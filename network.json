{
    "nodes": [
        {
            "abstract": "prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at",
            "group": 0,
            "name": "10.1.1.123.7607",
            "keyword": "",
            "title": "Optimization by simulated annealing"
        },
        {
            "abstract": "This paper contributes to this methodology by presenting an improvement over previous algorithms. Sections II and III give a short outline of previous Boltzmann annealing (BA) and fast Cauchy fast annealing (FA) algorithms. Section IV presents the new very fast algorithm. Section V enhances this algorithm with a re-annealing modification found to be extremely useful for multi-dimensional parameter-spaces. This method will be referred to here as very fast reannealing (VFR)",
            "group": 1,
            "name": "10.1.1.1.1635",
            "keyword": "",
            "title": "Very Fast Simulated Re-Annealing"
        },
        {
            "abstract": "Many inference and optimization tasks in machine  learning can be solved by sampling approaches  such as Markov Chain Monte Carlo  (MCMC) and simulated annealing. These  methods can be slow if a single target density  query requires many runs of a simulation  (or a complete sweep of a training data  set). We introduce a hierarchy of MCMC  samplers that allow most steps to be taken  in the solution space using only a small sample  of simulation runs (or training examples).",
            "group": 2,
            "name": "10.1.1.1.2907",
            "keyword": "",
            "title": "Efficient Hierarchical MCMC for Policy Search"
        },
        {
            "abstract": "When the systems under investigation are complex, the analytical solutions to these systems become impossible. Because of the complex stochastic characteristics of the systems, simulation can be used as an analysis tool to predict the performance of an existing system or a design tool to test new systems under varying circumstances. However, simulation is extremely time consuming for most problems of practical interest. As a result, it is impractical to perform any parametric study of system performance, especially for systems with a large parameter space. One approach to overcome this limitation is to develop a simpler model to explain the relationship between the inputs and outputs of the system. Simulation metamodels are increasingly being used in conjunction with the original simulation, to improve the analysis and understanding of decision-making processes. In this study, artificial neural networks (ANN) metamodel is developed for simulation model of an asynchronous assembly system and ANN metamodel together with simulated annealing (SA) is used to optimize the buffer sizes in the system.",
            "group": 3,
            "name": "10.1.1.1.4849",
            "keyword": "",
            "title": "Proceedings of the 2002 Winter Simulation Conference"
        },
        {
            "abstract": "We present a method for mapping a given Bayesian network to a Boltzmann machine architecture, in the sense that the the updating process of the resulting Boltzmann machine model provably converges to a state which can be mapped back to a maximum a posteriori (MAP) probability state in the probability distribution represented by the Bayesian network. The Boltzmann machine model can be implemented efficiently on massively parallel hardware, since the resulting structure can be divided into two separate clusters where all the nodes in one cluster can be updated simultaneously. This means that the proposed mapping can be used for providing Bayesian network models with a massively parallel probabilistic reasoning module, capable of finding the MAP states in a computationally efficient manner. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can be seen as a method for automatically determining the structure and the connection weights of a Boltzmann machine by incorporating high-level, probabilistic information directly into the neural network architecture, without recourse to a time-consuming and unreliable learning process.",
            "group": 4,
            "name": "10.1.1.1.4921",
            "keyword": "Boltzmann machinesprobabilistic reasoningBayesian networkssimulated annealing",
            "title": "Massively Parallel Probabilistic Reasoning with Boltzmann Machines"
        },
        {
            "abstract": "Supervised learning in neural networks based on the popular backpropagation method can be often trapped in a local minimum of the error function. The class of backpropagation-type training algorithms includes local minimization methods that have no mechanism that allows them to escape the influence of a local minimum. The existence of local minima is due to the fact that the error function is the superposition of nonlinear activation functions that mayhave minima at different points, which sometimes results in a nonconvex error function. This work investigates the use of global search methods for batch-mode training of feedforward multilayer perceptrons. Global search methods are expected to lead to \"optimal\" or \"near-optimal\" weight configurations byallowing the neural network to escape local minima during training and, in that sense, they improve the efficiency of the learning process. The paper reviews the fundamentals of simulated annealing, genetic and evolutionary algorithms as well as some recently proposed deflection procedures. Simulations and comparisons are presented.",
            "group": 5,
            "name": "10.1.1.1.5915",
            "keyword": "Local minimasimulated annealinggenetic algorithms",
            "title": "Supervised Training Using Global Search Methods"
        },
        {
            "abstract": "The thermal generator maintenance scheduling problem has been tackled by a variety of traditional optimisation techniques over the years. While these methods can give an optimal solution to small scale problems, they are often inefficient and impractical when applied to larger problems.  In this paper, we employ a multi-stage approach where the problem is decomposed into smaller sub-problems, each of which can be solved much more efficiently by existing algorithms. After each part of the problem has been solved, the results are then recombined to form the solution to the whole problem.  Both tabu search and a memetic algorithm have been observed to produce very good results but they take a significant amount of time to run. In this paper we utilise both techniques to form the basis of a multi-stage approach to solve the thermal generator maintenance scheduling problem.  The results will demonstrate that the multistage methodology is just as effective for this problem while achieving a significant reduction in run-time.  ",
            "group": 6,
            "name": "10.1.1.1.5961",
            "keyword": "",
            "title": "A Multi-Stage Approach for the Thermal Generator Maintenance Scheduling Problem"
        },
        {
            "abstract": "Packet radio (PR) is a technology that applies the packet switching technique to the broadcast radio environment. In a PR network, a single high-speed wideband channel is shared by all PR stations. When a time-division multi-access protocol is used, the access to the channel by stations' transmissions must be properly scheduled in both time and space domains in order to avoid collisions or interferences. It is proven in this paper that such a scheduling problem is NP-complete. Therefore, an efficient polynomial algorithm rarely exists, and a mean field annealing-based algorithm is proposed to schedule the stations' transmissions in a frame consisting of certain number of time slots. Numerical examples and comparisons with some existing scheduling algorithms have shown that the proposed scheme can find near-optimal solutions with reasonable computational complexity. Both time delay and channel utilization are calculated based on the found schedules.",
            "group": 7,
            "name": "10.1.1.1.6059",
            "keyword": "",
            "title": "Optimal Broadcast Scheduling in Packet Radio Networks Using Mean Field Annealing"
        },
        {
            "abstract": "Due to its low attenuation, #ber has become that medium of choice for point-to-point links. Using Wavelength-Division Multiplexing (WDM), manychannels can be created in the same fiber. A network node equipped with a tunable optical transmitter can select any of these channels for sending data. An optical interconnection combines the signal from the various receivers in the network, and makes it available to the optical receivers, whichmay also be tunable. By properly tuning transmitters and/or receivers, point-to-point links can be dynamically created and destroyed. Therefore, in a WDM network, the routing algorithm has an additional degree of freedom compared to traditional networks: it can modify the network topology to create the routes. In this report, we consider the problem of routing audio/video streams in WDM networks. We present a general linear integer programming formulation for the problem. However, since this is a complex solution, we propose simpler heuristic algorithms, both for the unicast case and for the multicast case. The performance of these heuristics is evaluated in a number of scenarios, with a realistic traffic model, and from the evaluation we derive guidelines for usage of the heuristic algorithms.",
            "group": 8,
            "name": "10.1.1.1.6200",
            "keyword": "i",
            "title": "Routing of Streams in WDM Reconfigurable Networks"
        },
        {
            "abstract": "This paper is devoted to the investigation of that question. We present two variants of local search algorithms where the search time can be set as an input parameter. These two approaches are: a time-predefined variant of simulated annealing and a specially designed local search that we have called the \"degraded ceiling\" method. We present a comprehensive series of experiments, which show that these approaches significantly outperform the previous best results (in terms of solution quality) on the most popular  benchmark exam timetabling problems. Of course there is a price to pay for such better results: increased execution time. We discuss the impact of this trade-off between quality and execution time. In particular we  discuss issues involving the proper estimation of the algorithm's execution time and assessing its importance",
            "group": 9,
            "name": "10.1.1.1.6410",
            "keyword": "",
            "title": "A Time-Predefined Local Search Approach to Exam Timetabling Problems"
        },
        {
            "abstract": ". In this paper we present a method called NOVEL (Nonlinear Optimization via External Lead) for solving continuous and discrete global optimization problems. NOVEL addresses the balance between global search and local search, using a trace to aid in identifying promising regions before committing to local searches. We discuss NOVEL for solving continuous constrained optimization problems and show how it can be extended to solve constrained satisfaction and discrete satisfiability problems. We first transform the problem using Lagrange multipliers into an unconstrained version. Since a stable solution in a Lagrangian formulation only guarantees a local optimum satisfying the constraints, we propose a global search phase in which an aperiodic and bounded trace function is added to the search to first identify promising regions for local search. The trace generates an information-bearing trajectory from which good starting points are identified for further local searches. Taking only a sm...",
            "group": 10,
            "name": "10.1.1.1.6946",
            "keyword": "Augmented Lagrange multiplier methodcontinuous nonlinear programming problemsconstraint satisfaction problemssatisfiability problemstrace functiontrajectory-based method",
            "title": "Trace-Based Methods for Solving Nonlinear Global Optimization and Satisfiability Problems"
        },
        {
            "abstract": "Meta-heuristics are methods that sit on top of local search algorithms. They perform the function of avoiding or escaping a local optimum and/or premature convergence. The aim of this paper is to survey, compare and contrast meta-heuristics for local search. First, we present the technique of local search (or hill climbing as it is sometimes known). We then present a table displaying the attributes of all the different meta-heuristics. After this, we give a short description and discussion of each meta-heuristic with pseudo code. Finally, we describe why, in general, these techniques work and present some ideas of what is needed from the next generation of meta-heuristics.",
            "group": 11,
            "name": "10.1.1.1.7027",
            "keyword": "",
            "title": "A survey of AI-based meta-heuristics for dealing with local optima in local search"
        },
        {
            "abstract": "The transport of multimedia streams in computer communication networks raises issues at all layers of the OSI model. At the physical layer, the main issue is one of providing the appropriate bandwidth at all levels of the network infrastructure, from the local area network to the campus and wide-area backbones. The data link layer must provide support for priorities to differentiate the various traffic types (data, video, audio, still images, etc.) and satisfy their specific requirements. The same requirements must be taken into account at the network layer, which is responsible for finding routes. The transport layer must provide new functions such as, for example, semi-reliability and multipoint transport. New functions at the session layer include connection control functions (call establishment and management), floor control functions and synchronization. This thesis considers some of the issues related to supporting multimedia streams at the network layer; in particular, the issue of routing algorithms appropriate for multimedia streams. The traffic",
            "group": 12,
            "name": "10.1.1.1.7146",
            "keyword": "",
            "title": "Routing of Video/Audio Streams in Packet-Switched Networks "
        },
        {
            "abstract": "Many systems in chemical engineering are difficult to optimize using gradient-based algorithms. These include process models with multimodalobjective functions and discontinuities. Herein, a stochastic algorithm is applied for the optimal design of a fermentation process, to determine multiphase equilibria, for the optimal control of a penicillin reactor, for the optimal control of a non-differentiable system, and for the optimization of a catalyst blend in a tubular reactor. The advantages of the algorithm for the efficient and reliable location of global optima are examined. The properties of these algorithms, as applied to chemical processes, are considered, with emphasis on the ease of handling constraints and the ease of implementation and interpretation of results. For the five processes, the efficiency of computation is improved compared with selected stochastic and deterministic algorithms. Results closer to the global optimum are reported for the optimal control of the penicillin reactor and the non-differentiable system.",
            "group": 13,
            "name": "10.1.1.1.7551",
            "keyword": "",
            "title": "Global Optimization of Chemical Processes using Stochastic Algorithms"
        },
        {
            "abstract": "In many cases the supervised neural network training using a backpropagation based learning rule can be trapped in a local minimum of the error function. These training algorithms are local minimization methods and have no mechanism that allows them to escape the influence of a local minimum. The existence of local minima is due to the fact that the error function is the superposition of nonlinear activation functions that may have minima at different points, which sometimes results in a non-convex error function. In this work global search methods for feed-forward neural network batch training are investigated. These methods are expected to lead to \"optimal\" or \"near-optimal\" weight configurations by allowing the network to escape local minima during training. The paper reviews the fundamentals of simulated annealing, genetic algorithms as well as some recently proposed deflection procedures. Simulations and comparisons are presented.",
            "group": 14,
            "name": "10.1.1.1.7657",
            "keyword": "Alleviation of local minimasimulated annealinggenetic algorithmsde ection",
            "title": "Global Search Methods for Neural Network Training"
        },
        {
            "abstract": "Traditionally, application software developers carry out their tests on their own local development databases. However, such local databases usually have only a small number of sample data and hence cannot simulate satisfactorily a live environment, especially in terms of performance and scalability testing. On the other hand, the idea of testing applications over live production databases is increasingly problematic in most situations primarily due to the fact that such use of live production databases has the potential to expose sensitive data to an unauthorized tester and to incorrectly update information in the underlying database. In this paper, we investigate techniques to generate mock databases for application software testing without revealing any confidential information from the live production databases. Specifically, we will design mechanisms to create the deterministic rule set    non-deterministic rule set    and statistic data set    for a live production database. We will then build a security Analyzer which will process the   together with security requirements (security policy) and output a new triplet        #.",
            "group": 15,
            "name": "10.1.1.1.7906",
            "keyword": "Categories and Subject Descriptors D.2.5 [Software EngineeringTesting and Debugging\u2014 testing toolsH.1.1 [Models and PrinciplesSystems and Information Theory\u2014information theoryH.2.8 [Database ManagementDatabase Applications\u2014statistical databases General Terms Algorithmsperformancesecuritytheory",
            "title": "Privacy Preserving Database Application Testing"
        },
        {
            "abstract": "One of the main questions concerning learning in Multi-Agent Systems is: \"(How)  can agents benefit from mutual interaction during the learning process?\". This paper  describes the study of an interactive advice-exchange mechanism as a possible way  to improve agents' learning performance. The advice-exchange technique, discussed  here, uses supervised learning (backpropagation), where reinforcement is not directly  coming from the environment but is based on advice given by peers with better performance  score (higher confidence), to enhance the performance of a heterogeneous  group of Learning Agents (LAs). The LAs are facing similar problems, in an environment  where only reinforcement information is available. Each LA applies a different,  well known, learning technique: Random Walk, Simulated Annealing, Evolutionary  Algorithms and Q-Learning. The problem used for evaluation is a simplified  traffic-control simulation. In the following text the reader can find a description of  the traffic simulation and Learning Agents (focused on the advice-exchange mechanism), a discussion of the first results obtained and suggested techniques to overcome  the problems that have been observed. Initial results indicate that advice-exchange  can improve learning speed, although \"bad advice\" and/or blind reliance can disturb  the learning performance. The use of supervised learning to incorporate advice given  from non-expert peers using different learning algorithms, in problems where no supervision  information is available, is, to the best of the authors' knowledge, a new  concept in the area of Multi-Agent Systems Learning.",
            "group": 16,
            "name": "10.1.1.1.8412",
            "keyword": "",
            "title": "On Learning by Exchanging Advice"
        },
        {
            "abstract": "A common drawback of local search metaheuristics such as simulated annealing in solving combinatorial optimisation problems is the necessity to set a number of uncertain parameters. This makes the algorithm problem-dependent and significantly increases the total time of solving the problem. On the other hand, the methods without any parameters (such as hill-climbing) usually produce results of inferior quality.",
            "group": 17,
            "name": "10.1.1.1.9155",
            "keyword": "",
            "title": "A New Local Search Approach with Execution Time as an Input Parameter"
        },
        {
            "abstract": "One of the main questions concerning learning in a Multi-Agent System's environment is: \"(How) can agents benefit from mutual interaction during  the learning process?\" This paper describes a technique that enables a heterogeneous  group of Learning Agents (LAs) to improve its learning performance by  exchanging advice. This technique uses supervised learning (backpropagation),  where the desired response is not given by the environment but is based on advice  given by peers with better performance score. The LAs are facing problems  with similar structure, in environments where only reinforcement information is  available. Each LA applies a different, well known, learning technique. The  problem used for the evaluation of LAs performance is a simplified trafficcontrol  simulation. In this paper the reader can find a summarized description of  the traffic simulation and Learning Agents (focused on the advice-exchange  mechanism), a discussion of the first results obtained and suggested techniques  to overcome the problems that have been observed.",
            "group": 18,
            "name": "10.1.1.1.9819",
            "keyword": "",
            "title": "Cooperative Learning Using Advice Exchange"
        },
        {
            "abstract": "Simulated annealing is an established method for global optimization. Perhaps its most salient feature is the statistical promise to deliver a globally optimal solution. In this work, we propose a technique which attempts to combine the robustness of annealing in rugged terrain with the efficiency of local optimization methods in simple search spaces. On a variety of benchmark functions, the proposed method seems to clearly outperform a parallel genetic algorithm and adaptive simulated annealing, two popular and powerful optimization techniques.  1. INTRODUCTION  The goal of optimization is, given a system, to find the setting of its parameters so as to obtain the optimal performance. The performance of the system is given by an evaluation function. Optimization problems are commonly found in a wide range of fields, and it is also of central concern to many problems in artificial intelligence and machine learning. In situations where the space of parameters cannot be searched exhaustiv...",
            "group": 19,
            "name": "10.1.1.1.9887",
            "keyword": "",
            "title": "Salo: Combining Simulated Annealing And Local Optimization For Efficient Global Optimization"
        },
        {
            "abstract": "Consider a set of non-cooperative agents acting in an environment in which each agent attempts to maximize a private utility function. As each agent maximizes its private utility we desire a global \"world\" utility function to in turn be maximized. The inverse problem induced from this situation is the following: How does each agent choose his move so that while he optimizes his private utility, the world utility is optimized as well? This problem has been considered by the theory of COllective INtelligence (COIN) (Wolpert & Tumer 2000).",
            "group": 20,
            "name": "10.1.1.2.416",
            "keyword": "",
            "title": "Intelligent Coordinates and Collective Intelligence: A case study in improving Simulated Annealing in the bin-packing domain"
        },
        {
            "abstract": "a rule-based expert system. This expert system checks results to ensure their consistency, identifies whether the results come from arterial or venous vessels, and then produces an interpretation of their meaning. This `crisp' expert system was validated, verified and commercially released, and has since been installed at twenty two hospitals all around the United Kingdom. The assessment of umbilical acid-base status is characterised by uncertainty in both the basic data and the knowledge required for its interpretation. Fuzzy logic provides a technique for representing both these forms of uncertainty in a single framework. A `preliminary' fuzzy-logic based expert system to interpret error-free results was developed, based on the knowledge embedded in the crisp expert system. Its performance was compared against clinicians in a validation test, but initially its performance was found to be poor in comparison with the clinicians and inferior to the crisp expert system. An automatic tuni",
            "group": 21,
            "name": "10.1.1.2.506",
            "keyword": "",
            "title": "Intelligent Techniques for Handling Uncertainty in the Assessment of Neonatal Outcome"
        },
        {
            "abstract": "There is a well-developed theory about the algorithmic complexity  of optimization problems. Complexity theory provides negative  results which typically are based on assumptions like NP!=P  or NP!=RP. Positive",
            "group": 22,
            "name": "10.1.1.2.842",
            "keyword": "",
            "title": "Towards a theory of randomized search heuristics"
        },
        {
            "abstract": "The combination of local search operators with a genetic algorithm has provided very good results in certain scheduling problems, particularly in timetabling and other maintenance scheduling problems. In fact, in previous work, the local search operators alone have been seen to produce good quality results for the thermal generator maintenance scheduling problem. However, even better results have been obtained (for this problem) by adopting a genetic algorithm/local search hybrid approach. The resulting algorithm from such an approach has been termed a Memetic Algorithm. This paper investigates the use of such an algorithm for the scheduling of transmission line maintenance for a known benchmark problem that has been previously addressed in the literature using a combination of a genetic algorithm and greedy optimisers. The problem discussed here is concerned with the scheduling of essential maintenance for an electricity transmission network where every transmission line must be mai...",
            "group": 23,
            "name": "10.1.1.2.846",
            "keyword": "transmission line maintenancehybrid evolutionary algorithms (memetics",
            "title": "A Memetic Algorithm to Schedule Planned Grid Maintenance"
        },
        {
            "abstract": "While searching for the global minimum of a cost function we have often to decide if a restart from a di#erent initial point would be more advantageous than continuing current optimization. This is a particular case of the e#ciency comparison between repeated minimizations and single extended search having the same total length.",
            "group": 24,
            "name": "10.1.1.2.934",
            "keyword": "Key wordsOptimization problemrestartrepeated searches",
            "title": "A Theoretical Approach to Restart"
        },
        {
            "abstract": "The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has led to an increase in the deployment of complex enterprise applications. These applications typically run on Web Application Servers, which assume the burden of managing many tasks, such as concurrency, memory management, database access, etc., required by these applications. The performance of an Application Server depends heavily on appropriate configuration. Configuration is a difficult and error-prone task due to the large number of configuration parameters and complex interactions between them. We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem. We propose a Smart Hill-Climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling (LHS). The algorithm is efficient in both searching and random sampling. It consists of estimating a local function, and then, hill-climbing in the steepest descent direction. The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling. We have carried out extensive experiments with an online brokerage application running in a WebSphere environment. Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods.",
            "group": 25,
            "name": "10.1.1.2.1211",
            "keyword": "Automatic TuningImportance SamplingSimulated AnnealingGradient Method Copyright is held by the author/owner(s",
            "title": "A Smart Hill-Climbing Algorithm for Application Server Configuration"
        },
        {
            "abstract": "The combination of local search operators with a genetic algorithm has provided very good results in certain scheduling problems, particularly in timetabling and other maintenance scheduling problems. In fact, in previous work, the local search operators alone have been seen to produce good quality results for the thermal generator maintenance scheduling problem. However, even better results have been obtained (for this problem) by adopting a genetic algorithm/local search hybrid approach.  The resulting algorithm from such an approach has been termed a Memetic Algorithm. This paper investigates the use of such an algorithm for the scheduling of transmission line maintenance for a known benchmark problem that has been previously addressed in the literature using a combination of a genetic algorithm and greedy optimisers.  The problem discussed here is concerned with the scheduling of essential maintenance for an electricity transmission network where every transmission line must be maintained once within a specified time period. The objective is both to avoid situations where sections of the network are disconnected, and to minimise the overloading of lines which are in service.  We look particularly at the \"four-node problem\" presented and we present and discuss in some detail the proposed memetic algorithm and compare it against a variety of algorithms, including the local search operators on their own, and also a range of algorithms which apply the local search operators to randomly generated solutions, thus demonstrating the utility of the memetic approach.  A comparison is made with the results from previous work, and it will be shown that, for this problem, the proposed memetic algorithm can produce good quality solutions in a very short space of time. ",
            "group": 26,
            "name": "10.1.1.2.1234",
            "keyword": "transmission line maintenancehybrid evolutionary algorithms (memetics",
            "title": "A Memetic Algorithm to Schedule Planned Grid Maintenance"
        },
        {
            "abstract": "Finding optimal three-dimensional molecular configurations based on a limited amount of experimental and/or theoretical data requires efficient nonlinear optimization algorithms. Optimization methods must be able to find atomic configurations that are close to the absolute, or global, minimum error and also satisfy known physical constraints such as minimum separation distances between atoms (based on van der Waals interactions). The most difficult obstacles in these types of problems are that 1) using a limited amount of input data leads to many possible local optima and 2) introducing physical constraints, such as minimum separation distances, helps to limit the search space but often makes convergence to a global minimum more difficult. We introduce a constrained global optimization algorithm that is robust and efficient in yielding near-optimal three-dimensional configurations that are guaranteed to satisfy known separation constraints. The algorithm uses an atom-based approach that reduces the dimensionality and allows for tractable enforcement of constraints while maintaining good global convergence properties. We evaluate the new optimization algorithm using synthetic data from the yeast phenylalanine tRNA and several proteins, all with known crystal structure taken from the Protein Data Bank. We compare the results to commonly applied optimization methods, such as distance geometry, simulated annealing, continuation, and smoothing. We show that compared to other optimization approaches, our algorithm is able combine sparse input data with physical constraints in an efficient manner to yield structures with lower root mean squared deviation.",
            "group": 27,
            "name": "10.1.1.2.1612",
            "keyword": "",
            "title": "Constrained Global Optimization for Estimating Molecular Structure from Atomic Distances"
        },
        {
            "abstract": " ",
            "group": 28,
            "name": "10.1.1.2.1694",
            "keyword": "Key wordsTime seriessegmentationnonstationarityhidden Markov modelsdynamical mode detectionEEGsleep",
            "title": "Hidden Markov Mixtures of Experts with an Application to EEG Recordings from Sleep"
        },
        {
            "abstract": "In this paper we propose an optimal anytime version of constrained  simulated annealing (CSA) for solving constrained nonlinear  programming problems (NLPs). One of the goals of the algorithm is  to generate feasible solutions of certain prescribed quality using an average  time of the same order of magnitude as that spentby the original  CSA with an optimal cooling schedule in generating a solution of similar  quality. Here, an optimal cooling schedule is one that leads to the  shortest average total number of probes when the original CSA with the  optimal schedule is run multiple times until it finds a solution. Our second  goal is to design an anytime version of CSA that generates gradually  improving feasible solutions as more time is spent, eventually finding a  constrained global minimum (CGM). In our study,wehaveobserved a  monotonically non-decreasing function relating the success probability  of obtaining a solution and the average completion time of CSA, and an  exponential function relating the objective target that CSA is looking  for and the average completion time. Based on these observations, we  have designed CSAAT;ID , the anytime CSA with iterative deepening  that schedules multiple runs of CSA using a set of increasing cooling  schedules and a set of improving objective targets. We then prove the  optimalityofourschedules and demonstrate experimentally the results  on four continuous constrained NLPs. CSAAT;ID can be generalized  to solving discrete, continuous, and mixed-integer NLPs, since CSA is  applicable to solve problems in these three classes. Our approach can  also be generalized to other stochastic search algorithms, suchasgenetic  algorithms, and be used to determine the optimal time for each run of  such algorithms.",
            "group": 29,
            "name": "10.1.1.2.1837",
            "keyword": "",
            "title": "Optimal Anytime Constrained Simulated Annealing For  Constrained Global Optimization  "
        },
        {
            "abstract": "We had a problem to be solved: the thermal generator maintenance scheduling problem [Yam82]. We wanted to look at stochastic methods and this paper will present three methods and discuss the pros and cons of each. We will also present evidence that strongly suggests that for this problem, tabu search was the most effective and efficient technique.  The problem is concerned with scheduling essential maintenance over a fixed length repeated planning horizon for a number of thermal generator units while minimising the maintenance costs and providing enough capacity to meet the anticipated demand.  Traditional optimisation based techniques such as integer programming [DM75], dynamic programming [ZQ75, YSY83] and branch-and-bound [EDM76] have been proposed to solve this problem. For small problems these methods give an exact optimal solution. However, as the size of the problem increases, the size of the solution space increases exponentially and hence also the running time of these algorit...",
            "group": 30,
            "name": "10.1.1.2.2115",
            "keyword": "",
            "title": "Four Methods for Maintenance Scheduling"
        },
        {
            "abstract": "Simulation of logic designs is a very important part of the VLSI-design process. The increasing size of the designs requires more efficient simulation strategies to accelerate the simulation process. Parallel logic simulation seems to be a promising approach in this direction. This paper describes the basic principles of parallel logic simulation, discusses different approaches, and surveys the research done in this field so far. 1 Introduction  In recent years the increasing demand for fast development of integrated circuits has caused many studies in the field of design of microelectronics and hardware units. A particular problem arises from the exploding number of transistors that can be placed on single chips. Design methodologies such as placement, floor planning, channel routing [SES85a] are one major topic in research. Another and even more important theme is verification of such large designs. \"Is the final product really going to show the expected behavior?\" is one of the most...",
            "group": 31,
            "name": "10.1.1.2.2118",
            "keyword": "",
            "title": "A Survey on Parallel Logic Simulation"
        },
        {
            "abstract": ". A multiway spatial join combines information found in three or more spatial relations with respect to some spatial predicates. Motivated by their close correspondence with constraint satisfaction problems (CSPs), we show how multiway spatial joins can be processed by systematic search algorithms traditionally used for CSPs. This paper describes two different strategies, window reduction and synchronous traversal, that take advantage of underlying spatial indexes to effectively prune the search space. In addition, we provide cost models and optimization methods that combine the two strategies to compute more efficient execution plans. Finally, we evaluate the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations.  Key Words. Spatial Databases, Spatial Joins, Constraint Satisfaction, R-trees  1. INTRODUCTION  Spatial DBMSs and GISs store large amounts of multi-dimensional data, such as points...",
            "group": 32,
            "name": "10.1.1.2.2147",
            "keyword": "",
            "title": "Constraint-based Processing of Multiway Spatial Joins"
        },
        {
            "abstract": "A global optimization method is introduced for the design of statistical classifiers that minimize the rate of misclassification. We first derive the theoretical basis for the method, based on which we develop a novel design algorithm and demonstrate its effectiveness and superior performance in the design of practical classifiers for some of the most popular structures currently in use. The method, grounded in ideas from statistical physics and information theory, extends the deterministic annealing approach for optimization, both to incorporate structural constraints on data assignments to classes and to minimize the probability of error as the cost objective. During the design, data are assigned to classes in probability, so as to minimize the expected classification error given a specified level of randomness, as measured by Shannon's entropy. The constrained optimization is equivalent to a free energy minimization, motivating a deterministic annealing approach in which the entropy...",
            "group": 33,
            "name": "10.1.1.2.2633",
            "keyword": "",
            "title": "A Global Optimization Technique for Statistical Classifier Design"
        },
        {
            "abstract": "Simulated tempering and swapping are two families of sampling algorithms in which a parameter representing temperature varies during the simulation. The hope is that this will overcome bottlenecks that cause sampling algorithms to be slow at low temperatures. Madras and Zheng demonstrate that the swapping and tempering algorithms allow efficient sampling from the low-temperature mean-field Ising model, a model of magnetism, and a class of symmetric bimodal distributions [10]. Local Markov chains fail on these distributions due to the existence of bad cuts in the state space.",
            "group": 34,
            "name": "10.1.1.2.3295",
            "keyword": "",
            "title": "Torpid Mixing of Simulated Tempering on the Potts Model"
        },
        {
            "abstract": "An assessment of neonatal outcome may be obtained from analysis of blood in the umbilical cord of an infant immediately after delivery. This can provide information on the health of the newborn infant and guide requirements for neonatal care, but there are problems with the technique. Samples frequently contain errors in one or more of the important parameters, preventing accurate interpretation and many clinical staff lack the expert knowledge required to interpret results. The development and validation of an expert system to overcome these difficulties is described. The initial development utilised conventional `crisp' logic within the rule base and this system was evaluated to commercial release. This expert system validates the raw data, provides an interpretation of the results for clinicians and archives all the results, including the quality control and calibration data, for permanent storage. Subsequent development went on to incorporate fuzzy logic into part of the expert system knowledge base, but tests of this preliminary fuzzy system showed that it performed worse than the original crisp expert system. A tuning algorithm was then employed to modify the fuzzy model and this process resulted in improved performance to a level comparable to clinicians and superior to the crisp system. Finally, the entire knowledge base was converted to utilise fuzzy logic and this `integrated' fuzzy expert system was validated against international expert opinion.",
            "group": 35,
            "name": "10.1.1.2.3618",
            "keyword": "Expert systemsfuzzy logicvalidationumbilical cord acid-base balance",
            "title": "The Development of a Fuzzy Expert System for the Analysis of Umbilical Cord Blood"
        },
        {
            "abstract": "The purpose of this reading paper is to empirically analyse and compare several heuristic algorithms that can be applied to combinatorial optimisation problems. The main focus of the paper is heuristic paradigms that are generally applicable to a wide variety of problem types. The three techniques investigated here are repeated local search, simulated annealing, and genetic algorithms. Each heuristic is investigated individually, with a small amount of theoretical discussion, to determine how the performance of each method can be changed by altering algorithm parameters. The Traveling Salesman Problem is used as a problem indicative of combinatorial optimisation to test the three heuristics. It is an NP-hard problem that shares many properties common to other combinatorial optimisation problems and it is hoped that the behaviour of the heuristics when applied to the Traveling Salesman Problem will be similar to that experienced for other combinatorial problems. To gain some insight int...",
            "group": 36,
            "name": "10.1.1.2.3853",
            "keyword": "",
            "title": "Generally Applicable Heuristics for Global Optimisation: An Investigation of Algorithm Performance for the Euclidean Traveling Salesman Problem"
        },
        {
            "abstract": "A new mapping heuristic is developed, based on the recently proposed Mean Field Annealing (MFA) algorithm. An efficient implementation scheme, which decreases the complexity of the proposed algorithm by asymptotical factors, is also given. Performance of the proposed MFA algorithm is evaluated in comparison with two wellknown heuristics; Simulated Annealing and Kernighan-Lin. Results of the experiments indicate that MFA can be used as an alternative heuristic for solving the mapping problem. Inherent parallelism of MFA is exploited by designing an efficient parallel algorithm for the proposed MFA heuristic.  1 Introduction  Today, with the aid of VLSI technology, parallel computers not only exist in research laboratories, but are also available on the market as powerful, general purpose computers. Wide use of parallel computers in various compute intensive applications makes the problem of mapping parallel programs to parallel computers more crucial. The mapping problem arises while d...",
            "group": 37,
            "name": "10.1.1.2.3930",
            "keyword": "",
            "title": "A New Mapping Heuristic Based On Mean Field Annealing"
        },
        {
            "abstract": "The maintenance scheduling problem has been previously tackled by various traditional optimisation techniques. While these methods can give an optimal solution to small scale problems, they are often inefficient when applied to larger scale problems. The memetic algorithm presented here is essentially a genetic algorithm with an element of local search. The effectiveness of the method is tested through its application to real scale problems.",
            "group": 38,
            "name": "10.1.1.2.4028",
            "keyword": "",
            "title": "A Memetic Algorithm for the Maintenance Scheduling Problem"
        },
        {
            "abstract": " ",
            "group": 39,
            "name": "10.1.1.2.4387",
            "keyword": "TABLE OF CONTENTS Signature Pageiii Dedicationiv Table of Contentsv List of Figuresviii List of Tablesx",
            "title": "Adaptive Global Optimization with Local Search"
        },
        {
            "abstract": "Achieving autonomous learning systems which can govern themselves is one of the goals of A.I. Most learning systems explore a fixed model space to explain a set of data. We believe that the \"best\" but most distinct models in the available space can provide insight into questions of autonomy such as when to change the model space and how to generate new data points (via experiments). We explore this idea by focusing on clustering problems where the initial data is known to be insufficient to find the true model. We propose a method to generate new data points via experiments. Our approach results in convergence to the true model using half as many additional data points than if they were randomly selected.",
            "group": 40,
            "name": "10.1.1.2.6456",
            "keyword": "Autonomous learningmachine discoveryclusteringunsupervised",
            "title": "Appearing in the 4"
        },
        {
            "abstract": "In this paper a number of improvements are suggested that can be applied to most k-medoids-based algorithms. These can be divided into two categories - conceptual / algorithmic improvements, and implementational improvements. These include the revisiting of the accepted cases for swap comparison and the application of partial distance searching and previous medoid indexing to clustering. We propose extensions to the problem of nearest neighbor search, by combining the previous medoid index with triangular inequality elimination and partial distance searching. An improved k-medoids algorithm using simulated annealing, CLASA, is also discussed, as is a novel mechanism for managing memory usage. Various hybrids of these search approaches are then applied to a number of k-medoids-based algorithms and we show that the method is generally applicable. For example, experimental results based on various datasets, including both artificial and real datasets, demonstrate that when applied to  CLARANS the number of distance calculations can be reduced by up to 98% with similar average distance per object. Importantly, these search approaches can also be applied to nearest neighbor searching and other clustering algorithms.",
            "group": 41,
            "name": "10.1.1.2.7149",
            "keyword": "Previous Medoid Index (PMITriangular Inequality Elimination (TIEPartial Distance Search (PDSCLASASimulated Annealing",
            "title": "Improved Search Strategies and . . . "
        },
        {
            "abstract": "In this paper we consider the unconstrained binary quadratic programming problem. This is the problem of maximising a quadratic objective by suitable choice of binary (zero-one) variables. We present two heuristic algorithms based upon tabu search and simulated annealing for this problem. Computational results are presented for a number of publically available data sets involving up to 2500 variables. An interesting feature of our results is that whilst for most problems tabu search dominates simulated annealing for the very largest problems we consider the converse is true. This paper typifies a \"multiple solution technique, single paper\" approach, i.e. an approach that within the same paper presents results for a number of different heuristics applied to the same problem. Issues relating to algorithmic design for such papers are discussed. Keywords: unconstrained binary (zero-one) quadratic programming; tabu search; simulated annealing 1 1. ",
            "group": 42,
            "name": "10.1.1.2.8453",
            "keyword": "unconstrained binary (zero-one",
            "title": ""
        },
        {
            "abstract": " ",
            "group": 43,
            "name": "10.1.1.2.9746",
            "keyword": "",
            "title": "An Analysis of Diversity in Genetic Programming"
        },
        {
            "abstract": "This paper will report on the adaptation of Simulated Annealing, previously used to extract partial classification rules [5], to the problem of finding an economic model for classification of organisms. The objective of the simulated annealing algorithm is to choose the classification model which allows for maximum di#erentiability of organisms, while minimising the total number of characteristics (or the total cost of characteristics, if this is given) required for classification. An alternative problem is that of \"economic\" identification of unnamed organisms. In this case, the objective is to choose a model which will maximise the chances of delivering an identity for any unknown organism while minimising the total cost of identification. The problem of identification will be modelled in this paper. Organisms may have a known frequency of occurrence, in which case the model chosen should ensure identification of most frequent organisms at a minimum cost. The approach has, for example, immediate application to the determination of the tests necessary to distinguish types of yeast, such as food spoilage agents, which are of great industrial interest",
            "group": 44,
            "name": "10.1.1.2.9865",
            "keyword": "",
            "title": "Classification/Identification on Biological Databases"
        },
        {
            "abstract": "We discuss the use of genetic algorithms (GAs) for the generation of music. We explain the structure of a typical GA, and outline existing work on the use of GAs in computer music. We propose that the addition of domain-specific knowledge can enhance the quality and speed of production of GA results, and describe two systems which exemplify this. However, we conclude that GAs are not ideal for the simulation of human musical thought (notwithstanding their ability to produce good results) because their operation in no way simulates human behaviour.",
            "group": 45,
            "name": "10.1.1.3.987",
            "keyword": "Genetic AlgorithmsMusic GenerationSearch Space",
            "title": "Evolutionary Methods for Musical Composition"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels  obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement  (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize  legibility. This problem occurs frequently in the production of many types of informational graphics,  though it arises most often in automated cartography. In this paper we present a comprehensive treatment of  the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals  that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help  inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have  exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely  on good heuristic methods. We propose two new methods, one based on a discrete form of gradient descent,  the other on simulated annealing, and report on a series of empirical tests comparing these and the other  known algorithms for the problem. Based on this study, the first to be conducted, we identify the best  approaches as a function of available computation time.",
            "group": 46,
            "name": "10.1.1.3.1004",
            "keyword": "Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealing. Submitted to ACM Trans",
            "title": "An Empirical Study of Algorithms for Point Feature Label Placement"
        },
        {
            "abstract": "Simulated annealing (SA) is an e#ective general heuristic method for solving many combinatorial optimization problems. This paper deals with the two problems in SA. One is the long computational time of the numerical annealings, and the solution to it is the parallel processing of SA. The other one is the determination of the appropriate temperature schedule in SA, and the solution to it is the introduction of an adaptive mechanism for changing the temperature. The multiple SA processes are performed in multiple processors, and the temperatures in the SA processes are determined by a genetic algorithms. The proposed method is applied to solve many TSPs (Traveling Salesman Problems), and it is found that the method is very e#ective and usefule. Keywords--- Simulated Annealing, Genetic Algorithm, Temperature, Traveling Salesman Problem. I. ",
            "group": 47,
            "name": "10.1.1.3.1990",
            "keyword": "Genetic AlgorithmTemperatureTraveling Salesman Problem",
            "title": "Parallel Simulated Annealing with Adaptive Temperature"
        },
        {
            "abstract": "Automating the scheduling of sport leagues has received considerable attention in recent  years, as these applications involve significant revenues and generate challenging combinatorial  optimization problems. This paper considers the traveling tournament problem (TTP)  which abstracts the salient features of major league baseball (MLB) in the United States. It  proposes a simulated annealing algorithm (TTSA) for the TTP that explores both feasible  and infeasible schedules, uses a large neighborhood with complex moves, and includes advanced  techniques such as strategic oscillation and reheats to balance the exploration of the  feasible and infeasible regions and to escape local minima at very low temperatures. TTSA  matches the best known solutions on the small instances of the TTP and produces significant  improvements over previous approaches on the larger instances. Moreover, TTSA is shown  to be robust, since its worst solution quality over 50 runs is always smaller or equal to the best  known solutions. Keywords: Sport Scheduling, Travelling Tournament Problems, Local Search, Simulated  Annealing.",
            "group": 48,
            "name": "10.1.1.3.2530",
            "keyword": "Sport SchedulingTravelling Tournament ProblemsLocal SearchSimulated Annealing",
            "title": "A Simulated Annealing Approach to"
        },
        {
            "abstract": " ",
            "group": 49,
            "name": "10.1.1.3.3109",
            "keyword": "",
            "title": "Random Search Under Additive Noise"
        },
        {
            "abstract": "The problem of radio frequency assignment is to provide communication channels from limited spectral resources whilst keeping to a minimum the interference suered by those whishing to communicate in a given radio communication network. This problem is a combinatorial (NP-hard) optimization problem. In 1993, the CELAR (the French \\Centre d'Electronique de l'Armement\") built a suite of simpli  ed versions of Radio Link Frequency Assignment Problems (RLFAP) starting from data on a real network [16]. Initially designed for assessing the performances of several Constraint Logic Programming languages, these benchmarks have been made available to the public in the framework of the European EUCLID project CALMA (Combinatorial Algorithms for Military Applications).",
            "group": 50,
            "name": "10.1.1.3.3853",
            "keyword": "Benchmarksradio link frequency assignmentconstraint satisfactionoptimization 1. Description of the problem",
            "title": "Radio Link Frequency Assignment"
        },
        {
            "abstract": "This paper addresses the optimization of pseudo-random planar point patterns for invariant-based identification or indexing. This is a novel problem and is formulated here as the maximization of the spacing of all the invariants when considered as points in a space. The task is of formidable complexity and a stochastic approximation strategy is proposed that yields interesting results.",
            "group": 51,
            "name": "10.1.1.3.4170",
            "keyword": "",
            "title": "Optimizing Random Patterns for Invariants-Based Identification"
        },
        {
            "abstract": "this paper, we explore this problem, which we call the network testbed mapping problem. We describe the interesting challenges that characterize this problem, and explore its application to other spaces, such as distributed simulation. We present the design, implementation, and evaluation of a solver for this problem, which is currently in use on the Netbed network testbed. It builds on simulated annealing to find very good solutions in a few seconds for our historical workload, and scales gracefully on large well-connected synthetic topologies",
            "group": 52,
            "name": "10.1.1.3.4186",
            "keyword": "",
            "title": "A Solver for the Network Testbed Mapping Problem"
        },
        {
            "abstract": "An off-line planning tool that supports the programmer in developing his real-time application is mandatory in the design of time-triggered real-time systems. This paper describes the architecture and the functions of such a tool, the Cluster Compiler, that is in developmentat  our institute. We emphasize on the principle of a strict separation of the local from the global parts of a distributed system and on the consequences for the structure of the design tool arising from this principle.",
            "group": 53,
            "name": "10.1.1.3.5295",
            "keyword": "",
            "title": "The Cluster Compiler -- A Tool for the Design of Time-Triggered . . . "
        },
        {
            "abstract": "The Ramsey number R(k; l) is the least integer n such that all graphs on n or more vertices contain a clique of k vertices or an independent set of l vertices as an induced subgraph. In this work we investigate computational methods for finding lower bounds for Ramsey numbers. Some constructions of lower bounds for multicolor Ramsey numbers, a generalization of Ramsey numbers, are also considered.",
            "group": 54,
            "name": "10.1.1.3.5547",
            "keyword": "Ramsey numbersgraph theorytabu search",
            "title": "Computational Methods for Ramsey Numbers"
        },
        {
            "abstract": "Introduction   The manufacturing and chemical industries have undergone significant changes during  the past 15 years due to the increased cost of energy and increasingly stringent  environmental regulations. Modification of both plant design procedures and plant  operating conditions have been implemented in order to reduce costs and meet the  constraints. Most industry observers believe that the emphasis in the near future will be  on improving efficiency and increasing profitability of existing plants rather than on plant  expansion. One of the most important engineering tools that can be employed in such  activities is optimization. As computers have become more powerful, the size and  complexity of problems which can be simulated and solved by optimization techniques,  have correspondingly expanded. Most of the traditional optimization techniques based on  gradient methods have the possibility of getting trapped at local optimum depending upon  the degree of non-linearity and init",
            "group": 55,
            "name": "10.1.1.3.6097",
            "keyword": "",
            "title": "Evolutionary Computation - At A Glance"
        },
        {
            "abstract": "Most problems in the design of real-time applications like task allocation or scheduling  belong to the class of NP-complete problems and can be solved efficiently only by heuristics.",
            "group": 56,
            "name": "10.1.1.3.6410",
            "keyword": "",
            "title": "Solving NP-Complete Problems in Real-Time System Design by Multichromosome Genetic Algorithms"
        },
        {
            "abstract": "This paper is concerned with hybrid flow-shop scheduling problem involving multiprocessor tasks. The hybrid flow-shop problem is a generalization of the classical flow-shop problem by permitting multiple parallel processors at each stage of task processing. The multiprocessor tasks, on the other hand, overcome the restriction of the classical scheduling problems by allowing tasks to be processed on more than one processor simultaneously. The hybrid flow-shop is defined by the set M =    ..., m} of m processing stages, in which each stage contains a set M i =    ..., m i    of m i identical processors. There is a given set J =    ..., n} of n jobs to be processed in this hybrid flow-shop system. Each job can be viewed as a sequence of m multiprocessor tasks. The i-th task in the sequence has to be processed for p ij time units without preemption by size ij processors of stage i, and can be started only after the completion of the previous task from this sequence. In other words, the i-th task of job j    J is defined by its processing time, p ij > 0, and the number of processors required, size ij . For convenience, we will regard the i-th task of job j to be comprised of size ij operations, which must be processed simultaneously at stage i. The remaining parameters of the jobs are as follows: r j - release date of job j, q j - delivery time of job j, t ij - transport time between stage i and i +1ofjob j. To be in line with transport times, we will refer to job release dates and delivery times as t 0j and t mj , respectively. Additionally, there are stage- and sequence-dependent set-up times. s ijk , i        0}, k    J , denotes the set-up time between job j and job k at stage i,ands i0k is a set-up time, which occurs before i-th task of job k if it is processed first on...",
            "group": 57,
            "name": "10.1.1.3.6471",
            "keyword": "",
            "title": "Metaheuristic Algorithms for Hybrid Flow-Shop"
        },
        {
            "abstract": "Builtin Data Types . . . . . . . . . . . . . . . . . . . . . . 49 4.11.2 Graph Invariants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5 A Denotational Semantics of Localizer 55 5.1 Little Localizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.2 Notations and Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 5.3 Semantic Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.4 The Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 5.4.1 Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 5.4.2 Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.4.3 Declarations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.4.4 Invariants maintenance . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.4.5 Neighborhood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5.4.6 Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 vi  6 Implementation 69 6.1 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 6.2 Static Invariants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.2.1 The Planning Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.2.2 The Execution Phase . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.2.3 Propagating the Invariants . . . . . . . . . . . . . . . . . . . . . . . 72 6.2.4 Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.3 Dynamic Invariants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 6.3.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 6.3.2 Overview of the Approach . . . ...",
            "group": 58,
            "name": "10.1.1.3.8338",
            "keyword": "",
            "title": "Localizer: A modeling language for local search"
        },
        {
            "abstract": "This paper presents a contextual classifier based on quadtree structures and Markov random fields theory. The initial classification is realized by a clustering algorithm, then for each level of the tree, boundary regions are found. Pixels of boundary regions are classified by using a combination of nearest class mean criterion, Mahalanobis distance criterion and finally a Markov model. Our scheme is simple to implement and performs well, giving satisfactory results for SAR images.",
            "group": 59,
            "name": "10.1.1.3.8722",
            "keyword": "",
            "title": "Segmentation Of SAR Images Using Quadtree And Potts Model"
        },
        {
            "abstract": "This thesis presents a concept for controlling the exchange of data in computational networks forming integrated design models. Its primary focus is on managing circular dependencies which may lead to an infinite propagation of changes of design parameter values through the system without converging to a final solution. The proposed technique applies graph algorithms to analyze the topology of the network and to locate its components with circular dependencies. These parts are encapsulated and solved separately. It is shown that by determining their feedback vertex sets their solving can be formulated as root finding problems of systems of equations of minimal dimensionality. The implementation of this concept, a software program that provides a collection of numerical search methods for solving systems of equations, is described. This collection includes algorithms specialized on root finding and techniques for local and global optimization.",
            "group": 60,
            "name": "10.1.1.3.8766",
            "keyword": "",
            "title": "Computational Strategies for Managing Circular Dependencies in Integrated Product Design Models"
        },
        {
            "abstract": "Automating the scheduling of sport leagues has received considerable attention in recent  years, as these applications involve significant revenues and generate challenging combinatorial  optimization problems. This paper considers the traveling tournament problem (TTP)  proposed in [10, 4] to abstract the salient features of major league baseball (MLB) in the  United States. It proposes a simulated annealing algorithm (TTSA) for the TTP that explores  both feasible and infeasible schedules, uses a large neighborhood with complex moves, and  includes advanced techniques such as strategic oscillation and reheats to balance the exploration  of the feasible and infeasible regions and to escape local minima at very low temperatures.",
            "group": 61,
            "name": "10.1.1.3.8839",
            "keyword": "",
            "title": "Proceedings CPAIOR'03"
        },
        {
            "abstract": "Data assimilation for nonlinear ocean models can be extremely complicated. A typical marine ecosystem model consists of equations that are both nonlinear and coupled to each other, and the data assimilation problem is therefore nontrivial.",
            "group": 62,
            "name": "10.1.1.3.9011",
            "keyword": "Data assimilationWeak constraint inverseZero-dimensional marine ecosystem model",
            "title": "A Weak Constraint Inverse For A Zero-Dimensional Marine Ecosystem Model"
        },
        {
            "abstract": "In this paper we present an automatic fairing algorithm  for bicubic B-spline surfaces. The fairing method consists of a knot removal  and knot reinsertion step which locally smoothes the surface. The  simulated-annealing search strategy is used to search for the global minimum  of the fairing measure.",
            "group": 63,
            "name": "10.1.1.4.192",
            "keyword": "",
            "title": "Fairing Bicubic B-Spline Surfaces"
        },
        {
            "abstract": "systems designers. It entails the determination of optimal buffer allocation plans in production lines  with the objective of maximizing their throughput. We present and compare two stochastic approaches  for solving the buffer allocation problem in large reliable production lines. The allocation plan is calculated  subject to a given amount of total buffer slots using simulated annealing and genetic algorithms.",
            "group": 64,
            "name": "10.1.1.4.257",
            "keyword": "Simulated annealinggenetic algorithmsbuffer allocation AMS Classification90C15 Stochastic programming",
            "title": "Stochastic Algorithms for Buffer Allocation in Reliable Production Lines"
        },
        {
            "abstract": "This paper presents a novel approach to the recovery of generic solid parts of objects from real 2D images. The part vocabulary chosen is the one of geons, which are qualitative volumetric part primitives that are defined by simple but perceptually relevant properties which are viewpoint quasi-invariant. Most previous works on detection and recognition of geons from 2D images relied on quasi-perfect line drawings. The use of aspects has also been proposed for matching fixed templates of synthetic images. Here we use parametrically deformable aspects as 2D models to be matched to real images of geons in the framework of Model-Based Optimisation. The use of parametric models allows us to efficiently represent geons, whereas the use of topologically different aspects yields more robustness in the optimisation process we use, which is Adaptive Simulated Annealing. A simple control strategy is developed that generates initial aspect hypotheses followed by a maximum a posteriori choice of th...",
            "group": 65,
            "name": "10.1.1.4.637",
            "keyword": "",
            "title": "Recovery of Generic Solid Parts by Parametrically Deformable Aspects"
        },
        {
            "abstract": "Model checking is the process of verifying whether a model o a coK452wG t system satisfies a specified tempomp property. Symbolic algoP90wG basedo n Binary Decisio Diagrams (BDDs) have significantly increased the sizeo the mo dels that can be verified. The mainprow42 in symbo licmo del checking is the image computVN7B problem, i.e., e#ciently co4j97Kw the successoK  o r predecesso5 o f a seto f states. This paper is an in-depth studyo the imagecoew5O7j5w pro4Kj4 We analyze and evaluate several newheuristics, metrics, and algo979wG fo thisprow0P0 The algoj25wG use co binato0wG oto0wG4Pj2 techniques such as hill climbing,simulat d annealing,andordering by recursive partWBBVN3F to oO0 better results than was previo4wG the case. Theo70wG42 analysis and systematic experimentatio are used to evaluate the algoPKwG47  ",
            "group": 66,
            "name": "10.1.1.4.873",
            "keyword": "",
            "title": " Using Combinatorial Optimization Methods for Quantification Scheduling    "
        },
        {
            "abstract": "The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we will focus on four areas that have been key in defining the design methodologies over time: physical design, simulation /verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment.",
            "group": 67,
            "name": "10.1.1.4.1019",
            "keyword": "synthesis",
            "title": "An Industrial View of Electronic Design Automation"
        },
        {
            "abstract": "The general design problem in serial production lines concerns the allocation of resources such  as the number of servers, their service rates, and buffers given production-specific constraints,  associated costs, and revenue projections. We describe the design of exPLOre: a modular, objectoriented,  production line optimization software architecture. An abstract optimization module can  be instantiated using a variety of stochastic optimization methods such as simulated annealing  and genetic algorithms. Its search space is constrained by a constraint checker while its search  direction is guided by a cost analyser which combines the output of a throughput evaluator with  the business model. The throughput evaluator can be instantiated using Markovian, generalised  queueing network methods, a decomposition, or an expansion method algorithm.",
            "group": 68,
            "name": "10.1.1.4.1460",
            "keyword": "AMS Classification90C15 Stochastic programming",
            "title": "Modular Production Line Optimization: The ExPLOre Architecture"
        },
        {
            "abstract": "Randomization is a standard technique for improving the performance  of local search algorithms for constraint satisfaction.",
            "group": 69,
            "name": "10.1.1.4.1708",
            "keyword": "",
            "title": "Robust Local Search for Spacecraft Operations Using Adaptive Noise"
        },
        {
            "abstract": "Variable Neighborhood Search (VNS) is a recent metaheuristic, or framework for  building heuristics, which exploits systematically the idea of neighborhood change,  both in the descent to local minima and in the escape from the valleys which contain  them. In this tutorial we first present the ingredients of VNS, i.e., Variable Neighborhood  Descent (VND) and Reduced VNS (RVNS) followed by the basic and then the  general scheme of VNS itself which contain both of them. Extensions are presented,  in particular Skewed VNS (SVNS) which enhances exploration of far away valleys and  Variable Neighborhood Decomposition Search (VNDS), a two-level scheme for solution  of large instances of various problems. In each case, we present the scheme, some  illustrative examples and questions to be addressed in order to obtain an e#cient implementation.",
            "group": 70,
            "name": "10.1.1.4.2350",
            "keyword": "metaheuristicsheuristicsdescentvalleyVariable Neighborhood Search",
            "title": "A Tutorial on Variable Neighborhood Search"
        },
        {
            "abstract": "The algorithm selection problem aims at selecting the best algorithm for a given computational problem instance according to some characteristics of the instance. In this dissertation, we first introduce some results from theoretical investigation of the algorithm selection problem. We show, by Rice's theorem, the nonexistence of an automatic algorithm selection program based only on the description of the input instance and the competing algorithms. We also describe an abstract theoretical framework of instance hardness and algorithm performance based on Kolmogorov complexity to show that algorithm selection for search is also incomputable. Driven by the theoretical results, we propose a machine learning-based inductive approach using experimental algorithmic methods and machine learning techniques to solve the algorithm selection problem. Experimentally, we have",
            "group": 71,
            "name": "10.1.1.4.2598",
            "keyword": "",
            "title": "Algorithm Selection for Sorting and Probabilistic Inference: A Machine Learning-Based Approach"
        },
        {
            "abstract": "A new design methodology that produces hardware solutions for performing real-time image processing is presented here. This design methodology provides significant advantages over traditional hardware design approaches by translating real-time image processing tasks into the gate-level resources of programmable logic-based hardware architectures. The use of programmable logic allows high-performance solutions to be realized with very efficient utilization of available logic and interconnection resources. These implementations provide comparable performance at a lower cost than other available programmable logic-based hardware architectures.",
            "group": 72,
            "name": "10.1.1.4.3814",
            "keyword": "methodologyreconfigurable computingMORRPHTRAVERSE",
            "title": "A Design Methodology for Creating Programmable Logic-based Real-Time Image Processing"
        },
        {
            "abstract": "this paper in [1], [2], [3], [8], [9], [10], [11]. These are supplemented with a new approaches and algorithms developed recently. The paper contains formulation and review of the analyzed multiple processors scheduling problems. Various implementations of several populationbased metaheuristics including evolution algorithms (EA), island-based evolution algorithm (IBEA), population learning algorithm (PLA), neural network algorithm (ANN) trained by population based metaheuristics, ant colony algorithm (ACA) as well as some hybrid algorithms involving population based methods are described. Final section includes computational experiment results",
            "group": 73,
            "name": "10.1.1.4.6761",
            "keyword": "",
            "title": "Population-Based Scheduling on Multiple Processors"
        },
        {
            "abstract": "Ring network design problems have many important applications, especially in the field of telecommunications and vehicle routing. Those problems generally consist of constructing a ring network by selecting a node subset and  corresponding direct links. Different requirements and objectives lead to  various specific types of NP-hard ring network design problems reported in the literature, each with its own algorithms. We exploit the similarities in  problems to produce a more general problem formulation and associated  solution methods that apply to a broad range of problems. Computational  results are reported for an implementation using a meta-heuristics framework  with generic components for heuristic search.",
            "group": 74,
            "name": "10.1.1.4.7512",
            "keyword": "",
            "title": " Solving General Ring Network Design Problems by Meta-Heuristics"
        },
        {
            "abstract": "This paper presents two automatic fairing algorithms for parametric C -continuous bi-cubic B-spline surfaces. The fairing method consists of a knot removal and knot reinsertion step which locally smooths the surface.",
            "group": 75,
            "name": "10.1.1.4.8948",
            "keyword": "",
            "title": "Knot-Removal Surface Fairing Using Search Strategies"
        },
        {
            "abstract": "Becoming trapped in suboptimal local minima is a perennial problem when optimizing visual models, particularly in applications like monocular human body tracking where complicated parametric models are repeatedly fitted to ambiguous image measurements. We show that trapping can be significantly reduced by building `roadmaps' of nearby minima linked by transition pathways -- paths leading over low `mountain passes' in the cost surface -- found by locating the transition state (codimension-1 saddle point) at the top of the pass and then sliding downhill to the next minimum. We present two families of transition-state-finding algorithms based on local optimization. In eigenvector tracking, unconstrained Newton minimization is modified to climb uphill towards a transition state, while in hypersurface sweeping, a moving hypersurface is swept through the space and moving local minima within it are tracked using a constrained Newton method. These widely applicable numerical methods, which appear not to be known in vision and optimization, generalize methods from computational chemistry where finding transition states is critical for predicting reaction parameters. Experiments on the challenging problem of estimating 3D human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently, but also underline the disturbingly large numbers of minima that can exist in this and similar model based vision problems.",
            "group": 76,
            "name": "10.1.1.4.9630",
            "keyword": "Model based visionglobal optimizationsaddle points3D human tracking",
            "title": "Building Roadmaps of Minima and Transitions in Visual Models"
        },
        {
            "abstract": " ",
            "group": 77,
            "name": "10.1.1.5.185",
            "keyword": "",
            "title": "An Empirical Analysis of Weight-Adaptation Strategies for Neighborhoods of Heuristics "
        },
        {
            "abstract": "The science return from future robotic exploration of the martian surface can be enhanced by performing routine processing using on board computers. This can be accomplished by using software that recognizes scientifically relevant surface features from imaging and other data and prioritizes the data for return transmission to Earth. Two algorithms have been designed and evaluated with field data to identify the properties of the environment that can be reliably detected with on board imaging and hyperspectral observation. One algorithm identifies variations in surface textures in images and successfully distinguishes between rocks and soil and between differences in grain size in a rock of a single composition. A second algorithm utilizes a neural net to recognize selected carbonate minerals from spectral reflectance data and successfully identifies carbonates from a set of spectra collected in the field. These types of algorithms will contribute to the efficiency of a landed instrument suite given the limited resources of time, data storage, and available communications opportunities.",
            "group": 78,
            "name": "10.1.1.5.681",
            "keyword": "",
            "title": "Strategies for autonomous rovers at Mars"
        },
        {
            "abstract": "In recent years, there has been much interest in phase transitions of combinatorial problems.",
            "group": 79,
            "name": "10.1.1.5.1038",
            "keyword": "",
            "title": "Phase Transitions and Backbones of the Asymmetric Traveling Salesman Problem"
        },
        {
            "abstract": "In this paper, we study the conditions in which the (1+1)-EA compares favorably  to other evolutionary algorithms (EAs) in terms of fitness distribution function  at given iteration and the average optimization time. Our approach is applicable  when the reproduction operator of an evolutionary algorithm is dominated by  the mutation operator of the (1+1)-EA. In this case one can extend the lower  bounds obtained for the expected optimization time of the (1+1)-EA to other  EAs based on the dominated reproduction operator. This method is exampled on  the sorting problem with HAM landscape and the exchange mutation operator.",
            "group": 80,
            "name": "10.1.1.5.1101",
            "keyword": "",
            "title": "Comparing Evolutionary Algorithms to the (1+1)-EA by Means of Stochastic Ordering"
        },
        {
            "abstract": "Nowadays, many real problems can be solved using  local search strategies. These algorithms incrementally  alter inconsistency value assignments  to all the variables using a repair or hill climbing  metaphor to move towards more and more complete  solutions. Furthermore, if the problem can be  modeled as a distributed problem, the advantages  can be even greater.",
            "group": 81,
            "name": "10.1.1.5.1191",
            "keyword": "",
            "title": "Stochastic Local Search for Distributed Constraint Satisfaction Problems"
        },
        {
            "abstract": "From a computational perspective, there is a close connection  between various probabilistic reasoning tasks and the  problem of counting or sampling satisfying assignments of  a propositional theory. We consider the question of whether  state-of-the-art satisfiability procedures, based on random  walk strategies, can be used to sample uniformly or nearuniformly  from the space of satisfying assignments. We first  show that random walk SAT procedures often do reach the  full set of solutions of complex logical theories. Moreover, by  interleaving random walk steps with Metropolis transitions,  we also show how the sampling becomes near-uniform.",
            "group": 82,
            "name": "10.1.1.5.3281",
            "keyword": "",
            "title": "Towards Efficient Sampling: Exploiting Random Walk Strategies"
        },
        {
            "abstract": "This paper presents a real-valued negative selection algorithm with  good mathematical foundation that solves some of the drawbacks of our previous  approach [11]. Specifically, it can produce a good estimate of the optimal  number of detectors needed to cover the non-self space, and the maximization  of the non-self coverage is done through an optimization algorithm with proven  convergence properties. The proposed method is a randomized algorithm based  on Monte Carlo methods. Experiments are performed to validate the assumptions  made while designing the algorithm and to evaluate its performance.",
            "group": 83,
            "name": "10.1.1.5.4314",
            "keyword": "",
            "title": "A Randomized Real-Valued Negative Selection Algorithm"
        },
        {
            "abstract": "This paper describes a novel image registration method for movement correction of fMR time-series. It is importanttoalignthefMR images in the time-series before time-dependent analyses. This registration method aligns the boundaries of brains extracted from the functional images. It uses a genetic algorithm to minimize the distance function obtained from the chamfer distance transform. The global search nature of genetic algorithm makes this method robust to the presence of local minima. 1. ",
            "group": 84,
            "name": "10.1.1.5.4574",
            "keyword": "",
            "title": "Boundary Based Movement Correction of Functional MR Data Using a Genetic Algorithm"
        },
        {
            "abstract": "at surprisingly, starting with \"good\" initial paths did not necessarily lead to better final solutions. The reason for this appears to be that the local search mechanism itself is powerful enough to improve upon the initial solutions --- often quickly giving better solutions than those generated using  other methods. As for the nature of the local changes considered, Lin and Kernighan found that a 3\\Gammachange approach is clearly better than considering only 2\\Gammachanges, but the additional computational cost of considering 4-changes does not appear to pay off. In general, finding the right local changes to consider requires an empirical evaluation of strategies. Another issue is that of how to select the actual changes made to the current solution. The two extremes are: first-improvement (also called \"hill-climbing\"), in which any favorable change is accepted, and steepest-descent, in which the best possible local improv",
            "group": 85,
            "name": "10.1.1.5.4879",
            "keyword": "",
            "title": "Greedy Local Search"
        },
        {
            "abstract": "Due to the recent changes of the European market including the opening up of Eastern Europe and its associated logistical (distribution) consequences, the economically significant question of reorganization of goods distribution networks represents an increasing problem for companies operating in Europe. The existing concepts indicate many weak points because of insufficient consideration of real, nonlinear storage and transportation costs, especially zone oriented transportation costs, and underlying strategies behind the distribution structure. In addition, the existing concepts are mostly limited to the optimization of only one warehousing level and they do not consider the important factor of time. The consideration of real conditions incl. simultaneous optimization of multiple distribution echelons increases the complexity enormously. The computational implementation of the following contribution - promoted by the DFG (Lu 373/18-2) - bases on application of heuristic optimization, i.e. Simulated Annealing, Evolutionary Strategies, Genetic Algorithms, and Tabu Search for solving this complex optimization problem.",
            "group": 86,
            "name": "10.1.1.5.4940",
            "keyword": "Heuristic AlgorithmsFacility LocationsDistribution NetworksNonlinear Optimization",
            "title": "HEURISTIC ALGORITHMS FOR OPTIMIZATION OF ZONE BASED TIME ORIENTED DISTRIBUTION NETWORKS"
        },
        {
            "abstract": "xi 1 ",
            "group": 87,
            "name": "10.1.1.5.6240",
            "keyword": "Acknowledgements.................................................... viii",
            "title": "Empirical and Analytic Approaches to Understanding Local Search Heuristics"
        },
        {
            "abstract": "Finding the maximum likelihood estimators for some distributional  parameters of intensity data in Synthetic Aperture Radar (SAR) images is a very difficult  optimisation problem due to, among other reason, the presence of several local maxima  in the objective function, the analytical intractability of the expressions involved and  numerical instabilities. A possible approach to this problem is the use of stochastic  optimisation techniques, such as simulated annealing and genetic algorithms, that do not  get trapped into local maxima hills and, thus, make it possible to deal with very general  distributions. This work shows the results of such approach in real situations, with  images obtained from urban areas.",
            "group": 88,
            "name": "10.1.1.5.6606",
            "keyword": "Remote SensingSAR imagesmultiplicative modelparameter estimationstochastic",
            "title": "Genetic-Annealing Parameter Estimation for Intensity SAR Data"
        },
        {
            "abstract": "This paper attempts to cover the main algorithms used for clustering, with a brief and simple description of each. For each algorithm, I have selected the most common version to represent the entire family. Advantages and drawbacks are commented for each case, and the general idea of possible sub-variations is presented",
            "group": 89,
            "name": "10.1.1.5.7425",
            "keyword": "",
            "title": "A Comprehensive Overview of Basic Clustering Algorithms"
        },
        {
            "abstract": "Caches have become increasingly important with the widening gap between main memory and processor speeds. Small and fast cache memories are designed to bridge this discrepancy. However, they are only effective when programs exhibit sufficient data locality. Performance of memory hierarchy...",
            "group": 90,
            "name": "10.1.1.5.8327",
            "keyword": "",
            "title": "Optimizing Program Locality through CMEs and GAs"
        },
        {
            "abstract": "We describe two Go programs,  and  , developed by a Monte-Carlo approach that is simpler than Bruegmann's (1993) approach. Our method is based on Abramson (1990). We performed experiments to assess ideas on (1) progressive pruning, (2) all moves as first heuristic, (3) temperature, (4) simulated annealing, and (5) depth-two tree search within the Monte-Carlo framework. Progressive pruning and the all moves as first heuristic are good speed-up enhancements that do not deteriorate the level of the program too much. Then, using a constant temperature is an adequate and simple heuristic that is about as good as simulated annealing. The depth-two heuristic gives deceptive results at the moment. The results of our Monte-Carlo programs against knowledge-based programs on 9x9 boards are promising. Finally, the ever-increasing power of computers lead us to think that Monte-Carlo approaches are worth considering for computer Go in the future.",
            "group": 91,
            "name": "10.1.1.5.8562",
            "keyword": "",
            "title": "Monte-Carlo Go Developments"
        },
        {
            "abstract": "Motivation: Although many network inference algorithms have been presented in the bioinformatics literature, no suitable approach has been formulated for evaluating their effectiveness at recovering models of complex biological systems from limited data. To overcome this limitation, we propose an approach to evaluate network inference algorithms according to their ability to recover a complex functional network from biologically reasonable simulated data.",
            "group": 92,
            "name": "10.1.1.5.9173",
            "keyword": "Bayesian networknetwork inference algorithmmodel inductionmodel inferen",
            "title": "Evaluating functional network inference using simulations of complex biological systems V."
        },
        {
            "abstract": "The recovery of generic solid parts is a fundamental step towards the realization of general-purpose vision systems. This thesis investigates issues in grouping, segmentation and recognition of parts from two-dimensional edge images.",
            "group": 93,
            "name": "10.1.1.5.9716",
            "keyword": "",
            "title": "Part-based Grouping and Recognition: A Model-Guided Approach"
        },
        {
            "abstract": "this paper.  In noisy optimization in general, it is possible to observe a sample drawn from distribution F x at each x, with F x possibly di#erent for each x. The mean of F x is Q(x). If there are just two x's, and the probe points selected by us are X 1 ,...,X n ,whereeachoftheX i 's is one of the x's, then the purpose in bandit problems is to minimize  An =      Q(X i )  in some sense (by, e.g., keeping E{An    small). This minimization is with respect to the sequential choice of the X i 's. Obviously, we would like all X i 's to be exactly at the best x, but that is impossible since some sampling of the non-optimal value or values x is necessary. Similarly, we may sometimes wish to minimize  Bn =      1 [X i  #=x  # ]  where x # is the global minimum of Q. This is relevant whenever we want to optimize a system on the fly, such as an operational control system or a game-playing program. Strategies have been developed based upon certain parametric assumptions on the F x 's or in a purely nonparametric setting. A distinction is also made between finite horizon and infinite horizon solutions. With a finite number of bandits, if at least one F x is nondegenerate, then for any algorithm, we must have EBn    c log n for some constant c>0 on some optimization problem (Robbins, 1952; Lai and Robbins, 1985)",
            "group": 94,
            "name": "10.1.1.5.9739",
            "keyword": "",
            "title": "Random Search Under Additive Noise"
        },
        {
            "abstract": "We report on some exceptionally good results in the solution of randomly generated 3-satisfiability instances using the \"record-to-record travel (RRT)\" local search method. When this simple, but less-studied algorithm is applied to random instances from the problem's satisfiable phase, it seems to find satisfying truth assignments almost always in linear time, with the coefficient of linearity depending on the ratio &alpha; of clauses to variables in the generated instances. RRT has a parameter for tuning...",
            "group": 95,
            "name": "10.1.1.6.938",
            "keyword": "",
            "title": "An Efficient Local Search Method for Random 3-Satisfiability"
        },
        {
            "abstract": "This paper underlines the association of two computer go approaches,  a domain-dependent knowledge approach and Monte Carlo. First, the  strengthes and weaknesses of the two existing approaches are related.",
            "group": 96,
            "name": "10.1.1.6.1059",
            "keyword": "",
            "title": "Associating Domain-Dependent Knowledge and Monte Carlo Approaches within a Go Program"
        },
        {
            "abstract": "One of the main tasks software testing includes is the generation of the test cases to be used  during the test. Due to its expensive cost, the automatization of this task has become one of the  key issues in the area. The field of Evolutionary Testing deals with this problem by means of  combinatorial optimization search techniques. An evolutionary ",
            "group": 97,
            "name": "10.1.1.6.1190",
            "keyword": "evolutionary computationssoftware testingbranch coveragescatter searchestimation of distribution algorithms",
            "title": "Scatter Search and Estimation of Distribution Algorithms in Software Testing: Competition and Cooperation"
        },
        {
            "abstract": "This paper discusses a method for estimating glottal flow derivative model parameters using the wavelet-smoothed excitation. The excitation is first estimated using the Weighted Recursive Least Squares with Variable Forgetting Factor algorithm. The raw excitation is then smoothed by applying a Discrete Wavelet Transform (DWT) using Biorthogonal Quadrature filters, and a thresholding operation done on the DWT amplitude coefficients, followed by an inverse DWT. The pitch period and the instant of glottal closure (IGC) are estimated from the wavelet-smoothed excitation. A sixparameter glottal flow derivative model consisting of three amplitude and three timing parameters is aligned with the IGC and optimized by minimum square error fitting to the speech waveform. The optimization is done by the method of simulated annealing The model is then used to reestimate the vocal-tract filter parameters in an ARX procedure followed by further stages of voice source-vocal tract estimation. The results of analysis of speech utterances from the BK_TIMIT database will be presented. 1. ",
            "group": 98,
            "name": "10.1.1.6.2434",
            "keyword": "",
            "title": "Glottal Flow Derivative Modeling With The"
        },
        {
            "abstract": "We hypothesize that evolutionary algorithms can effectively schedule coordinated fleets of Earth observing satellites. The constraints are complex and the bottlenecks are not well understood, a condition where evolutionary algorithms are often effective. This is, in part, because evolutionary algorithms require only that one can represent solutions, modify solutions, and evaluate solution fitness.",
            "group": 99,
            "name": "10.1.1.6.2835",
            "keyword": "",
            "title": "Scheduling Earth Observing Satellites with Evolutionary Algorithms"
        },
        {
            "abstract": "this paper, we discuss, from a practical point of view, the e#ectiveness of applying reusable metaheuristics software components to the continuous flow-shop scheduling problem. This includes analyzing the knowledge and e#orts needed to adapt the metaheuristics and analyzing by experiments the trade-o# between running time and solution quality. Our goal is to gain general insights in the e#ectiveness of applying di#erent types of metaheuristics with respect to di#erent demands for solution quality and di#erent amounts of available resources such as knowledge about algorithms, implementation e#orts and running time. In Section 2, we first describe the continuous flow-shop scheduling problem. Then, in Sections 3 and 4, we review di#erent kinds of construction methods and metaheuristics. The implementation is briefly discussed in Section 5. In Section 6, we provide and discuss extensive computational results. Finally, we draw some conclusions and give directions for future research",
            "group": 100,
            "name": "10.1.1.6.3002",
            "keyword": "MetaheuristicsHeuristicsTabu SearchSimulated AnnealingContinuous Flow-Shop Scheduling",
            "title": "Solving the Continuous Flow-Shop Scheduling Problem Metaheuristics"
        },
        {
            "abstract": "The main goal of this paper is to attempt an unbiased comparison of the peformance of straightforward implementations of five different metaheuristics on a university course timetabling problem. In particular the metaheuristics under consideration are Evolutionary Algorithms, Ant Colony Optimization, Iterated Local Search, Simulated Annealing, and Tabu Search. To attempt fairness the implementations of all the algorithms use a common solution representation, and a common neighbourhood structure or local search. The results show that no metaheuristic is best on all the timetabling instances considered. Moreover, even when instances are very similar, from the point of view of the instance generator, it is not possible to predict the best metaheuristic, even if some trends appear when focusing on particular instance classes. These results underline...",
            "group": 101,
            "name": "10.1.1.6.3087",
            "keyword": "",
            "title": "A Comparison of the Performance of Different Metaheuristics on the Timetabling Problem"
        },
        {
            "abstract": " ",
            "group": 102,
            "name": "10.1.1.6.4164",
            "keyword": "",
            "title": "An Ant Colony System Hybridized with a New Local Search for the Sequential Ordering Problem"
        },
        {
            "abstract": "This paper reviews the various studies that have introduced adaptive and selfadaptive parameters into Evolutionary Computations. A formal definition of an adaptive evolutionary computation is provided with an analysis of the types of adaptive and self-adaptive parameter update rules currently in use. Previous studies are reviewed and placed into a categorization that helps to illustrate their similarities and differences.  Introduction",
            "group": 103,
            "name": "10.1.1.6.4594",
            "keyword": "",
            "title": "Adaptive and Self-adaptive Evolutionary Computations"
        },
        {
            "abstract": "Partitioning a large set of objects into homogeneous clusters is a fundamental operation in data mining. The k-means algorithm is best suited for implementing this operation because of its efficiency in clustering large data sets. However, working only on numeric values limits its use in data mining because data sets in data mining often contain categorical values. In this paper we present an algorithm, called k-modes, to extend the k-means  paradigm to categorical domains. We introduce new dissimilarity measures to deal with categorical objects, replace means of clusters with modes, and use a frequency based method to update modes in the clustering process to minimise the clustering cost function. Tested with the well known soybean disease data set the algorithm has demonstrated a very good classification performance. Experiments on a very large health insurance data set consisting of half a million records and 34 categorical attributes show that the algorithm is scalable in terms of ...",
            "group": 104,
            "name": "10.1.1.6.4718",
            "keyword": "",
            "title": "A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining"
        },
        {
            "abstract": "As modem software systems are large and complex,  appropriate abstractions of their structure  are needed to make them more understandable  and, thus, easier to maintain. Software clustering  tools are useful to support the creation of these  abstractions. In this paper we describe our search  algorithms for software clustering, and conduct a  case study to demonstrate how altering the clustering  parameters impacts the behavior and performance  of our algorithms.",
            "group": 105,
            "name": "10.1.1.6.4923",
            "keyword": "",
            "title": "Using Heuristic Search Techniques to Extract  Design Abstractions from Source Code  "
        },
        {
            "abstract": "Pythagorean--hodograph (PH) curves admit the formulation of  real--time CNC interpolators that are extremely accurate, flexible,  and robust. Motivated by the practical benefits of these algorithms  in high--speed machining applications, we study the approximation  of \"traditional\" (piecewise--linear/circular) G code part programs  by PH curve tool paths. A least--squares fitting approach, entailing  the solution of a non--linear system of equations in four variables, is  employed to accomplish this approximation. We discuss both the  Newton--Raphson and simulated annealing methods for solving this  system. We also address issues of tolerance computation, footpoint  parameter refinement, penalization of the objective function by the  absolute rotation numbers or bending energies of PH curves, and  extension of the fitting procedure  to3  D tool paths.",
            "group": 106,
            "name": "10.1.1.6.4984",
            "keyword": "",
            "title": "Least-Squares Tool Path Approximation with Pythagorean-Hodograph Curves for High-Speed CNC Machining"
        },
        {
            "abstract": "This paper describes the application of stochastic search techniques to the production scheduling of a group of linked oil and gas fields. The goal was the maximisation of total net present value and a genetic algorithm using problem-specific crossover operators was particularly successful in this respect.",
            "group": 107,
            "name": "10.1.1.6.5812",
            "keyword": "",
            "title": "Presented at the \"European 3-D Reservoir Modelling Conference\", Society of Petroleum Engineers, Stavanger (Norway), April 1996."
        },
        {
            "abstract": "This paper presents spade, a system for partitioning designs onto multi-fpga architectures. The input to spade  is a task graph, that is composed of computational tasks, memory tasks and the communication and synchronization between tasks. spade consists of an iterative partitioning engine, an architectural constraint evaluator, and a throughput optimization and rtl design space exploration heuristic. We show how various architectural constraints can be effectively handled using an iterative partitioning engine. 1 Introduction  The primary focus of existing multi-fpga partitioning research is rtl or gate level partitioning. A popular and advantageous system-synthesis approach is to perform behavioral partitioning prior to, or during, the high-level synthesis process [1]. Although several interesting features may be found in existing multi-fpga partitioning techniques, it is very difficult to adapt any of them to suite partitioning for a generic multi-fpga board. This is because, stateo...",
            "group": 108,
            "name": "10.1.1.6.6028",
            "keyword": "",
            "title": "Task-level Partitioning and RTL Design Space Exploration for Multi-FPGA Architectures"
        },
        {
            "abstract": "Domain-independent planning is a hard combinatorial problem. Taking into account  plan quality makes the task even more difficult. This article introduces Planning by Rewriting  (PbR), a new paradigm for efficient high-quality domain-independent planning. PbR  exploits declarative plan-rewriting rules and e#cient local search techniques to transform  an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition  to addressing the issues of planning e#ciency and plan quality, this framework offers  a new anytime planning algorithm. We have implemented this planner and applied it to  several existing domains. The experimental results show that the PbR approach provides  significant savings in planning effort while generating high-quality plans.",
            "group": 109,
            "name": "10.1.1.6.7048",
            "keyword": "",
            "title": " Planning by Rewriting"
        },
        {
            "abstract": "The analogy between combinatorial optimization and statistical  mechanics has proven to be a fruitful object of study. Simulated annealing,  a metaheuristic for combinatorial optimization problems, is based on this  analogy. In this paper",
            "group": 110,
            "name": "10.1.1.6.7139",
            "keyword": "combinatorial problemasymptotic behaviorprobabilistic analysisstatistical mechanics",
            "title": "An Asymptotical Study of Combinatorial Optimization by Means of Statistical Mechanics"
        },
        {
            "abstract": "This paper reviews a number of different approaches aiming at introducing some robustness into source coders/decoders when the coded bit-stream is to be sent through a noisy channel. Various methods are presented with reference to a scheme in which all tasks that have to be completed in sequence are explicitly shown. Depending on the assumptions on which these methods rely, some blocks are merged, and have to perform a more complex task which is then to be optimized for minimum distortion under noisy channel conditions. Past and current studies are outlined in this context. 1 Introduction  Roughly speaking, source coding is a data compression process that aims at removing as much as possible redundancy from the source signal, while channel coding is the process of intelligent redundancy insertion which creates some kind of protection against the channel noise. In this aspect, these two processes seem to act in opposition. The joint source-channel coding theorem of Shannon consists of t...",
            "group": 111,
            "name": "10.1.1.6.8081",
            "keyword": "",
            "title": "Combined Source-Channel Coding: Panorama of Methods"
        },
        {
            "abstract": "We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated.",
            "group": 112,
            "name": "10.1.1.6.8250",
            "keyword": "Markov chain Monte CarloLangevin modelstempered diffusionsexponential ergodicityOzaki discretisation",
            "title": "Tempered Langevin diffusions and algorithms"
        },
        {
            "abstract": "As the Internet infrastructure grows to support a variety of services (eg: VPNs), its legacy protocols (eg: OSPF, BGP) are being overloaded with new functions such as traffic engineering. Today, operators engineer such capabilities through clever, but manual parameter tuning. In this paper, we propose a back-end support tool for large-scale parameter configuration that is based on efficient parameter state space search techniques and on-line simulation. The framework is useful when the network protocol performance is sensitive to its parameter settings, and its performance can be reasonably modeled in simulation. In particular, our system imports the network topology, relevant protocol models and latest monitored traffic patterns into a simulation that runs on-line in a network operations center (NOC). Each simulation evaluates the network performance for a particular setting of protocol parameters. A recursive random search (RRS) technique is proposed to efficiently explore the large-dimensional parameter state space, where each sample point results in a single simulation. An important feature of this framework is its flexibility: it allows arbitrary choices in terms of the simulation engines used (eg: ns-2, SSFnet, future scalable simulators etc), network protocols to be simulated (eg: OSPF, BGP, RED, MPLS etc), and in the specification of the optimization objectives. We demonstrate the flexibility and relevance of this framework in three scenarios: joint tuning of the RED buffer management parameters at multiple bottlenecks, traffic engineering using OSPF link weight tuning, and outbound load-balancing of traffic at peering/transit points using BGP LOCAL PREF parameter. The on-line simulation framework has been prototyped in Linux using SNMP as the configuration inte...",
            "group": 113,
            "name": "10.1.1.6.8438",
            "keyword": "",
            "title": "Large-Scale Network Parameter Configuration Using an On-line Simulation Framework"
        },
        {
            "abstract": "We show how coupling of local optimization processes can lead to better solutions  than multi-start local optimization consisting of independent runs. This is achieved  by minimizing the average energy cost of the ensemble, subject to synchronization  constraints between the state vectors of the individual local minimizers. From an  augmented Lagrangian which incorporates the synchronization constraints both as  soft and hard constraints, a network is derived wherein the local minimizers interact  and exchange information through the synchronization constraints. From the viewpoint  of neural networks, the array can be considered as a Lagrange programming  network for continuous optimization and as a cellular neural network (CNN). The  penalty weights associated with the soft state synchronization constraints follow from  the solution to a linear program. This expresses that the energy cost of the ensemble  should maximally decrease. In this way successful local minimizers can implicitly  impose their state to the others through a mechanism of master-slave dynamics resulting  into a cooperative search mechanism. Improved information spreading within  the ensemble is obtained by applying the concept of small-world networks. We illustrate  the new optimization method on two dierent problems: supervised learning  of multilayer perceptrons and optimization of Lennard-Jones clusters. The initial  distribution of the local minimizers plays an important role. For the training of multilayer  perceptrons this is related to the choice of the prior on the interconnection  weights in Bayesian learning methods. Depending on the choice of this initial distribution,  coupled local minimizers (CLM) can avoid over  tting and produce good  generalization, i.e. reach a state of intelligen...",
            "group": 114,
            "name": "10.1.1.6.8893",
            "keyword": "",
            "title": "Intelligence and Cooperative Search by Coupled Local Minimizers"
        },
        {
            "abstract": "Over the years different interpretations of Rent's rule and different ways of estimating the Rent parameters have emerged. In general, these parameters are extracted from the average terminal-gate relationship for a set of circuit modules. We show that this relationship (the Rent characteristic) strongly depends on the definition of the circuit modules. These can be generated in many different ways, either from the topology of the circuit graph or, in a geometric way, by cutting regions from a circuit layout. The resulting Rent parameters can be quite far apart. This paper discusses the fundamental differences between the topological and the two geometric interpretations of the Rent characteristic that are expected to be most appropriate for current wirelength estimation techniques. Our discussion is based on experimental data, as well as on a theoretical model that can be used to estimate certain geometric Rent characteristics from the topological Rent parameters. Using this model, we derive a theoretical lower limit to the value of the average geometric Rent exponent. We also study the impact of the placement approach and placement quality on the geometric Rent characteristics.",
            "group": 115,
            "name": "10.1.1.6.9516",
            "keyword": "",
            "title": " A Comparison of Various Terminal-gate Relationships For Interconnect . . ."
        },
        {
            "abstract": "This thesis describes a new, efficient, and general purpose parallel  simulated annealing algorithm. The algorithm is based on periodic  mixing steps, in which favorable states reproduce and unfavorable  ones are destroyed. It runs on a distributed memory Multiple  Instructions Multiple Data architecture parallel computer. Parallel  eciency is controlled by the interval between mixing steps. In  this thesis, it is shown that for certain values of this interval found  by exhaustive search, the algorithm can give up to 100% parallel  eciency on up to 50 processors and 80% parallel efficiency on 100  processors. Moreover, for a given number of processors, there is a range of mixing interval which gives high parallel efficiency. In  this thesis, two efficient statistical estimators, namely, the cross-correlation and variance among processors are defined for finding  efficient mixing intervals are constructed which give parallel efficiency of 75% without exhaustive search. This is done by tracking  the two statistical estimators right after communication, so as to  obtain the lower and upper bounds for the optimal mixing interval.",
            "group": 116,
            "name": "10.1.1.6.9557",
            "keyword": "State Mixing by",
            "title": "Optimal Parallelization of Simulated Annealing by State Mixing "
        },
        {
            "abstract": "A contextual segmentation technique to detect brain activation from functional brain images is presented in the Bayesian framework. Unlike earlier similar approaches [Holmes and Ford (1993) and Descombes et al. (1998)], a Markov random field (MRF) is used to represent configurations of activated brain voxels, and likelihoods given by statistical parametric maps (SPM's) are directly used to find the maximum a posteriori (MAP) estimation of segmentation. The iterative segmentation algorithm, which is based on a simulated annealing scheme, is fully data-driven and capable of analyzing experiments involving multiple-input stimuli. Simulation results and comparisons with the simple thresholding and the statistical parametric mapping (SPM) approaches are presented with synthetic images, and functional MR images acquired in memory retrieval and event-related working memory tasks. The experiments show that an MRF is a valid representation of the activation patterns obtained in functional brain images, and the present technique renders a superior segmentation scheme to the context-free approach and the SPM approach.",
            "group": 117,
            "name": "10.1.1.7.792",
            "keyword": "imaging",
            "title": "Bayesian Approach to Segmentation of Statistical"
        },
        {
            "abstract": "We describe an end-to-end real-time S&P futures trading system. Inner-shell stochastic nonlinear dynamic models are developed, and Canonical Momenta Indicators (CMI) are derived from a fitted Lagrangian used by outer-shell trading models dependent on these indicators. Recursive and adaptive optimization using Adaptive Simulated Annealing (ASA) is used for fitting parameters shared across these shells of dynamic and trading models.",
            "group": 118,
            "name": "10.1.1.7.1041",
            "keyword": "",
            "title": "Optimization of Trading Physics Models of Markets"
        },
        {
            "abstract": "We propose an integrated framework for the design of SOC test solutions, which includes a set of algorithms for early design space exploration as well as extensive optimization for the final solution. The framework deals with test scheduling, test access mechanism design, test sets selection, and test resource placement. Our approach minimizes the test application time and the cost of the test access mechanism while considering constraints on tests and power consumption. Themain feature of our approach is that it provides an integrated design environment to treat several different tasks at the same time, which were traditionally dealt with as separate problems. We have made an implementation of the proposed heuristic used for the early design space exploration and an implementation based on Simulated Annealing for the extensive optimization. Experiments on several benchmarks and industrial designs show the usefulness and efficiency of our approach.",
            "group": 119,
            "name": "10.1.1.7.1489",
            "keyword": "SOC testtest schedulingtest access mechanism designtest resource partitioningtest resource placement",
            "title": "JOURNAL OF ELECTRONIC TESTING: Theory and Applications 18, 385--400, 2002 c"
        },
        {
            "abstract": "We propose a new method for training an ensemble of neural networks. A population of networks is created and maintained  such that more probable networks replicate and less probable networks vanish. Each individual network is updated using random  weight changes. This produces a diversity among the networks which is important for the ensemble prediction using the population.",
            "group": 120,
            "name": "10.1.1.7.1675",
            "keyword": "",
            "title": "A New Learning Scheme for Neural Network Ensembles"
        },
        {
            "abstract": "A paradigm of statistical mechanics of financial markets (SMFM) using nonlinear nonequilibrium algorithms, first published in L. Ingber, Mathematical Modelling, 5, 343-361 (1984), is fit to multivariate financial markets using Adaptive Simulated Annealing (ASA), a global optimization algorithm, to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta are thereby derived and used as technical indicators in a recursive ASA optimization process to tune trading rules. These trading rules are then used on out-ofsample data, to demonstrate that they can profit from the SMFM model, to illustrate that these markets are likely not efficient.",
            "group": 121,
            "name": "10.1.1.7.2186",
            "keyword": "EconomicsSimulated AnnealingStatistical Mechanics Nonlinear Nonequilibrium Financial Markets-2- Lester Ingber",
            "title": "Statistical Mechanics of Nonlinear Nonequilibrium Financial Markets: Applications to Optimized Trading"
        },
        {
            "abstract": "We present a theory of population based optimization methods using approximations of search distributions. We prove convergence of the search distribution to the global optima for the Factorized Distribution Algorithm FDA if the search distribution is a Boltzmann distribution and the size of the population is large enough. Convergence is defined in a strong sense -- the global optima are attractors of a dynamical system describing mathematically the algorithm. We investigate an adaptive annealing schedule and show its similarity to truncation selection. The inverse temperature beta is changed inversely proportionally to the standard deviation of the population. We extend FDA by using a Bayesian hyper parameter. The hyper parameter is related to mutation in evolutionary algorithms. We derive an upper bound on the hyper parameter to ensure that FDA still generates the optima with high probability. We discuss the relation of the FDA approach to methods used in statistical physics to approximate a Boltzmann distribution and to belief propagation in probabilistic reasoning. In the last part, we apply the algorithm to an important practical problem, the bipartioning of large graphs. We assume that the graphs are sparsely connected. Our empirical results are as good or even better than any other method used for this problem.",
            "group": 122,
            "name": "10.1.1.7.2262",
            "keyword": "linkage equilibriumfactorization of distributions",
            "title": "Evolutionary Optimization and the Estimation of Search Distributions with Applications to Graph Bipartitioning"
        },
        {
            "abstract": "We show that when agents cooperate in a distributed search problem, they can solve it faster than any  agent working in isolation. This is accomplished by having agents exchange hints within a computational  ecosystem. We present a quantitative assessment of the value of cooperation for solving constraint  satisfaction problems through a series of experiments. Our results suggest an alternative methodology  to existing techniques for solving constraint satisfaction problems in computer science and distributed  artificial intelligence.",
            "group": 123,
            "name": "10.1.1.7.2521",
            "keyword": "",
            "title": "Better Than The Best: The Power of Cooperation"
        },
        {
            "abstract": "Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search --- a semi-distributed search algorithm that is widely applicable --- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.",
            "group": 124,
            "name": "10.1.1.7.3546",
            "keyword": "",
            "title": "Reinforcement Learning in Distributed Domains: Beyond Team Games"
        },
        {
            "abstract": "As modern software systems are large and complex,  appropriate abstractions of their structure  are needed to make them more understandable  and, thus, easier to maintain. Software clustering  tools are useful to support the creation of these  abstractions. In this paper we describe our search  algorithms for software clustering, and conduct a  case study to demonstrate how altering the clustering  parameters impacts the behavior and performance  of our algorithms.",
            "group": 125,
            "name": "10.1.1.7.3651",
            "keyword": "",
            "title": "Using Heuristic Search Techniques to Extract Design Abstractions from Source Code"
        },
        {
            "abstract": "This paper deals with synchronization problems arising in the context of cellular  networks. It presents and compares several algorithms that can be used for solving  these problems",
            "group": 126,
            "name": "10.1.1.7.3813",
            "keyword": "",
            "title": "Some Algorithms for Synchronizing Clocks of Base Transceiver Stations in a Cellular Network"
        },
        {
            "abstract": "In k-means clustering we are given a set of n data points in d-dimensional space R^d and an integer k, and the problem is to determine a set of k points in R^d, called centers, to minimize the mean squared distance from each data point to its nearest center. No exact polynomial-time algorithms are known for this problem. Although asymptotically efficient approximation algorithms exist, these algorithms are not practical due to the very high constant factors involved. There are many heuristics that are used in practice, but we know of no bounds on their performance. We consider the question of...",
            "group": 127,
            "name": "10.1.1.7.3865",
            "keyword": "Clusteringk-meansapproximation algorithmslocal searchcomputational geometry",
            "title": "A Local Search Approximation Algorithm for k-Means Clustering"
        },
        {
            "abstract": "Scheduling a casting sequence involving a number of orders with dierent casting weights  and satisfying due dates of production is an important optimization problem often encountered  in foundries. In this paper, we attempt to solve this complex, multi-variable, and multiconstraint  optimization problem using different implementations of genetic algorithms (GAs). In",
            "group": 128,
            "name": "10.1.1.7.3883",
            "keyword": "",
            "title": "Optimal Scheduling of Casting Sequence Using Genetic Algorithms  "
        },
        {
            "abstract": "A series of papers has developed a statistical mechanics of neocortical interactions (SMNI), deriving aggregate behavior of experimentally observed columns of neurons from statistical electrical-chemical properties of synaptic interactions. While not useful to yield insights at the single neuron level, SMNI has demonstrated its capability in describing large-scale properties of short-term memory and electroencephalographic (EEG) systematics. The necessity of including nonlinear and stochastic structures in this development has been stressed. In this paper, a more stringent test is placed on SMNI: The algebraic and numerical algorithms previously developed in this and similar systems are brought to bear to fit large sets of EEG and evoked potential data being collected to investigate genetic predispositions to alcoholism and to extract brain \u201csignatures\u201d of short-term memory. Using the numerical algorithm of Very Fast Simulated Re-Annealing, it is demonstrated that SMNI can indeed fit this data within experimentally observed ranges of its underlying neuronal-synaptic parameters, and use the quantitative modeling results to examine physical neocortical mechanisms to discriminate between high-risk and low-risk populations genetically predisposed to alcoholism. Since this first study is a control to span relatively long time epochs, similar to earlier attempts to establish such correlations, this discrimination is inconclusive because of other neuronal activity which can mask such effects. However, the SMNI model is shown to be consistent",
            "group": 129,
            "name": "10.1.1.7.3947",
            "keyword": "",
            "title": "Statistical mechanics of neocortical interactions: A scaling paradigm applied to electroencephalography"
        },
        {
            "abstract": "This paper describes local probing, an algorithm hybridization form that combines backtrack search enhanced with local consistency techniques (BT+CS) with local search (LS) via probe backtracking.",
            "group": 130,
            "name": "10.1.1.7.4269",
            "keyword": "",
            "title": "Local Probing Applied to Scheduling"
        },
        {
            "abstract": "A paradigm of statistical mechanics of financial markets (SMFM) is fit to multivariate financial markets using Adaptive Simulated Annealing (ASA), a global optimization algorithm, to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta are thereby derived and used as technical indicators in a recursive ASA optimization process to tune trading rules. These trading rules are then used on out-of-sample data, to demonstrate that they can profit from the SMFM model, to illustrate that these markets are likely not efficient. This methodology can be extended to other systems, e.g., electroencephalography. This approach to complex systems emphasizes the utility of blending an intuitive and powerful mathematical-physics formalism to generate indicators which are used by AI-type rule-based models of management.",
            "group": 131,
            "name": "10.1.1.7.5825",
            "keyword": "",
            "title": "Canonical Momenta Indicators of Financial Markets and Neocortical EEG"
        },
        {
            "abstract": "This paper overviews recent work on ant algorithms, that is, algorithms for discrete  optimization which took inspiration from the observation of ant colonies foraging  behavior, and introduces the ant colony optimization (ACO) meta-heuristic. In the  rst part of the paper the basic biological  ndings on real ants are overviewed, and  their arti  cial counterparts as well as the ACO meta-heuristic are de  ned. In the  second part of the paper a number of applications to combinatorial optimization and  routing in communications networks are described. We conclude with a discussion of  related work and of some of the most important aspects of the ACO meta-heuristic.",
            "group": 132,
            "name": "10.1.1.7.6152",
            "keyword": "",
            "title": "Ant Algorithms for Discrete Optimization"
        },
        {
            "abstract": "In this paper, a method for optimal test sequence generation using Multiple Unique Input/Output Sequences (MUIOS) is presented. The test sequence generation problem using MUIOS is viewed as an asymmetric travelling sales person problem and is solved using simulated annealing methodology. This technique is applied to various test cases and the results are compared with the results obtained by using rural Chinese postman algorithm.",
            "group": 133,
            "name": "10.1.1.7.6790",
            "keyword": "formal methods for test sequence generationmultiple",
            "title": "Protocol Test Sequence Generation Using MUIOS Based on TSP Problem"
        },
        {
            "abstract": "Metaheuristic parallel search methods -- tabu search, simulated annealing and genetic algorithms, essentially -- are reviewed, classified and examined not according to particular methodological characteristics, but following the unifying approach of the level of parallelization. It is hoped that by examining the commonalities among parallel implementations across the field of metaheuristics, insights may be gained, trends may be discovered, and research challenges may be identified. Particular attention is paid to applications of parallel metaheuristics to transportation problems.",
            "group": 134,
            "name": "10.1.1.7.7491",
            "keyword": "R'esum'e",
            "title": "Parallel Metaheuristics"
        },
        {
            "abstract": "The model-based methodology has proven to be effective for fast and low-cost development of embedded software. In the model-based development process, transforming a software structural model that describes the underlying application, to an implementable runtime model is a critical issue. Since the designed software will finally run on the target platform, non-functional issues like schedulability, timing constraints and resource requirements have to be considered during the transformation. In this paper, we propose a generic runtime model architecture that can best satisfy the non-functional requirements of the system, and a generic transformation method to convert a structural model to a runtime model in such an architecture. The transformation approach is based on the notion of end-toend computations performed by the system in response to external stimuli. We demonstrate the advantages and effectiveness of the proposed method by constructing a software runtime model for a combined electronic throttle and airfuel ratio control system.",
            "group": 135,
            "name": "10.1.1.7.7593",
            "keyword": "",
            "title": "Transforming Structural Model to Runtime Model of Embedded Software with Real-Time Constraints"
        },
        {
            "abstract": "Traditional design rules for cellular networks are not directly applicable to code division multiple access (CDMA) networks where intercell interference is not mitigated by cell placement and careful frequency planning. For transmission quality requirements, a minimum signal-to-interference ratio (SIR) must be achieved. The base-station location, its pilot-signal power (which determines the size of the cell), and the transmission power of the mobiles all affect the received SIR. In addition, because of the need for power control in CDMA networks, large cells can cause a lot of interference to adjacent small cells, posing another constraint to design. In order to maximize the network capacity associated with a design, we develop a methodology to calculate the sensitivity of capacity to base-station location, pilot-signal power, and transmission power of each mobile. To alleviate the problem caused by different cell sizes, we introduce the power compensation factor, by which the nominal power of the mobiles in every cell is adjusted. We then use the calculated sensitivities in an iterative algorithm to determine the optimal locations of the base stations, pilot-signal powers, and power compensation factors in order to maximize capacity. We show examples of how networks using these design techniques provide higher capacity than those designed using traditional techniques.",
            "group": 136,
            "name": "10.1.1.7.7628",
            "keyword": "",
            "title": "Multicell CDMA Network Design"
        },
        {
            "abstract": "We compare Genetic Algorithms (GA) with a functional search method, Very Fast Simulated Reannealing (VFSR), that not only is efficient in its search strategy, but also is statistically guaranteed to find the function optima. GA previously has been demonstrated to be competitive with other standard Boltzmann-type simulated annealing techniques. Presenting a suite of six standard test functions to GA and VFSR codes from previous studies, without any additional fine tuning, strongly suggests that VFSR can be expected to be orders of magnitude more efficient than GA.",
            "group": 137,
            "name": "10.1.1.7.7777",
            "keyword": "",
            "title": "Genetic Algorithms And Very Fast Simulated Reannealing: A Comparison"
        },
        {
            "abstract": "This article describes a numerical method that may be used to efficiently  locate and track underwater sonar targets in the near-field, with  both bearing and range estimation, for the case of very large passive arrays.",
            "group": 138,
            "name": "10.1.1.7.8175",
            "keyword": "",
            "title": "Numerical Techniques for Efficient Sonar Bearing"
        },
        {
            "abstract": "Automatic graphic object layout methods have long been studied in many application areas in which graphic objects should be laid out to satisfy the constraints specific to each application. In those areas, carefully designed layout algorithms should be used to satisfy each application's constraints. However, those algorithms tend to be complicated and not reusable for other applications. Moreover, it is difficult to add each user's preferences to the layout scheme of the algorithm. To overcome these difficulties, we developed a general-purpose interactive graphic layout system GALAPAGOS based on genetic algorithms. GALAPAGOS is general-purpose because graphic objects are laid out not by specifying how to lay them out, but just by specifying the preferences for the layout. GALAPAGOS can not only lay out complicated graphs automatically, but also allow users to modify the constraints at run time so that users can tell the system their own preferences.",
            "group": 139,
            "name": "10.1.1.7.9341",
            "keyword": "",
            "title": "Graphic Object Layout with Interactive Genetic Algorithms"
        },
        {
            "abstract": "this paper to note that the projects discussed here, modeling NTC and Janus(T) as described above, and modeling teleoperated vehicles, both have brought powerful mathematical machinery to bear to the stages of numerical specificity with state-of-the-art successful description of realistic empirical data",
            "group": 140,
            "name": "10.1.1.7.9641",
            "keyword": "",
            "title": "A. Necessity of Comparing Computer Models to Exercise Data"
        },
        {
            "abstract": "In this thesis, we develop constrained simulated annealing (CSA), a global optimization algorithm that asymptotically converges to constrained global minima (CGM dn ) with probability one, for solving discrete constrained nonlinear programming problems (NLPs). The algorithm is based on the necessary and su#cient condition for constrained local minima (CLM dn ) in the theory of discrete constrained optimization using Lagrange multipliers developed in our group. The theory proves the equivalence between the set of discrete saddle points and the set of CLM dn , leading to the first-order necessary and su#cient condition for CLM dn .",
            "group": 141,
            "name": "10.1.1.7.9822",
            "keyword": "W. Wahfor his invaluable guidanceadvice and encouragement during the course of my",
            "title": " \t Global Optimization For Constrained Nonlinear Programming "
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an  important but very approximate zeroth-order description of actual market behavior. We  generalize the functional form of the diffusion of these systems and also consider multi-factor  models including stochastic volatility. We use a previous development of a statistical  mechanics of financial markets to model these issues. Daily Eurodollar futures prices and  implied volatilities are fit to determine exponents of functional behavior of diffusions using  methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits  across moving time windows of Eurodollar contracts. These short-time fitted distributions  are then developed into long-time distributions using a robust non-Monte Carlo path-integral  algorithm, PATHINT, to generate prices and derivatives commonly used by option traders. The results",
            "group": 142,
            "name": "10.1.1.8.856",
            "keyword": "",
            "title": "Statistical Mechanics of Financial Markets: Exponential Modifications to Black-Scholes"
        },
        {
            "abstract": "This paper considers the importance of endfragment constraints in the construction of contig restriction maps. A representation for such maps is used in conjunction with an objective function based on minimum message length (MML) principles and two stochastic optimization methods. Results from the optimization of real and simulated data sets with and without end fragment constraints are given, and it is shown that better scores can be obtained if end fragment constraints are violated. The effectiveness of the MML objective function is illustrated by its ability to balance a number of conflicting constraints. 1 Introduction  Large genetic mapping projects are currently underway for a range of organisms [8] [10] [13] [4]. These projects have adopted a bottom-up mapping strategy in which large numbers of short DNA extracts called clones are obtained from random and unknown locations in the genome of the organism. Typically, the number of clones obtained will be sufficient to ensure an ave...",
            "group": 143,
            "name": "10.1.1.8.2334",
            "keyword": "",
            "title": "End Fragment Constraints in Stochastic Assembly of Contig Restriction Maps"
        },
        {
            "abstract": "The parameter configuration of a network protocol can be formulated as a black-box optimization problem with network simulation evaluating the performance of the blackbox, i.e., the network. This paper proposes a unified search framework (USF) to handle such large-scale black-box optimization problems. The framework is designed to provides a general platform on which tailored optimization algorithms can be constructed easily for various types of problems. Therefore, it can be applied to the configuration of di#erent network protocols. In the USF, various samplers are provided as basic building blocks and each of them implements a certain search technique. For a specific problem, a selection of samplers can be used to construct an appropriate search algorithm. These samplers are run in parallel and coordinated with various type of memories which selectively store the samples generated by samplers. The USF also include a resource management mechanism, which can manage parallel computing devices, for example, a network of workstations, and allocate the available computing resources to samplers according to the predefined allocation strategy. The benchmark tests are presented in this paper to demonstrate the flexibility and advantages of the USF.",
            "group": 144,
            "name": "10.1.1.8.3151",
            "keyword": "",
            "title": "A Unified Search Framework for Large-scale Black-box"
        },
        {
            "abstract": "Recent work in statistical mechanics has developed new analytical and numerical techniques to solve coupled stochastic equations. This paper applies the very fast simulated re-annealing and path-integral methodologies to the estimation of the Brennan and Schwartz two-factor term structure model. It is shown that these methodologies can be utilized to estimate more complicated n-factor nonlinear models.",
            "group": 145,
            "name": "10.1.1.8.4098",
            "keyword": "",
            "title": "Application of statistical mechanics methodology to term-structure bond-pricing models"
        },
        {
            "abstract": "able nonlinear stochastic development [1-13]. The basic approach of the SMNI has been to statistically aggregate synaptic and neuronal interactions, from microscopic systematics to mesoscopic interactions among minicolumns of hundreds of neurons, to macroscopic macrocolumnar interactions among thousands of minicolumns, to approach regional spatial scales of several millimeters to several centimeters. The theory has been tested by verifying observations at the mesoscopic scale, e.g., shortterm -memory phenomena [4,6], and at the macroscopic scale, e.g., electroencephalography (EEG) [4,5,13]. A current description of the theory to date is in Ref. [13], where extensions were made to the SMNI to correlate human behavioral states to circuitries measured by EEG electrode recordings, an ongoing project. While the experimental resolution of EEG typically is on the order of several centimeters, new work has shown that under some circumstances this resolution can be sharpened to several millimet",
            "group": 146,
            "name": "10.1.1.8.4138",
            "keyword": "",
            "title": "Generic Mesoscopic Neural Networks Based on Statistical Mechanics of Neocortical Interactions"
        },
        {
            "abstract": "In this paper, we perform a comparative study of different heuristics used to design combinational logic circuits. The use of local search hybridized with a genetic algorithm and the effect of parallelism are of particular interest in the study conducted. Our results indicate that a hybridization of a genetic algorithm with simulated annealing is benefitial and that the use of parallelism does not only introduce a speedup (as expected) in the algorithms, but also allows to improve the quality of the solutions found.",
            "group": 147,
            "name": "10.1.1.8.4977",
            "keyword": "",
            "title": "Comparing Different Serial and Parallel"
        },
        {
            "abstract": "Real world engineering design problems are usually characterized by the presence of many conflicting objectives. Therefore, it is natural to look at the engineering design problem as a multiobjective optimization problem. This report summarizes a survey of techniques to conduct multiobjective optimization in an engineering design context.",
            "group": 148,
            "name": "10.1.1.8.5638",
            "keyword": "",
            "title": "A Survey of Multiobjective Optimization in Engineering Design Johan"
        },
        {
            "abstract": "As the Internet infrastructure grows to support a variety of services (eg: VPNs), its legacy protocols (eg: OSPF, BGP) are being overloaded with new functions such as traffic engineering. Today, operators engineer such capabilities through clever, but manual parameter tuning. In this paper, we propose a back-end support tool for large-scale parameter configuration that is based on efficient parameter state space search techniques and online simulation. The framework is useful when the network protocol performance is sensitive to its parameter settings, and its performance can be reasonably modeled in simulation. In particular, our system imports the network topology, relevant protocol models and latest monitored traffic patterns into a simulation that runs online in a network operations center (NOC). Each simulation evaluates the network performance for a particular setting of protocol parameters. A recursive random search (RRS) technique is proposed to explore the large-dimensional parameter state space, where each sample point results in a single simulation. In other words, the overall parameter configuration problem is modeled as a \"black-box\" optimization problem. The goal of RRS is efficiency, i.e., to find \"good\" network protocol configurations for the current traffic conditions quickly. An important feature of the framework is its flexibility: it allows arbitrary choices in terms of the simulation engines used (eg: ns-2, SSFnet, future scalable simulators etc), network protocols to be simulated (eg: OSPF, BGP, RED, MPLS etc), and in the specification of the optimization objectives. We demonstrate the flexibility and relevance of this framework in three scenarios: joint tuning of the RED buffer management parameters at multiple bottlenecks, traffic engineering usi...",
            "group": 149,
            "name": "10.1.1.8.5917",
            "keyword": "",
            "title": "Large-Scale Network Parameter Configuration Using An On-line Simulation Framework"
        },
        {
            "abstract": "Spacecraft design optimization is a difficult problem, due to the complexity of optimization cost surfaces and the human expertise in optimization that is necessary in order to achieve good results. In this paper, we propose the use of a set of generic, metaheuristic optimization algorithms (e.g., genetic algorithms, simulated annealing), which is configured for a particular optimization problem by an adaptive problem solver based on artificial intelligence and machine learning techniques. We describe work in progress on OASIS, a system for adaptive problem solving based on these principles.",
            "group": 150,
            "name": "10.1.1.8.6494",
            "keyword": "TABLE OF CONTENTS",
            "title": "Automating the Process of Optimization in Spacecraft Design"
        },
        {
            "abstract": "One of the most difficult and time-consuming steps in the creation of an FPGA is its transistor-level design and physical layout. Modern commercial FPGAs typically consume anywhere from 50 to 200 man-years simply in the layout step. To date, automated tools have only been employed in small parts of the periphery and programming circuitry. The core tiles, which are repeated many times, are subject to painstaking manual design and layout. In this paper we present a new system (called GILES, for Good Instant Layout of Erasable Semiconductors) that automatically generates a transistor-level schematic from a high-level architectural specification of an FPGA. It also generates a cell-level netlist that is placed and routed automatically. The architectural specification is the one used as input to the VPR [3] architectural exploration tool. The output is the mask-level layout of a single tile that can be replicated to form an FPGA array. We describe a new placement tool that simultaneously places and compacts the layout to minimize white space and wiring demand, and a specialpurpose router built for this task. GILES can",
            "group": 151,
            "name": "10.1.1.8.6589",
            "keyword": "",
            "title": "Automatic Transistor and Physical Design of FPGA Tiles from an Architectural Specification"
        },
        {
            "abstract": "  The FPGAs of today are being used to implement large, system - sized circuits. Systems often require significant memory resources, and vendors have responded to these needs by embedding block memories onto their FPGAs. A proportion of these user circuits are  communications-based, which have slightly different requirements from standard systems. Most notably, these circuits often contain wide data signals that must be buffered and processed efficiently and quickly. Wide signals also often originate from recently developed high-speed I/O that allow FPGAs to communicate with higher frequency circuits. This",
            "group": 152,
            "name": "10.1.1.8.6629",
            "keyword": "",
            "title": "A Novel FPGA Architecture Supporting Wide, Shallow Memories"
        },
        {
            "abstract": "Parameter configuration is a common procedure used in large-scale network protocols to support multiple operational  goals. This problem can be formulated as a black-box optimization problem and solved with an efficient  search algorithm. This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for  large-scale network parameter optimization. The RRS algorithm is based on the initial high-efficiency property of  random sampling and attempts to maintain this high-efficiency by constantly \"restarting\" random sampling with  adjusted sample spaces. Due to its root in random sampling, the RRS algorithm is robust to the effect of random  noises in the objective function and is advantageous in optimizing the objective function with negligible parameters. These features are",
            "group": 153,
            "name": "10.1.1.8.7070",
            "keyword": "",
            "title": "A Recursive Random Search Algorithm for Large-Scale Network Parameter Configuration"
        },
        {
            "abstract": "A new approach is presented for finding the pose of an object model in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can forseeably be used in a wide variety of imaging situations. Experiments are presented that demonstrate the approach registering MR images, aligning a smooth 3D object model to images, aligning a complex 3D object model to real scenes including clutter and occlusion, and aligning a view-based 2D object model to real images. The method is based on a new formulation of the mutual information between the model and the image. As applied here the technique is intensity-based, rather than feature-based. It works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than...",
            "group": 154,
            "name": "10.1.1.8.7452",
            "keyword": "",
            "title": "Alignment by Maximization of Mutual Information"
        },
        {
            "abstract": "This paper presents an hybrid algorithm for deriving 3-D structures of cyclic polypeptides. The algorithm combines constraint-based techniques with the most widely used methods for non-cyclic polypeptides. The empirical results demonstrate that the proposed hybrid algorithm outperforms traditional methods especially with respect to running times.",
            "group": 155,
            "name": "10.1.1.8.7620",
            "keyword": "constraint reasoningsimulated annealingoptimisation problemsprotein folding problems",
            "title": "A Constraint-Based Approach for Deriving 3-D Structures of Cyclic Polypeptides"
        },
        {
            "abstract": "The Bayesian Ying-Yang (BYY) harmony learning acts as a general statistical learning framework, featured by not only new regularization techniques for parameter learning but also a new mechanism that implements model selection either automatically during parameter learning or via a new class of model selection criteria used after parameter learning. In this paper, further advances on BYY harmony learning by considering modular inner representations are presented in three parts. One consists of results on unsupervised mixture models, ranging from Gaussian mixture based Mean Square Error (MSE) clustering, elliptic clustering, subspace clustering to NonGaussian mixture based clustering not only with each cluster represented via either Bernoulli -- Gaussian mixtures or independent real factor models, but also with independent component analysis implicitly made on each cluster. The second consists of results on supervised mixture-ofexperts (ME) models, including Gaussian ME, Radial Basis Function nets, and Kernel regressions. The third consists of two strategies for extending the above structural mixtures into self-organized topological maps. All these advances are introduced with details on three issues, namely, (a) adaptive learning algorithms, especially elliptic, subspace, and structural rival penalized competitive learning algorithms, with model selection made automatically during learning; (b) model selection criteria for being used after parameter learning, and (c) how these learning algorithms and criteria are obtained from typical special cases of BYY harmony learning.",
            "group": 156,
            "name": "10.1.1.8.9499",
            "keyword": "",
            "title": "BBY Harmony Learning,  Structural RPCL, . . . "
        },
        {
            "abstract": "A modern calculus of multivariate nonlinear multiplicative Gaussian-Markovian  systems provides models of many complex systems faithful to their nature, e.g., by not  prematurely applying quasi-linear approximations for the sole purpose of easing analysis. To  handle these complex algebraic constructs, sophisticated numerical tools have been  developed, e.g., methods of adaptive simulated annealing (ASA) global optimization and of  path integration (PATHINT). In-depth application to three quite different complex systems  have yielded some insights into the benefits to be obtained by application of these algorithms  and tools, in statistical mechanical descriptions of neocortex (short-term memory and  electroencephalography), financial markets (interest-rate and trading models), and combat  analysis (baselining simulations to exercise data).",
            "group": 157,
            "name": "10.1.1.8.9526",
            "keyword": " statistical mechanics...-2- Lester Ingber",
            "title": "Data mining and knowledge discovery via statistical mechanics in nonlinear stochastic systems"
        },
        {
            "abstract": "Genetic programming suffers difficulty  in discovering useful numeric constants  for the terminal nodes of its sexpression  trees. In earlier work we postulated  a solution to this problem called  numeric mutation. Here, we provide empirical  evidence to demonstrate that this  method provides a statistically significant  improvement in GP system performance  on a variety of problems.",
            "group": 158,
            "name": "10.1.1.8.9806",
            "keyword": "",
            "title": "Numeric Mutation Improves the Discovery of Numeric Constants in Genetic Programming "
        },
        {
            "abstract": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering  is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean--field approximation. A new algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyse dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images.  \"Pairwise Data Clustering...",
            "group": 159,
            "name": "10.1.1.8.9812",
            "keyword": "",
            "title": "Pairwise Data Clustering by Deterministic Annealing"
        },
        {
            "abstract": "Ant Colony Optimization (ACO) is a recent metaheuristic method that  is inspired by the behavior of real ant colonies. In this paper, we review the  underlying ideas of this approach that lead from the biological inspiration  to the ACO metaheuristic, which gives a set of rules of how to apply ACO  algorithms to challenging combinatorial problems. We present some of the  algorithms that were developed under this framework, give an overview of  current applications, and analyze the relationship between ACO and some of  the best known metaheuristics. In addition, we describe recent theoretical  developments in the  eld and we conclude by showing several new trends and  new research directions in this  eld.",
            "group": 160,
            "name": "10.1.1.9.80",
            "keyword": "",
            "title": "A Review on the Ant Colony Optimization Metaheuristic: Basis, Models and New Trends"
        },
        {
            "abstract": "Crosstalk is generally recognized as a major problem in IC design. This paper presents a novel approach to the efficient measurement of the effect of crosstalk on the delay of a net using an algorithm whose worst-case complexity is polynomial-time in the number of nets. The cost of the algorithm is seen to be O(n log n) in practice, where n is the number of nets, and it is amenable to being incorporated into the inner loop of a timing optimizer. To illustrate this, the method is applied to reduce the effects of crosstalk in channel routing, whereitisseen to give an average improvement of 23% in the delay in a channel as compared to the worst case, as measured by SPICE.",
            "group": 161,
            "name": "10.1.1.9.663",
            "keyword": "",
            "title": "A Timing Model Incorporating the Effect of Crosstalk on Delay and its Application to Optimal Channel Routing"
        },
        {
            "abstract": "As System-on-a-Chip (SoC) design enters into mainstream usage, the ability to make post-fabrication changes will become more and more attractive. This ability can be realized using programmable logic cores. These cores are like any other intellectual property (IP) in the SoC design methodology, except that their function can be changed after fabrication. In many cases, non-rectangular programmable logic cores are required, either to better mesh with the other IP cores, or because of I/O constraints. However, most CAD algorithm and programmable logic architecture research targets stand-alone field programmable gate arrays (FPGA's), which are invariably square or rectangular. In this thesis, we enable researchers...",
            "group": 162,
            "name": "10.1.1.9.777",
            "keyword": "",
            "title": "Non-Rectangular Embedded Programmable Logic Cores"
        },
        {
            "abstract": "A standard cell library typically contains several versions of any given gate type, each of which has a di#erent gate size. We consider the problem of choosing optimal gate sizes from the library to minimize a cost function #such as total circuit area# while meeting the timing constraints imposed on the circuit.",
            "group": 163,
            "name": "10.1.1.9.1870",
            "keyword": "",
            "title": "Timing and Area Optimization for Standard-Cell VLSI Circuit Design"
        },
        {
            "abstract": "The application of robots in critical missions in hazardous environments requires the development of reliable or fault tolerant manipulators. In this paper, we define fault tolerance as the ability to continue the performance of a task after immobilization of a joint due to failure. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical design procedure can be used, as is illustrated through an example.  1: Introduction  Consider the use of manipulators in a nuclear environment where equipment has to be repaired or space has to be searched for radioactive contamination. A manipulator that is deployed for this kind of critical missions must have many desirable attributes. It must be versatile. T...",
            "group": 164,
            "name": "10.1.1.9.2212",
            "keyword": "",
            "title": "Mapping Tasks into Fault Tolerant Manipulators"
        },
        {
            "abstract": "A new learning theory derived from the study of the dynamics of an abstract system of masses, moving in a multidimensional space under an external force field, is presented. The set of equations describing system's dynamics may be directly interpreted as a learning algorithm for neural layers. Relevant properties of the proposed learning theory are discussed within the paper, along with results of computer simulations performed in order to assess its effectiveness in applied fields.",
            "group": 165,
            "name": "10.1.1.9.2440",
            "keyword": "",
            "title": "A Theory for Learning Based on Rigid Bodies Dynamics"
        },
        {
            "abstract": "A fundamental assumption inmach5 vision isthx th spatial arrangement of pixels is given. Inch2x5GT thh assumption wehGx utilised a general relationsh thl exists between space and beh52GT Thh relationsh5 presents itself as spatial redundancy,whdu othu researchy, hes considered problematic. We present amathxGT5 model and empirical investigations into tho relationsh and develop analgorith JIGSAW,whSA uses it to build spatial representations.Th phresenta underpinning JIGSAW takes signal behlG27 rathl thh position, as primary. JIGSAW is an unsupervised learning algorith tho is efficient in time and space and thG makes minimal assumptions about its operating domain.Tha algorith offers engineering potential, opportunities in th understanding of biological vision, and a contribution to th wider field of cognitive science. IntroductiN Machine vDvDkDvkDkDvDjjkDjjjDv physical space, despite the technical excellence of real-time v:D3UUU2kDv hardware. EvDvdespite the technical excellence of real-time v:D3UUU2kDv the same problem domain. This is a sobering observj2:Dvtime v:D3UUU2kDv animals do it for themselvD3Uq55qDvbering observj2:Dvtime v:D3UUU2kDv some fundamental assumptions of machine vg observj2:Dvtime v:D3UUU2kDv biological ve it rivptions of m We describe JIGSAW, a new approach to vvg observj2:Dvtime v:D3UUU2kDv to be aggravproblems that seemD:6U:5D324j6:Dvervj2:Dvtime v:D3UUU2kDv implications for biological v2264kDv324j6:Dvervj2:Dvtime v:D3UUU2kDv significant new way of processing vv324j6:Dvervj2 Contemporary image processing and machine vDvervj2:Dvtime v:D3UUU2kDv formulation giv:jkD32j22:Dv and machine 993) in which an image is described by a list of pixels in a notionally orthogonal matrix. In order to read the vD3UUU2kDv position, in either coordinate or list position ...",
            "group": 166,
            "name": "10.1.1.9.3405",
            "keyword": "",
            "title": "Jigsaw:Th Unsupervised Construction of Spatial Representations"
        },
        {
            "abstract": "Logic replication is known to be an effective technique to reduce the number of cut nets in partitioned circuits. A new replication model called functional replication is particularly useful for partitioning technology mapped circuits [7]. Functional replication differs from traditional replication because it considers the functional dependency of the different output signals of a logic cell on its input signals. Functional replication can lead to a higher reduction in the number of cut nets than traditional replication. In this paper, we give the first theoretical treatment of the min-cut partitioning problem with functional replication. We present a novel two-phase algorithm to compute a min-cut bipartition of a technology mapped circuit with functional replication using minimum amount of area overhead. And we show that our algorithm can be applied to improve the solution produced byany area-constrained functional replication partitioning heuristic.",
            "group": 167,
            "name": "10.1.1.9.3925",
            "keyword": "Categories and Subject Descriptors B.7.2 [Integrated CircuitsDesign Aids \ufffd J.6 [Computer- Aided EngineeringCAD General Terms Algorithms Keywords Circuit partitioninglogic replicationminimum cut",
            "title": "Min-Cut Partitioning with Functional Replication for Technology Mapped Circuits Using Minimum Area Overhead"
        },
        {
            "abstract": "Many problems of combinatorial optimization belong to the class of NP-complete problems and can be solved efficiently only by heuristics. Both, Genetic Algorithms and Evolution Strategies have a number of drawbacks that reduce their applicability to that kind of problems. During the last decades plenty of work has been investigated in order to introduce new coding standards and operators especially for Genetic Algorithms. All these approaches have one thing in common: They are very problem specific and mostly they do not challenge the basic principle of Genetic Algorithms. In the present paper we take a different approach and look upon the concepts of a Standard Genetic Algorithm (SGA) as an artificial self organizing process in order to overcome some of the fundamental problems Genetic Algorithms are concerned with in almost all areas of application. With the purpose of providing concepts which make the algorithm more open for scalability on the one hand, and which fight premature convergence on the other hand, this paper presents an extension of the Standard Genetic Algorithm that does not introduce any problem specific knowledge. On the basis of an Evolution-Strategy-like selective pressure handling some further improvements like the introduction of a concept of segregation and reunification of smaller subpopulations during the",
            "group": 168,
            "name": "10.1.1.9.4333",
            "keyword": "",
            "title": "Segregative Genetic Algorithms (SEGA): A Hybrid Superstructure Upwards Compatible to Genetic Algorithms for Retarding Premature Convergence"
        },
        {
            "abstract": "... In this paper, we explore this problem, which we call the network testbed mapping problem. We describe the interesting challenges that characterize it, and explore its application to emulation and other spaces, such as distributed simulation. We present the design, implementation, and evaluation of a solver for this problem, which is in production use on the Netbed shared network testbed. Our solver builds on simulated annealing to find very good solutions in a few seconds for our historical workload, and scales gracefully on large well-connected synthetic topologies",
            "group": 169,
            "name": "10.1.1.9.4816",
            "keyword": "",
            "title": "A Solver for the Network Testbed Mapping Problem"
        },
        {
            "abstract": "With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with softwareexposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute e#ciently across a range of wire-exposed architectures.",
            "group": 170,
            "name": "10.1.1.9.5379",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "... of block placement called sequence pair. All block placement algorithms which are based on sequence pairs use simulated annealing where the generation and evaluation of a large number of sequence pairs is required. Therefore, a fast algorithm is needed to evaluate each generated sequence pair, i.e. to translate the sequence pair to its corresponding block placement. This paper presents a new approach to evaluate a sequence pair based on computing longest common subsequence in a pair of weighted sequences. We present a very simple and   problem. We also show that using a more sophisticated   in [1]. For example, we achieve 60X speedup over the previous algorithm when input size n = 128.",
            "group": 171,
            "name": "10.1.1.9.6207",
            "keyword": "",
            "title": "Fast Evaluation of Sequence Pair in Block Placement by Longest Common Subsequence Computation"
        },
        {
            "abstract": "COMPUTATIONAL ENGINEERING AND DESIGN CENTER  SCHOOL OF ENGINEERING SCIENCES  Doctor of Philosophy  Artificial Intelligence Technologies in Complex Engineering Design  by Yew Soon Ong  Engineering design optimization is an emerging technology whose application both tends to shorten design-cycle time and finds new designs that are not only feasible, but also nearer to optimum, based on specified design criteria. Its gain in attention in the field of complex designs is fuelled by advancing computing power now allowing increasingly accurate analysis codes to be deployed. Unfortunately, the optimization of complex engineering design problems remains a di#cult task, due to the complexity of the cost surfaces and the human expertise necessary in order to achieve high quality results. This research is concerned with the e#ective use of past experiences and chronicled data from previous designs to mitigate some of the limitations of present engineering design optimization process. In particular, the present work leverages well established artificial intelligence technologies and extends recent theoretical and empirical advances, particularly in machine learning, adaptive hybrid evolutionary computation, surrogate modeling, radial basis functions and transductive inference, to mitigate the issues of i) choice of optimization methods and ii) dealing with expensive design problems. The resulting approaches are studied using commonly employed benchmark functions. Further demonstrations on realistic aerodynamic aircraft and ship design problems reveal that the proposed techniques not only generate robust design performance, they can also greatly decrease the cost of design space search and arrive at better designs as compared to conventional approaches.",
            "group": 172,
            "name": "10.1.1.9.6278",
            "keyword": "",
            "title": "Artificial Intelligence Technologies in Complex Engineering Design"
        },
        {
            "abstract": "Today's theories of computing and machine learning developed within a nineteenth-century  mechanistic mindset. Although digital computers would be impossible without quantum physics,  their physical and logical architecture is based on the view of a computer as an automaton  executing pre-programmed sequences of operations exactly as instructed. Recent innovations in  representations and algorithms suggest that a shift in viewpoint may be occurring. In the newly  emerging view, a computer program executes a stochastic process that transforms inputs and  internal state into a sequence of trial solutions, as it evolves toward an improved world model and  better task performance. A full realization of this vision requires a new logic for computing that  incorporates learning from experience as an intrinsic part of the logic, and that permits full  exploitation of the quantum nature of the physical world. Knowledge representation languages  based on graphical probability and decision models have now attained sufficient expressive power  to support general computing applications. At the same time, research is progressing rapidly on  hardware and software architectures for quantum computing. It is hypothesized that a sufficiently  expressive probabilistic logic executing on quantum hardware could perform Bayesian learning  and decision-theoretic reasoning with efficiency far surpassing that of classical computers. Moreover, a",
            "group": 173,
            "name": "10.1.1.9.6874",
            "keyword": "Bayesian networksdecision theorygraphical modelsquantum computingquantum",
            "title": "Quantum Physical Symbol Systems"
        },
        {
            "abstract": "Some apparently powerful algorithms for automatic label placement on maps use heuristics that capture considerable cartographic expertise but are hampered by provably inefficient methods of search and optimization. On the other hand, no approach to label placement that is based on an efficient optimization technique has been applied to the production of general cartographic maps -- those with labeled point, line, and area features -- and shown to generate labelings of acceptable quality. We present an algorithm for label placement that achieves the twin goals of practical efficiency and high labeling qualityby combining simple cartographic heuristics with effective stochastic optimization techniques.",
            "group": 174,
            "name": "10.1.1.9.7029",
            "keyword": "",
            "title": "A General Cartographic Labeling Algorithm"
        },
        {
            "abstract": "Simulated annealing (SA) is a provably convergent optimiser for single-objective (SO) problems. Previously proposed MO extensions have mostly taken the form of an SO SA optimising a composite function of the objectives. We propose an MO SA utilising the relative dominance of a solution as the system energy for optimisation, eliminating problems associated with composite objective functions. We also propose a method for choosing perturbation scalings promoting search both towards and across the Pareto front.",
            "group": 175,
            "name": "10.1.1.9.7706",
            "keyword": "",
            "title": "Dominance Measures for Multi-Objective Simulated Annealing"
        },
        {
            "abstract": "We present a computational approach to a practical problem occurring in the mobile telecommunications industry. Due to changes in cellular technology, there is sometimes the necessity of moving users from one network to another (in this case from TDMA to GSM). In this paper we address the problem of minimizing the number of users a#ected by changes in cellular technology, subject to the constraint that the selected users must be uniformly spread along the covered area. The resulting problem is shown to be NP-hard and a Lagrangian relaxation method is used to find approximate results. We also propose a heuristic algorithm, which is shown to give good results for some instances.",
            "group": 176,
            "name": "10.1.1.9.8742",
            "keyword": "TelecommunicationsCellular TelephonyHeuristicsSimulated AnnealingMassive Data Sets",
            "title": "A Randomized Algorithm for Minimizing User"
        },
        {
            "abstract": "This paper presents the application of Differential Evolution (DE) for the optimal design of shell-and-tube heat  exchangers. The main objective in any heat exchanger design is the estimation of the minimum heat transfer area  required for a given heat duty, as it governs the overall cost of the heat exchanger. Lakhs of configurations are  possible with various design variables such as outer diameter, pitch, and length of the tubes; tube passes; baffle  spacing; baffle cut etc. Hence the design engineer needs an efficient strategy in searching for the global minimum. In   the present study for the first time DE, an improved version of Genetic Algorithms (GAs), has been successfully  applied with different strategies for 1,61,280 design configurations using Bells method to find the heat transfer area.  In the application of DE 9680 combinations of the key parameters are considered. For comparison, GAs are also  applied for the same case study with 1080 combinations of its parameters. For this optimal design problem, it is found   that DE, an exceptionally simple evolution strategy, is significantly faster compared to GA and yields the global  optimum for a wide range of the key parameters.",
            "group": 177,
            "name": "10.1.1.10.1117",
            "keyword": "Genetic AlgorithmsDifferential Evolution",
            "title": "Optimal Design of Shell-and-Tube Heat Exchangers by Different Strategies of Differential Evolution"
        },
        {
            "abstract": "Multimedia data storage is a critical issue in relation to the overall system's performance and functionality. This paper studies a multimedia document application which effectively guides data placement towards improving the quality of presentation of multimedia data. Several storage policies are proposed, towards better response and service times. Multimedia data dependencies, access frequencies and timing constraints are used to guide the storage policies under a certain representation model. The proposed placement policies are based on the simulated annealing algorithm and an extended improved version of the typical simulated annealing approach is presented. Experimentation results are presented and their impact on the total system 's performance is commented and evaluated.",
            "group": 178,
            "name": "10.1.1.10.1190",
            "keyword": "multimedia documents applicationsmultimedia data",
            "title": "Multimedia documents Storage: An Evolutionary based Application"
        },
        {
            "abstract": "This thesis addresses the application of nonlinear optimization to three different  problems in computer graphics: the generation of gait cycles for legged creatures, the  generation of models of truss structures, and the generation of models of constant mean  curvature structures.",
            "group": 179,
            "name": "10.1.1.10.1682",
            "keyword": "",
            "title": "Three Applications of Optimization in Computer Graphics"
        },
        {
            "abstract": "In mobile computing scenarios, context-aware applications are more effective in relieving from the mobile user the burden of introducing information that can be automatically derived from the environment. In particular, the physical position of the mobile system (and hence of the user) is fundamental for many types of applications. User position estimation methods based on strength of the radio signals received from multiple wireless access points have been recently proposed and implemented by several independent research groups. In this paper a new approach to wireless access point placement is proposed. While previous proposals focus on optimal coverage aimed at connectivity, the proposed method integrates coverage requirements with the reduction of the error of the user position estimate. In particular, this paper proposes a mathematical model of user localization error based on the variability of signal strength measurements. This model has been designed to be independent from the actual localization technique, therefore it is only based on generic assumptions on the behavior of the localization algorithm employed. The proposed error model is used by local search heuristic techniques, such as local search, a prohibition-based variation and simulated annealing. Near-optimal access point placements are computed for various kinds of optimization criteria: localization error minimization, signal coverage maximization, a mixture of the two. The different criteria are not expected to be compatible: maximizing signal coverage alone can lead to degradation of the average positioning error, and vice versa. Some experiments have been dedicated to quantify this phenomenon and to introduce possible trade-offs.",
            "group": 180,
            "name": "10.1.1.10.2203",
            "keyword": "",
            "title": "Optimal Wireless Access Point Placement For . . ."
        },
        {
            "abstract": "Simulated annealing has proven to be a good technique for solving hard combinatorial  optimization problems. Some attempts at speeding up annealing algorithms  have been based on shared memory multiprocessor systems. Also parallelizations  for certain problems on distributed memory multiprocessor systems are known.",
            "group": 181,
            "name": "10.1.1.10.3266",
            "keyword": "distributed memory",
            "title": "Problem Independent Distributed Simulated Annealing and Its Applications"
        },
        {
            "abstract": "Designers have no way of establishing, and hence controlling, the likely production  consequences of design decisions during the early stages of new product introduction. Aggregate  process modelling is a newly developed methodology for the identication and manufacturability  assessment of production routeings for partially specied product congurations. The novelty of the  proposed approach lies in the close integration of process models with existing product and  resource information and the provision of feedback regarding manufacturing issues to control  downstream design processes. A description of the methods for early estimation of product  manufacturability and their potential application in a design support system, able to prevent the  progression of design ideas that would be costly, di # cult or even impossible to manufacture, is  presented in this paper.",
            "group": 182,
            "name": "10.1.1.10.4335",
            "keyword": "automated process planningmanufacturability evaluation",
            "title": "Assessing the Manufacturability of Early Product Designs Using Aggregate Process Models"
        },
        {
            "abstract": "Caches have become increasingly important with the widening gap between main memory and processor speeds. Small and fast cache memories are designed to bridge this discrepancy. However, they are only effective when programs exhibit sufficient data locality. Performance of",
            "group": 183,
            "name": "10.1.1.10.4553",
            "keyword": "",
            "title": "Optimizing Program Locality through CMEs and GAs"
        },
        {
            "abstract": "In this paper we describe the application of a so called \"Self-Generating\" Memetic Algorithm to the Maximum Contact Map Overlap problem (MAX-CMO). The maximum overlap of contact maps is emerging as a leading modeling technique to obtain structural alignment among pairs of protein structures. Identifying structural alignments (and hence similarity among proteins) is essential to the correct assessment of the relation between proteins structure and function. A robust methodology for structural comparison could have impact on the process of rational drug design.",
            "group": 184,
            "name": "10.1.1.10.4622",
            "keyword": "",
            "title": "Self Generating Metaheuristics in Bioinformatics: The Proteins Structure Comparison Case"
        },
        {
            "abstract": "This article addresses the problem of defining working and protection paths for Scheduled Lightpath Demands (SLDs) in an optical transport network. An SLD is a demand for a set of lightpaths (connections), defined by a tuple (s, d, n, #, #), where s and d are the source and destination nodes of the lightpaths, n is the number of requested lightpaths and #, # are the set-up and tear-down dates of the lightpaths. The problem is formulated as a combinatorial optimization problem where the objective is to minimize the number of channels required to instantiate the lightpaths. Two techniques are used to achieve this goal: channel reuse and backup-multiplexing. The former consists of assigning the same channel (either working or spare) to several lightpaths, provided that these lightpaths are not simultaneous in time. The latter consists of sharing a spare channel among multiple lightpaths. A spare channel cannot be shared if two conditions hold: a) the working paths of these lightpaths have at least one span in common and b) these lightpaths are simultaneous in time. In the other cases, the spare channel can be shared. We propose a Simulated Annealing (SA) based algorithm to find approximate solutions to this optimization problem since finding exact solutions is computationally intractable. The results show that backup-multiplexing improves the utilization of channels but requires significant computing capacity. Under a fixed computing capacity budget, the technique is useful in cases where there is little time disjointness among SLDs.",
            "group": 185,
            "name": "10.1.1.10.4785",
            "keyword": "",
            "title": "Diverse Routing of Scheduled Lightpath Demands in an Optical Transport Network"
        },
        {
            "abstract": "In this paper, a simulated-annealing-based method called Filter Simulated Annealing  (FSA) method is proposed to deal with the constrained global optimization  problem. The considered problem is reformulated so as to take the form of optimizing  two functions; the objective function and the constraint violation function. Then, the  FSA method is applied to solve the reformulated problem. The FSA method invokes a  multi-start diversification scheme in order to achieve an e#cient exploration process.",
            "group": 186,
            "name": "10.1.1.10.5539",
            "keyword": "Key wordsConstrained global optimizationMetaheuristicsSimulated annealingFilter SetApproximate descent direction",
            "title": "Derivative-Free Filter Simulated Annealing Method for Constrained Continuous Global Optimization"
        },
        {
            "abstract": "Randomized search heuristics like local search, tabu search, simulated  annealing or all kinds of evolutionary algorithms have many  applications. However, for most problems the best worst-case expected  run times are achieved by more problem-specific algorithms. This raises",
            "group": 187,
            "name": "10.1.1.10.5599",
            "keyword": "",
            "title": "Upper and Lower Bounds for Randomized Search Heuristics . . ."
        },
        {
            "abstract": "The game of Go has a high branching factor that defeats the tree  search approach used in computer chess, and long-range spatiotemporal  interactions that make position evaluation extremely  difficult. Development of conventional Go programs is hampered  by their knowledge-intensive nature. We demonstrate a viable  alternative by training networks to evaluate Go positions via temporal  difference (TD) learning.",
            "group": 188,
            "name": "10.1.1.10.5706",
            "keyword": "",
            "title": "Temporal Difference Learning of"
        },
        {
            "abstract": "The vehicle routing problem (VRP) and job shop scheduling  problem (JSP) are two common combinatorial problems that can  be naturally represented as graphs. A core component of solving each  problem can be modeled as finding a minimum cost Hamiltonian path  in a complete weighted graph. The graphs extracted from VRPs and  JSPs have different characteristics however, notably in the ratio of edge  weight to node weight. Our long term research question is to determine  the extent to which such graph characteristics impact the performance of  algorithms commonly applied to VRPs and JSPs. As a preliminary step,  in this paper we investigate five transformations for complete weighted  graphs that preserve the cost of Hamiltonian paths. These transformations  are based on increasing node weights while reducing edge weights  or the inverse. We demonstrate how the transformations affect the ratio  of edge to node weight and how they change the relative weights of edges  at a node. Finally, we conjecture how the different transformations will  impact the performance of existing VRP and JSP solving techniques.",
            "group": 189,
            "name": "10.1.1.10.6300",
            "keyword": "",
            "title": "Graph Transformations for the Vehicle Routing and Job Shop Scheduling Problems"
        },
        {
            "abstract": "The compact and harmonious layout of ads and text is a fundamental and  costly step in the production of commercial telephone directories (\"Yellow  Pages\"). We formulate a canonical version of Yellow-Pages pagination and  layout (YPPL) as an optimization problem in which the task is to position  ads and text-stream segments on sequential pages so as to minimize total page  length and maximize certain layout aesthetics, subject to constraints derived  from page-format requirements and positional relations between ads and text. We present",
            "group": 190,
            "name": "10.1.1.10.6548",
            "keyword": "directory paginationpage layoutheuristic searchstochastic optimizationsimulated",
            "title": "Automatic Yellow-Pages Pagination and Layout"
        },
        {
            "abstract": "In mobile computing scenarios, context-aware applications are more effective in relieving from the mobile user the burden of introducing information that can be automatically derived from the environment. In particular, the physical position of the mobile system (and hence of the user) is fundamental for many types of applications. User position estimation methods based on strength of the radio signals received from multiple wireless access points have been recently proposed and implemented by several independent research groups. In this paper a new approach to wireless access point placement is proposed. While previous proposals focus on optimal coverage aimed at connectivity, the proposed method integrates coverage requirements with the reduction of the error of the user position estimate. In particular, this paper proposes a mathematical model of user localization error based on the variability of signal strength measurements. This model has been designed to be independent from the actual localization technique, therefore it is only based on generic assumptions on the behavior of the localization algorithm employed. The proposed error model is used by local search heuristic techniques, such as local search, a prohibition-based variation and simulated annealing. Near-optimal access point placements are computed for various kinds of optimization criteria: localization error minimization, signal coverage maximization, a mixture of the two. The different criteria are not expected to be compatible: maximizing signal coverage alone can lead to degradation of the average positioning error, and vice versa. Some experiments have been dedicated to quantify this phenomenon and to introduce possible trade-offs. I. ",
            "group": 191,
            "name": "10.1.1.10.6631",
            "keyword": "",
            "title": "Optimal Wireless Access Point Placement for Location-Dependent Services"
        },
        {
            "abstract": "A common framework for 3D image registration consists in minimizing a cost (or energy) function  that expresses the pixel or voxel similarity of the images to be aligned. Standard cost functions, based  on voxel similarity measures, are highly non-linear, non-convex, exhibit many local minima and thus  yield hard optimization problems. Local, deterministic optimization algorithms are known to be very  sensitive to local minima. Global optimization methods (like simulated annealing or evolutionay  algorithms), yield better, often close to the optimal solutions, but are time consuming. In this paper",
            "group": 192,
            "name": "10.1.1.10.7124",
            "keyword": "",
            "title": "Parallelizing Differential Evolution for 3D Medical Image Registration"
        },
        {
            "abstract": "Fuzzy logic and fuzzy set theory provide an important framework for representing and managing imprecision and uncertainty in medical expert systems, but the need remains to optimise such systems to enhance performance. This paper presents a general technique for optimizing fuzzy models in fuzzy expert systems by simulated annealing and N-dimensional hill climbing simplex method. The application of the technique to a fuzzy expert system for the interpretation of the acid-base balance of blood in the umbilical cord of new born infants is presented. The Spearman Rank Order Correlation statistic was used to assess and to compare the performance of a commercially available crisp expert system, an initial fuzzy expert system and a tuned fuzzy expert system with experienced clinicians.",
            "group": 193,
            "name": "10.1.1.10.7580",
            "keyword": "Index Terms \u2014 Fuzzy ModellingSimulated AnnealingSimplex MethodNeonatal OutcomeAcid-base Balance Jonathan M",
            "title": "Application of Simulated Annealing Fuzzy Model Tuning to Umbilical Cord Acid-Base Interpretation"
        },
        {
            "abstract": "Problems of Combinatorial Optimization distinguish themselves by their well-structured problem description as well as by their huge number of possible action alternatives. Especially in the area of production and operational logistics these problems frequently occur. Their advantage lies in their subjective understanding of action alternatives and their objective functions. The use of classical optimization methods for problems of combinatorial optimization often fails because of the exponentially growing computational effort. Therefore, even if they are not able to ensure a global solution, heuristic methods like Genetic Algorithms (GAs) or Evolution Strategies (ESs) are massively utilized in practice because of their significant lower computational effort. Both, GAs and ESs have a number of drawbacks that reduce their applicability to that kind of problems. During the last decades plenty of work has been investigated in order to introduce new coding standards and operators especially for Genetic Algorithms. All these approaches have one thing in common: They are rather problem specific and often they do not challenge the basic principle of Genetic Algorithms. In the present paper we take a different approach and look upon the concepts of a Standard Genetic Algorithm (SGA) as an artificial self organizing process in order to overcome some of the fundamental problems Genetic Algorithms are concerned with in almost all areas of application.\r\nWith the purpose of providing concepts which make the algorithm more open for scalability on the one hand, and which \u00ef\u00ac\u0081ght premature convergence on the other hand, this paper presents an extension of the SGA that does not introduce any problem speci\u00ef\u00ac\u0081c knowledge: On the basis of an\r\nEvolution-Strategy-like selective pressure handling a concept of dynamically dealing with multiple crossover operators in parallel is introduced. In contrast to contributions in the \u00ef\u00ac\u0081eld of Genetic Algorithms that introduce new coding standards and operators for certain problems, the introduced approach should be considered as a novel heuristic appliable to multiple problems of Combinatorial Optimization using exactly the same coding standards and operators for crossover and mutation as done when treating a certain problem with a SGA. Furthermore, the corresponding Genetic Algorithm is unrestrictedly included in all of the newly proposed hybrid variants under especial parameter settings.\r\nIn the present paper the new algorithm is discussed for the Traveling Salesman Problem (TSP) as a well documented instance of a multimodal combinatorial optimization problem. Even if we did not presuppose any information about the quality of the involved operators we were able to achieve results superior to the results obtained with a corresponding Genetic Algorithm for all considered benchmark problems and operators.",
            "group": 194,
            "name": "10.1.1.10.9540",
            "keyword": "",
            "title": "New Variants of Genetic Algorithms Applied to Problems of Combinatorial Optimization"
        },
        {
            "abstract": "A common weakness of local search metaheuristics, such as Simulated Annealing, in solving combinatorial optimisation problems, is the necessity of setting a certain number of parameters. This tends to make significantly increase the total amount of time required to solve the problem and often requires a high level of experience from the user. This paper is motivated by the goal of overcoming this drawback by employing \"parameter-free\" techniques in the context of automatically solving course timetabling problems.",
            "group": 195,
            "name": "10.1.1.11.713",
            "keyword": "Combinatorial optimisationMetaheuristicLocal searchTimetabling",
            "title": "A Time-Predefined Approach to Course Timetabling"
        },
        {
            "abstract": "This paper presents the application of Differential Evolution (DE) for the optimal design of shell-and-tube heat  exchangers. A primary objective in the heat exchanger (HE) design is the estimation of the minimum heat transfer  area required for a given heat duty, as it governs the overall cost of the heat exchanger. However, many number of  discrete combinations of the design variables are possible. Hence the design engineer needs an efficient strategy in  searching for the global minimum heat exchanger cost. In the present study, for the first time DE, an improved  version of Genetic Algorithms (GA), has been successfully applied with 1,61,280 design configurations obtained  by varying the design variables: tube outer diameter, tube pitch, tube length, number of tube passes, baffle spacing   and baffle cut. Bells method is used to find the heat transfer area for a given design configuration. For a case  study taken up, it is observed that DE, an exceptionally simple evolution strategy, is significantly faster compared  to GA and is also much more likely to find a functions true global optimum.  ",
            "group": 196,
            "name": "10.1.1.11.758",
            "keyword": "",
            "title": "Differential Evolution for the Optimal Design of Heat Exchangers"
        },
        {
            "abstract": "In the context of part segmentation from 2D edge images, this paper presents some interesting results with a novel method that addresses the problem of filtering a redundant set of part hypotheses that retains only those that are likely to correspond to actual parts. In the proposed method, supporting evidence for hypotheses are put in competition in a Minimum Description-Length (MDL) framework to select part hypotheses that most economically represent supporting edges in the \"language\" of generic parts. Keywords: Part Segmentation, MDL, Point Distribution Models  Abbreviated Title: 2D Part Segmentation by MDL Note: This paper is an expanded version of the short-listed paper with the same title/authors that will appear at BMVC96.  1 Introduction  The recovery of generic solid parts is a hard fundamental step towards the realization of general-purpose vision systems. A wealth of previous works has been proposed in the past to perform part segmentations that, with a couple of exceptions...",
            "group": 197,
            "name": "10.1.1.11.953",
            "keyword": "Part SegmentationMDLPoint Distribution Models Abbreviated Title2D Part Segmentation by MDL",
            "title": "Part Segmentation from 2D Edge Images by the MDL Criterion"
        },
        {
            "abstract": "The calculation and optimization of product manufacturability during the preliminary stages of design are critical to achieving reduced time to market, high quality and low cost. An aggregate planning method, which translates early product characteristics into manufacturing requirements, forms the basis of a new intelligent support system for which the manufacturing evaluation, optimization and reporting functions are described in this paper. The system `intelligently explores' the many alternative processing  technologies and equipment choices available, seeking solutions that best satisfy a multi-criteria objective function encapsulating quality, cost, delivery and knowledge criteria. The addition of knowledge factors means that both quantitative and qualitative factors can be used to rank alternatives. Importantly, the system prioritizes each element according to its potential for improvement. The designer is thus presented with the opportunity to redefine the design elements or process specifications, which would yield the greatest improvements in production.",
            "group": 198,
            "name": "10.1.1.11.2508",
            "keyword": "",
            "title": "Manufacturability Analysis of Early Product Designs"
        },
        {
            "abstract": " ",
            "group": 199,
            "name": "10.1.1.11.3273",
            "keyword": "",
            "title": "Criticality and parallelism in structured SAT instances"
        },
        {
            "abstract": "In this work we introduce a multiagent architecture conceived as a conceptual and practical framework for metaheuristic algorithms (MAGMA, MultiAGent Metaheuristics Architecture). Metaheuristics can be seen as the result of the interaction among di erent kinds of agents: level 0 agents constructing initial solutions, level-1 agents improving solutions and level-2 agents providing the high level strategy. In this framework, classical metaheuristic algorithms can be smoothly accommodated and extended, and new algorithms can be easily designed by defining which agents are involved and their interactions. Furthermore, with the introduction of a fourth level of agents, coordinating lower level agents, MAGMA can also describe, in a uniform way, cooperative search and, in general, any combination of metaheuristics. We propose",
            "group": 200,
            "name": "10.1.1.11.3439",
            "keyword": "Metaheuristicsagentscombinatorial optimization Contents",
            "title": "MAGMA: A Multiagent Architecture for Metaheuristics"
        },
        {
            "abstract": "The complex and dynamic feature of the Internet requires scalable and effective network control. In this paper, a collaborative on-line simulation scheme is proposed to provide the automated and pro-active control functions for networks. This scheme introduces autonomous on-line simulators into local networks, which continuously monitor the surrounding network conditions, collect the relevant information, communicate with other simulators and execute collaborative on-line simulation. Based on the simulation results, the on-line simulators keep tuning the network parameters to the better operation point to fit the current network conditions. In this paper, we describe the basic concepts and investigate the solutions to the challenges faced in the realization of this scheme, particularly in the areas of network modeling, on-line simulation and parameter search. We also discuss the applicability of this scheme, and present the simulation results under ns and the test results of a preliminary implementation on a real network.",
            "group": 201,
            "name": "10.1.1.11.3464",
            "keyword": "",
            "title": "Network Management and Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": "Black-box optimization algorithms cannot use the specific parameters of the problem instance, i.e., of the fitness function f. Their run time is measured as the number of f-evaluations. This implies that the usual algorithmic complexity of a problem cannot be used in the black-box scenario. Therefore, a new framework for the valuation of algorithms for black-box optimization is presented allowing the notion of the black-box complexity of a problem. For several problems upper and lower bounds on their black-box complexity are presented. Moreover, it can can be concluded that randomized search heuristics whose (worst-case) expected optimization time for some problem is close to the black-box complexity of the problem are provably efficient (in the black-box scenario). The new approach is applied to several problems based on typical example functions and further interesting problems. Run times of general EAs for these problems are compared with the black-box complexity of the problem.",
            "group": 202,
            "name": "10.1.1.11.6047",
            "keyword": "",
            "title": "A New Framework for the Valuation of Algorithms for  Black-Box-Optimization"
        },
        {
            "abstract": "Iterative algorithms for numerical optimization in continuous spaces typically  need to adapt their step lengths in the course of the search. While some  strategies employ fixed schedules for reducing the step lengths over time,  others attempt to adapt interactively in response to either the outcome of trial  steps or to the history of the search process. Evolutionary algorithms are of  the latter kind. One of the control strategies that is commonly used in evolution  strategies is the cumulative step length adaptation approach. This paper  presents a first theoretical analysis of that adaptation strategy by considering  the algorithm as a dynamical system. The analysis includes the practically  relevant case of noise interfering in the optimization process. Recommendations  are made with respect to the problem of choosing appropriate popula-  tion sizes.",
            "group": 203,
            "name": "10.1.1.11.6098",
            "keyword": "",
            "title": "Evolutionary Optimization with Cumulative Step Length Adaptation: A Performance Analysis"
        },
        {
            "abstract": "We extend in this paper the concept of the P-admissible floorplan representation to that of the P*-admissible one. A P*-admissible representation can model the most general floorplans. Each of the currently existing P*-admissible representations, SP, BSG, and TCG, has its strengths as well as weaknesses. We show the equivalence of the two most promising P*-admissible representations, TCG and SP, and integrate TCG with a packing sequence (part of SP) into a new representation, called TCGS. TCG-S combines the advantages of SP and TCG and at the same time eliminates their disadvantages. With the property of SP, faster packing and perturbation schemes are possible. Inherited nice properties from TCG, the geometric relations among modules are transparent to TCGS (implying faster convergence to a desired solution), placement with position constraints becomes much easier, and incremental update for cost evaluation can be realized. These nice properties make TCG-S a superior representation which exhibits an elegant solution structure to facilitate the search for a desired floorplan/placement. Extensive experiments show that TCG-S results in the best area utilization, wirelength optimization, convergence speed, and stability among existing works and is very flexible in handling placement with special constraints.",
            "group": 204,
            "name": "10.1.1.11.8727",
            "keyword": "",
            "title": "TCG-S: Orthogonal Coupling of P*-admissible  Representations for General Floorplans"
        },
        {
            "abstract": "For most computationally intractable problems there exists no simple heuristic which consistently outperforms all other heuristics. One remedy is to bundle simple heuristics into composite ones in a fixed and predetermined way (Barman 1997). Adaptive control schemes take this approach one step further by dynamically combining algorithms. Several such algorithms have been proposed recently in various settings, yet an experimental investigation comparing them to  other contemporary methods has been lacking. We aim to close this gap by a comprehensive  computational study on the field of resource-constrained project scheduling. Also we show how  to improve effectiveness of the best algorithm by means of randomized sampling. Finally, we  expose several advantages of adaptive control schemes over other algorithms which facilitate  the OR practitioner's task of designing good algorithms for newly arising problems.",
            "group": 205,
            "name": "10.1.1.11.8966",
            "keyword": "",
            "title": "Resource-Constrained Project Scheduling: An Evaluation of Adaptive Control Schemes for Parameterized Sampling Heuristics"
        },
        {
            "abstract": "We present in this paper an efficient, flexible, and effective data structure,  B*-trees, for non-slicing floorplans. B*-trees are based on ordered binary trees and the admissible placement presented in [1]. Inheriting from the nice properties of ordered binary trees, B*-trees are very easy for implementation and can perform the respective primitive tree operations search, insertion, and deletion in only      , and  78  times while existing representations for non-slicing floorplans need at least   6  time for each of these operations, where    is the number of modules. The correspondence between an admissible placement and its induced B*-tree is 1-to-1 (i.e., no redundancy); further, the transformation between them takes only linear time. Unlike other representations for non-slicing floorplans that need to construct constraint graphs for cost evaluation, in particular, the evaluation can be performed on B*- trees and their corresponding placements directly and incrementally. We further show the flexibility of B*-trees by exploring how to handle rotated, pre-placed, soft, and rectilinear modules. Experimental results on MCNC benchmarks show that the B*-tree representation runs about 4.5 times faster, consumes about 60% less memory, and results in smaller silicon area than the O-tree one [1]. We also develop a B*-tree based simulated annealing scheme for floorplan design; the scheme achieves near optimum area utilization even for rectilinear modules.",
            "group": 206,
            "name": "10.1.1.11.9430",
            "keyword": "",
            "title": "B*-Trees: A New Representation for Non-Slicing Floorplans"
        },
        {
            "abstract": "Thresholding is an important topic for image processing, pattern recognition and computer vision. Selecting thresholds is a critical issue for many applications. The fuzzy set theory has been successfully applied to many areas, such as control, image processing, pattern recognition, computer vision, medicine, social science, etc. It is generally believed that image processing bears some fuzziness in nature. In this paper, we use the concept of fuzzy c-partition and the maximum fuzzy entropy principle to select threshold values for gray-level images. We have conducted experiments on many images. The experimental results demonstrate that the proposed approach can select the thresholds automatically and e#ectively, and the resulting images can preserve the main features of the components of the original images very well. # 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved  Thresholding Fuzzy logic Fuzzy c-partition Maximum entropy principle  Simulated annealing  1. ",
            "group": 207,
            "name": "10.1.1.11.9656",
            "keyword": "",
            "title": "Threshold Selection Based on Fuzzy c-Partition Entropy Approach"
        },
        {
            "abstract": "In this work, we will develop probabilistic classifiers for two challenging and diverse natural language processing (NLP) tasks using a common set of techniques. One classifier will be capable of disambiguating a large vocabulary of words with respect to full sets of sense distinctions from published sources, e.g., Longman's on-line dictionary. The second will perform a discourse processing task that involves segmentation, reference resolution, and belief: segmenting a text into blocks that express the beliefs and opinions of a single agent, and identifying noun phrases that refer to that agent. Both systems will be fully automatic. Exploiting recent",
            "group": 208,
            "name": "10.1.1.11.9667",
            "keyword": "",
            "title": "Methods of Category Classification Applied to Word-Sense Disambiguation and Discourse Analysis"
        },
        {
            "abstract": "This article proposes a new population-based stochastic search algorithm called Evolutionary Diffusion Optimization (EDO) inspired by diffusion in nature. Each entity in EDO makes the decision to diffuse based on the information shared between its parent and its siblings. The behavior of EDO when solving a typical optimization problem is also discussed.",
            "group": 209,
            "name": "10.1.1.12.101",
            "keyword": "",
            "title": "Evolutionary Diffusion Optimization, Part I: Description of the Algorithm"
        },
        {
            "abstract": "While most of the stable learning algorithms perform well on domains with relevant information, they degrade in the presence of irrelevant or redundant information. Selective or focused learning presents a solution to this problem. Two components of selective learning are selective attention (feature selection) and selective utilization (example selection).",
            "group": 210,
            "name": "10.1.1.12.1010",
            "keyword": "Feature SelectionExample SelectionNaive Bayes learnerNearest NeighborsClass Imbalance",
            "title": "Enhancing Learning using Feature and Example Selection"
        },
        {
            "abstract": "A new population-based stochastic search algorithm called Evolutionary Diusion Optimization (EDO) inspired by diffusion in nature has been proposed [10]. This article compares the performance of EDO with simulated annealing and fast evolutionary programming. Experimental results show that EDO performs better than SA and FEP in some cases.",
            "group": 211,
            "name": "10.1.1.12.1171",
            "keyword": "",
            "title": "Evolutionary Diffusion Optimization, Part II: Performance Assessment"
        },
        {
            "abstract": "The need for improvement in process operations, logistics and supply chain management has created a great demand for the development of optimization models for planning and scheduling. In this paper we first review the major classes of planning and scheduling models that arise in process operations, and establish the underlying mathematical structure of these problems. As will be shown, the nature of these models is greatly affected by the time representation (discrete or continuous), and is often dominated by discrete decisions. We then briefly review the major recent developments in mixed-integer linear and nonlinear programming, disjunctive programming and constraint programming, as well as general decomposition techniques for solving these problems. We present a general formulation for integrating planning and scheduling to illustrate the models and methods discussed in this paper.",
            "group": 212,
            "name": "10.1.1.12.1826",
            "keyword": "",
            "title": "Discrete Optimization Methods and their Role in the Integration of Planning and Scheduling"
        },
        {
            "abstract": "... tool support for the development of parallel and distributed embedded real-time system software. The presented approach comprises the complete design flow from the modeling of a distributed controller system by means of a high--level graphical language down to the synthesis of executable code for a given target hardware, whereby the implementation is verified to meet hard real-time constraints. The methodology is mainly based upon the tools SEA (System Engineering and Animation) and CHaRy (The  C-LAB Hard Real-Time System).",
            "group": 213,
            "name": "10.1.1.12.2149",
            "keyword": "C\u2013LAB Hard Real\u2013Time System",
            "title": "From High-Level Specifications down to Software Implementations of Parallel Embedded Real-Time Systems  "
        },
        {
            "abstract": "Simulated Annealing (SA) is a powerful stochastic search algorithm applicable to a wide range of problems. This paper presents some experiments of applying SA to an NP-hard problem, i.e., call routing in circuit-switched telecommunications networks. The call routing problem considered here can be described as assigning calls to paths in a network so that the number of calls blocked can be minimised. Previously, the problem has been approached by linear programming and some heuristic algorithms. However, none of these algorithms has solved the problem satisfactorily due to the nonlinear nature of the problem. Linear programming can only find a rough approximation to the actual global optimal solution. This paper applies SA to the call routing problem in circuit-switched telecommunication networks. We have carried out two sets of experiments. The first set investigates random selection of paths in SA with uniform distribution. The second set studies non-uniform probabilistic selection of...",
            "group": 214,
            "name": "10.1.1.12.3311",
            "keyword": "",
            "title": "Call Routing by Simulated Annealing"
        },
        {
            "abstract": "Local Search (LS) and Evolutionary Algorithms  (EA) are probabilistic search algorithms, widely  used in global optimization, where selection is important  as it drives the search. In this paper, we  introduce acceptance, a metric measuring the selective  pressure in LS and EA, that is the tradeo  # between exploration and exploitation. Informally,  acceptance is the proportion of accepted  non-improving transitions in a selection.",
            "group": 215,
            "name": "10.1.1.12.3930",
            "keyword": "",
            "title": "Acceptance Driven Local Search and Evolutionary Algorithms"
        },
        {
            "abstract": "Recent studies (Alizadeh et al. 2000, Bittner et al. 2000, Golub et al. 1999) demonstrate the  discovery of disease subtypes from gene expression data. In this paper, we propose a principled  and systematic approach to address the computational problem of partitioning the set of  sample tissues into statistically meaningful classes. We start by describing a method, called  overabundance analysis, for assessing how informative a given expression data set is with respect  to a partition of the samples. As we show, in several published expression datasets, an  overabundance of genes separating known classes is observed. Then, we use this method as  the foundation to a novel approach to class discovery. In this approach, we search for partitions  that have statistically significant overabundance score. We evaluate the performance of  our approach on synthetic data, where we show it can recover planted partitions. Finally, we  apply it to several published tumor expression datasets, and show that we find several highly  pronounced partitions.",
            "group": 216,
            "name": "10.1.1.12.4680",
            "keyword": "",
            "title": "Overabundance Analysis and Class Discovery in Gene Expression Data"
        },
        {
            "abstract": "Motivation: The pairwise alignment of biological sequences obtained from an algorithm will in general contain both correct and incorrect parts. Hence, to allow for a valid interpretation of the alignment, the local trustworthiness of the alignment has to be quantified. Results: ",
            "group": 217,
            "name": "10.1.1.12.5813",
            "keyword": "",
            "title": "A Novel Approach to Local Reliability of Sequence Alignments  "
        },
        {
            "abstract": "Testing is a major phase in the software life cycle. One of the most important problems testing raises is the automatic generation of input cases to be applied to the software under test. An alternative for tackling this problem is evolutionary testing, a eld which aims at creating software test cases using combinatorial optimization search methods.",
            "group": 218,
            "name": "10.1.1.12.6190",
            "keyword": "evolutionary testingestimation of distribution algorithms",
            "title": "Dealing with Software Testing via Estimation of Distribution Algorithms: a preliminary research"
        },
        {
            "abstract": "This paper reviews several problems that arise in the area of design automation. Most of these problems are shown to be NP-hard. Further, it is unlikely that any of these problems can be solved by fast approximation algorithms that guarantee solutions that are always within some fixed relative error of the optimal solution value. This points out the importance of heuristics and other tools to obtain algorithms that perform well on the problem instances of interest.",
            "group": 219,
            "name": "10.1.1.12.6452",
            "keyword": "complexityNP-hardapproximation algorithm",
            "title": "The Complexity of Design Automation Problems"
        },
        {
            "abstract": "FACULTY OF ENGINEERING  DEPARTMENT OF ELECTRONICS AND COMPUTER SCIENCE  Doctor of Philosophy  Power Minimisation Techniques for Testing Low Power VLSI Circuits  by Nicola Nicolici  Testing low power very large scale integrated (VLSI) circuits has recently become an area of concern due to yield and reliability problems. This dissertation focuses on minimising power dissipation during test application at logic level and register-transfer level (RTL) of abstraction of the VLSI design flow.",
            "group": 220,
            "name": "10.1.1.12.6652",
            "keyword": "",
            "title": "Power Minimisation Techniques for Testing Low Power VLSI Circuits"
        },
        {
            "abstract": "One of the most important issues in software testing is the generation of the input cases used  during the test. Due to the expensive cost of this task its automatization has become a key aspect.",
            "group": 221,
            "name": "10.1.1.12.7097",
            "keyword": "of distribution algorithms",
            "title": "On the performance of Estimation of Distribution Algorithms applied to Software Testing"
        },
        {
            "abstract": "The effectiveness of the memory hierarchy is critical for the performance of current processors. The performance of the memory hierarchy can be improved by means of program transformations such as loop tiling, which is a code transformation targeted to reduce capacity misses. This paper presents a novel systematic approach to perform nearoptimal loop tiling based on an accurate data locality analysis (Cache Miss Equations) and a powerful technique to search the solution space that is based on a genetic algorithm. The results show that this approach can remove practically all capacity misses for all considered benchmarks. The reduction of replacement misses results in a decrease of the miss ratio that can be as significant as a factor of 7 for the matrix multiply kernel.",
            "group": 222,
            "name": "10.1.1.12.9220",
            "keyword": "",
            "title": "Near-Optimal Loop Tiling by means of Cache Miss Equations and Genetic Algorithms"
        },
        {
            "abstract": "module packing using the Transitive Closure Graph (TCG) representation. The geometric meanings of modules are transparent to TCG and its induced operations, which makes TCG an ideal representation for floorplanning /placement with arbitrary rectilinear modules. We first partition a rectilinear module into a set of submodules and then derive necessary and sufficient conditions of feasible TCG for the submodules. Unlike most previous works that process each submodule individually and thus need post processing to fix deformed rectilinear modules, our algorithm treats a set of submodules as a whole and thus not only can guarantee the feasibility of each perturbed solution but also can eliminate the need of the post processing on deformed modules, implying better solution quality and running time. Experimental results show that our TCG-based algorithm is capable of handling very complex instances; further, it is very efficient and results in better area utilization than previous work.",
            "group": 223,
            "name": "10.1.1.12.9508",
            "keyword": "",
            "title": "Arbitrary Convex and Concave Rectilinear Module Packing Using TCG"
        },
        {
            "abstract": "Most shotgun sequencing projects undergo a long and costly phase of finishing, in which a partial assembly forms several contigs whose order, orientation and relative distance is unknown. We propose here a new technique that supplements the shotgun assembly data by experimentally simple and commonly used complete restriction digests of the target. By computationally combining information from the contig sequences and the fragment sizes measured for several different enzymes, we seek to form a \"scaffold\" on which the contigs will be placed in their correct orientation, order and distance. We give a heuristic search algorithm for solving the problem and report on promising preliminary simulation results. The key to the success of the search scheme is the very rapid solution of two time-critical subproblems that are solved to optimality in linear time.",
            "group": 224,
            "name": "10.1.1.13.360",
            "keyword": "",
            "title": "The Restriction Scaffold Problem"
        },
        {
            "abstract": "This article presents a novel Chinese class n-gram model for contextual postprocessing of handwriting recognition results. The word classcs in the modcl are automatically discovered by a corpus-based simn latcd annealing procedure. Three other language models, least-word, wordfreqnency, and the powerlift interword character bigram model, have been constrncted for comparison. Extensive cxperiments on large text corpora show that the discovered class bigram model outperforms the other three competing modcls",
            "group": 225,
            "name": "10.1.1.13.1394",
            "keyword": "",
            "title": "Word Class Discovery For Postprocessing"
        },
        {
            "abstract": "this paper we propose a new methodology of coupled local minimizers (CLM) for solving continuous nonlinear optimization problems. We pose a somewhat similar challenge as for committee networks but within a di#erent and broader context of solving di#erentiable optimization problems. The aim is to (online) combine the results from local optimizers in order to let the ensemble generate a local minimum that is better than the best result obtained from all individual local minimizers. We show how improved local minima can be obtained by having interaction and information exchange between the local search processes. This is realized through state synchronization constraints that are imposed between the local minimizers by incorporating principles of master--slave dynamics. Synchronization theory has been intensively studied within the area of chaotic systems and secure communications [Chen & Dong, 1998; Pecora & Carroll, 1990; Suykens et al., 1996, 1997, 1998; Wu & Chua, 1994]. The CLM method is related to Lagrange programming network approaches for chaos synchronization [Suykens & Vandewalle, 2000], where identical or generalized synchronization constraints are imposed on dynamical systems. CLMs also fit within the framework of Cellular Neural Networks (CNN) [Chua & Roska, 1993; Chua et al., 1995; Chua, 1998]. By considering the objective of minimizing the average cost of an ensemble of local  minimizers subject to pairwise synchronization constraints, a continuous-time optimization algorithm is studied according to Lagrange programming networks [Cichocki & Unbehauen, 1994; Zhang & Constantinides, 1992]. The resulting continuoustime optimization algorithm is described by an array of coupled nonlinear cells or a one-dimensional CNN with bidirectional coupling",
            "group": 226,
            "name": "10.1.1.13.1737",
            "keyword": "",
            "title": "Intelligence and Cooperative Search by Coupled Local Minimizers"
        },
        {
            "abstract": "We describe a linear-time algorithm that recovers absolute camera orientations and positions,  along with uncertainty estimates, for networks of terrestrial image nodes spanning hundreds of meters  in outdoor urban scenes. The algorithm produces pose estimates globally consistent to roughly 0.1 # (2  milliradians) and 5 centimeters on average, or about four pixels of epipolar alignment.",
            "group": 227,
            "name": "10.1.1.13.2226",
            "keyword": "Exterior orientationegomotionstructure from motionpanoramas",
            "title": "Scalable Extrinsic Calibration of Omni-Directional Image Networks"
        },
        {
            "abstract": "Developing grid applications is a challenging endeavor, which at the moment requires both extensive labor and expertise. The Grid Application Development Software Project (GRADS) provides a system to simplify grid application development. This system incorporates tools at all stages of the application development and execution cycle. In this chapter we focus on application scheduling, and present the three scheduling approaches developed in GRADS: development of an initial application schedule (launch-time scheduling), modification of the execution platform during execution (rescheduling), and negotiation between multiple applications in the system (metascheduling). These approaches have been developed and evaluated for platforms that consist of distributed networks of shared workstations, and applied to real-world parallel applications.",
            "group": 228,
            "name": "10.1.1.13.3210",
            "keyword": "scheduling rescheduling metascheduling grid data-parallel",
            "title": "Scheduling In The Grid Application Development Software Project"
        },
        {
            "abstract": "Recent studies (Alizadeh et al. 2000, Bittner et al. 2000, Golub et al. 1999) demonstrate the  discovery of disease subtypes from gene expression data. In this paper, we propose a principled  and systematic approach to address the computational problem of partitioning the set of  sample tissues into statistically meaningful classes. We start by describing a method, called  overabundance analysis, for assessing how informative a given expression data set is with respect  to a partition of the samples. As we show, in several published expression datasets, an  overabundance of genes separating known classes is observed. Then, we use this method as  the foundation to a novel approach to class discovery. In this approach, we search for partitions  that have statistically significant overabundance score. We evaluate the performance of  our approach on synthetic data, where we show it can recover planted partitions. Finally, we  apply it to several published tumor expression datasets, and show that we find several highly  pronounced partitions.",
            "group": 229,
            "name": "10.1.1.13.3618",
            "keyword": "",
            "title": "Overabundance Analysis and Class Discovery in Gene Expression Data"
        },
        {
            "abstract": "of block placement called sequence pair. All block placement algorithms which are based on sequence pairs use simulated annealing where the generation and evaluation of a large number of sequence pairs is required. Therefore, a fast algorithm is needed to evaluate each generated sequence pair, i.e. to translate the sequence pair to its corresponding block placement. This paper presents a new approach to evaluate a sequence pair based on computing longest common subsequence in a pair of weighted sequences. We present a very simple and   problem. We also show that using a more sophisticated   in [1]. For example, we achieve 60X speedup over the previous algorithm when input size n # ###.",
            "group": 230,
            "name": "10.1.1.13.3955",
            "keyword": "",
            "title": "Fast Evaluation of Sequence Pair in Block Placement by Longest Common Subsequence Computation"
        },
        {
            "abstract": "A fundamental assumption in machine vision is that the spatial arrangement of pixels is given. In challenging this assumption we have utilised a general relationship that exists between space and behaviour. This relationship presents itself as spatial redundancy, which other researchers have considered problematic. We present a mathematical model and empirical investigations into this relationship and develop an algorithm, JIGSAW, which uses it to build spatial representations. The philosophy underpinning JIGSAW takes signal behaviour, rather than position, as primary. JIGSAW is an unsupervised learning algorithm that is efficient in time and space and that makes minimal assumptions about its operating domain. This algorithm offers engineering potential, opportunities in the understanding of biological vision, and a contribution to the wider field of cognitive science. 1 ",
            "group": 231,
            "name": "10.1.1.13.4639",
            "keyword": "",
            "title": "Jigsaw: The Unsupervised Construction of Spatial Representations"
        },
        {
            "abstract": "We consider a median pyramidal transform for denoising applications. Traditional techniques of pyramidal denoising are similar to those in wavelet-based methods. In order to remove noise, they use the thresholding of transform coefficients. We propose to model the structure of the transform coefficients as a Markov random field. The goal of modeling transform coefficients is to retain significant coefficients on each scale and to discard the rest. Estimation of the transform coefficient structure is obtained via a Markov chain sampler. A technique is proposed to estimate the parameters of the field's distribution. The advantage of our method is that we are able to utilize the interactions between transform coefficients, both within each scale and among the scales, which leads to denoising improvement as demonstrated by numerical simulations.",
            "group": 232,
            "name": "10.1.1.13.5041",
            "keyword": "median pyramidal transformMarkov random fieldadditive modeldenoising",
            "title": "Markov Random Field Modeling in Median Pyramidal Transform Domain for Denoising Applications"
        },
        {
            "abstract": "In this paper, the placement problem on FPGAs is faced using Thermodynamic Combinatorial Optimization (TCO). TCO is a new combinatorial optimization method based on both Thermodynamics and Information Theory. In TCO two kinds of processes are considered: microstate and macrostate transformations. Applying the Shannon's definition of Entropy to microstate reversible transformations, a probability of acceptance based on Fermi-Dirac statistics is derived. On the other hand, applying thermodynamic laws to reversible macrostate transformations, an efficient annealing schedule is provided. TCO has been compared with Simulated Annealing (SA) on a set of benchmark circuits for the FPGA placement problem. TCO has achieved large time reductions with respect to SA, while providing interesting adaptive properties.",
            "group": 233,
            "name": "10.1.1.13.5193",
            "keyword": "",
            "title": "FPGA Placement by Thermodynamic Combinatorial Optimization"
        },
        {
            "abstract": "This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.",
            "group": 234,
            "name": "10.1.1.13.7133",
            "keyword": "",
            "title": "An Introduction to MCMC for Machine Learning"
        },
        {
            "abstract": "Simulated Annealing (SA) is a powerful stochastic search method applicable to a wide range of problems for which little prior knowledge is available. It can produce very high quality solutions for hard combinatorial optimization problems. However, the computation time required by SA is very large. Various methods have been proposed to reduce the computation time, but they mainly deal with the careful tuning of SA's control parameters. This paper first analyzes the impact of SA's neighbourhood on SA's performance and shows that SA with a larger neighbourhood is better than SA with a smaller one. The paper also gives a general model of SA, which has both dynamic generation probability and acceptance probability, and proves its convergence. All variants of SA can be unified under such a generalization. Finally, a method of extending SA's neighbourhood is proposed, which uses a discrete approximation to some continuous probability function as the generation function in SA, and several impo...",
            "group": 235,
            "name": "10.1.1.13.7847",
            "keyword": "Stochastic SearchAlgorithm AnalysisCombinatorial Optimization",
            "title": "Simulated Annealing with Extended Neighbourhood"
        },
        {
            "abstract": "During the product design process, the optimization of the parameters (dimensions, shapes, materials) has been, until recently, the exclusive domain of the part design stage. This optimization is now spreading into two directions: first, to the conceptual design stage during which approximated mathematical models are solved to find the global parameters driving the design; second to the sub-assembly and assembly design stage during which the components are united to form the final product. It is only at this stage that system level characteristics, strongly linked to the desire of the customers, can be checked. These characteristics, considered as objectives by the engineer, and reflecting different knowledge fields, link parameters at various levels of the system and thus break the well established hierarchical organization of the full system. At present, no satisfactory solution is implemented to optimize these additional relations that transform the assembly design from a constraint satisfaction problem into a non-linear constrained multi objective optimization problem.",
            "group": 236,
            "name": "10.1.1.13.7992",
            "keyword": "",
            "title": "Configuration Design Optimization Method"
        },
        {
            "abstract": "Multiplicative weight-update algorithms such as Winnow and Weighted Majority have  been studied extensively due to their on-line mistake bounds' logarithmic dependence on N ,  the total number of inputs, which allows them to be applied to problems where N is exponential.",
            "group": 237,
            "name": "10.1.1.13.8111",
            "keyword": "1",
            "title": "On Approximating Weighted Sums with Exponentially Many Terms"
        },
        {
            "abstract": "this paper, we introduce an algorithm that attempts to produce aesthetically-pleasing, two-dimensional pictures of graphs by doing simplified simulations of physical systems. We are concerned with drawing undirected graphs according to some generally accepted aesthetic criteria:  1. Distribute the vertices evenly in the frame. 2. Minimize edge crossings. 3. Make edge lengths uniform. 4. Reflect inherent symmetry. 5. Conform to the frame. Our algorithm does not explicitly strive for these goals, but does well at distributing vertices evenly, making edge lengths uniform, and reflecting symmetry. Our goals for the implementation are speed and simplicity. PREVIOUS WORK Our algorithm for drawing undirected graphs is based on the work of Eades   which, in turn, evolved from a VLSI technique called force-directed placement",
            "group": 238,
            "name": "10.1.1.13.8444",
            "keyword": "",
            "title": "Graph Drawing by Force-directed Placement"
        },
        {
            "abstract": " ",
            "group": 239,
            "name": "10.1.1.13.9336",
            "keyword": "",
            "title": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies"
        },
        {
            "abstract": "We describe two systems: GATE (General Architecture for Text Engineering), an architecture to aid in the production and delivery of language engineering systems which significantly reduces development time and ease of reuse in such systems. We also describe a sense tagger which we implemented within the GATE architecture, and which achieves high accuracy (92% of all words in text to a broad semantic level). We used the implementation of the sense tagger as a real-world task on which to evaluate the usefulness of the GATE architecture and identified strengths and weaknesses in the architecture.",
            "group": 240,
            "name": "10.1.1.14.595",
            "keyword": "",
            "title": "Implementing a Sense Tagger in a General Architecture for Text Engineering"
        },
        {
            "abstract": "this paper describes our approach from the system point of view. The programming interface is described in detail in the next section, following  which the design and salient implementation aspects are discussed. Representative results from a few simulation systems are then reported, and the concluding section discusses some of the critical issues in such an approach, the implications for applications other than stochastic simulation, and ongoing and future work",
            "group": 241,
            "name": "10.1.1.14.782",
            "keyword": "",
            "title": "EcliPSe: A System for High Performance Concurrent Simulation"
        },
        {
            "abstract": "Integer and combinatorial optimization problems constitute a major challenge for algorithmics. They arise when a large number of discrete organizational decisions have to be made, subject to constraints and optimization criteria. This thesis",
            "group": 242,
            "name": "10.1.1.14.1430",
            "keyword": "",
            "title": "Domain-Independent Local Search For Linear Integer Optimization"
        },
        {
            "abstract": "In this paper, we explore the algorithmic problems which are induced by the extension of the CSP  framework, for dealing with preferences between solutions and with over-constrained problems. We show that the optimization induced problem is incredibly more difficult than the classical satisfaction problem, and that the current algorithmic methods are particularly inefficient. We try to give reasons for such a situation, and to draw up an inventory of possible solutions.",
            "group": 243,
            "name": "10.1.1.14.1833",
            "keyword": "",
            "title": "Algorithmic problems and solutions in the Valued Constraint Satisfaction Problem framework"
        },
        {
            "abstract": "We present an algorithm to recover three-dimensional shape, i.e., surface orientation and relative depth from a single segmented image. It is assumed that the scene is composed of opaque regular solid objects bounded by piecewlse smooth surfaces with no markings or texture. Also it is assumed that the reflectance map R(n) is known. For the canonical case of Lambertlan surfaces illuminated by a point light source, this implies knowing the light source direction. Solutions for simplified versions of this problem have been proposed by Sugihara for the case of polyhedra, and by Horn et al. for the case of a single smooth surface patch given the surface orientation on the patch bonndary. This work presents the first solution for plecewise smooth surfaces. A variational h}rmulation of line drawing and shading constraints in a common framework is developed. Finding the 3-D shape which best satisfies these constraints is a difficult global optimization prob- lem. The problem is made tractable by precomputing line labels at junctions using the algorithm developed by Malik. The global constraints are partitioned into constraint sets corresponding to the faces, edges, and vertices in the scene. For a face, the constraints are given by Horn's image irradiance eqnation. We develop a variational formulation of the constraints at an edge--bnth from the known direction of the image curve corresponding to the edge and the shading. The associated Euler-l,agrange differential equation completely captures the local information. At a vertex, the constraints are modeled by a system of nonlinear equations. An algorithm has been developed to solve this system of constraints. We present experimental results demonstrating the feasibility of the approach.",
            "group": 244,
            "name": "10.1.1.14.2549",
            "keyword": "",
            "title": "Recovering Three-Dimensional Shape from a Image of Curved Objects"
        },
        {
            "abstract": "A clustering and ordination algorithm suitable for mining extremely large databases, including those produced by microarray expression studies, is described and analyzed for stability. Data from a yeast cell cycle experiment with 6000 genes and 18 experimental measurements per gene are used to test this algorithm under practical conditions. The process of assigning database objects to an X, Y coordinate, ordination, is shown to be stable with respect to random starting conditions, and with respect to minor perturbations in the starting similarity estimates. Careful analysis of the way clusters typically co-locate, versus the occasional large displacements under different starting conditions are shown to be useful in interpreting the data. This extra stability information is lost when only a single cluster is reported, which is currently the accepted practice. However, it is believed that the approaches presented here should become a standard part of best practices in analyzing computer clustering of large data collections.",
            "group": 245,
            "name": "10.1.1.14.2764",
            "keyword": "",
            "title": "Cluster Stability and the Use of Noise in Interpretation of Clustering"
        },
        {
            "abstract": "Constraint Violation Minimization Problems arise when dealing with  over-constrained CSPs. Unfortunately, experiments and practice show that they  quickly become too large and too difficult to be optimally solved. In this context,  multiple methods (limited tree search, heuristic or stochastic local search) are  available to produce non-optimal, but good quality solutions, and thus to provide  the user with anytime upper bounds of the problem optimum. On the other hand,  few methods are available to produce anytime lower bounds of this optimum.",
            "group": 246,
            "name": "10.1.1.14.3598",
            "keyword": "",
            "title": "Anytime Lower Bounds for Constraint Violation Minimization Problems"
        },
        {
            "abstract": "We present a computational approach to a practical problem occurring in the mobile telecommunications industry. Due to changes in cellular technology, there is sometimes the necessity of moving users from one network to another (in this case from TDMA to GSM). In this paper we address the problem of minimizing the number of users affected by changes in cellular technology, subject to the constraint that the selected users must be uniformly spread along the covered area. The resulting problem is shown to be NP-hard and a Lagrangian relaxation method is used to find approximate results. We also propose a heuristic algorithm, which is shown to give good results for some instances.",
            "group": 247,
            "name": "10.1.1.14.3761",
            "keyword": "TelecommunicationsCellular TelephonyHeuristicsSimulated AnnealingMassive Data Sets",
            "title": "A Randomized Algorithm for Minimizing User Disturbance Due to Changes in Cellular Technology"
        },
        {
            "abstract": "We present a force-based simulated annealing algorithm to heuristically  solve the NP-hard label number maximization problem LNM: Given a set of rectangular  labels, each of which belongs to a point-feature in the plane, the task is to find  a labeling for a largest subset of the labels. A labeling is a placement such that none  of the labels overlap and each is placed at its point-feature. The",
            "group": 248,
            "name": "10.1.1.14.4391",
            "keyword": "",
            "title": "Force-Based Label Number Maximation"
        },
        {
            "abstract": "One of the challenges of financial research is to develop models that are capable of explaining and forecasting market price movements and returns.Agent based models focus directly on the underlying structure of the market. The basic idea is, that the market price dynamics arises from the interaction of many individual agents. Approaching financial markets in this manner, one starts off with the modeling of the agents\u00b4 decision making schemes on the microeconomic level of the market. Thereafter, market price changes can be determined on the macroeconomic level by a superposition of the agents\u00b4 buying and selling decisions. The aim of a (micro-)economic model is to explain market prices by a detailed causal analysis of the agents\u00b4 decision making behavior. The market price results from an aggregation of the agents\u00b4 decisions. Remarkably, agent-based financial markets provide a new explanatory framework supplementing the traditional economic concepts of equilibrium theory and efficient markets. Such a supplementing framework is needed, because in real-world financial markets the underlying assumptions of equilibrium or efficient market theory are often violated.As we will show, neural networks allow the integration of the decision behavior of individual economic agents into a market model. Based on the perspective of interacting agents, the resulting market model allows us to capture the underlying dynamics of financial markets, to fit real-world financial data, and to forecast future market price movements.In addition, we point out that neural networks allow to set up a joint framework of econometric model building. Besides the learning from data, one may integrate prior knowledge about the underlying dynamical system and first principles into the modeling. These elements are incorporated into the neural networks in form of architectural enhancements. This way of model building helps to overcome the drawbacks of purely data driven approaches.",
            "group": 249,
            "name": "10.1.1.14.4777",
            "keyword": "",
            "title": "Multi-Agent Market Modeling Based On Neural Networks"
        },
        {
            "abstract": "This paper provides a comprehensive survey of the most popular constraint-handling techniques currently used with evolutionary algorithms. We review approaches that go from simple variations of a penalty function, to others, more sophisticated, that are biologically inspired on emulations of the immune system, culture or ant colonies. Besides describing briefly each of these approaches (or groups of techniques), we provide some criticism regarding their highlights and drawbacks. A small comparative study is also conducted, in order to assess the performance of several penalty-based approaches with respect to a dominance-based technique proposed by the author, and with respect to some mathematical programming approaches. Finally, we provide some guidelines regarding how to select the most appropriate constraint-handling technique for a certain application, ad we conclude with some of the the most promising paths of future research in this area.",
            "group": 250,
            "name": "10.1.1.14.4812",
            "keyword": "evolutionary algorithmsconstraint handlingevolutionary optimization",
            "title": "Theoretical and Numerical Constraint-Handling Techniques used with Evolutionary Algorithms: A Survey of the State of the Art"
        },
        {
            "abstract": "this paper, the authors modify the multilevel algorithms to optimize a cost function based on the aspect ratio. Several variants of the algorithms are tested and shown to provide excellent results",
            "group": 251,
            "name": "10.1.1.14.5487",
            "keyword": "",
            "title": "Computing Applications Mesh Partitioning For Domain Shape Multilevel Mesh Partitioning For Optimizing Domain Shape"
        },
        {
            "abstract": "This paper proposes that the process of language understanding can be modeled as a collective phenomenon that emerges from a myriad of microscopic and diverse activities. The process is analogous to the crystallization process in chemistry. The essential features of this model are: asynchronous parallelism; temperature-controlled randomness; and statistically emergent active symbols. A computer program that tests this model on the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences is presented. The program adopts a holistic approach in which word identification forms an integral component of sentence analysis. Various types of knowledge, from statistics to linguistics, are seamlessly integrated for the tasks of word boundary disambiguation as well as sentential analysis. Our experimental results showed that the model is able to address the word boundary ambiguity problems effectively.  ",
            "group": 252,
            "name": "10.1.1.14.5930",
            "keyword": "",
            "title": "A Statistically Emergent Approach For Language Processing: Application to . . . "
        },
        {
            "abstract": " Genetic algorithms (GAs) have been extensively used as a means for performing global optimization in a simple yet reliable manner. However, in some realistic engineering design optimization domains a general purpose GA is often inefficient and unable to reach the global optimum. In this thesis we describe a GA for continuous designspace optimization that uses new GA operators and strategies tailored to the structure and properties of engineering design domains. Empirical results in several realistic engineering design domains as well as benchmark design domains demonstrate that using our system can greatly decrease the cost of design space search, and can also improve the quality of the resulting designs.",
            "group": 253,
            "name": "10.1.1.14.7503",
            "keyword": "",
            "title": "GADO: A Genetic Algorithm For Continuous Design Optimization"
        },
        {
            "abstract": "In this paper we describe a physics-based method for synthesis of bird flight animations. Our method computes a realistic set of wingbeats that enables a bird to follow the specified trajectory. We model the bird as an articulated skeleton with elastically deformable feathers. The bird motion is created by applying joint torques and aerodynamic forces over time in a forward dynamics simulation. We solve for each wingbeat motion separately by optimizing for wingbeat parameters that create the most natural motion. The final animation is constructed by concatenating a series of optimal wingbeats. This detailed bird flight model enables us to produce flight motions of different birds performing a variety of maneuvers including taking off, cruising, rapidly descending, turning, and landing.",
            "group": 254,
            "name": "10.1.1.14.7689",
            "keyword": "CR CategoriesI.3.7 [Computer GraphicsThree-Dimensional Graphics and Realism\u2014AnimationI.6.8 [Simulation and ModelingTypes of Simulation\u2014Animation Keywordscomputer animationbird flightphysically based animationforward",
            "title": "Realistic Modeling of Bird Flight Animations"
        },
        {
            "abstract": "The problem of radio frequency assignment is to provide communication channels from limited spectral resources whilst keeping to a minimum the interference suffered by those whishing to communicate in a given radio communication network. This problem is a combinatorial (NP-hard) optimization problem. In 1993, the CELAR (the French \"Centre d'Electronique de l'Armement\") built a suite of simplified versions of Radio Link Frequency Assignment Problems (RLFAP) starting from data on a real network (Roisnel 93). Initially designed for assessing the performances of several Constraint Logic Programming languages, these benchmarks have been made available to the public in the framework of the European EUCLID project CALMA (Combinatorial Algorithms for Military Applications).",
            "group": 255,
            "name": "10.1.1.14.8545",
            "keyword": "Benchmarksradio link frequency assignmentconstraint satisfactionoptimization",
            "title": "Radio Link Frequency Assignment"
        },
        {
            "abstract": "We have developed two go programs, Olga and Oleg, using a Monte Carlo approach, simpler than Bruegmann's [Bruegmann, 1993], and based on [Abramson, 1990]. We have set up experiments to assess ideas such as progressive pruning, transpositions, temperature, simulated annealing and depth-two tree search within the Monte Carlo framework. We have shown that progressive pruning alone gives better results than Bruegmann's Monte Carlo go, which uses transpositions, temperature and simulated annealing. Nevertheless, transpositions and temperature are good speed up enhancements that do not lower the level of the program too much. Moreover, the results of our Monte Carlo programs against knowledgebased programs on 9x9 boards and the ever-increasing power of computers lead us to think that Monte Carlo approaches are worth considering for computer go in the future.",
            "group": 256,
            "name": "10.1.1.15.452",
            "keyword": "",
            "title": "Developments On Monte Carlo Go"
        },
        {
            "abstract": "this paper \"ergodic\" is used in a very weak sense, as it is not proposed, theoretically or practically, that all states of the system are actually to be visited",
            "group": 257,
            "name": "10.1.1.15.1046",
            "keyword": "",
            "title": "Simulated annealing: Practice versus theory"
        },
        {
            "abstract": "Recent results in the application of... this paper. The review takes the form of an analysis of the problems presented by different application requirements and characteristics. Issues covered include uniprocessor and multiprocessor systems, periodic and aperiodic processes, static and dynamic algorithms, transient overloads and resource usage. Protocols that limit and reduce blocking are discussed. Considerations are also given to scheduling Ada tasks.",
            "group": 258,
            "name": "10.1.1.15.1118",
            "keyword": "",
            "title": "Scheduling Hard Real-Time Systems: A Review"
        },
        {
            "abstract": "This paper describes a methodology for designing in-  terconnected LAN-MAN networks with the objective  of minimizing the average network delay. We consider  IEEE 802.3-5 LANs interconnected by transparent  bridges. These bridges are required to form a  spanning tree topology. The optimization algorithm  for finding a minimum delay spanning tree topology  is based on simulated annealing. In order to measure  the quality of the solutions, we find a lower bound  for the average network delay. The comparison of our  results with this lower bound and several other goodness  measures show that the solutions are not very far  from the global minimum. We extend our algorithm  for finding minimum delay LAN-MAN topologies consisting  of FDDI MANs or Switched Multi-Megabit  Data Service (SMDS) interconnecting several clusters  of bridged LANs.",
            "group": 259,
            "name": "10.1.1.15.1283",
            "keyword": "",
            "title": "Topological Design Of Interconnected Lan-Man Networks*"
        },
        {
            "abstract": "Crosstalk is generally recognized as a major problem in IC design. This paper presents a novel approach to the efficient measurement of the effect of crosstalk on the delay of a net using an algorithm whose worst-case complexity is polynomial-time in the number of nets. The cost of the algorithm is seen to be O#nlogn# in practice, where n is the number of nets, and it is amenable to being incorporated into the inner loop of a timing optimizer. To illustrate this, the method is applied to reduce the effects of crosstalk in channel routing, where it is seen to give an average improvement of 23% in the delay in a channel as compared to the worst case, as measured by SPICE.",
            "group": 260,
            "name": "10.1.1.15.1739",
            "keyword": "",
            "title": "Capturing the Effect of Crosstalk on Delay"
        },
        {
            "abstract": "This paper describes an evolutionary neural network approach to Hang Seng stock index forecast. In this approach, a feedforward neural network is evolved using an evolutionary programming algorithm. Both the weights and architectures (i.e., connectivity of the network) are evolved in the same evolutionary process. The network may grow as well as shrink. The experimental results show that the evolutionary neural network approach can produce very compact neural networks with good prediction.",
            "group": 261,
            "name": "10.1.1.15.1853",
            "keyword": "",
            "title": "Evolving Neural Networks for Hang Seng Stock Index Forecast"
        },
        {
            "abstract": "Adaptive simulated annealing (ASA) is a global optimization algorithm based on an  associated proof that the parameter space can be sampled much more efficiently than by  using other previous simulated annealing algorithms. The author's ASA code has been  publicly available for over two years. During this time the author has volunteered to help  people via e-mail, and the feedback obtained has been used to further develop the code.",
            "group": 262,
            "name": "10.1.1.15.2777",
            "keyword": "",
            "title": "Adaptive simulated annealing (ASA): Lessons learned"
        },
        {
            "abstract": "We consider the planning problem for a mobile manipulator system that must perform a sequence of tasks defined by position, orientation, force, and moment vectors at the end-effector. Each task can be performed in mnltiple configurations dne to the redundancy introduced by mobility. We formulate the planning problem as an optimization problem in which the decision variables for mobility (base position) are separated from the manipulator join1 angles in the cost function. The resnlfing numerical problem is nonlinear with nonconvex, unconnected feasible regions in the decision space. Simulated annealing is proposed as a general solution method for obtaining near-optimal results. The problem formulation and numerical solution by simulated annealing are illustrated for a manipulator system with three degrees of freedom monnted on a base with two degrees of freedom. These results are compared with results obtained by conventional nonlinear programming techniques customized for the particular example system.",
            "group": 263,
            "name": "10.1.1.15.3326",
            "keyword": "",
            "title": "Path Planning for Mobile Manipulators for"
        },
        {
            "abstract": "This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for blackbox  optimization problems. Specifically, this algorithm is designed for the dynamical parameter optimization  of network protocols which emphasizes on obtaining good solutions within a limited time frame  rather than full optimization. The RRS algorithm is based on the initial high-efficiency property of random  sampling and attempts to maintain this high-efficiency by constantly \"restarting\" random sampling  with adjusted sample spaces. Due to its basis on random sampling, the RRS algorithm is robust to the  effect of random noises in the objective function and it performs especially efficiently when handling  the objective functions with negligible parameters. These properties have been demonstrated with the  tests on a suite of benchmark functions. The RRS algorithm has been successfully applied to the optimal  configuration of several network protocols. One application to a network routing algorithm is presented.",
            "group": 264,
            "name": "10.1.1.15.3418",
            "keyword": "",
            "title": "A Recursive Random Search Algorithm for Black-box Optimization"
        },
        {
            "abstract": "In this paper we introduce a rather straightforward but fundamental observation concerning the convergence of the general iteration process a?-} -1 a7 k -- O(xk)B(xk)--l v f (x k) for minimizing a function f(x). We give necessary and sufficient conditions for a stationary point of f(x) to be a point of strong attraction of the iteration process. We will discuss various ramifications of this fundamental result particularly for nonlinear least squares problems.",
            "group": 265,
            "name": "10.1.1.15.3914",
            "keyword": "YIN ZHANG tRICHARD TAPIAAND LETICIA VELAZQUEZ",
            "title": "On Convergence of Minimization Methods: Attraction Repulsion and Selection"
        },
        {
            "abstract": "Motivated by the success of genetic algorithms and simulated annealing in hard optimization problems, the authors propose a new Markov chain Monte Carlo (MCMC) algorithm so called an evolutionary Monte Carlo algorithm. This algorithm has incorporated several attractive features of genetic algorithms and simulated annealing into the framework of MCMC. It works by simulating a population of Markov chains in parallel, where each chain is  attached to a different temperature. The population is updated by mutation (Metropolis update), crossover (partial state swapping) and exchange operators (full state swapping). The algorithm is illustrated through examples of the Cp-based model selection and change-point identification. The numerical results and the extensive comparisons show that evolutionary  Monte Carlo is a promising approach for simulation and optimization.",
            "group": 266,
            "name": "10.1.1.15.4052",
            "keyword": "Change-point identificationCrossoverExchangeGenetic algorithmMarkov Chain Monte CarloMetropolis algorithmMutationParallel temperingRegression variable selectionSimulated annealing",
            "title": "Evolutionary Monte Carlo: Applications to C_p Model Sampling and Change Point Problem"
        },
        {
            "abstract": "Evolutionary algorithms (EAs) are a class of stochastic search algorithms applicable to a wide range of problems in learning and optimisation. They have been applied to numerous problems in combinatorial optimisation, function optimisation, artificial neural network learning, fuzzy logic system learning, etc. This paper first introduces EAs and their basic operators. Then an overview of three major branches of EAs, i.e., genetic algorithms (GAs), evolutionary programming(EP) and evolution strategies (ESs) is given. Different search operators and selection mechanisms are described. The emphasis of all the discussions is on global optimisation by EAs. The paper also presents three simple models for parallel EAs. Finally, some open issues and future research directions in evolutionary optimisation and evolutionary computation in general are discussed. 1 Introduction  Evolutionary algorithms (EAs) are a class of general stochastic search algorithms applicable to problems for which little a...",
            "group": 267,
            "name": "10.1.1.15.5185",
            "keyword": "",
            "title": "Global Optimisation by Evolutionary Algorithms"
        },
        {
            "abstract": "This paper contributes to this methodology by presenting an improvement over previous algorithms. Sections II and III give a short outline of previous Boltzmann annealing (BA) and fast Cauchy fast annealing (FA) algorithms. Section IV presents the new very fast algorithm. Section V enhances this algorithm with a re-annealing modification found to be extremely useful for multi-dimensional parameter-spaces. This method will be referred to here as very fast reannealing (VFR)",
            "group": 268,
            "name": "10.1.1.15.5347",
            "keyword": "",
            "title": "Very Fast Simulated Re-Annealing"
        },
        {
            "abstract": "We propose a new evolutionary method of extracting user preferences from examples shown to an automatic graph layout system. Using stochastic methods such as simulated annealing and genetic algorithms, automatic layout systems can find a good layout using an evaluation function which can calculate how good a given layout is. However, the evaluation function is usually not known beforehand, and it might vary from user to user. In our system, users show the system several pairs of good and bad layout examples, and the system infers the evaluation function from the examples using genetic programming technique. After the evaluation function evolves to reflect the preferences of the user, it is used as a general evaluation function for laying out graphs. The same technique can be used for a wide range of adaptive user interface systems.",
            "group": 269,
            "name": "10.1.1.15.6667",
            "keyword": "Graphic Object LayoutGraph LayoutGenetic AlgorithmsGenetic ProgrammingProgramming by ExampleAdaptive User Interface",
            "title": "Evolutionary Learning of Graph Layout Constraints from Examples"
        },
        {
            "abstract": "Statistical modeling of sequences is a central paradigm of machine learning that # nds multiple uses in computational molecular biology and many other domains. The probabilistic  automata typically built in these contexts are subtended by uniform, # xed-memory Markov models. In practice, such automata tend to be unnecessarily bulky and computationally imposing both during their synthesis and use. Recently, D. Ron, Y. Singer, and N. Tishby built much more compact, tree-shaped variants of probabilistic automata under the assumption of an underlying Markov process of variable memory length. These variants, called Probabilistic  Suf# x Trees (PSTs) were subsequently adapted by G. Bejerano and G. Yona and  applied successfully to learning and prediction of protein families. The process of learning the automaton from a given training set of sequences requires    worst-case time, where is the total length of the sequences in and is the length of a longest substring of to be considered for a candidate state in the automaton. Once the automaton is built, predicting the likelihood of a query sequence of characters may cost time    in the worst case. The main contribution of this paper is to introduce automata equivalent to PSTs but having the following properties:  Learning the automaton, for any , takes time.",
            "group": 270,
            "name": "10.1.1.15.7314",
            "keyword": "",
            "title": "Optimal Amnesic Probabilistic Automata or How to Learn and Classify Proteins in Linear Time and Space"
        },
        {
            "abstract": "This article describes a simple and e#ective algorithm for constructing mixed-level orthogonal and nearly orthogonal arrays. It can construct a variety of small-run designs with good statistical properties e#ciently. KEY WORDS: D-optimality; Exchange algorithm; Interchange algorithm; J 2 -optimality. Consider an experiment to screen factors that may influence the blood glucose readings of a clinical laboratory testing device. One two-level factor and eight three-level factors are included in the experiments. The nine factors (Wu and Hamada 2000, Table 7.3) are (A) wash (no or yes), (B) microvial volume (2.0, 2.5, or 3.0 ml), (C) caras H 2 O level (20, 28, or 35 ml), (D) centrifuge RPM (2100, 2300, 2500), (E) centrifuge time (1.75, 3, 4.5 min), (F) sensitivity (0.10, 0.25, 0.50), (G) temperature (25, 30, 37   C), (H) dilution ratio (1:51, 1:101, 1:151), and (I) absorption (2.5, 2, 1.5). To ensure that all the main e#ects are estimated clearly from each other, it is desirable to use an orthogonal array (OA). The smallest OA found for one two-level factor and eight three-level factors requires 36 runs. However, the scientist wants to reduce the cost of this experiment and plans to use an 18-run design. A good solution then is to use an 18-run nearly orthogonal array  (NOA). The concept of OA dates back to Rao (1947). OAs have been used widely in manufacturing and high-technology industries for quality and productivity improvement experiments as evidenced by many industrial case studies and recent design textbooks (Myers and Montgomery 1995; Wu and Hamada 2000). Applications of NOAs can be found in Wang and Wu (1992), Nguyen (1996b) and the references therein. Formally, an orthogonal array (OA) (of strength two), denoted by OA(N, s 1     s n ), is an N   matrix of which the...",
            "group": 271,
            "name": "10.1.1.15.8104",
            "keyword": "",
            "title": "An Algorithm for Constructing Orthogonal and Nearly Orthogonal Arrays with Mixed Levels and Small Runs"
        },
        {
            "abstract": "In this paper, we consider approximating global minima of zero or small residual,  nonlinear least-squares problems. We propose a selective search approach based on  the concept of selective minimization recently introduced in Zhang et al [16]. To test  the viability of the proposed approach, we construct a simple implementation using a  Levenberg-Marquardt type method combined with a multi-start scheme, and compare it  with several existing global optimization techniques. Numerical experiments were performed  on zero residual nonlinear least-squares problems chosen from structural biology  applications and from the literature. On the problems of significant sizes, the performance  of the new approach compared favorably with other tested methods, indicating  that the new approach is promising for the intended class of problems.",
            "group": 272,
            "name": "10.1.1.15.9065",
            "keyword": "Global minimizationzero or small residual least-squares problemsselective minimizationLevenberg-Marquardt methodmulti-start",
            "title": "Selective Search for Global Optimization of Zero or Small Residual Least-Squares Problems: A Numerical Study"
        },
        {
            "abstract": "Sensing to recognize and locate objects is a critical need for robotic operations in unstructured environments. An accurate 3-D model of objects in the scene is necessary for efficient high level control of robots. Drawing upon concepts from supervisory control, we have developed an interactive system for creating object models from range data, based on simulated annealing. Site modeling is a task that is typically performed using purely manual or autonomous techniques, each of which has inherent strengths and weaknesses. However, an interactive modeling system combines the advantages of both manual and autonomous methods, to create a system that has high operator productivity as well as high flexibility and robustness. Our system is unique in that it can work with very sparse range data, tolerate occlusions, and tolerate cluttered scenes. We have performed an informal evaluation with four operators on 16 different scenes, and have shown that the interactive system is superior to either manual or automatic methods in terms of task time and accuracy.",
            "group": 273,
            "name": "10.1.1.15.9387",
            "keyword": "",
            "title": "An Interactive System for Creating Object Models from Range Data based on Simulated Annealing"
        },
        {
            "abstract": "Local search is one of the fundamental approaches to tackle large combinatorial optimization problems. Yet relatively little support is available to facilitate the design and implementation of local search algorithms. This paper introduces Localizer , an extensible object-oriented library for local search. Localizer supports both declarative abstractions to describe the neighborhood and high-level search constructs to specify local moves and meta-heuristics. It also supports a variety of features typically found only in modeling languages and its extensibility allows for an easy integration of new, user-defined, abstractions. Of particular interest is the conciseness and readability of Localizer statements and the efficiency of the Localizer implementation.",
            "group": 274,
            "name": "10.1.1.15.9425",
            "keyword": "",
            "title": "Localizer++: An Open Library for Local Search"
        },
        {
            "abstract": "We present a novel approach to model inter-processor communication in multi-DSP systems. In most multi-DSP systems, inter-processor communication is realized by transferring data over point-to-point links with hardware FIFO buffers. Direct memory access (DMA) is additionally used to concurrently transfer data to the FIFO buffers and perform computation. Our model accounts for the limited size of the communication buffers as well as concurrent DMA transfer.",
            "group": 275,
            "name": "10.1.1.15.9614",
            "keyword": "communication modelmapping and schedulingmulti-DSPrapid prototyping",
            "title": "A New Approach to Model Communication for Mapping and Scheduling DSP-Applications"
        },
        {
            "abstract": "This article presents and evaluates the Slack Method, a new constructive heuristic for  the allocation (mapping) of periodic hard real-time tasks to multiprocessor or distributed  systems. The Slack Method is based on task deadlines, in contrast with other constructive  heuristics, such as List Processing. The presented evaluation shows that the Slack Method  is superior to list-processing-based approaches with regard to both finding more feasible  solutions as well as finding solutions with better objective function values.",
            "group": 276,
            "name": "10.1.1.15.9905",
            "keyword": "",
            "title": "The Slack Method: A New Method for Static Allocation of Hard Real-Time Tasks"
        },
        {
            "abstract": "Local Search (LS) and Evolutionary Algorithms  (EA) are probabilistic search algorithms, widely  used in global optimization, where selection is important  as it drives the search. In this paper, we  introduce acceptance, a metric measuring the selective  pressure in LS and EA, that is the tradeoff  between exploration and exploitation. Informally,  acceptance is the proportion of accepted  non-improving transitions in a selection.",
            "group": 277,
            "name": "10.1.1.16.200",
            "keyword": "",
            "title": "Acceptance Driven Local Search and Evolutionary Algorithms"
        },
        {
            "abstract": "We propose a novel methodology for designing fault-tolerant real-time system to achieve optimal productivity on a single-chip multiprocessor platform using the heterogeneous builtin -self-repair(BISR) based graceful degradation and yield enhancement technique as an embedded optimization engine which exploits task-level scheduling and algorithm selection flexibility. Wealsodeveloped a hardware fault model for modern superscalar processors and multi-processors which enables an efficient treatment of the synthesis and compilation goals.",
            "group": 278,
            "name": "10.1.1.16.417",
            "keyword": "",
            "title": "Heterogeneous BISR-approach using System Level Synthesis Flexibility"
        },
        {
            "abstract": "The need for design space exploration on different levels of abstraction during synthesis of electronic systems has received wide attention recently. Unfortunately, there are almost no tools available on the EDA market that allow a designer to enhance his synthesis tool suite by design space exploration capabilities easily. A versatile tool for design space exploration must be targetable to di erent synthesis tools. Also, different optimization (exploration) algorithms should be able to be connected to such a versatile tool. Here, we present an approach that enables design space exploration with support to couple different exploration algorithms and synthesis tools. Our JAVA based tool called EXPLORA is also able to visualize the exploration results and can be adapted to new problems and abstraction levels within hours.",
            "group": 279,
            "name": "10.1.1.16.418",
            "keyword": "",
            "title": "EXPLORA - Generic Design Space Exploration During Embedded System Synthesis"
        },
        {
            "abstract": "We propose a new optimization paradigm for solving intractable combinatorial problems. The technique, named probabilistic constructive, combines the advantages of both constructive and probabilistic algorithms. The constructive aspect provides relatively short runtime and makes the technique amenable for the inclusion of insights through heuristic rules. The probabilistic nature facilitates a flexible trade-off between runtime and the quality of solution.",
            "group": 280,
            "name": "10.1.1.16.763",
            "keyword": "",
            "title": "A Probabilistic Constructive Approach To Optimization Problems"
        },
        {
            "abstract": "This paper describes a new technique for minimising power dissipation  in full scan sequential circuits during test application. The  technique increases the correlation between successive states during  shifting in test vectors and shifting out test responses by reducing spurious  transitions during test application. The reduction is achieved by  freezing the primary input part of the test vector until the smallest  transition count is obtained which leads to lower power dissipation. This paper",
            "group": 281,
            "name": "10.1.1.16.1226",
            "keyword": "",
            "title": "Minimisation of Power Dissipation During Test Application in Full Scan Sequential Circuits Using Primary Input Freezing"
        },
        {
            "abstract": "Reduction of the number of operations optimizes the important design metrics such as area, cost, throughput, and power consumption for both custom ASIC and programmable processor implementations. We propose a novel technique to minimize the number of operations in DSP computations. The first step of the approach logically partitions a computation into strongly connected components. The second step optimizes each component separately. In the third step the components are merged to further optimize. Finally, the components are scheduled to minimize memory consumption. The effectiveness of our approach is  demonstrated on real-life examples.",
            "group": 282,
            "name": "10.1.1.16.1277",
            "keyword": "",
            "title": "Minimizing the number of  operations in DSP computations"
        },
        {
            "abstract": "In the last several years, system and integrated circuits (IC) semiconductor industry and research has started refocusing from the general purpose computing platform toward application specific devices and appliances. This shift, compounded with the exponentially growing gap between IC potential and design productivity imposes an urgent need for new design methodologies and technologies. There are four main phases in development of application specific systems (ASS): algorithm, architecture, implementation, and semiconductor realization. The last phase is mainly related to the technology CAD field and is out of main scope of the research presented in this paper.",
            "group": 283,
            "name": "10.1.1.16.1315",
            "keyword": "",
            "title": "Probabilistic Control Search Strategies For Hardware And Software Optimization During Solution Space Exploration"
        },
        {
            "abstract": "In this thesis we explore some industrial pattern sequencing problems arising in settings as distinct as the scheduling of flexible machines, the design of VLSI circuits, and the sequencing of cutting patterns. This latter setting presents us the minimization of open stacks problem, which is the main focus of our study. Some complexity results are presented for these sequencing problems, establishing a surprising connection between previously unrelated fields. New local search methods are also presented to deal with these problems, and their effectiveness is evaluated by comparisons with results previously obtained in the literature. The first method is derived from the simulated annealing algorithm, bringing new ideas from statistical physics. The second method advances these ideas, by proposing a collective search model based on two themes: (i) to explore the search space while simultaneously exhibiting search intensity and search diversity, and (ii) to explore the search space in proportion to the perceived quality of each region. Some preliminaries, given by coordination policies (to guide the search processes) and distance metrics, are introduced to support the model.",
            "group": 284,
            "name": "10.1.1.16.1359",
            "keyword": "",
            "title": "Industrial Pattern Sequencing Problems: some complexity results and new local search models"
        },
        {
            "abstract": "This article presents an in-depth survey of CAD methodologies and techniques for designing low power digital CMOS circuits and systems and describes the many issues facing designers at architectural, logical, and physical levels of design abstraction. It reviews some of the techniques and tools that have been proposed to overcome these difficulties and outlines the future challenges that must be met to design low power, high performance systems",
            "group": 285,
            "name": "10.1.1.16.1697",
            "keyword": "Categories and Subject DescriptorsJ.6 [Computer ApplicationsComputer-Aided Engineering General TermsDesignExperimentationPerformance Additional Key Words and PhrasesComputer-aided design of VLSICMOS circuitssystem designsynthesislayoutpower analysis and estimation",
            "title": "Power Minimization in IC Design: Principles and Applications"
        },
        {
            "abstract": "We propose a novel methodology for designing fault-tolerant real-time multi-processor systems-on-a-chip to achieve optimal productivity. The methodology employs the heterogeneous built-in-self-repair(BISR) based graceful degradation and yield enhancement technique as an embedded optimization engine. The technique exploits the flexibility provided in task-level scheduling and algorithm selection steps. We have developed a hardware fault model for modern superscalar processors and multi-processors which enables an efficient treatment of the synthesis and compilation goals. For the first time heterogeneous BISR is used at the task level. The key idea is to adapt scheduling and algorithm selection to the available non-faulty resources. If there is a fault in memory, the algorithms that use less memory are selected and the scheduler exploits the other abundant resource, namely the processors more vigorously to compensate for the loss of part of memory. Similarly, fault in a processor is backed up by memory. The synthesis approach minimizes the degradation in performance for single or multiple faults using simulated annealing-based algorithm selection, scheduling and assignment algorithms. On the large set of examples our adaptive algorithm selection and scheduling technique has achieved significant improvement of throughput compared to conventional nonadaptive schemes. The extensive experimental results also indicate that significant improvement in productivity can be achieved by utilizing the extra throughput gained from the technique.",
            "group": 286,
            "name": "10.1.1.16.2305",
            "keyword": "",
            "title": "Heterogeneous BISR-approach using System Level Synthesis Flexibility"
        },
        {
            "abstract": "AppART is an Adaptive Resonance Theory low parameterized neural model that incrementally approximates continuous-valued multidimensional functions from noisy data using biologically plausible processes. AppART performs a higher-order Nadaraya-Watson regression and can be interpreted as a fuzzy logic Standard Additive Model. In this work we describe AppART dynamics and training. We also discuss the approach it makes to hybrid neural systems and deal with its theoretical foundations as a function approximation method. Three benchmark problems are solved in order to study AppART from an application point of view and to compare its results with the ones obtained from other models. Finally two modi  cations to the AppART formulation aimed at improving AppART eciency are proposed and tested.",
            "group": 287,
            "name": "10.1.1.16.2745",
            "keyword": "",
            "title": "AppART: A hybrid neural network based on Adaptive Resonance Theory for universal function approximation"
        },
        {
            "abstract": "Some statistical methods which have been shown to have direct neural network analogs are surveyed here; we discuss sampling, optimization, and representation methods which make them feasible when applied in conjunction with, or in place of, neural networks. We present the foremost of these, the Gibbs sampler, both in its successful role as a convergence heuristic derived from statistical physics and under its probabilistic learning interpretation. We then review various manifestations of Gibbs sampling in Bayesian learning; its relation to \"traditional\" simulated annealing; specializations and instances such as EM; and its application as a model construction technique for the Bayesian network formalism. Next, we examine the ramifications of recent advances in Markov chain Monte Carlo methods for learning by backpropagation. Finally, we consider how the Bayesian network formalism informs the causal reasoning interpretation of some neural networks, and how it prescribes optimizations for efficient random sampling in Bayesian learning applications.",
            "group": 288,
            "name": "10.1.1.16.2746",
            "keyword": "",
            "title": "A Position Paper on Statistical Inference Techniques Which Integrate Neural Network and Bayesian Network Models"
        },
        {
            "abstract": "A communication network is composed by a set of centers which transmit and receive data, and a set of links which transport this data. To evaluate the capacity of a communication network architecture to resist to the possible failures of some of its components, several reliability metrics are currently used.",
            "group": 289,
            "name": "10.1.1.16.2909",
            "keyword": "",
            "title": "Simulated Annealing for Communication Network Reliability Improvement"
        },
        {
            "abstract": "Most shotgun sequencing projects undergo a long and costly phase of finishing, in which a partial assembly forms several contigs whose  order, orientation and relative distance is unknown. We propose here a new technique that supplements the shotgun assembly data by cheap and simple complete restriction digests of the target. By computationally combining information from the contig sequences and the fragment sizes measured for several different enzymes, we seek to form a \"scaffold\" on which the contigs will be placed in their correct orientation, order and distance. We give a heuristic search algorithm for solving the problem and report on promising preliminary simulation results. The key to the success of the search scheme is the very rapid solution of two time-critical subproblems that are solved precisely in linear time.",
            "group": 290,
            "name": "10.1.1.16.4319",
            "keyword": "",
            "title": "The Restriction Scaffold Problem"
        },
        {
            "abstract": "We propose two efficient heuristics for minimizing the number of oligonucleotide probes needed for analyzing populations of ribosomal RNA gene (rDNA) clones by hybridization experiments on DNA microarrays. Such analyses have applications in the study of microbial communities. Unlike in the classical SBH (sequencing by hybridization) procedure, where multiple probes are on a DNA chip, in our applications we perform a series of experiments, each one consisting of applying a single probe to a DNA microarray containing a large sample of rDNA sequences from the studied population. The overall cost of the analysis is thus roughly proportional to the number of experiments, underscoring the need for minimizing the number of probes. Our algorithms are based on two well-known optimization techniques, i.e. simulated annealing and Lagrangian relaxation, and our preliminary tests demonstrate that both algorithms are able to find satisfactory probe sets for real rDNA data.",
            "group": 291,
            "name": "10.1.1.16.4428",
            "keyword": "",
            "title": "Probe Selection Algorithms with Applications in the Analysis of Microbial Communities (Extended Abstract)"
        },
        {
            "abstract": "This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes# efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing) . Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speedup factors are typically obtained when comparing this approach with the equivalent brute-force formulation# we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardwarespecific implementations of the proposed method as far as SIMD parallelism is concerned.",
            "group": 292,
            "name": "10.1.1.16.4493",
            "keyword": "",
            "title": "Real-Time Object Detection for Smart Vehicles"
        },
        {
            "abstract": "The optimization of spatial indexing is an increasingly important issue considering the fact that spatial databases, in suchdiverse areas as geographical, CAD/CAM and image applications, are growing rapidly in size and often contain on the order of millions of items or more. This necessitates the storage of the index on disk, which has the potential of slowing down the access time significantly. In this paper, we discuss ways of minimizing the disk access frequency by grouping together data items which are close to one another in the spatial domain (\"packing\"). The data structure which we seek to optimize here is the R-tree for a given set of data objects.",
            "group": 293,
            "name": "10.1.1.16.4573",
            "keyword": "",
            "title": "R-Tree Index Optimization"
        },
        {
            "abstract": "A new Reactive Local Search (RLS ) algorithm is proposed for the solution of the Maximum-Clique problem. RLS is based on local search complemented by a feedback (history-sensitive) scheme to determine the amount of diversification. The reaction acts on the single parameter that decides the temporary prohibition of selected moves in the neighborhood, in a manner inspired by Tabu Search. The performance obtained in computational tests appears to be significantly better with respect to all algorithms tested at the the second DIMACS implementation challenge. The worst-case complexity per iteration of the algorithm is O(max{n, m}) where n and m are the number of nodes and edges of the graph. In practice, when a vertex is moved, the number of operations tends to be proportional to its number of missing edges and therefore the iterations are particularly fast in dense graphs.",
            "group": 294,
            "name": "10.1.1.16.4667",
            "keyword": "",
            "title": "Reactive Local Search for the Maximum Clique Problem"
        },
        {
            "abstract": "With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp, SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wireexposed architectures.",
            "group": 295,
            "name": "10.1.1.16.5644",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "We discuss a new nonlinear feature extraction algorithm, the solution of which can be obtained in closed-form. The features can be used for general pattern recognition applications including those involving class representation, class discrimination, or for both simultaneously. The feature extraction method is called the maximum representation and discrimination feature (MRDF) method. A new discrimination measure is used that can handle classes with multiple clusters in the input space. A computationally efficient nonlinear MRDF procedure to extract nonlinear features from high-dimensional input images is devised. This nonlinear feature extraction procedure is shown to provide very general nonlinear surfaces, in contrast with other nonlinear techniques. The nonlinear MRDF is also shown to generalize to well-known linear and nonlinear image processing operations. We show the use of the MRDF feature extraction technique for classification of product inspection items, classification and pose estimation of objects (machined-parts), and pose-invariant face recognition. Comparisons of the results obtained with our MRDF features with other well-known feature extraction procedures such as the Karhunen-Loeve (KL) transform, and the Fisher linear discriminant are also made.",
            "group": 296,
            "name": "10.1.1.16.5931",
            "keyword": "",
            "title": "Nonlinear Feature Extraction For Pattern Recognition Applications"
        },
        {
            "abstract": "In modern embedded systems including communication and multimedia applications, large fraction of power is consumed during memory access and data transfer. Thus, buses should be designed and optimized to consume reasonable power while delivering sufficient performance. In this paper, we address a bus ordering problem for low-power application-specific systems. A heuristic algorithm is proposed to determine the order in a way that effective lateral component of capacitance is reduced, thereby reducing the power consumed by buses. Experimental results for various examples indicate that the average power saving from 30% to 46.7% depending on capacitance components can be obtained without any  circuit overhead.",
            "group": 297,
            "name": "10.1.1.16.7740",
            "keyword": "",
            "title": "Coupling-Driven Bus Design for Low-Power Application-Specific Systems"
        },
        {
            "abstract": "We propose a new approach to the problem of workload partitioning and assignment for very large distributed real-time systems, in which software components are typically organized hierarchically, and hardware components potentially span several shared and/or dedicated links. Existing approaches for load partitioning and assignment are based on either schedulability or communication. The first category attempts to construct a feasible schedule for various assignments and chooses the one that minimizes task lateness (or other similar criteria), while the second category partitions the workload heuristically in accordance with the amount of intertask communication. We propose, and argue for, a (new) third category based on task periods, which, among others, combines the ability of handling heterogeneity with excellent scalability. Our algorithm is a recursive-...",
            "group": 298,
            "name": "10.1.1.16.8584",
            "keyword": "Key Words--- Task allocationtask partitioninginhomogeneous networksreal-time scheduling. 1",
            "title": "Period-Based Load Partitioning and Assignment for Large Real-Time Applications"
        },
        {
            "abstract": " ",
            "group": 299,
            "name": "10.1.1.16.9091",
            "keyword": "",
            "title": "Impact of structure on parallel local search for SAT"
        },
        {
            "abstract": "Delay estimation of a code-spread code-division multiple-access (CS-CDMA) system using low-rate maximum free distance (MFD) convolutional codes for bandwidth expansion is studied. The MFD codes used, have a repetitive structure, which is utilized for non-data-aided synchronization. The repetition pattern of the code is optimized for good delay estimation properties, using an optimization criterion based on the Cramer-Rao bound for the delay estimation problem. An approximate maximum-likelihood estimator that jointly estimates the channel gain, the codewords and the delay is proposed. A corresponding estimator for direct-sequence CDMA (DS-CDMA) is also derived. The performance, in terms of estimation error, outlier rate and bit-error rate, for both the systems are compared.",
            "group": 300,
            "name": "10.1.1.16.9340",
            "keyword": "",
            "title": "Synchronization of Code-Spread CDMA Systems"
        },
        {
            "abstract": "An important issue in data warehouse development is the selection of a set of views to materialize in order to accelerate OLAP queries, given certain space and maintenance time constraints. Existing methods provide good results but their high execution cost limits their applicability for large problems. In this paper, we explore the application of randomized, local search algorithms to the view selection problem. The efficiency of the proposed techniques is evaluated using synthetic datasets, which cover a wide range of data and query distributions. The results show that randomized search methods provide near-optimal solutions in limited time, being robust to data and query skew. Furthermore, they can be easily adapted for various versions of the problem, including the simultaneous existence of size and time constraints, and view selection in dynamic environments. The proposed heuristics scale well with the problem size, and are therefore particularly useful for real life warehouses, which need to be analyzed by numerous business perspectives.",
            "group": 301,
            "name": "10.1.1.17.256",
            "keyword": "View selectionOLAPData warehouse",
            "title": "View Selection Using Randomized Search"
        },
        {
            "abstract": "The ever growing number of wireless communications systems deployed around the globe have made the optimal assignment of a limited radio frequency spectrum a problem of primary importance. At issue are planning models for permanent spectrum allocation, licensing, regulation, and network design. Further at issue are on-line algorithms for dynamically assigning frequencies to users within an established network. Applications include aeronautical mobile, land mobile, maritime mobile, broadcast, land fixed (pointto -point), and satellite systems. This paper surveys research conducted by theoreticians, engineers, and computer scientists regarding the frequency assignment problem (FAP) in all of its guises. The paper begins by defining some of the more common types of FAPs. It continues with a discussion on measures of optimality relating to the use of spectrum, models of interference, and mathematical representations of the many FAPs, both in graph theoretic terms, and as mathematical pro...",
            "group": 302,
            "name": "10.1.1.17.1041",
            "keyword": "",
            "title": "Frequency Assignment Problems"
        },
        {
            "abstract": "We consider a median pyramidal transform for denoising applications. Traditional techniques of pyramidal alenoising are similar to those in wavelet-based methods. In order to remove noise, they use the thresholding of transform coefficients. We propose to model the structure of the transform coefficients as a Markov random field. The goal of modeling transform coefficients is to retain significant coefficients on each scale and to discard the rest. Estimation of the transform coefficient structure is obtained via a Markov chain sampler. The advantage of our method is that we are able utilize the interactions between transform coefficients, both within each scale and among the scales, which leads to alenoising improvement as demonstrated by numerical simulations.",
            "group": 303,
            "name": "10.1.1.17.1409",
            "keyword": "",
            "title": "Markov Random Field Modeling in Median Pyramidal Transform Domain for Denoising Applications"
        },
        {
            "abstract": "This paper presents a survey of connectionist inference systems.",
            "group": 304,
            "name": "10.1.1.17.2333",
            "keyword": "",
            "title": "Connectionist Inference Systems"
        },
        {
            "abstract": "Sequential read-write memories (SRWMs) are RAMs  without an address decoder. A shift register is used instead  to point at subsequent memory locations. SRWMs consume  less power than RAMs of the same size. Algorithms are presented  to check whether a set of storage values fits in a single  SRWM and to automatically map storage values in as  few SRWMs as possible. Benchmark results show that good  assignments can be obtained in spite of the limited addressing  capabilities.",
            "group": 305,
            "name": "10.1.1.17.3353",
            "keyword": "",
            "title": "Assignment of Storage Values to Sequential Read-Write Memories"
        },
        {
            "abstract": "Local search is a traditional technique to solve combinatorial search problems which has raised much interest in recent years. The design and implementation of local search algorithms is not an easy task in general and may require considerable experimentation and programming effort. However, contrary to global search, little support is available to assist the design and implementation of local search algorithms. This paper is an attempt to support the implementation of local search. It presents the preliminary design of Localizer, a modeling language which makes it possible to express local search algorithms in a notation close to their informal descriptions in scientific papers. Experimental results on our first implementation show the feasibility of the approach.   ",
            "group": 306,
            "name": "10.1.1.17.4006",
            "keyword": "",
            "title": "LOCALIZER -- A Modeling Language for Local Search"
        },
        {
            "abstract": ". A multiway spatial join combines information found in three or more spatial relations with respect to some spatial predicates. Motivated by their close correspondence with constraint satisfaction problems (CSPs), we show how multiway spatial joins can be processed by systematic search algorithms traditionally used for CSPs. This paper describes two different strategies, window reduction and synchronous traversal, that take advantage of underlying spatial indexes to prune the search space effectively. In addition, we provide cost models and optimization methods that combine the two strategies to compute more efficient execution plans. Finally, we evaluate the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations.  Key Words. Spatial databases, Spatial joins, Constraint satisfaction, R-trees.  1. ",
            "group": 307,
            "name": "10.1.1.17.4112",
            "keyword": "",
            "title": "Constraint-based Processing of Multiway Spatial Joins"
        },
        {
            "abstract": "this paper we study a new information-theoretically justified approach to missing data estimation for multivariate categorical data. The approach discussed is a model-based imputation procedure relative to a model class (i.e., a functional form for the probability distribution of the complete data matrix), which in our case is the set of multinomial models with some independence assumptions. Based on the given model class assumption an information-theoretic criterion can be derived to select between the different complete data matrices. Intuitively this general criterion, called stochastic complexity, represents the shortest code length needed for coding the complete data matrix relative to the model class chosen. Using this information-theoretic criteria, the missing data problem is reduced to a search problem, i.e., finding the data completion with minimal stochastic complexity. In the experimental part of the paper we present empirical results of the approach using two real data sets, and compare these results to those achived by commonly used techniques such as case deletion and imputating sample averages. Introduction",
            "group": 308,
            "name": "10.1.1.17.4234",
            "keyword": "",
            "title": "Stochastic Complexity Based Estimation of Missing Elements in Questionnaire Data"
        },
        {
            "abstract": "A stochastic global optimization approach is presented for skew minimization in  CMOS VLSI circuits. This is a direct search strategy for the best design among feasible  ones, with the designer determining when the search is stopped. Through examples,  we show the power of this technique in quickly obtaining very good designs, even for  constrained problems.",
            "group": 309,
            "name": "10.1.1.17.4361",
            "keyword": "",
            "title": "Global Optimization Approach to Transistor Sizing for High Performance CMOS VLSI Circuits"
        },
        {
            "abstract": "Simulated annealing is a computational technique reported to give good results  in coping with complex combinatorial problems. These problems usually consist of  finding a global minimum of a cost function on a (large) set of states (also known as  feasible solutions). In this paper we explore modifications to the standard simulated  annealing method that we apply to the design of dense interconnection networks,  that is networks that have as many nodes as possible for a given maximum number  of links from each node to other nodes, and a given maximum distance between  all pair of nodes. We are particularly interested in the directed-symmetric case in  wich all links have a direction, and the network, roughly speaking, looks the same  from any node. In that case each node may execute, without modifications, the  same communication software.",
            "group": 310,
            "name": "10.1.1.17.5267",
            "keyword": "",
            "title": "Using Simulated Annealing to Design Interconnection Networks"
        },
        {
            "abstract": "The complexity of circuit designs has necessitated a top-down approach to layout synthesis. A large body of work shows that a good layout hierarchy, or partitioning tree, as measured by the associated Rent parameter, will correspond to an area-efficient layout. We define the intrinsic Rent parameter of a netlist to be the minimum possible Rent parameter of any partitioning tree for the netlist. Experimental results show that spectra-based ratio cut partitioning algorithms yield partitioning trees with the lowest observed Rent parameter over all benchmarks and over all algorithms tested. For examples where the intrinsic Rent parameter is known, spectral ratio cut partitioning yields a partitioning tree with Rent parameter essentially identical to this theoretical optimum. These results have deep implications withrespect to both the choice of partitioning algorithms for top-down layout, as well as new approaches to layout area estimation. The paper concludes with directions for future research, including several promising techniques for fast estimation of the (intrinsic) Rent parameter.",
            "group": 311,
            "name": "10.1.1.17.5344",
            "keyword": "",
            "title": "On the Intrinsic Rent Parameter and Spectra-Based Partitioning Methodologies"
        },
        {
            "abstract": "The application of Genetic Algorithms (GAs) to the automated design of artificial neural networks has received much attention in recent years. A number of common network models have been studied. This paper presents empirical results on the application of GAs to the Probabilistic RAM (pRAM) network model. Pattern recognition tasks are presented to the pRAM networks. These results are compared with those using Simulated Annealing (SA) as well as the reinforcement learning algorithm originally proposed for pRAM networks. We found that both SA and GA do not perform as well as the reinforcement learning algorithm. Nevertheless, GA is able to achieve an average recognition rate of 83.78% at the expense of long running time. 1 Introduction  There is increasing interest in the application of machine learning algorithms to assist with design and training of neural networks. One common technique is the use of Genetic Algorithms (GAs) [7], in which a population of individuals in a parallel searc...",
            "group": 312,
            "name": "10.1.1.17.5478",
            "keyword": "",
            "title": "An Empirical Study of Two Approaches to Automated pRAM Network Design"
        },
        {
            "abstract": "Image restoration is an essential preprocessing step for many image analysis  applications. So far, the majority of works have been devoted to image  denoising. For this issue, the most common problem is that some interesting  structures in the image will be removed from the concerned image during noise  suppression. Such interesting structures in an image often correspond to the  discontinuities in the image. In this paper, we propose a novel pixon-based  multiresolution method for image denoising. The key idea to our approach is  that a pixon map is embedded into a MRF model under a Bayesian framework.",
            "group": 313,
            "name": "10.1.1.17.5720",
            "keyword": "Markov Random FieldsGibbs Random Fieldsfuzzy pixonsimage denoisingBayesian estimationsimulated annealingmultiresolution technique",
            "title": "Pixon-Based Image Denoising with Markov Random Fields"
        },
        {
            "abstract": "Optimal structure element extraction is a key step in the application of mathematical morphology to various image processing tasks and shape description problem in computer vision. In this paper, We propose a novel optimization technique called evolutionary tabu search (ETS) to solve optimal structure element extraction problem for MST-based shape description. Specifically, we incorporates \"the survival of strongest\" idea of evolution algorithm into tabu search. This new method has the ability to find the global optimum, which not only keeps the advantages of tabu search and Genetic Algorithms, but also overcomes some of their shortages. Specifically, by comparing our algorithm with the existing other global optimization methods (such as genetic algorithm, Simulated annealing and tabu search), we find that the ETS is more practical and effective, which also yields good near-optimal solutions and has better convergence speed.",
            "group": 314,
            "name": "10.1.1.17.7261",
            "keyword": "tabu searchmodel-based vision",
            "title": "A New Approach to Optimal Structuring Element Extraction for MST-Based Shapes Description"
        },
        {
            "abstract": "In this introductory paper to the field, it is our goal to provide a new unified look at synthesis problems that is independent from the level of abstraction like system, RTL, and logic (for refinements targeted to hardware), or processand basic block level (for refinements targeted to software). For each level of our model called \"double roof\", synthesis requires the solution of three basic problems, namely allocation (of resources), binding, and scheduling. Based on the \"double roof\" model, we present a graph-based formulation of the tasks of system-synthesis: Contrary to former approaches that consider system-synthesis as a bi-partition problem (e.g., earlier hardware/software partitioning algorithms), we consider also as well the allocation of components like micro- and hardware coprocessors as part of the optimization problem as scheduling of tasks including communication scheduling. The approach is flexible enough to be applied to different other abstraction levels. Finally, we introduce the problem of design space exploration as a new challenge in synthesis. For the typically multi-objective nature of synthesis problems, not only one optimum is wanted, but an exploration of a complete front of optimal solutions called Pareto points.",
            "group": 315,
            "name": "10.1.1.17.7301",
            "keyword": "",
            "title": "Embedded System Synthesis and Optimization"
        },
        {
            "abstract": "",
            "group": 316,
            "name": "10.1.1.17.7587",
            "keyword": "",
            "title": "Survey of Job Shop Scheduling Techniques"
        },
        {
            "abstract": "OFDM based Single Frequency Networks (SFNs) have been standardized for terrestrial broadcasting systems, for digital audio broadcasting (DAB) as well as for digital video broadcasting (DVB). Due to the multipath tolerance of the OFDM scheme, the receiver is able to combine signals coming from several transmitters, despite of the varying propagation delays, i.e., heavy artificial multipath propagation. In order to take full advantage of the diversity gain provided by the SFN architecture, proper network design is required. In the paper we focus on the cost efficient design of an SFN providing broadcasting services over a predefined service area with requirements both on the received signal quality and on the allowable interference level experienced by existing services in the same spectrum. We formulate the problem as a discrete optimization problem, where the network design parameters such as power, antenna heights and transmitter locations are the decision variables. The general stochastic optimi .ation algorithm Simulated Annealing has been adapted for solving the above problem. The novelty of our method is that cost factors and interference constraints are embedded in the optimization procedure. Through numerical examples we demonstrate that significant reduction in network cost can be achieved by our approach.",
            "group": 317,
            "name": "10.1.1.17.8115",
            "keyword": "OptimizationSimulated Annealing",
            "title": "Minimal Cost Coverage Planning for Single Frequency Networks"
        },
        {
            "abstract": "In this paper, we propose a transitive closure graph-based representation for general floorplans, called TCG, and show its superior properties. TCG combines the advantages of popular representations such as sequence pair, BSG, and B*-tree. Like sequence pair and BSG, but unlike O-tree, B*-tree, and CBL, TCG is P-admissible. Like B*-tree, but unlike sequence pair, BSG, O-tree, and CBL, TCG does not need to construct additional constraint graphs for the cost evaluation during packing, implying faster runtime. Further, TCG supports incremental update during operations and keeps the information of boundary modules as well as the shapes and the relative positions of modules in the representation. More importantly, the geometric relation among modules is transparent not only to the TCG representation but also to its operations, facilitating the convergence to a desired solution. All these properties make TCG an effective and flexible representation for handling the general floorplan/placement design problems with various constraints. Experimental results show the promise of TCG.",
            "group": 318,
            "name": "10.1.1.17.8412",
            "keyword": "",
            "title": "TCG: A Transitive Closure Graph-Based Representation for Non-Slicing Floorplans"
        },
        {
            "abstract": "This thesis investigates the problem of growing decision trees from data, for the purposes of classification and prediction.",
            "group": 319,
            "name": "10.1.1.17.9426",
            "keyword": "AromaBabaji ooo",
            "title": "On Growing Better Decision Trees from Data"
        },
        {
            "abstract": "Logic emulation enables designers to functionally verify complex integrated circuits prior to chip fabrication. However, traditional FPGA-based logic emulators have poor inter-chip communication bandwidth, commonly limiting gate utilization to less than 20 percent. Global routing contention mandates the use of expensive crossbar and PC-board technology in a system of otherwise low-cost, commodity parts. Even with crossbar technology, current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). Virtual Wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. The resulting increase in bandwidth allows effective use of low dimension, direct interconnect. The size of the FPGA array can be decreased as well, resulting in low cost logic emulation.",
            "group": 320,
            "name": "10.1.1.17.9545",
            "keyword": "",
            "title": "Logic Emulation with Virtual Wires"
        },
        {
            "abstract": "We report the results of testing the performance of a new, efficient, and highly general- purpose parallel optimization method based upon simulated annealing. This optimization algorithm was applied to analyze the network of interacting genes that control embryonic de- velopment and other fundamental biological processes. We found several sets of algorithmic parameters that lead to optimal parallel efficiency for up to 100 processors on distributed- memory MIMD architectures. Our strategy contains two major elements. First, we monitor and pool performance statistics obtained simultaneously on all processors. Second, we mix states at intervals to ensure a Boltzmann distribution of energies. The central scientific issue is the inverse problem, the determination of the parameters of a set of nonlinear or- dinary differential equations by minimizing the total error between the model behavior and experimental observations.",
            "group": 321,
            "name": "10.1.1.17.9743",
            "keyword": "Simulated annealingParallel processingInverse problems",
            "title": "Parallel Simulated Annealing by Mixing of States"
        },
        {
            "abstract": "In this paper the task of training sub-symbolic systems is considered as a combinatorial optimization prob- lem and solved with the heuristic scheme of the Reactive Tabu Search. An iterative optimization process based on a \"modified greedy search\" component is complemented with a meta-strategy to realize a discrete dynamical system that discourages limit cycles and the confinement of the search trajectory in a limited portion of the search space. The possible cycles are discouraged by prohibiting (i.e., making tabu) the execution of moves that reverse the ones applied in the most recent part of the search, for a prohibition period that is adapted in an automated way. The confinement is avoided and a proper exploration is obtained by activating a diversification strategy when too many configurations are repeated excessively often. The RTS method is applicable to non-differentiable functions, it is robust with respect to the random initialization and effective in continuing the search after local minima. Three tests of the technique on feedforward and feedback systems are presented.",
            "group": 322,
            "name": "10.1.1.18.10",
            "keyword": "searchmultilayer perceptronfeedback neural networks",
            "title": "Training Neural Nets with the Reactive Tabu Search"
        },
        {
            "abstract": "The graph partitioning problem is as follows.  Given a graph G = (N; E) (where N is a set of weighted  nodes and E is a set of weighted edges) and a positive  integer p, find p subsets N 1 ; N 2 ; : : : ; N p of N such that  1. [  p  i=1 N i = N and N i \" N j = ; for i 6= j,  2. W (i)  W=p, i = 1; 2; : : : ; p, where W (i) and W are  the sums of the node weights in N i and N , respectively,  3. the cut size, i.e., the sum of weights of edges crossing  between subsets is minimized.  This problem is of interest in areas such as VLSI placement and routing, and efficient parallel implementations of finite element methods. In this survey we summarize the state-of-the-art of sequential and parallel graph partitioning algorithms.  Keywords: Graph partitioning, sequential algorithms, parallel algorithms. The work presented here is funded by CENIIT (the Center for  Industrial Information Technology) at Linkoping University.  1 1 Introduction  The graph partitioning problem is as follows.  Given...",
            "group": 323,
            "name": "10.1.1.18.232",
            "keyword": "Recommended citationAuthor?.!Title?. Linkoping Electronic Articles in",
            "title": "Algorithms for Graph Partitioning: A Survey"
        },
        {
            "abstract": "Image segmentation by energy-minimizing active contour models (snakes) suffers from the fact that classic numerical optimization algorithms find only local energy minima, which makes the approachvery sensible to noise and initialization. This paper presents a robust adaptive snake model using a stochastic relaxation technique, Simulated Annealing (SA), to find a global energy minimum in noisy cine MR images. Once a reliable segmentation has been done, a much faster probabilistic relaxation technique, Iterated Conditional Modes (ICM), is used for energy minimization in the following time frames.",
            "group": 324,
            "name": "10.1.1.18.931",
            "keyword": "",
            "title": "Contour fitting using stochastic and probabilistic relaxation for Cine MR Images"
        },
        {
            "abstract": "This thesis addresses a number of issues concerned with the restoration of one type of image sequence, namely archived black and white motion pictures. These are often a valuable historical record, but because of the physical nature of the film they can suffer from a variety of degradations which reduce their usefulness. The main visual defects are `dirt and sparkle' due to dust and dirt becoming attached to the film, or abrasion removing the emulsion, and `line scratches' due to the film running against foreign bodies in the camera or projector. For an image",
            "group": 325,
            "name": "10.1.1.18.1048",
            "keyword": "image sequence processingMarkov random fieldsMarkov chain Monte",
            "title": "Image Sequence Restoration Using Gibbs Distributions"
        },
        {
            "abstract": "The brain is perhaps the most complex system to have ever been subjected to rigorous scientific investigation. The scale is staggering: over 1011 neurons, each making an average of 10 3 synapses, with computation occurring on scales ranging from a single dendritic spine, to an entire cortical area. Slowly, we are beginning to acquire experimental tools that can gather the massive amounts of data needed to characterize this system. However, to understand and interpret these data will also require substantial strides in inferential and statistical techniques. This dissertation attempts to meet this need, extending and applying the modern tools of latent variable modeling to problems in neural data analysis. It is divided",
            "group": 326,
            "name": "10.1.1.18.1712",
            "keyword": "Acknowledgements Many thanks are due to my advisorsRichard Andersen and John Hopfield. Richard has been",
            "title": "Latent Variable Models for Neural Data Analysis"
        },
        {
            "abstract": "We present a new heuristic algorithm for graph bisection, based on an implicit notion of clustering. We describe",
            "group": 327,
            "name": "10.1.1.18.1916",
            "keyword": "",
            "title": "A Seed-Growth Heuristic for Graph Bisection"
        },
        {
            "abstract": "Many problems in early vision can be formulated in terms of minimizing an' energy or cost function. Examples are shape-from-shading, edge detection, motion snatysis, structure from motion and surface interpolation (Poggio, Torre and Koch, 1985). It has been shown that all quadratic variational problems, an important subset of early vision tasks, can be \"solved\" by linear, analog electrical or chemical networks (Poggio and Koch, 1985). In a variety of situations the cost function is non-quadratic, however, for instance in the presence of discontinuities. The use of non-quadratic cost functions raises the question of designing efficient algorithms for computing the optimal solution. Recently. Hopfield and Tank (1985) have shown that networks of nonlinear analog \"neurons\" can be effect. lye in computing the solution of optimization problems, In this paper, we show how these networks can be generalized to solve the non-convex energy functionals of early vision. We illustrate this approach by implementing a specific network solving the problem of reconstructing a smooth surface while preserving its discontinuities from sparsely sampled data (Geman and Geman, 1984; Marroquin, 1984; Terzopoulos, 1984). These results suggest a novel computational strategy for solving such problems for both biological and artificial vision systems.",
            "group": 328,
            "name": "10.1.1.18.2303",
            "keyword": "",
            "title": "Analog \"Neuronal\" Networks in Early Vision"
        },
        {
            "abstract": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.",
            "group": 329,
            "name": "10.1.1.18.2720",
            "keyword": "General TermsAlgorithms Additional Key Words and PhrasesCluster analysisclustering applicationsexploratory data analysis",
            "title": "Data Clustering: A Review"
        },
        {
            "abstract": " Several factors contribute to making search problems easy or difficult. One of these factors is the modality of the fitness landscape. Quite often, when local search algorithms fail to locate the global optimum, it is because the algorithm converged to a local optimum. The majority of local search methods in use today maneuver through the search space using local neighborhood information around a single point to guide the search. In that search paradigm, the number of local optima that occur in the search space has a tremendous effect on search performance. Genetic",
            "group": 330,
            "name": "10.1.1.18.2879",
            "keyword": "",
            "title": "Examining The Role Of Local Optima And Schema Processing In Genetic Search"
        },
        {
            "abstract": "A method is presented for using connectionist networks of simple computing elements  to discover a particular type of constraint in multidimensional data. Suppose  that some data source provides samples consisting of n-dimensional feature-vectors,  but that this data all happens to lie on an m-dimensional surface embedded in the  n-dimensional feature space. Then occurrences of data can be more concisely described  by specifying an m-dimensional location on the embedded surface than  by reciting all n components of the feature vector. The recoding of data in such  a way is known as dimensionality-reduction. This paper describes a method for  performing dimensionality-reduction in a wide class of situations for which an assumption  of linearity need not be made about the underlying constraint surface.",
            "group": 331,
            "name": "10.1.1.18.2985",
            "keyword": "",
            "title": "Dimensionality-Reduction Using Connectlonist Networks"
        },
        {
            "abstract": "We investigate two ways of solving the correspondence problem for motion using the assumptions of minimal mapping and rigidity. Massively parallel analog networks are designed to implement these theories. Their effectiveness is demonstrated with mathematical proofs and computer simulations. We discuss relevant psychophysical experiments.",
            "group": 332,
            "name": "10.1.1.18.3006",
            "keyword": "",
            "title": "Massively Parallel Implementations of Theories for Apparent Motion"
        },
        {
            "abstract": "The work described in this thesis began as an inquiry into the nature and use of optimization programs based on \"genetic algorithms.\" That inquiry led, eventually, to three powerful heuristics that are broadly applicable in gradient-ascent programs: First, remember the locations of local maxima and restart the optimization program at a place distant from previously located local maxima. Second, adjust the size of probing steps to suit the local nature of the terrain, shrinking when probes do poorly and growing when probes do well. And third, keep track of the directions of recent successes, so as to probe preferentially in the direction of most rapid ascent. These algorithms lie at the core of a novel optimization program that illustrates the power to be had from deploying them together. The efficacy of this program is demonstrated on several test problems selected from a variety of fields, including De Jong's famous testproblem suite, the traveling salesman problem, the problem of coo...",
            "group": 333,
            "name": "10.1.1.18.3038",
            "keyword": "",
            "title": "From Genetic Algorithms To Efficient Optimization"
        },
        {
            "abstract": "Active visual based scene exploration as well as speech understanding and dialogue are important skills of a service robot which is employed in natural environments and has to interact with humans. In this paper we suggest a knowledge based approach for both scene exploration and spoken dialogue using semantic networks. For scene exploration the knowledge base contains information about camera movements and objects. In the dialogue system the knowledge base contains information about the individual dialogue steps as well as about syntax and semantics of utterances. In order to make use of the knowledge, an iterative control algorithm which has real-- time and any--time capabilities is applied. In addition, we propose appearance based object models which can substitute the object models represented in the knowledge base for scene exploration. We show the applicability of the approach for exploration of office scenes and for spoken dialogues in the experiments. The integration of the multi--sensory input can easily be done, since the knowledge about both application domains is represented using the same network formalism.",
            "group": 334,
            "name": "10.1.1.18.3197",
            "keyword": "",
            "title": "Knowledge Based Image and Speech Analysis for Service Robots"
        },
        {
            "abstract": "We argue that the modern computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information-processing tasks. In particular, we show that a simple, theory-neutral linguistic model ofsyntactic agreement and lexical ambiguity demonstrates that natural language parsing may be computationally intractable, extending the classic work of Chomsky and Miller (1963). Signitcantly, we show that it may be syntactic features rather than complex rules that can cause this difficulty. Informally, human languages and the computationally intractable satistability problem (SAT) share two costly computational mechanisms: both enforce agreement among terminal symbols across unbounded distances and both allow terminal symbol ambiguity. In natural languages, lexical elements may be required to agree (or disagree) on such features as person, number, and gender (e.g., subject/verb agreement in English); in SAT, agreement ensures the consistency of variable truth assignments. Lexical ambiguity can appear freely in natural language utterances (can may be a noun, verb, or auxiliary), while a variable in a SAT formula may be either true or false. When coupled with a deterministic performance model, this complexity result explains a subtle psycholinguistic distinction between discovering and verifying the grammaticality of an utterance. Finally, the applicability of computational complexity analysis to other cognitive faculties such as vision is discussed.",
            "group": 335,
            "name": "10.1.1.18.3221",
            "keyword": "",
            "title": "Computational Consequences of Agreement and Ambiguity in Natural Language"
        },
        {
            "abstract": "A fast simulated annealing algorithm is developed for automatic object recognition. The object recognition  problem is addressed as the problem of best describing a matchbetween a hypothesized object and an  image. The normalized correlation coefficient is used as a measure of the match. Templates are generated  on-line during the searchby transforming model images. Simulated annealing reduces the searchtimeby  orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of  how landmarks, for example, traffic signs, can be recognized by an autonomous vehicle or a navigating  robot. Images are assumed to be taken while the robot or the vehicle is moving through its environment.",
            "group": 336,
            "name": "10.1.1.18.3255",
            "keyword": "",
            "title": "Fast Object Recognition in Noisy Images Using Simulated Annealing"
        },
        {
            "abstract": "Image analysis problems, posed mathematically as variational principles or as partial differential equations, are amenable to numerical solution by relaxation algorithms that are local, iterative, and often parallel. Although they are well suited structurally for implementation on massively parallel, locally-interconnected computational architectures, such distributed algorithms are seriously handicapped by an inherent inefficiency at propagating constraints between widely separated processing elements. Hence, they converge extremely slowly when confi'onted by the large representations necessary for low-level vision. Application of multigrid methods can overcome this drawback, as we establisheel in previous work oil 3-D surface reconstruction. In this paper, we develop efficient multiresolution Jtcrative algorithms for computing lightncss, shape-from-shading, and optical flow, and we evaluate the performance of these algorithms on synthetic images. The multigrid methodology that we describe is broadly applicable in low-level vision. Notably, it is an appealing strategy to use in conjunction with regularization analysis for the efficient solution of a wide range of ill-posed visual reconstruction problems.",
            "group": 337,
            "name": "10.1.1.18.3445",
            "keyword": "",
            "title": "Multigrid Relaxation Methods and . . . "
        },
        {
            "abstract": "When a function is mapped onto a bit representation,  the structure of the fitness landscape  can change dramatically. Techniques  such as Delta Coding have tried to dynamically  adapt the representation during the  search process in hopes of making the problem  easier for a genetic algorithm to solve.",
            "group": 338,
            "name": "10.1.1.18.3704",
            "keyword": "",
            "title": "Bit Representations with a Twist"
        },
        {
            "abstract": "We describe a method for automatically building statistical shape models from a training set of exam- ple boundaries / surfaces. These models show considerable promise as a basis for segmenting and interpreting images. One of the drawbacks of the approach is, however, the need to establish a set of dense correspondences between all members of a set of training shapes. Often this is achieved by locating a set of qandmarks manually on each training image, which is time-consuming and subjective in 2D, and almost impossible in 3D. We describe how shape models can be built automatically by posing the correspondence problem as one of finding the parameterization for each shape in the training set. We select the set of parameterizations that build the best model. We define best as that which min- imizes the description length of the training set, arguing that this leads to models with good compactness, specificity and generalization ability. We show how a set of shape parameterizations can be represented and manipulated in order to build a minimum description length model. Results are given for several different training sets of 2D boundaries, showing that the proposed method constructs better models than other approaches including manual landmarking - the current gold standard. We also show that the method can be extended straightforwardly to 3D.",
            "group": 339,
            "name": "10.1.1.18.4146",
            "keyword": "Minimum Description LengthAutomatic LandmarkingActive Shape ModelsCorrespondence problemPoint Distribution Mod- els",
            "title": "A Minimum Description Length Approach to Statistical Shape Modelling"
        },
        {
            "abstract": "The goal of unsupervised learning algorithms is to discover concise yet informative representations of large data sets; the minimum description length principle and exploratory projection pursuit are two representative attempts to formalize this notion. When implemented with neural networks, both suggest the minimization of entropy at the network's output as an objective for unsupervised learning. The empirical computation of entropy or its derivative with respect to parameters of a neural network unfortunately requires explicit knowledge of the local data density; this information is typically not available when learning from data samples. This dissertation discusses and applies three methods for making density information accessible in a neural network: parametric modelling, probabilistic networks, and nonparametric estimation. By imposing their own structure on the data, parametric density models implement impoverished but tractable forms of entropy such as the log-variance. We have used this method to improve the adaptive dynamics of an anti-Hebbian learning rule which has proven successful in extracting disparity from random stereograms. In probabilistic networks, node activities are interpreted as the defining parameters of a stochastic process. The entropy of the process can then be calculated from its parameters, and hence optimized. The popular logistic activation function defines a binomial process in this manner; by optimizing the information gain of this process we derive a novel nonlinear Hebbian learning algorithm. The nonparametric technique of Parzen window or kernel density estimation leads us to an entropy optimization algorithm in which the network adapts in response to the distance between pairs of data samples. We discuss distinct implementations for data-limited or memory-limited operation, and describe a maximum likelihood approach to setting the kernel shape, the regularizer for this technique. This method has been applied with great success to the problem of pose alignment in computer vision. These experiments demonstrate a range of techniques that allow neural networks to learn concise representations of empirical data by minimizing its entropy. We have found that simple gradient descent in various entropy-based objective functions can lead to novel and useful algorithms for unsupervised neural network learning.",
            "group": 340,
            "name": "10.1.1.18.4269",
            "keyword": "",
            "title": "Optimization of Entropy with Neural Networks"
        },
        {
            "abstract": "Existing FPGA-based logic emulators suffer from limited inter-chip communication bandwidth, resulting in low gate utilization (10 to 20 percent). This resource imbalance increases the number of chips needed to emulate a particular logic design and thereby decreases emulation speed, since signals must cross more chip boundaries. Current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). These logical wires are not active simultaneously and are only switched at emulation clock speeds.",
            "group": 341,
            "name": "10.1.1.18.4666",
            "keyword": "FPGAlogic emulationprototypingreconfigurable architecturesstatic routingvirtual wires",
            "title": "Virtual Wires' Overcoming Pin Limitations in FPGA-based Logic Emulators"
        },
        {
            "abstract": "We investigate two ways of solving the correspondence problem for motion using the assumptions of minimal mapping and rigidity. Massively parallel analog networks are designed to implement these theories. Their effectiveness is demonstrated with mathematical proofs and computer simulations. We discuss relevant psychophysicM experiments.",
            "group": 342,
            "name": "10.1.1.18.4810",
            "keyword": "",
            "title": "Massively parallel implementations of theories for apparent motion"
        },
        {
            "abstract": "This paper describes attempts to model the modules of early  vision in terms of minimizing energy functions, in particular energy functions  allowing discontinuities in the solution. It examines the success of using  Hopfield-style analog networks for solving such problems. Finally it discusses  the limitations of the energy function approach.",
            "group": 343,
            "name": "10.1.1.18.5055",
            "keyword": "",
            "title": "Energy Functions for Early Vision and Analog Networks."
        },
        {
            "abstract": "This paper presents a review of advances that have taken place in the mathematical programming approach to process design and synthesis. A review is first presented on the algorithms that are available for solving MINLP problems, and its most recent variant, Generalized Disjunctive Programming models. The formulation of superstructures, models and solution strategies is also discussed for the effective solution of the corresponding optimization problems. The rest of the paper is devoted to reviewing recent mathematical programming models for the synthesis of reactor networks, distillation sequences, heat exchanger networks, mass exchanger networks, utility plants, and total flowsheets. As will be seen from this review, the progress that has been achieved in this area over the last decade is very significant.",
            "group": 344,
            "name": "10.1.1.18.5465",
            "keyword": "",
            "title": "Advances in Mathematical Programming for Automated Design Integration"
        },
        {
            "abstract": "With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in  microprocessor performance. To address this issue, a number of emerging architectures contain replicated  processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp,  SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler  technology that enables a portable, high-level language to execute e#ciently across a range of wireexposed  architectures.",
            "group": 345,
            "name": "10.1.1.18.6271",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "The Methodology to Parallelize Simulated Annealing (MPSA) leads  to massive parallelization by executing each temperature cycle of the Simulated  Annealing (SA) algorithm in parallel. The initial solution for each internal cycle  is set through a Monte Carlo random sampling to adjust the Boltzmann  distribution at the cycle beginning. MPSA uses an asynchronous communication  scheme and any implementation of MPSA leads to a parallel Simulated  Annealing algorithm that is in general faster than its sequential implementation  version while the precision is held. This paper illustrates the advantages of the  MPSA scheme by parallelizing a SA algorithm for the Traveling Salesman  Problem.",
            "group": 346,
            "name": "10.1.1.18.6572",
            "keyword": "Simulated AnnealingCombinatorial OptimizationParallel Algorithmsand Traveling Salesman Problem",
            "title": "MPSA: A Methodology to Parallelize Simulated Annealing and its Application to the Traveling Salesman Problem"
        },
        {
            "abstract": "Deformable inter-subject matching of 3D MR images remains a difficult problem due to the high dimension of both transformations and data. The matching problem is generally expressed as the minimization of a highly non-linear energy function, yielding a hard computationally expensive optimization problem.",
            "group": 347,
            "name": "10.1.1.18.6841",
            "keyword": "parallelism",
            "title": "Parallel Sampling with Stochastic Differential Equations for 3D Deformable Matching of Medical Images"
        },
        {
            "abstract": "This paper introduces an application of GA's to scheduling iterative, or repeating operations. Iterative schedules occur in many tasks including manufacturing, production control and compiling iterative programs. A petri net representation of schedules is introduced that enables the GA to search the space of legal schedules by incorporating resource and dependency constraints",
            "group": 348,
            "name": "10.1.1.18.6919",
            "keyword": "Key Wordssoftware p",
            "title": "Petri Net Representation for Parallel Loop Scheduling Using a . . ."
        },
        {
            "abstract": "The problem of minimizing a multivariate function is recurrent in many  disciplines as Physics, Mathematics, Engeneering and, of course, Computer  Science. Both deterministic and nondeterministic algorithms have been devised  to perform this task. It is common practice to use the nondeterministic  algorithms when the function to be minimized is not smooth or depends on  binary variables, as in the case of combinatorial optimization. In this paper  we describe a simple nondeterministic algorithm which is based on the idea of  adaptive noise, and that proved to be particularly effective in the minimization  of a class of multivariate, continuous valued, smooth functions, associated with  some recent extension of regularization theory by Poggio and Girosi (1990).",
            "group": 349,
            "name": "10.1.1.18.7154",
            "keyword": "",
            "title": "A nondeterministic minimization algorithm"
        },
        {
            "abstract": " ",
            "group": 350,
            "name": "10.1.1.18.7277",
            "keyword": "",
            "title": "Learning and Vision Algorithms for Robot Navigation"
        },
        {
            "abstract": "This paper applies a methodr Genetic algorithm with Search area Adaptation (GSA), to the function optimization. In previous studyr GSA has proposed for the floorplan design problem and it has shown better performance than several existing methods. We believe that investigation of the searching behavior of the algorithm is important. However r since the floorplan design problem is combinatorial optimization problem r we do not know in detail why GSA works well. Thusr in this paper r we apply GSA to the function optimization in order to study the searching behavior in detail. In the function optimizationr several benchmarks have been proposedr and their optima and landscapes are known. There is another purpose to apply GSA to the function optimization. We would like to propose a superior method for the function optimization. Through several experimentsr we have confirmed that GSA works adaptively and it shows higher performance than one of existing methods.",
            "group": 351,
            "name": "10.1.1.18.7384",
            "keyword": "",
            "title": "Genetic Algorithm with Search Area Adaptation for the Function Optimization And Its"
        },
        {
            "abstract": "Introduction  1.1 The Game of Go  Go was developed four millennia ago in China; it is one of the oldest and most popular board games in the world. Like chess, it is a deterministic, perfect information, zero-sum game of strategy between two players. They alternate in placing black and white stones on the intersections of a 19x19 grid (smaller for beginners) with the objective of surrounding more board area (territory) with their stones than the opponent. Adjacent stones of the same color form groups; an empty intersection adjacent to a group is called a liberty of that group. A group is captured and removed from the board when its last liberty is occupied by the opponent. To prevent loops, it is illegal to make certain moves which recreate a prior board position. A player may pass at any time; the game ends when both players pass in succession. Each player's score is then calculated as the territory (number of empty intersections) they have surrounded plus the number of enemy stones th",
            "group": 352,
            "name": "10.1.1.18.7478",
            "keyword": "",
            "title": "Learning To Evaluate Go Positions Via Temporal Difference Methods"
        },
        {
            "abstract": "Learning an input-output mapping from a set of examples, of the type that many  neural networks have been constructed to perform, can be regarded as synthesizing  an approximation of a multi-dimensional function, that is solving the problem of hypersurface  reconstruction. From this point of view, this form of learning is closely  related to classical approximation techniques, such as generalized splines and regularization  theory. This paper considers the problems of an exact representation and, in  more detail, of the approximation of linear and nonlinear mappings in terms of simpler  functions of fewer variables. Kolmogorov's theorem concerning the representation  of functions of several variables in terms of functions of one variable turns out to be  almost irrelevant in the context of networks for learning. Wedevelop a theoretical  framework for approximation based on regularization techniques that leads to a class  of three-layer networks that we call Generalized Radial Basis Functions (GRBF), since  they are mathematically related to the well-known Radial Basis Functions, mainly used  for strict interpolation tasks. GRBF networks are not only equivalent to generalized  splines, but are also closely related to pattern recognition methods suchasParzen  windows and potential functions and to several neural network algorithms, suchas  Kanerva's associative memory,backpropagation and Kohonen's topology preserving  map. They also haveaninteresting interpretation in terms of prototypes that are  synthesized and optimally combined during the learning stage. The paper introduces  several extensions and applications of the technique and discusses intriguing analogies  with neurobiological data.",
            "group": 353,
            "name": "10.1.1.18.7546",
            "keyword": "",
            "title": "A Theory of Networks for Approximation and Learning"
        },
        {
            "abstract": "In this paper we present a knowledge based approach to a speech understanding and dialog system which possesses any--time and real--time capabilities. Knowledge is represented by means of a semantic--network formalism; the concept--centered knowledge base is automatically compiled into a fine--grained network which allows an efficient exploitation of parallelism. The interpretation problem is defined as a combinatorial optimization problem and solved by means of iterative optimization methods. By using iterative methods, any--time capabilities are provided since after each iteration step a (sub--)optimal solution is always available. At the moment, the real--time factor for interpreting an initial user's utterance is 0.7. 1 INTRODUCTION  Automatic recognition of complex patterns, specifically motion sequences and spontaneous speech,become increasingly relevant for a great number of applications, such as service robots to aid handicapped individuals, multi--modal telecooperation tasks, ...",
            "group": 354,
            "name": "10.1.1.18.7558",
            "keyword": "",
            "title": "A Real-Time And Any-Time Approach For A Dialog System"
        },
        {
            "abstract": "Modifiable Boltzmann selective pressure is investigated as a tool to  control vaxiability in optimizations using genetic algorithms. An implementation  of variable selective pressure, modeled agter the use of  temperature as a parameter in simulated annealing approaches, is described.",
            "group": 355,
            "name": "10.1.1.18.7834",
            "keyword": "genetic algorithmssimulated annealingfunction optimization",
            "title": "Boltzmann weighted selection improves performance of genetic algorithms"
        },
        {
            "abstract": "We argue that the modern computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information-processing tasks. In particular, we show that a simple, theory-neutral linguistic model of syntactic agreement and lexical ambiguity demonstrates that natural language parsing may be computationally intractable, extending the classic work of Chomsky and Miller (1963). Significantly, we show that it may be slmtactic features rather than complex rules that can cause this difficulty. Informally, human languages and the computationally intractable satisfiability problem (SAT) share two costly computational mechanisms: both enforce agreement among terminal symbols across unbounded distances and both allow terminal symbol ambiguity. In natural languages, lexical elements may be required to agree (or disagree) on such features as person, number, and gender (e.g., subject/verb agreement in English); in SAT, agreement ensures the consistency of variable truth assignments. Lexical ambiguity can appear freely in natural language utterances (can may be a noun, verb, or auxiliary), while a variable in a SAT formula may be either true or false. When coupled with a deterministic performance model, this complexity result explains a subtle psycholinguistic distinction between discovering and verifying the grammaticality of an utterance. Finally, the applicability of computational complexity analysis to other cognitive faculties such as vision is discussed.",
            "group": 356,
            "name": "10.1.1.18.7954",
            "keyword": "",
            "title": "Computational Consequences of Agreement and Ambiguity in Natural Language"
        },
        {
            "abstract": "With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in  microprocessor performance. To address this issue, a number of emerging architectures contain replicated  processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp,  SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler  technology that enables a portable, high-level language to execute eciently across a range of wireexposed  architectures. In this",
            "group": 357,
            "name": "10.1.1.18.8271",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "The gene expression process in nature produces different proteins in different cells from different portions of the DNA. Since proteins control almost every important activity in a living organism, at an abstract level, gene expression can be viewed as a process that evaluates the merit or \"fitness\" of the DNA. This distributed evaluation of the DNA would not be possible without a decomposed representation of the fitness function defined over the DNAs. This paper argues that, unless the living body was provided with such a representation, we have every reason to believe that it must have an efficient mechanism to construct this distributed representation. This paper demonstrates polynomial-time computability of such a representation by proposing a class of efficient algorithms. The main contribution of this paper is two-fold. On the algorithmic side, it offers a way to scale up evolutionary search by detecting the underlying structure of the search space. On the biological side, it proves that the distributed representation of the evolutionary fitness function in gene expression can be computed in polynomial-time.",
            "group": 358,
            "name": "10.1.1.18.8355",
            "keyword": "representation constructionWalsh analysislinkage learning",
            "title": "Gene Expression and Fast Construction of Distributed Evolutionary Representation"
        },
        {
            "abstract": "A method is presented that uses grouping to improve local classification of image primitives. The grouping process is based upon a spin-glass system, where the image primitives are treated as possessing a spin.",
            "group": 359,
            "name": "10.1.1.18.8588",
            "keyword": "Statistical LearningBayesian Grouping",
            "title": "Grouping in Images: From Local to Multi-Local Classification"
        },
        {
            "abstract": "this paper, we suggest an analog model of computation in electrical or chemical netwo'ks for a I[trge class of vision problems, that maps more easily into biologically plausible mechanisms. Poggio and Torre (1984) have recently recognized that early vision problems such as motion analysis (!-lorn and Schunck, '19S1; Hildreth, 1984a,b), edge detection ('Forre and Poggio, 1984), surface interpolation (Grimson, 1981; Terzopoulos 1984), shape-from-shading (Ikeuchi and Horn, '1981) and stereoms. tching can be characterized as m,.qthematica!ly ill-posed problems in tte sense of Hadamard (1923). Ill-posed problems can be \"solved\", according to re.quttrization theories, by variational principles of a specific type. A natural way of implementing variational problems are electrical, chemical or neuronal networks. We present specific networks for solving several low-level vision problems, SLch as the computation ol visual motion and edge detection",
            "group": 360,
            "name": "10.1.1.18.8591",
            "keyword": "",
            "title": "An Analog Model of Computation for the Ill-Posed Problems of Early Vision"
        },
        {
            "abstract": "Un algorithme de propagation d'intervalles r'esoud de facon approch'ee sur les intervalles des Probl`emes de Satisfaction de Contraintes (CSPs) r'eels. Il met en oeuvre des op'erateurs de cloture, appel'es op'erateurs de narrowing (CNOs), associ'es `a chaque contrainte et utilis'es pour r'eduire les domaines des variables. Un tel algorithme est confluent et les domaines en sortie sont obtenus en it'erant des applications de CNOs, jusqu'`a ne plus pouvoir les r'eduire. L'ordre d'application des CNOs influence notablement le nombre total d'applications de CNOs (N) et le temps de r'esolution du probl`eme (T ). Dans le but de minimiser ces deux param`etres, nous proposons d'impl'ementer un algorithme de propagation d'intervalles bas'e sur des m'ethodes de recherche Tabou. Les r'esultats exp'erimentaux montrent une am'elioration des temps de calcul pour un ensemble de tests propos'es.",
            "group": 361,
            "name": "10.1.1.18.9006",
            "keyword": "",
            "title": "Optimisation De La Propagation D'intervalles par des m\u00e9thodes de recherche locale"
        },
        {
            "abstract": "In this thesis we study constraint relaxations of various nonlinear programming (NLP) algorithms in order to improve their performance. For both stochastic and deterministic algorithms, we study the relationship between the expected time to find a feasible solution and the constraint relaxation level, build an exponential model based on this relationship, and develop a constraint relaxation schedule in such a way that the total time spent to find a feasible solution for all the relaxation levels is of the same order of magnitude as the time spent for finding a solution of similar quality using the last relaxation level alone. When the",
            "group": 362,
            "name": "10.1.1.19.203",
            "keyword": "",
            "title": "Improving Constrained Nonlinear Search Algorithms Through Constraint Relaxation"
        },
        {
            "abstract": "A central task in the study of molecular evolution is the reconstruction of a phylogenetic tree from sequences of current-day taxa. The most established approach to tree reconstruction is maximum likelihood (ML) analysis. Unfortunately, searching for the maximum likelihood phylogenetic tree is computationally prohibitive for large data sets. In this paper, we describe a new algorithm that uses Structural EM for learning maximum likelihood phylogenetic trees.",
            "group": 363,
            "name": "10.1.1.19.491",
            "keyword": "",
            "title": "A Structural EM Algorithm for Phylogenetic Inference"
        },
        {
            "abstract": "A new method is described for optimising graph partitions which arise in mapping unstructured mesh  calculations to parallel computers. The method employs a combination of iterative techniques to both  evenly balance the workload and minimise the number and volume of interprocessor communications. It  is designed to work efficiently in parallel as well as sequentially and can be applied directly to dynamically  refined meshes. In addition, when combined with a fast direct partitioning technique (such as the Greedy  algorithm) to give an initial partition, the resulting two-stage process proves itself to be both a powerful and  flexible solution to the static graph-partitioning problem. A clustering technique can also be employed to  speed up the whole process. Experiments, on graphs with up to a million nodes, indicate that the resulting  code is up to an order of magnitude faster than existing state-of-the-art techniques such as Multilevel  Recursive Spectral Bisection, whilst providing partitions of equivalent quality.",
            "group": 364,
            "name": "10.1.1.19.1531",
            "keyword": "",
            "title": "A Parallelisable Algorithm for Optimising Unstructured Mesh Partitions"
        },
        {
            "abstract": "In this thesis, we present a new theory of discrete constrained optimization using Lagrange multipliers and an associated first-order search procedure (DLM) to solve general constrained optimization problems in discrete, continuous and mixed-integer space. The constrained problems are general in the sense that they do not assume the differentiability or convexity of functions. Our proposed theory and methods are targeted at discrete problems and can be extended to continuous and mixed-integer problems by coding continuous variables using a floating-point representation (discretization). We have characterized the errors incurred due to such discretization and have proved that there exists upper bounds on the errors. Hence, continuous and mixed-integer constrained problems, as well as discrete ones, can be handled by DLM in a unified way with bounded errors.",
            "group": 365,
            "name": "10.1.1.19.1930",
            "keyword": "",
            "title": "The Theory And Applications Of Discrete Constrained Optimization Using Lagrange Multipliers"
        },
        {
            "abstract": "A central task in the study of molecular evolution is the reconstruction of a phylogenetic  tree from sequences of current-day taxa. The most established approach to tree reconstruction  is maximum likelihood (ML) analysis. Unfortunately, searching for the maximum likelihood  phylogenetic tree is computationally prohibitive for large data sets. In this paper, we describe  a new algorithm that uses Structural EM for learning maximum likelihood phylogenetic trees. This algorithm is",
            "group": 366,
            "name": "10.1.1.19.2204",
            "keyword": "",
            "title": "A Structural EM Algorithm for Phylogenetic Inference"
        },
        {
            "abstract": "A central task in the study of molecular evolution is the reconstruction of a phylogenetic tree from sequences of current-day taxa. The most established approach to tree reconstruction is maximum likelihood (ML) analysis . Unfortunately, searching for the maximum likelihood phylogenetic tree is computationally prohibitive for large data sets. In this paper, we describe a new algorithm that uses Structural Expectation Maximization (EM) for learning maximum likelihood phylogenetic trees. This algorithm is similar to the standard EM method for edgelength estimation, except that during iterations of the Structural EM algorithm the topology is improved as well as the edge length. Our algorithm performs iterations of two steps. In the E-step, we use the current tree topology and edge lengths to compute expected sufficient statistics, which summarize the data. In the M-Step, we search for a topology that maximizes the likelihood with respect to these expected sufficient statistics. We show that searching for better topologies inside the M-step can be done efficiently, as opposed to standard methods for topology search. We prove that each iteration of this procedure increases the likelihood of the topology, and thus the procedure must converge. This convergence point, however, can be a suboptimal one. To escape from such \"local optima,\" we further enhance our basic EM procedure by incorporating moves in the flavor of simulated annealing. We evaluate these new algorithms on both synthetic and real sequence data and show that for protein sequences even our basic algorithm finds more plausible trees than existing methods for searching maximum likelihood phylogenies. Furthermore, our algorithms are dramatically faster than such methods, enabling, for the first time, phylogenetic ...",
            "group": 367,
            "name": "10.1.1.19.2329",
            "keyword": "Key wordsphylogenyevolutionary treeinferenceStructural EM",
            "title": "A Structural EM Algorithm for Phylogenetic Inference"
        },
        {
            "abstract": "A common approach to parallelizing simulated annealing to generate several perturbations  to the currentsolutionsimultaneously, requiring synchronization to guarantee correct evaluation  of the cost function. The cost of this synchronization may be reduced by allowing inaccuracies  in the cost calculations. We provide a framework for understanding the theoretical implications  of this approach based on a model of processor interaction under reduced synchronization that  demonstrates how errors in cost calculations occur and how to estimate them. We showhow  bounds on error in the cost calculations in asimulated annealing algorithm can be translated  into worst-case bounds on perturbations in the parameters which describe the behavior of the  algorithm.",
            "group": 368,
            "name": "10.1.1.19.3662",
            "keyword": "",
            "title": "Trading Accuracy for Speed in Parallel Simulated Annealing with Simultaneous Moves"
        },
        {
            "abstract": "In this thesis, we study optimal anytime stochastic search algorithms (SSAs) for solving general constrained nonlinear programming problems (NLPs) in discrete, continuous and mixed-integer space. The algorithms are general in the sense that they do not assume di#erentiability or convexity of functions. Based on the search algorithms, we develop the theory of SSAs and propose optimal SSAs with iterative deepening in order to minimize their expected search time. Based on the optimal SSAs, we then develop optimal anytime SSAs that generate improved solutions as more search time is allowed. Our SSAs",
            "group": 369,
            "name": "10.1.1.19.4017",
            "keyword": "",
            "title": "Optimal Anytime Search For Constrained Nonlinear Programming"
        },
        {
            "abstract": "In this paper we present a Lagrange-multiplier formulation of discrete constrained optimization problems, the associated discrete-space first-order necessary and sufficient conditions for saddle points, and an efficient first-order search procedure that looks for saddle points in discrete space. Our new theory provides a strong mathematical foundation for solving general nonlinear discrete optimization problems. Specifically, we propose a new vector-based definition of descent directions in discrete space and show that the new definition does not obey the rules of calculus in continuous space. Starting from the concept of saddle points and using only vector calculus, we then prove the discrete-space first-order necessary and sufficient conditions for saddle points. Using well-defined transformations on the constraint functions, we further prove that the set of discrete-space saddle points is the same as the set of constrained local minima, leading to the first-order necessary and sufficient conditions for constrained local minima. Based on the first-order conditions, we propose a local-search method to look for saddle points that satisfy the first-order conditions.",
            "group": 370,
            "name": "10.1.1.19.4370",
            "keyword": "",
            "title": "The Theory of Discrete Lagrange Multipliers for Nonlinear Discrete Optimization"
        },
        {
            "abstract": "Clustering in data mining is used to group similar objects based on their distance, connectivity, relative density, or some specific characteristics. The k-medoids-based algorithms have been shown to be effective to spherical-shaped clusters with outliers. However, they are not efficient for large database. In this paper, we propose a novel Multi-Centroids with Multi-Runs Sampling Scheme MCMRS to improve the performance of many k-medoids-based algorithms, including PAM, CLARA and CLARANS. MCMRS is also further improved by combining with the CLASA algorithm presented in earlier work. Experimental results demonstrate the efficiency of the proposed MCMRS and MCMRS-CLASA algorithms.",
            "group": 371,
            "name": "10.1.1.19.4961",
            "keyword": "",
            "title": "Efficient K-medoids Algorithms Using Multi-Centroids With Multi-Runs Sampling Scheme"
        },
        {
            "abstract": "The effectiveness of the memory hierarchy is critical for the performance of current processors. The performance of the memory hierarchy can be improved by means of program transformations such as padding, which is a code transformation targeted to reduce conflict misses. This paper presents a novel approach to perform near-optimal padding for multi-level caches. It analyzes programs, detecting conflict misses by means of the Cache Miss Equations. A genetic algorithm is used to compute the parameter values that enhance the program. Our results show that it can remove practically all conflicts among variables in the SPECfp95, targeting all the different cache levels simultaneously.",
            "group": 372,
            "name": "10.1.1.19.5256",
            "keyword": "",
            "title": "Near-Optimal Padding for Removing Conflict Misses"
        },
        {
            "abstract": "This paper is a first step to formal comparisons of several leading optimization algorithms, establishing guidance to practitioners for when to use or not use a particular method. The focus in this paper is four general algorithm forms: random search, simultaneous perturbation stochastic approximation, simulated annealing, and evolutionary computation. We summarize the available theoretical results on rates of convergence for the four algorithm forms and then use the theoretical results to draw some preliminary conclusions on the relative efficiency. Our aim is to sort out some of the competing claims of efficiency and to suggest a structure for comparison that is more general and transferable than the usual problem-specific numerical studies. Much work remains to be done to generalize and extend the results to problems and algorithms of the type frequently seen in practice.",
            "group": 373,
            "name": "10.1.1.19.5683",
            "keyword": "",
            "title": " Theoretical Comparisons of Evolutionary . . . "
        },
        {
            "abstract": "In this thesis, we develop constrained simulated annealing (CSA), a global optimization algorithm that asymptotically converges to constrained global minima (CGM dn ) with probability one, for solving discrete constrained nonlinear programming problems (NLPs). The algorithm is based on the necessary and sufficient condition for constrained local minima (CLM dn ) in the theory of discrete constrained optimization using Lagrange multipliers developed in our group. The theory proves the equivalence between the set of discrete saddle points and the set of CLM dn, leading to the first-order necessary and sufficient condition for CLM dn. To find",
            "group": 374,
            "name": "10.1.1.19.5691",
            "keyword": "",
            "title": "Global Optimization For Constrained Nonlinear Programming"
        },
        {
            "abstract": "Accurate determination of the input function is essential for absolute quantification of physiological parameters in PET and SPECT imaging but it requires an invasive and tedious procedure of blood sampling that is impractical in clinical studies. We previously proposed a technique that simultaneously estimates kinetic parameters and the input function from the tissue impulse response functions and which, requires only two blood samples. A nonlinear least squares method was used to estimate all the parameters in the impulse response functions and the input function but it fails occasionally due to high noise levels in the data causing an ill-conditioned cost function. This study investigates the feasibility of applying a Monte Carlo method called simulated annealing to estimate kinetic parameters in the impulse response functions and the input function. Time-activity curves of teboroxime, which is very sensitive to changes in the input function, were simulated based on published data obtained from a canine model. The equations describing the tracer kinetics in different regions were minimised simultaneously by simulated annealing and nonlinear least squares. We found that the physiological parameters obtained with simulated annealing are more accurate and the estimated input function more closely resembled the simulated curve. We conclude that simulated annealing reduces bias in the estimation of physiological parameters and determination of the input function.",
            "group": 375,
            "name": "10.1.1.19.5807",
            "keyword": "cardiac perfusion",
            "title": "Numerical Deconvolution by a Monte Carlo Approach with Application to"
        },
        {
            "abstract": "In On-line Analytical Processing (OLAP), one often works with large, highdimensional data sets whose bulk makes for slow processing. One common OLAP approach stores data in chunked multidimensional arrays. Indices into an array are obtained by a normalization process that maps attribute values to integers, often arbitrarily. Typically, the array may be sparse in some areas and have dense clusters of data in others. If certain chunks of the array are sufficiently dense, existing systems store these chunks more efficiently than with the usual sparse representation. This report considers a new...",
            "group": 376,
            "name": "10.1.1.19.5880",
            "keyword": "",
            "title": "Compressing MOLAP Arrays by Attribute-Value Reordering: An Experimental Analysis"
        },
        {
            "abstract": "Splitting jobs among machines often result in improved customer service and reduction in throughput time. Implicit in determining a schedule, there is a lot-sizing decision specifying how jobs are to be split. This research considers the problem of lot-sizing and scheduling jobs with varying processing times, non-common due dates, and sequencedependent set-up times on parallel machines. The objective is to minimize the sum of total tardiness. The system is non-preemptive. Most of the research work performed on parallel machines scheduling does not consider job  splitting. This research proposes a simulated annealing method. Computational simulations indicate that this  procedure can be used to solve large-scale problems of practical size.",
            "group": 377,
            "name": "10.1.1.19.6609",
            "keyword": "SchedulingParallel Machines",
            "title": "Simulated Annealing for Lot Sizing and Scheduling on Parallel Machines With Sequence-Dependent Set-up Times"
        },
        {
            "abstract": "this paper, the authors modify the multilevel algorithms to optimize a cost function based on the aspect ratio. Several variants of the algorithms are tested and shown to provide excellent results",
            "group": 378,
            "name": "10.1.1.19.7235",
            "keyword": "",
            "title": "Multilevel Mesh Partitioning For Optimizing Domain Shape"
        },
        {
            "abstract": "We consider noisy Euclidean traveling salesman problems in the  plane, which are random combinatorial problems with underlying  structure. Gibbs sampling is used to compute average trajectories,  which estimate the underlying structure common to all instances.",
            "group": 379,
            "name": "10.1.1.19.8037",
            "keyword": "",
            "title": "The Noisy Euclidean Traveling Salesman Problem and Learning"
        },
        {
            "abstract": "One of the main questions concerning learning in Multi-Agent Systems is: \"(How) can agents benefit from mutual interaction during the learning process?\". This paper describes the study of an interactive advice-exchange mechanism as a possible way to improve agents' learning performance. The advice-exchange technique, discussed here, uses supervised learning (backpropagation), where reinforcement is not directly coming from the environment but is based on advice given by peers with better performance score (higher confidence), to enhance the performance of a heterogeneous group of Learning Agents (LAs). The LAs are facing similar problems, in an environment where only reinforcement information is available. Each LA applies a different, well known, learning technique: Random Walk (hill-climbing), Simulated Annealing, Evolutionary Algorithms and Q-Learning. The problem used for evaluation is a simplified traffic-control simulation. In the following text the reader can find a description of the traffic simulation and Learning Agents (focused on the advice-exchange mechanism) , a discussion of the first results obtained and suggested techniques to overcome the problems that have been observed. Initial results indicate that advice-exchange can improve learning speed, although \"bad advice\" and/or blind reliance can disturb the learning performance. The use of supervised learning to incorporate advice given from non-expert peers using different learning algorithms, in problems where no supervision information is available, is, to the best of the authors' knowledge, a new concept in the area of Multi-Agent Systems Learning.",
            "group": 380,
            "name": "10.1.1.19.9690",
            "keyword": "",
            "title": "On Learning by Exchanging Advice"
        },
        {
            "abstract": "We investigate the setting in which Monte Carlo methods are used and draw a parallel to the formal setting of statistical inference. In particular, we find that Monte Carlo approximation gives rise to a bias-variance dilemma. We show that it is possible to construct a biased approximation schemes with a lower approximation error than a related unbiased algorithms.",
            "group": 381,
            "name": "10.1.1.20.1050",
            "keyword": "",
            "title": "The Bias-Variance dilemma of the Monte Carlo method"
        },
        {
            "abstract": "We are investigating the problem of removing 3D objects from a heap without  having recourse to object models. Aswe are relying on geometric information  alone, the use of range data is a natural choice. To ensure that we see  opposite patches of the object surfaces, we use up to three range views  from different directions. These views are triangulated using the data points  as vertices. After merging the views, the resulting surface description is  segmented into patches which correspond to object parts. We present a novel  approach to search for grasping opportunities on a selected part. We allow  all combinations of two vertices to be possible contact points for the two  finger gripper. Based on evidence accumulation of local features, a quality  measure is defined for each of these vertex pairs. A discrete optimization  algorithm which is based on Tabu-Search then tries to find several good  grasping configurations. Global constraints which are not part of the objective  function (e.g. collisions) have to be respected.",
            "group": 382,
            "name": "10.1.1.20.1356",
            "keyword": "",
            "title": "Searching for Grasping Opportunities on Unmodeled 3D Objects"
        },
        {
            "abstract": "Over the last decade, significant advances have been made in compilation technology and microprocessor architectures for capitalizing on instruction- level parallelism (ILP). While most of the processors containing superscalar VLIW and SIMD structures that are well matched to the needs and capabilities of the ILP compilers are targeted at embedded applications, the majority of all ILP compilation work has been conducted in the context of general-purpose computing. As a consequence, there currently exists a gap between the compiler community and embedded application developers. We present a quantitative benchmark selection and validation technique to close the gap. It is based on more quantitatively rigorous metrics to accurately measure usefulness and effectiveness of benchmarks. We assemble a collec-  tion of DSP and communication applications written in a high-level language. An experimental selection of benchmarks from the collected applications is conducted and its applicability and usefulness are verified through an application specific system synthesis.",
            "group": 383,
            "name": "10.1.1.20.1376",
            "keyword": "",
            "title": "Quantitative Selection of Media Benchmarks"
        },
        {
            "abstract": "Digital filters with power-of-two or a sum of power-of-two coefficients can be built using simple and fast shift registers instead of slower floating-point multipliers, such a strategy can reduce both the VLSI silicon area and the computational time. Due to the quantization and the nonuniform distribution of the coefficients through their domain, in the case of adaptive filters, classical steepest descent based approaches cannot be successfully applied. Methods for adaptation processes, as in the least mean squares (LMS) error and other related adaptation algorithms, can actually lose their convergence properties. In this brief, we present a customized Tabu Search (TS) adaptive algorithm that works directly on the power-of-two filter coefficients domain, avoiding any rounding process. In particular, we propose TS for a time varying environment, suitable for real time adaptive signal processing. Several experimental results demonstrate the effectiveness of the proposed method.",
            "group": 384,
            "name": "10.1.1.20.2452",
            "keyword": "",
            "title": "Power-of-Two Adaptive Filters Using Tabu Search"
        },
        {
            "abstract": "Monte Carlo methods, such as importance sampling, have become a major tool in Bayesian inference. However, in order to produce an accurate estimate, the sampling distribution is required to be close to the target distribution. Several adaptive importance sampling algorithms, proposed over the last years, attempt to learn a good sampling distribution automatically, but their performance is often unsatisfactory. In addition, a theoretical analysis, which takes into account the computational cost of the sampling algorithms, is still lacking. In this paper, we present a first attempt of such analysis, and we propose some modifications of existing adaptive importance sampling algorithms, which produce significantly more accurate estimates.",
            "group": 385,
            "name": "10.1.1.20.2537",
            "keyword": "nonparametric importance samplingannealed importance sampling",
            "title": "Efficient Nonparametric Importance Sampling for Bayesian Learning"
        },
        {
            "abstract": "This study empirically investigates variations of hill climbing algorithms for training artificial neural networks on the 5-bit parity classification task. The experiments compare the algorithms when they use different combinations of random number distributions, variations in the step size and changes of the neural networks ' initial weight distribution. A hill climbing algorithm which uses inline search is proposed. In most experiments on the 5-bit parity task it performed better than simulated annealing and standard hill climbing. 1 Introduction  Hill climbing is an optimisation algorithm which historically was first used for maximisation tasks. Like gradient descent algorithms in continuous spaces it approaches a local extremum but instead of using the gradient, hill climbing uses random local search to determine the direction and size of each new step. The terminology in the literature is not uniform. In this study hill climbing (HC) is the algorithm presented in Table 1 and hill ...",
            "group": 386,
            "name": "10.1.1.20.3438",
            "keyword": "",
            "title": "A Study On Hill Climbing Algorithms For Neural Network Training"
        },
        {
            "abstract": "The main claim of this paper is that connectionism offers cognitive science a number of excellent  opportunities for turning methodological, theoretical. and meta-theoretica! schisms into powerfnl  integrations--opportunities for forging constructive synergy out of the destructive interference  which plagues the field. The paper begins with an analysis of the rifts in tile field and what  it would take to overcome them. We argue that while connectionism ha,s often contributed to  the deepexLing of these schisms, ]t is nonetheless possible to turn this trend around--possible for  connectionism to play a central role in a unification of cognitive science. Essential o this process  is the development of strong theoretical principles founded (in part) on connectionist computation;  a main goal of this paper is to demonstrate that such principles are indeed within the reach of  a connectionist-grounded theory of cognition. The enterprise rests on a willingness to entertain,  analyze, and extend characterizations of cognitive problems, and hypothesized solutions, which  are deliberately overly simple and general--in order to disco4'er the insights they can offer through  mathematical a.na.lyses which this simplicity and generality are makes possible.",
            "group": 387,
            "name": "10.1.1.20.4352",
            "keyword": "1 egendretramp. colorado. edu",
            "title": "Principles for an Integrated Connectionist/Symbolic Theory of Higher Cognition"
        },
        {
            "abstract": "Monte Carlo methods, such as importance sampling, have become a major tool in Bayesian inference. However, in order to produce an accurate estimate, the sampling distribution is required to be close to the target distribution. Several adaptive importance sampling algorithms, proposed over the last years, attempt to learn a good sampling distribution automatically, but their performance is often unsatisfactory. In addition, a theoretical analysis, which takes into account the computational cost of the sampling algorithms, is still lacking. In this paper, we present a first attempt of such analysis, and we propose some modifications of existing adaptive importance sampling algorithms, which produce significantly more accurate estimates.",
            "group": 388,
            "name": "10.1.1.20.4450",
            "keyword": "adaptive importance sampling",
            "title": "Efficient Nonparametric Importance Sampling for Bayesian Inference"
        },
        {
            "abstract": "Most cost function based clustering or partitioning methods  measure the compactness of groups of data. In contrast to this picture  of a point source in feature space, some data sources are spread out on  a low-dimensional manifold which is embedded in a high dimensional  data space. This property is adequately captured by the criterion of  connectedness which is approximated by graph theoretic partitioning  methods.",
            "group": 389,
            "name": "10.1.1.20.4596",
            "keyword": "",
            "title": "Path Based Pairwise Data Clustering with Application to Texture Segmentation"
        },
        {
            "abstract": "A deterministic annealing algorithm for the design of tiedmixture HMM recognizers is proposed, which reduces the training sensitivity to parameter initialization, automatically smoothes the classification error cost function to allow gradientbased optimization, and seeks better solutions than known techniques. The new approach introduces randomness into the classification rule during the training process, and minimizes the expected error rate while controlling the level of randomness via a constraint on the Shannon entropy. As the entropy constraint is gradually relaxed, the effective cost function converges to the classification error rate and the system becomes a hard (nonrandom) recognizer. Experiments show that the proposed method outperforms design by maximum likelihood reestimation and by generalized probabilistic descent.",
            "group": 390,
            "name": "10.1.1.20.4661",
            "keyword": "",
            "title": "Discriminative Training of Tied-Mixture HMM by Deterministic Annealing"
        },
        {
            "abstract": "A new motion field representation based on the boundary-control vector (BCV) scheme for  video coding is examined in this work. With this scheme, the motion field is characterized by  a set of control vectors and boundary functions. The control vectors are associated with the  center points of blocks to control the overall motion behavior. We use the boundary functions  to specify the continuity of the motion field across adjacentblocks. For BCV-based motion field  estimation, an optimization framework based on the Markov random field model and maximum  aposterior (MAP) criterion is used. The new scheme effectively represents complex motions  such as translation, rotation, zooming and deformation and does not require complex scene  analysis. Compared with MPEG of similar decoded SNR (signal-to-noise ratio) quality, 15-65%  bit rate saving can be achieved in the proposed scheme with a more pleasant visual quality.",
            "group": 391,
            "name": "10.1.1.20.4933",
            "keyword": "motion estimationmotion representationvideo codingboundary-control vector (BCVMarkov",
            "title": "Boundary-Control Vector (BCV) Motion Field Representation and Estimation By Using A Markov Random Field Model"
        },
        {
            "abstract": "The paradigm of simulated annealing is applied to the problem of drawing graphs \"nicely.\" Our algorithm deals with general graphs with straigh-line edges, and employs several simple criteria for the aesthetic quality of the result. The algorithm is flexible, in that the relative weights of the criteria can be changed. For graphs of modest size it produces good results, competitive with those produced by other methods, notably, the \"spring method\" and its variants.",
            "group": 392,
            "name": "10.1.1.20.5663",
            "keyword": "neousF.m [Theory of ComputationMiscellaneous General TermsAlgorithmsTheory Additional Key Words and PhrasesAestheticsgraph drawingsimulated annealing",
            "title": "Drawing Graphs Nicely Using Simulated Annealing"
        },
        {
            "abstract": "This paper presents an Instance Based filter approach called Selection Construction and  Ranking using Attribute Pattern (SCRAP). The SCRAP filter is a conservative filtering  scheme that tries to identify the features that change at the decision boundaries and include  them in the feature sub set. SCRAP does this by performing a sequentially search on the  instance space. SCRAP arranges the instances as neighborhoods, which are pure clusters.",
            "group": 393,
            "name": "10.1.1.20.5878",
            "keyword": "FilterWrapperNeural NetworksDecision TreesPruningNearest Neighbor learnerNaive Bayes",
            "title": "Instance Based Filter for Feature Selection"
        },
        {
            "abstract": "Designing shifts is one of the important stages in the general workforce scheduling  process. In this paper we consider solving the shift design problem by using local  search methods. First we propose a set of move types that give rise to a composite neighborhood  relation. In the move selection process, we make use of the basic prohibition  mechanisms of tabu search. In addition, in order to avoid having to explorate the whole  neighborhood which could be prohibitively large, we evaluate the moves in decreasing order  of their promise to yield some improvement. Furthermore, we propose an algorithm for  generating a good initial solution, which also exploits knowledge about requirements and  shift structure. Experimental results on both real-life and randomly-generated problems  show the advantages of these ingredients. The solver is part of a commercial product and  has shown to work well in practical cases.",
            "group": 394,
            "name": "10.1.1.20.6270",
            "keyword": "",
            "title": "Local Search for Shift Design"
        },
        {
            "abstract": " ",
            "group": 395,
            "name": "10.1.1.20.6426",
            "keyword": "workforce schedulingshift designlocal searchconstraint satisfaction",
            "title": "Rota: A Research Project on Algorithms for Workforce Scheduling and Shift Design Optimization"
        },
        {
            "abstract": "Constraints on downside risk, measured by shortfall probability, expected shortfall etc., lead to optimal asset allocations which differ from the mean-variance optimum. The resulting optimization problem can become quite complex as it exhibits multiple local extrema and discontinuities, in particular if we also introduce constraints restricting the trading variables to integers, constraints on the holding size of assets or on the maximum number of different assets in the portfolio. In such situations classical optimization methods fail to work efficiently and heuristic optimization techniques can be the only way out. The paper shows how a particular optimization heuristic, called threshold accepting, can be successfully used to solve complex portfolio choice problems.",
            "group": 396,
            "name": "10.1.1.20.7255",
            "keyword": "Portfolio OptimizationDownside Risk MeasuresHeuristic Optimization",
            "title": "A Global Optimization Heuristic for Portfolio Choice with VaR and Expected Shortfall"
        },
        {
            "abstract": "This paper presents a formal approach to the evolution of a representation for use in a design process. The approach adopted is based on concepts associated with genetic engineering. An initial set of genes representing elementary building blocks is evolved into a set of complex genes representing targeted building blocks. These targeted building blocks have been evolved because they are more likely to produce designs which exhibit desired characteristics than the commencing elementary building blocks. The targeted building blocks can then be used in a design process. The paper presents a formal evolutionary model of design representations based on genetic algorithms and uses pattern recognition techniques to execute aspects of the genetic engineering. The paper describes how the state space of possible designs changes over time and illustrates the model with an example from the domain of two-dimensional layouts. It concludes with a discussion of style in design. ",
            "group": 397,
            "name": "10.1.1.20.7694",
            "keyword": "",
            "title": "Evolving building blocks for design using genetic engineering: A Formal Approach"
        },
        {
            "abstract": "In this paper we present and analyze new sequential and parallel heuristics to  approximate the Minimum Linear Arrangement problem (MinLA). The heuristics  consist in obtaining a first global solution using Spectral Sequencing and improving  it locally through Simulated Annealing. In order to accelerate the annealing process,  we present a special neighborhood distribution that tends to favor moves with high  probability to be accepted. We show how to make use of this neighborhood to parallelize  the Metropolis stage on distributed memory machines by mapping partitions  of the input graph to processors and performing moves concurrently. The paper  reports the results obtained with this new heuristic when applied to a set of large  graphs, including graphs arising from finite elements methods and graphs arising  from VLSI applications. Compared to other heuristics, the measurements obtained  show that the new heuristic improves the solution quality, decreases the running  time and o#ers an excellent speedup when ran on a commodity network made of  nine personal computers.   ",
            "group": 398,
            "name": "10.1.1.20.7915",
            "keyword": "",
            "title": "Combining Spectral Sequencing and Parallel Simulated Annealing for the MinLA Problem"
        },
        {
            "abstract": ": It is often important to be able to explain the reasoning behind a machine learning algorithm, such as an articial neural network, especially in the medical domain. In this paper we develop a case-based sensitivity analysis method for neural networks. The method uses a trained neural network committee in order to nd both important and unimportant input variables for individual cases. The sensitivity analysis is formulated as combinatorial optimization problem, where the mean eld annealing method is used as a tool for nding good solutions. The approach is tested on a problem from the medical domain; namely the problem of identifying patients suering from acute myocardial infarction in the presence of left bundle branch block. We feel that the case-based sensitivity analysis developed here can be used to understand the complex functioning of a neural network and that the method can be applied on other problems from the medical domain. ",
            "group": 399,
            "name": "10.1.1.20.8196",
            "keyword": "",
            "title": "Case-Based Sensitivity Analysis for Artificial Neural Networks with Applications in Medicine"
        },
        {
            "abstract": "Due to the layout complexity in deep sub-micron technology, integrated circuit blocks are often not rectangular. However, literature on general rectilinear block placement is still quite limited. In this paper, we present approaches for handling the placement for arbitrarily shaped rectilinear blocks, based on a newly developed data structure called B*-trees [1]. Experimental results show that our algorithm achieves optimal or near optimal block placement for benchmarks with multiple shaped blocks.  1 ",
            "group": 400,
            "name": "10.1.1.20.8271",
            "keyword": "",
            "title": "Rectilinear Block Placement Using B*-Trees"
        },
        {
            "abstract": "The complexity and dynamics of the Internet is driving the demand for scalable and effective network control. This paper proposes a collaborative on-line simulation architecture to provide pro-active and automated control functions for networks. The general model includes autonomous on-line simulators which continuously monitor/model the network conditions and execute a search in the parameter state space for better settings of protocol parameters. The protocol parameters are then tuned by the on-line simulation system. In this paper, we describe the building blocks of this architecture and investigate the implementation challenges in the areas of network modeling, on-line simulation and parameter search. We also discuss the applicability of this system and present the simulation and test results of a preliminary implementation.  I. ",
            "group": 401,
            "name": "10.1.1.20.8979",
            "keyword": "",
            "title": "Traffic Management and Network Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": "The timetabling problem consists in fixing a sequence of meetings between teachers and  students in a prefixed period of time (typically a week), satisfying a set of constraints of various  types. A large number of variants of the timetabling problem have been proposed in the  literature, which differ from each other based on the type of institution involved (university  or high school) and the type of constraints. This problem, that has been traditionally considered  in the operational research field, has recently been tackled with techniques belonging  also to artificial intelligence (e.g. genetic algorithms, tabu search, simulated annealing, and  constraint satisfaction). In this paper, we survey the various formulations of the problem,  and the techniques and algorithms used for its solution.  ",
            "group": 402,
            "name": "10.1.1.20.9160",
            "keyword": "TimetablingHeuristicsCombinatorial optimizationScheduling",
            "title": "A Survey of Automated Timetabling"
        },
        {
            "abstract": "An approach is presented for treating discrete optimization problems mapped on  the architecture of the Hopfield neural network. The method constitutes a modification  to the local minima escape (LME) algorithm which has been recently proposed  as a method that uses perturbations in the network's parameter space in order to  escape from local minimum states of the Hofield network. Our approach (LMESA)  adopts this perturbation mechanism but, in addition, introduces randomness in the  selection of the next local minimum state to be visited in a manner analogous with the  case of Simulated Annealing. Experimental results using instances of the Weighted  Maximum Independent Set problem indicate that the proposed method leads to significant  improvement over the conventional LME approach in terms of quality of the  obtained solutions, while requiring a comparable amount of computational effort.  ",
            "group": 403,
            "name": "10.1.1.20.9348",
            "keyword": "",
            "title": "Improved Exploration in Hopfield Network State-Space through Parameter Perturbation Driven by Simulated Annealing"
        },
        {
            "abstract": "This paper addresses the implementation of a Markovian model on a programmable  digital retina. The original contribution is to adapt the coding of the local energy in order  to get compact implementations of deterministic (Iterated Conditional Mode), and stochastic  (Metropolis Dynamics) relaxation algorithms for energy minimization. The logical conciseness  of the procedures, together with the ne-grained parallelism of the retinal architecture, makes  possible the real-time computation of the simulated annealing on a low-power vision system. The  proposed model is applied to the segmentation of moving objects over a static background.  1 ",
            "group": 404,
            "name": "10.1.1.21.757",
            "keyword": "",
            "title": "Markovian-Based Modeling On Programmable Retina"
        },
        {
            "abstract": "The thermal generator maintenance scheduling problem has been tackled by a variety of traditional optimisation techniques over the years. While these methods can give an optimal solution to small scale problems, they are often inefficient and impractical when applied to larger problems. In this paper, we employ a multi-stage approach where the problem is decomposed into smaller sub-problems, each of which can be solved much more efficiently by existing algorithms. After each part of the problem has been solved, the results are then recombined to form the solution to the whole problem. Both tabu search and a memetic algorithm have been observed to produce very good results but they take a significant amount of time to run. In this paper we utilise both techniques to form the basis of a multi-stage approach to solve the thermal generator maintenance scheduling problem. The results will demonstrate that the multistage methodology is just as effective for this problem while achieving a significant reduction in run-time.",
            "group": 405,
            "name": "10.1.1.21.794",
            "keyword": "",
            "title": "A Multi-Stage Approach for the Thermal Generator Maintenance Scheduling Problem"
        },
        {
            "abstract": "We investigate the setting in which Monte Carlo methods are used and draw a parallel to the formal setting of statistical inference. In particular, we find that Monte Carlo approximation gives rise to a bias-variance dilemma. We show that it is possible to construct a biased approximation schemes with a lower approximation error than a related unbiased algorithms.",
            "group": 406,
            "name": "10.1.1.21.962",
            "keyword": "",
            "title": "The Bias-Variance dilemma of the Monte Carlo method"
        },
        {
            "abstract": "We develop a normative theory of interaction---negotiation in particular---among selfinterested  computationally limited agents where computational actions are game theoretically  treated as part of an agent's strategy. We focus on a 2-agent setting where each  agent has an intractable individual problem, and there is a potential gain from pooling  the problems, giving rise to an intractable joint problem. At any time, an agent can  compute to improve its solution to its own problem, its opponent's problem, or the  joint problem. At a deadline the agents then decide whether to implement the joint solution,  and if so, how to divide its value (or cost). We present a fully normative model  for controlling anytime algorithms where each agent has statistical performance profiles  which are optimally conditioned on the problem instance as well as on the path of  results of the algorithm run so far. Using this model, we introduce a solution concept,  which we call deliberation equilibrium. It is the perfect Bayesian equilibrium of the  game where deliberation actions are part of each agent's strategy. The equilibria differ  based on whether the performance profiles are deterministic or stochastic, whether  the deadline is known or not, and whether the proposer is known in advance or not.  We present algorithms for finding the equilibria. Finally, we show that there exist instances  of the deliberation--bargaining problem where no pure strategy equilibria exist  and also instances where the unique equilibrium outcome is not Pareto efficient.  #  A short early version of this paper appeared in the proceedings of the National Conference on Artificial Intelligence (AAAI), Austin, TX, August 2000.  1  Keywords: Automated negotiation, multiagent systems, game theory, resource--boun...",
            "group": 407,
            "name": "10.1.1.21.1307",
            "keyword": "Automated negotiationmultiagent systemsgame theoryresource\u2013bounded reasoning",
            "title": "Bargaining with Limited Computation: Deliberation Equilibrium"
        },
        {
            "abstract": " ",
            "group": 408,
            "name": "10.1.1.21.1523",
            "keyword": "",
            "title": "An Empirical Analysis of Weight-Adaptation Strategies for Neighborhoods of Heuristics"
        },
        {
            "abstract": "The maintenance scheduling problem has been previously tackled by various traditional optimisation techniques. While these methods can give an optimal solution to small scale problems, they are often inefficient when applied to larger scale problems. The memetic algorithm presented here is essentially a genetic algorithm with an element of local search. The effectiveness of the method is tested through its application to real scale problems. ",
            "group": 409,
            "name": "10.1.1.21.1562",
            "keyword": "",
            "title": "A Memetic Algorithm for the Maintenance Scheduling Problem"
        },
        {
            "abstract": "The vehicle routing problem with time windows is a hard combinatorial optimization problem  which has received considerable attention in the last decades. This paper proposes a two-stage  hybrid algorithm for this transportation problem. The algorithm rst minimizes the number of  vehicles using simulated annealing. It then minimizes travel cost using a large neighborhood  search which may relocate a large number of customers. Experimental results demonstrate the  eectiveness of the algorithm which has improved 13 (23%) of the 58 best published solutions to  the Solomon benchmarks, while matching or improving the best solutions in 47 problems (84%).  More important perhaps, the algorithm is shown to be very robust. With a xed conguration  of its parameters, it returns either the best published solutions (or improvements thereof) or  solutions very close in quality on all Solomon benchmarks. Results on the extended Solomon  benchmarks are also given.  1 ",
            "group": 410,
            "name": "10.1.1.21.1628",
            "keyword": "",
            "title": "A Two-Stage Hybrid Local Search for the Vehicle Routing Problem with Time Windows"
        },
        {
            "abstract": "This paper describes a simulated annealing algorithm to compute k-connected graphs that minimize a linear combination of graph edge-cost and diameter. Replicas of Internet information services can use graphs with these properties to propagate updates among themselves. We report the algorithm's performance on 2- and 3connected, 50-node graphs, where edge-cost and diameter correspond to physical distance in a plane. For these graphs, the algorithm finds solutions whose edge-cost and diameter are 10% and 70% lower than our graph construction algorithm that returns feasible but unoptimized solutions. When optimizing edge-cost only, the resulting graphs show 25 and 30% reductions edge-cost and diameter, respectively. 1 Introduction  This paper describes a simulated annealing algorithm to construct low diameter, low edge-cost, 2- and 3-connected graphs. We need graphs with these properties to propagate updates between replicas of Internet information systems [2]. For maintaining replicas acr...",
            "group": 411,
            "name": "10.1.1.21.1994",
            "keyword": "",
            "title": "Finding Low-Diameter, Low Edge-Cost, Networks"
        },
        {
            "abstract": "We advocate a new methodology for empirically analysing the behaviour of Las  Vegas Algorithms, a large class of probabilistic algorithms comprising prominent  methods such as local search algorithms for SAT and CSPs, like WalkSAT and the  Min-Conflicts Heuristic, as well as more general metaheuristics like Genetic Algorithms,  Simulated Annealing, Iterated Local Search, and Ant Colony Optimization.  Our method is based on measuring and analysing run-time distributions (RTDs) for  individual problem instances. We discuss this empirical methodology and its application  to Las Vegas Algorithms for various problem domains. Our experience so far  strongly suggests that using this approach for studying the behaviour of Las Vegas  Algorithms can provide a basis for improving the understanding of these algorithms  and thus facilitate further successes in their development and application. ",
            "group": 412,
            "name": "10.1.1.21.2133",
            "keyword": "Tabu Search [2Simulated Annealing [11Genetic Algorithms [3Evolution Strategies [1415Ant Colony Optimisation [1or Iterated Local Sea",
            "title": "On the Empirical Evaluation of Las Vegas Algorithms "
        },
        {
            "abstract": "It has recently been shown that local search is surprisingly good at finding satisfying assignments for certain classes of CNF formulas [24]. In this paper we demonstrate that the power of local search for satisfiability testing can be further enhanced by employinga new strategy, called \"mixed random walk\", for escaping from local minima. We present experimental results showing how this strategy allows us to handle formulas that are substantially larger than those that can be solved with basic local search. We also present a detailed comparison of our random walk strategy with simulated annealing. Our results show that mixed random walk is the superior strategy on several classes of computationally difficult problem instances. Finally, we present results demonstrating the effectiveness of local search with walk for solving circuit synthesis and diagnosis problems.",
            "group": 413,
            "name": "10.1.1.21.2207",
            "keyword": "",
            "title": "Local Search Strategies for Satisfiability Testing"
        },
        {
            "abstract": "We describe two systems: GATE (General Architecture  for Text Engineering), an architecture to aid in the production and delivery  of language engineering systems which significantly reduces development time and ease of  reuse in such systems. We also describe a  sense tagger which we implemented within the  GATE architecture, and which achieves high accuracy (92% of all words in text to a broad  semantic level). We used the implementation of the sense tagger as a real-world task on  which to evaluate the usefulness of the GATE architecture and identified strengths and weaknesses in the architecture.",
            "group": 414,
            "name": "10.1.1.21.2560",
            "keyword": "",
            "title": "  Implementing a Sense Tagger in a General Architecture for Text Engineering  "
        },
        {
            "abstract": "We had a problem to be solved: the thermal generator maintenance scheduling problem [Yam82]. We wanted to look at stochastic methods and this paper will present three methods and discuss the pros and cons of each. We will also present evidence that strongly suggests that for this problem, tabu search was the most effective and efficient technique. The problem is concerned with scheduling essential maintenance over a fixed length repeated planning horizon for a number of thermal generator units while minimising the maintenance costs and providing enough capacity to meet the anticipated demand. Traditional optimisation based techniques such as integer programming [DM75], dynamic programming [ZQ75, YSY83] and branch-and-bound [EDM76] have been proposed to solve this problem. For small problems these methods give an exact optimal solution. However, as the size of the problem increases, the size of the solution space increases exponentially and hence also the running time of these algorithm...",
            "group": 415,
            "name": "10.1.1.21.2648",
            "keyword": "",
            "title": "Four Methods for Maintenance Scheduling"
        },
        {
            "abstract": " Assume that some objects are present in an image but can be seen only partially and are overlapping each other. To recognize the objects, we have to firstly separate the objects from one another, and then match them against the modeled objects using partial observation. This paper presents a probabilistic approach for solving this problem. Firstly, the task is formulated as a two-stage optimal estimation process. The first stage, matching, separates different objects and finds feature correspondences between the scene and each potential model object. The second stage, recognition, resolves inconsistencies among the results of matching to different objects and identifies object categories. Both the matching and recognition are formulated in terms of the maximum a posteriori (MAP) principle. Secondly, contextual constraints, which play an important role in solving the problem, are incorporated in the probabilistic formulation. Specifically, between-object constraints are encoded in the prior distribution modeled as a Markov random field, and withinobject constraints are encoded in the likelihood distribution modeled as a Gaussian. They are combined into the posterior distribution which defines the MAP solution. Experimental results are presented for matching and recognizing jigsaw objects under partial occlusion, rotation, translation and scaling.  ",
            "group": 416,
            "name": "10.1.1.21.2940",
            "keyword": "",
            "title": "A Two-Stage Probabilistic Approach For Object Recognition"
        },
        {
            "abstract": "In this work we investigate the effects of the parallelization of a local search algorithm for MAX-SAT. The variables of the problem are divided in subsets and local search is applied to each of them in parallel, supposing that variables belonging to other subsets remain unchanged. We show empirical evidence for the existence of a critical level of parallelism which leads to the best performance. This result allows to improve local search and adds new elements to the investigation of criticality and parallelism in combinatorial optimization problems. ",
            "group": 417,
            "name": "10.1.1.21.3276",
            "keyword": "",
            "title": "Critical Parallelization of Local Search for MAX-SAT"
        },
        {
            "abstract": "This paper describes how Fractal Coding Theory may be applied to compress video images using an image resampling sequencer (IRS) in a video compression system on a modular image processing system. The first part of the paper describes the background theory of image coding using a form of fractal equation known as Iterated Function System (IFS) codes. The second part deals with the modular image processing system on which to implement these operations. The third part briefly covers how IFS codes may be calculated. Finally, how the IRS and 2  nd  order geometric transformations may be used to describe inter-frame changes to compress motion video.  Appendix E The Use of Fractal Theory in a Video Compression System 261 ",
            "group": 418,
            "name": "10.1.1.21.4234",
            "keyword": "",
            "title": "Appendix E The Use of Fractal Theory in a Video Compression System"
        },
        {
            "abstract": "Since the beginning of AI, mind games have been studied as relevant  application fields. Nowadays, some programs are better than human  players in most classical games. Their results highlight the efficiency of  AI methods that are now quite standard. Such methods are very useful  to Go programs, but they do not enable a strong Go program to be built.  The problems related to Computer Go require new AI problem solving  methods. Given the great number of problems and the diversity of  possible solutions, Computer Go is an attractive research domain for  AI. Prospective methods of programming the game of Go will probably  be of interest in other domains as well. The goal of this paper is to  present Computer Go by showing the links between existing studies on  Computer Go and different AI related domains: evaluation function,  heuristic search, machine learning, automatic knowledge generation,  mathematical morphology and cognitive science. In addition, this paper  describes both the practical aspects of Go programming, such as  program optimization, and various theoretical aspects such as  combinatorial game theory, mathematical morphology, and MonteCarlo  methods.  B. Bouzy T. Cazenave page 2 08/06/01  1. ",
            "group": 419,
            "name": "10.1.1.21.4871",
            "keyword": "",
            "title": "Computer Go: an AI Oriented Survey"
        },
        {
            "abstract": "Standard graphics systems encode pictures by assigning an address and colour attribute for each point of the object resulting in a long list of addresses and attributes. Fractal geometry enables a newer class of geometrical shapes to be used to encode whole objects, thus image compression is achieved. Compression ratios of 10,000:1 have been claimed by researchers  1  in this field. The fractal equations describing these shapes are very simple equations. Specifically, iterated function system (IFS)  codes are investigated. The difficult inverse problem of finding a suitable IFS code whose fractal image is to represent the real image and hence achieve compression is investigated through the use of: a) a library of IFS codes and complex moments,  b) the method of simulated annealing, for solving non-linear equations of many parameters. Image Compression  Image compression is reducing the number of bits required to represent an image in such a way that either an exact replica of the image (lossless compression) or an approximate replica (lossy  compression) of the image can be retrieved. 1  M.F. BARNSLEY, A.D. SLOAN, \"A better way to compress images\", BYTE, Jan 1988, p.215-223.  Proc. 1  st  Seminar on Information Technology and its Applications (ITA `91), Markfield Conf. Centre, Leicester, U.K., 29 Sept., 1991. 1  Canonical Representation of Digital Images  A digital picture consists of an n  m array of integer numbers or picture elements (pels), see Fig.1. n m  pixels pixels Fig.1 Canonical Representation of Digital Images. If it takes B bits to encode each pel, then: n  m  B bits are required to represent the picture digitally. Thus for a 512 512 raster with 8 bits/pel: 512 512 8 = 2,097,152 bits. (A large number!) Reasons for Compressing Images  1) To reduce the speed...",
            "group": 420,
            "name": "10.1.1.21.5160",
            "keyword": "",
            "title": "Fractal Image Compression"
        },
        {
            "abstract": "In this thesis we study constraint relaxations of various nonlinear programming (NLP) algorithms in order to improve their performance. For both stochastic and deterministic algorithms, we study the relationship between the expected time to find a feasible solution and the constraint relaxation level, build an exponential model based on this relationship, and develop a constraint relaxation schedule in such a way that the total time spent to find a feasible solution for all the relaxation levels is of the same order of magnitude as the time spent for finding a solution of similar quality using the last relaxation level alone. When the objective and constraint functions are stochastic, we define new criteria of constraint satisfaction and similar constraint relaxation schedules. Similar to the case when functions are deterministic, we build an exponential model between the expected time to find a feasible solution and the associated constraint relaxation level. We develop an anytime constraint relaxation schedule in such a way that the total time spent to solve a problem for all constraint relaxation levels is of the same order of magnitude as the time spent for finding a feasible solution using the last relaxation level alone. Finally, we study the asymptotic behavior of our new algorithms and prove their asymptotic convergence, based on the theory of asymptotic convergence in simulated annealing and constrained simulated annealing. iii  To my wife and my parents. iv  Acknowledgments First, I would like to thank my research advisor, Professor Benjamin Wah, for his guidance, encouragement and valuable ideas in this work. He introduced me to this area and inspired me all the time. I would like to thank all the member in my research group for their ideas and comments in ...",
            "group": 421,
            "name": "10.1.1.21.5197",
            "keyword": "",
            "title": "Improving Constrained Nonlinear Search Algorithms Through Constraint Relaxation"
        },
        {
            "abstract": "Domain-independent planning is a hard combinatorial problem. Taking into account plan quality makes the task even more difficult. This article introduces Planning by Rewriting (PbR), a new paradigm for efficient high-quality domain-independent planning. PbR exploits declarative plan-rewriting rules and efficient local search techniques to transform an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition to addressing the issues of planning efficiency and plan quality, this framework offers a new anytime planning algorithm. We have implemented this planner and applied it to several existing domains. The experimental results show that the PbR approach provides significant savings in planning effort while generating high-quality plans.",
            "group": 422,
            "name": "10.1.1.21.6222",
            "keyword": "",
            "title": "Planning by Rewriting"
        },
        {
            "abstract": "A Simulated Annealing method is presented for the solution of nonlinear time series estimation problems, by maximization of the a Posteriori Likelihood function. Homogeneous temperature annealing is proposed for smoothing problems and inhomogeneous temperature annealing for filtering problems. Both methods of annealing guarantee convergence to the Maximum A Posteriori Likelihood (MAP) estimate. Entropy change with temperature provides a heuristic evaluation of speed of convergence. 1",
            "group": 423,
            "name": "10.1.1.21.6788",
            "keyword": "",
            "title": "Some Properties of Nonlinear MAP Estimation by Simulated Annealing"
        },
        {
            "abstract": "We describe the Trianus software system which consists of a  suite of tightly integrated tools for the efficient design and implementation  of algorithms using a custom computing machine. The software is  built upon a generic framework for FPGA circuit design and comprises  a compiler for the Lola hardware description language, a layout editor, a  circuit checker, a technology mapper, a placer, a router, and a bit-stream  generator and loader for the Xilinx XC6200 architecture. We argue that  a tight coupling of design tools provides a base for fast iterative and  interactive circuit design, a feature which current systems provide only  in a very limited form.  1 ",
            "group": 424,
            "name": "10.1.1.21.7239",
            "keyword": "",
            "title": "The Trianus System and its Application to Custom Computing"
        },
        {
            "abstract": "We propose a new optimization paradigm for solving intractable combinatorial problems. The technique, named Probabilistic Constructive (PC), combines the advantages of both constructive and probabilistic algorithms. The Constructive aspect provides relatively short runtime and makes the technique amenable for inclusion of insights through heuristic rules. The probabilistic nature facilitates a flexible trade-off between runtime and the quality of solution, and super imposition of a variety of control strategies and solution selection mechanisms.  In addition to presenting the generic technique, we apply it to two generic NP-complete problems (Maximal Independent Set, Graph Coloring) and a synthesis problem (Scheduling). Extensive experimentation indicates that the new approach provides very attractive trade-offs between the quality of the solution and runtime, often outperforming the best previously published approaches.  1. ",
            "group": 425,
            "name": "10.1.1.21.7977",
            "keyword": "",
            "title": "A Probabilistic Constructive Approach To Optimization Problems"
        },
        {
            "abstract": "A growing body of work demonstrates that syntactic structure can evolve in populations of genetically identical agents. Traditional explanations for the emergence of syntactic structure employ an argument based on genetic evolution: syntactic structure is specified by an innate Language Acquisition Device (LAD). Knowledge of language is complex, yet the data available to the language learner is sparse. This incongruous situation, termed the \"poverty of the stimulus\", is accounted for by placing much of the specification of language in the LAD. The assumption is that the characteristic structure of language is somehow coded genetically. The effect of language evolution on the cultural substrate, in the absence of genetic change, is not addressed by this explanation. We show that the poverty of the stimulus introduces a pressure for compositional language structure when we consider language evolution resulting from iterated observational learning. We use a mathematical model to map the space of parameters that result in compositional syntax. Our hypothesis is that compositional syntax cannot be explained by understanding the LAD alone: compositionality is an emergent property of the dynamics resulting from sparse language exposure.",
            "group": 426,
            "name": "10.1.1.21.8867",
            "keyword": "LanguageEvolutionSyntaxLearningCompression",
            "title": "Compositional Syntax From Cultural Transmission"
        },
        {
            "abstract": "This paper presents a novel approach for registering two sets of 3-D range data points, using an optimization algorithm that is both robust and efficient. The algorithm combines the speed of an iterative closest point algorithm with the robustness of a simulated annealing algorithm. Additionally, a robust error function is incorporated to deal with outliers. Index terms: Registration, range data, simulated annealing, iterative closest point, optimization, model construction. 1  This work was supported by a grant from Sandia National Laboratory.  Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence Luck, Hoff, Underwood, Little 2  1 ",
            "group": 427,
            "name": "10.1.1.21.9432",
            "keyword": "",
            "title": "Registration of Range Data Using a Hybrid Simulated Annealing and Iterative Closest Point Algorithm"
        },
        {
            "abstract": "This paper introduces Multiple Ant Colony Systems, an optimization algorithm  based on the Ant Colony System. The performance of this new  algorithm is compared with the performance of a Greedy Algorithm and  Simulated Annealing on a new optimization problem called the Busstop Allocation  Problem (BAP). In the BAP, the goal is to construct a set of buslines  (sequences of busstops) so that the average travel time of all passengers is  minimized. Results show that the new algorithm outperforms the Greedy  Algorithm and Simulated Annealing, which indeed makes it a promising  acquisition to the range of existing Ant Algorithms.  1 ",
            "group": 428,
            "name": "10.1.1.21.9587",
            "keyword": "",
            "title": "Multiple Ant Colony Systems for the Busstop Allocation Problem"
        },
        {
            "abstract": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean--field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images.  ",
            "group": 429,
            "name": "10.1.1.22.403",
            "keyword": "",
            "title": "Pairwise Data Clustering by Deterministic Annealing"
        },
        {
            "abstract": "The objective of this project was to develop and  test a software system for the kinematic analysis  of total joint arthroplasty (TJA) implants. Using a supervised iterative optimization algorithm (simulated annealing), the system iteratively adjusts the pose of an implant model to maximize correlation between the model's pose and the pose of an actual implant in a x-ray image. A graphical user interface (GUI) was developed that provides visualization of the model-fitting process, and permits human guidance for error correction. The resulting system was evaluated to determine its accuracy and repeatability. On synthetic images, the mean translational and rotational errors were found to be 0.005 mm and 0.0015 degrees, respectively.  On in vitro images, the repeatability was found  to be 0.15 mm of translation and 0.17 degrees of  rotation. The outcome is a working jointmeasurement software system and quantitative data to support comparison with other methods.  1 ",
            "group": 430,
            "name": "10.1.1.22.926",
            "keyword": "arthroplastyimplantoptimizationsoftwarepolyethylene",
            "title": "An Interactive System For Kinematic Analysis Of Artificial Joint Implants"
        },
        {
            "abstract": "In this paper, we revisit the convergence properties of the iteration process  x  k+1  = x  k  \\Gamma ff(x  k  )B(x  k  )  \\Gamma1  rf(x  k  ) for minimizing a function f(x). After reviewing some classic results and introducing the notion of strong attraction, we give necessary and sufficient conditions for a stationary point of f(x) to be a point of strong attraction for the iteration process. This result not only gives a new algorithmic interpretation to the classic Ostrowski theorem, but also provides insight into the interesting phenomenon called selective minimization. We also present illustrative numerical examples for nonlinear least squares problems. Keywords: Attraction, repulsion, selective minimization 1. ",
            "group": 431,
            "name": "10.1.1.22.1095",
            "keyword": "Attractionrepulsionselective minimization",
            "title": "On Convergence of Minimization Methods: Attraction, Repulsion and Selection"
        },
        {
            "abstract": "In this paper, we present a framework where a learning rule can be optimized  within a parametric learning rule space. We define what we call  parametric learning rules and present a theoretical study of their generalization   properties when estimated from a set of learning tasks and tested over  another set of tasks. We corroborate the results of this study with practical  experiments.   ",
            "group": 432,
            "name": "10.1.1.22.1135",
            "keyword": "",
            "title": "On the Search for New Learning Rules for ANNs"
        },
        {
            "abstract": "This paper presents a methodology to solve the Just-in-Time (JIT) sequencing problem for multiple product scenarios when set-ups between products are required. Problems of this type are combinatorial, and complete enumeration of all possible solutions is computationally prohibitive. Therefore, Genetic Algorithms are often employed to nd desirable, although not necessarily optimal, solutions. This research, through experimentation, shows that Genetic Algorithms provide formidable solutions to the multi-product JIT sequencing problem with set-ups. The results also compare favourably to those found using the search techniques of Tabu Search and Simulated Annealing.  1. Introduction  The bene ts of successful Just-in-Time (JIT) production systems such as reduced inventory levels, shorter lead times, and improved responsiveness are well documented. One important part of successful JIT implementation , production scheduling, is the focus of this research. Speci cally, this re",
            "group": 433,
            "name": "10.1.1.22.1283",
            "keyword": "",
            "title": "Patrick R."
        },
        {
            "abstract": "Simulated annealing placement algorithms which use minimum wire length metrics based on rectilinear approximations fail to accurately account for an FPGA's routing resources since the number of logic block interconnections could be limited, causing certain placements to rely on resources which may not exist. In this paper we present a simulated annealing-based placement algorithm which performs a simple but effective route after each swap. We will show that, on wire-constrained FPGAs, our algorithm is a better evaluator for a given placement than the faster wire length metrics. In particular, on the Triptych 3-input RLB 4 \\Theta 16 array the algorithm achieves final delays ranging from 3.5% to 21.5% faster than delays yielded by a cost function tailored for the architecture. In addition, the algorithm demonstrates its adaptability by producing even better results on the most recent variant of the Triptych architecture. Finally, we will show that our method can be implemented without an...",
            "group": 434,
            "name": "10.1.1.22.1948",
            "keyword": "",
            "title": "Simultaneous Place and Route for Wire-Constrained FPGAs"
        },
        {
            "abstract": "This paper demonstrates how Simulated Annealing can be used to obtain line balancing solutions when one or more objectives are important. The experimental results showed that Simulated Annealing approaches yielded signi cantly better solutions on cycle time performance but average solutions on cost performance. When cycle time performance and total unit cost are weighted equally, performance rankings showed that Simulated Annealing approaches still showed better mean performance than the other approaches.  1. Introduction and literature review  1.1. Background  Assembly line balancing is an area of research that has received relatively little attention in recent years. With the number of simplifying assumptions that most traditional line balancing approaches make, it is unsurprising that production managers today are often reluctant to use these old approaches. Modern production environments are often fast paced and exible, and an increasing number of co",
            "group": 435,
            "name": "10.1.1.22.3316",
            "keyword": "",
            "title": "Using Simulated Annealing to Solve a Multiobjective Assembly Line Balancing Problem With Parallel Workstations"
        },
        {
            "abstract": "In this paper we investigate the design of a coarse-grained  parallel implementation of Cga-LK, a hybrid heuristic for the Traveling  Salesman Problem (TSP). Cga-LK exploits a compact genetic algorithm  in order to generate high-quality tours which are then refined by  means of an e#cient implementation of the Lin-Kernighan local search  heuristic. The results of several experiments conducted on a cluster  of workstations with di#erent TSP instances show the e#cacy of the  parallelism exploitation.  Keywords: Parallel algorithms, TSP, compact genetic algorithm, LinKernighan  algorithm, hybrid GA.  1 ",
            "group": 436,
            "name": "10.1.1.22.3427",
            "keyword": "",
            "title": "A Parallel Hybrid Heuristic for the TSP"
        },
        {
            "abstract": "The Sequence Pair (SP), which was introduced by Murata [1], appears to be an efficient and powerful structure for general module placement representation. Unfortunately, most of the known algorithms that apply to the sequence pair still have O(M  2  ) complexity or worse, which renders the algorithms often impractical for large problem instances. In this paper we propose a method to improve the average run-time complexity of sequence-pair operations using the recently introduced DV algorithm [2]. Additionally, we reduce the computational overhead of some basic sequence-pair operations, exploiting symmetry properties or precomputed information. All operations are embedded in a Very Fast Simulated Annealing algorithm, which is described in detail. The new approach, which has significantly lower complexity components of order M  1:25  , allows faster convergence of the annealing algorithm, substantially improving over previous results. Keywords--- Sequence Pair, floorplanning, placement, layout, simulated annealing, perturbation  I. ",
            "group": 437,
            "name": "10.1.1.22.3957",
            "keyword": "floorplanningplacementlayoutsimulated annealingperturbation",
            "title": "A More Efficient Sequence Pair Perturbation Scheme"
        },
        {
            "abstract": "The performance of several network protocols can be significantly enhanced by tuning their parameters.  The optimization of network protocol parameters can be modeled as a \"black-box\" optimization  problem with unknown, multi-modal and noisy objective functions. In this paper, a recursive random  search algorithm is proposed to address this type of optimization problems. The new algorithm takes advantage  of the favorable statistical properties of random sampling and achieves high efficiency without  imposing extra restrictions, e.g., differentiability, on the objective function. It is also robust to noise in  the evaluation of objective function since it use no traditional noise-susceptible local search techniques.  The proposed algorithm is tested on classical benchmark functions and its performance compared with a  multi-start hillclimbing algorithm. The algorithm is also integrated with a new on-line simulation system  which attempts to automate network management by tuning protocol parameters when network conditions  change significantly. We present the application of this on-line simulation system in enhancing the  performance of network protocols, such as, RED and OSPF.  1  1 ",
            "group": 438,
            "name": "10.1.1.22.4016",
            "keyword": "",
            "title": "A Recursive Random Search Algorithm for Optimizing Network Protocol Parameters"
        },
        {
            "abstract": "The typical planning, design or operations problem has multiple objectives and constraints. Such problems can be solved using only autonomous agents, each specializing in a small and distinct subset of the overall objectives and constraints. No centralized control is necessary. Instead, agents collaborate by observing and modifying one another's work. Convergence to good solutions for a variety of real and academic problems has been obtained by embedding a few simple rules in each agent. The paper develops these rules and illustrates their use. ",
            "group": 439,
            "name": "10.1.1.22.4329",
            "keyword": "A-TeamsAgentAsynchronousAutonomousCollaborationConstraintCooperationNonlinearOptimizationSpecialized",
            "title": "A Collaboration Strategy for Autonomous, Highly Specialized Agents"
        },
        {
            "abstract": "The NP-hard graph bisection problem is to partition the nodes of an undirected graph into two equal-sized groups so as to minimize the number of edges that cross the partition. The more general graph l-partition problem is to partition the nodes of an undirected graph into l equal-sized groups so as to minimize the total number of edges that cross between groups.  We present a simple, linear-time algorithm for the graph l-partition problem and analyze it on a random \"planted l-partition\" model. In this model, the n nodes of a graph are partitioned into l groups, each of size n=l; two nodes in the same group are connected by an edge with some probability p, and two nodes in different groups are connected by an edge with some probability r ! p. We show that if p \\Gamma r  n  \\Gamma1=2+ffl  for some constant ffl, then the algorithm finds the optimal partition with probability 1 \\Gamma exp(\\Gamman  \\Theta(ffl)  ).  1 ",
            "group": 440,
            "name": "10.1.1.22.4340",
            "keyword": "",
            "title": "Algorithms for Graph Partitioning on the Planted Partition Model"
        },
        {
            "abstract": "A flurry of modern communications, video, and DSP applications, such as world wide web, interactive high resolution television, video-on-demand, video conferencing, and wireless communications, are mainly data management oriented. In this type of systems, magnetic disk performance is quickly becoming a primary bottleneck which dictates key design metrics of the overall system. At the same time, technology trends imply reduced importance of classical design objectives, such as area and throughput, and together with consumers demand for portability promote power consumption as the principal design metric. While numerous power optimization techniques have been proposed at all levels of design process abstractions for electronic components, until now, power minimization in mixed mechanical-electronic subsystems, such as disks, has not been addressed. We first analyze optimization degrees of freedom for power minimization in disk-based application specific systems. Next, we propose a concep...",
            "group": 441,
            "name": "10.1.1.22.4908",
            "keyword": "",
            "title": "Power Optimization in Disk-Based Real-Time Application Specific Systems"
        },
        {
            "abstract": "FDA (the Factorized Distribution Algorithm) is an evolutionary algorithm that combines mutation and recombination by using a distribution. The distribution is estimated from a set of selected points. It is then used to generate new points for the next generation. FDA uses a factorization to be able to compute the distribution in polynomial time. Previously, we have shown a convergence theorem for FDA . But it is only valid using Boltzmann selection. Boltzmann selection was not used in practice because a good annealing schedule was lacking. Using a Taylor expansion of the average fitness of the Boltzmann distribution, we have developed an adaptive annealing schedule called SDS . The inverse temperature  is changed inversely proportional to the standard deviation. In this work, we compare the resulting scheme to truncation selection both theoretically and experimentally with a series of test functions. We find that it behaves similar in terms of complexity, robustness and efficiency.  Keywords: genetic algorithms, Boltzmann distribution, Boltzmann selection, truncation selection  1 ",
            "group": 442,
            "name": "10.1.1.22.5413",
            "keyword": "genetic algorithmsBoltzmann distributionBoltzmann selectiontruncation selection",
            "title": "Comparing the adaptive Boltzmann selection schedule SDS to truncation selection"
        },
        {
            "abstract": "Recognizing and locating objects is crucial to robotic operations in unstructured environments. To satisfy this need, we have developed an interactive system for creating object models from range data based on simulated annealing and supervisory control  a  . This interactive modeling system maximizes the advantages of both manual and autonomous methods while minimizing their weaknesses . Therefore, it should outperform purely autonomous or manual techniques. We have designed and executed experiments for the purpose of evaluating the  performance of our application as compared to an identical but purely manually driven application. These experiments confirmed the following hypotheses:  . Interactive modeling should outperform purely manual modeling in total task time and fitting accuracy.  . Operator effort decreases significantly when utilizing interactive modeling.  . User expertise does not have a significant effect on interactive modeling task time.  . Minimal human interaction will increase performance on \"easy\" scenes.  Using 14 subjects and 8 synthetically generated scenes, we recorded the task times and pose data and, from them, used analysis of variance (ANOVA) to test a set of hypotheses.  a  This project has been funded by the INEL University Research Consortium. The INEL is managed by Lockheed Martin Idaho Technologies Company for the U.S. Department of Energy, Idaho Operations Office, under Contract No. DE-AC07-94ID13223  II. ",
            "group": 443,
            "name": "10.1.1.22.5764",
            "keyword": "",
            "title": "Evaluation Of An Interactive Technique For Creating Site Models From"
        },
        {
            "abstract": "Markovian approaches to early vision processes need a huge...",
            "group": 444,
            "name": "10.1.1.22.5777",
            "keyword": "",
            "title": "Image Segmentation Using Markov Random Field Model in Fully Parallel . . ."
        },
        {
            "abstract": "In this paper, we present a basic model for describing hard- and software of heterogeneous multi-DSP systems. The software part consists of a task model that has been tuned to suit typical, cyclic DSP applications. Hardware is described by a processor model adapted to multi-DSP systems with point-to-point communication structure, which also includes an exact model of communication timing behavior. Additionally a special cost function allows to rate each mapping of tasks onto processors. By using the heuristic optimization algorithm Simulated Annealing, different optimization goals can be achieved here. Several examples shows the suitability and applicability or our model to these multi-DSP systems.",
            "group": 445,
            "name": "10.1.1.22.5949",
            "keyword": "",
            "title": "A Model for Scheduling and Mapping DSP Applications onto Multi-DSP Platforms"
        },
        {
            "abstract": " ",
            "group": 446,
            "name": "10.1.1.22.5950",
            "keyword": "",
            "title": "Statistical Physics of Clustering Algorithms"
        },
        {
            "abstract": ". Simulated annealing is an algorithm which generates near-optimal outcomes to combinatorial optimization problems. It is commonly thought to be slow. Cost-function approximation and parallel processing increase simulated annealing speed, but they can cause inaccuracies that degrade the outcome. Prior theoretical work has not adequately related cost-function inaccuracy to the run-time or quality of the outcome. We prove these results about annealing with inaccurate cost-functions: 1) Expected cost at equilibrium is exponentially affected by fl=T , where fl limits cost-function rangeerrors and T gives the temperature. 2) Expected cost at equilibrium is exponentially affected by (oe  2  \\Gamma oe  2  )=2T  2  , when the errors have a Gaussian distribution. 3) Constraining  fl to a constant factor of T guarantees convergence under a 1= log t temperature schedule. 4) A similar constraint guarantees convergence for a fractal space with a geometric temperature schedule. 5) Inaccuracies worse...",
            "group": 447,
            "name": "10.1.1.22.6062",
            "keyword": "General TermsAlgorithmsperformance. Additional Key Words and PhrasesParallel simulated annealingGaussian noisecombinatorial optimizationstatistical mechanicsBoltzmann machineconductancemixing speed",
            "title": "Simulated Annealing with Inaccurate Cost Functions"
        },
        {
            "abstract": "This paper describes the application of stochastic search techniques to the production scheduling of a group of linked oil and gas fields. The goal was the maximisation of total net present value and a genetic algorithm using problem-specific crossover operators was particularly successful in this respect. Introduction  Reservoir engineers must decide upon a \"best\" management strategy for the exploitation of each petroleum resource, typically the strategy which will maximise economic return. In particular, a production schedule, specifying rates of extraction over the lifetime of the reservoir(s), must be chosen. From this, decisions about the construction of processing facilities and pipelines will follow. A quantitative approach to the search for the \"best\" production schedule requires the construction of a mathematical model, which must capture enough characteristics of the reservoir(s) to predict the costs and benefits of any given schedule. Even when a single independent field is ...",
            "group": 448,
            "name": "10.1.1.22.6163",
            "keyword": "",
            "title": "SPE 35518 Optimisation of Production Strategies using Stochastic Search Methods"
        },
        {
            "abstract": "This paper investigates a form of algorithm hybridization that combines constraint satisfaction algorithms with local search algorithms. On one hand, constraint satisfaction algorithms are effective at finding feasible solutions for tightly constrained problems with complex constraints. On the other hand, local search algorithms are usually better at finding solutions that are good with respect to an optimization function when the problem is loosely constrained, or has simple constraints that can be embedded into neighbourhood operators. The tight hybridization of these two algorithm classes may lead to more powerful algorithms for certain classes of large scale combinatorial optimization problems (LSCOs). A new hybrid algorithm, local probing, is introduced for resource constrained scheduling where the aim is to optimize an objective function while satisfying both resource and temporal constraints. The problem is solved in a backtracking branch-and-bound tree. In each node of the tree, a relaxed temporal sub-problem is created by removing resource constraints, and a \"prober\" which, in this paper, consists of a local search algorithm with a dynamic neighbourhood operator generates a partial solution that satisfies the remaining constraints and gives a good value to the objective function. Possible resource violations are solved in the backtracking search tree by posting (or removing when backtracking) temporal precedence constraints which are guaranteed to be satisfied by the neighbourhood operator. For the local search prober, we introduce dynamic neighbourhood operators that satisfy the constraints of the temporal sub-problem using techniques based on limited discrepancy search and linear programming. Experiments are carried out on a class of resourc...",
            "group": 449,
            "name": "10.1.1.22.6218",
            "keyword": "",
            "title": "Local Probing for Resource Constrained Scheduling"
        },
        {
            "abstract": "The local search with orthogonal design of experiment in its neighborhood determination (ODLS) outperforms the local search with the conventional neighborhood when the objective function includes noise. This models practical optimization problems that contains uncontrolledorunobserved  variables. ODLS is robust and efficient since it shares all evaluations for direction determinationofeach variable. We illustrate the characteristics and demonstrate its performanceinsimple  quadratic function plus random noise, and discuss their improvedparallel processing capability from the conventional local search.  Keywords: Orthogonal Experimental Design, Local Search  1 ",
            "group": 450,
            "name": "10.1.1.22.6278",
            "keyword": "Orthogonal Experimental DesignLocal Search",
            "title": "Local Search Using Orthogonal Design of Experiment"
        },
        {
            "abstract": "class for SeedMakerBase. Generell functionality to store a solution and the instance. // Derived classes must provide MakeSeed which is providing a new SeedSolution. // ==================================================================================================== class SeedMakerBase -- public: SeedMakerBase (MinWOutlierProblem &Inst) : Inst(Inst) -- SolData = NULL; ~SeedMakerBase () -- MinWOutlierProblem &Instance () -- return Inst;  virtual void MakeSeed (MixedSolution &SeedSol) = 0; // Start the creation of the seeds void CollectData (MixedSolution &Sol) -- if (SolData != NULL) delete SolData; SolData = new MixedSolution (Sol);  void CleanSolution (MixedSolution &Solution); // Empty a solution protected: int FindAvailableCluster(MixedSolution &Solution); private: MixedSolution *SolData; MinWOutlierProblem &Inst; // Local storage of the reference for the problem ; // ==================================================================================================== // Fill a given solution with random seeds. // ==================================================================================================== class RandomSeedsMaker : public SeedMakerBase -- public: RandomSeedsMaker (MinWOutlierProblem& Inst, int a); ~RandomSeedsMaker () -- void MakeSeed (MixedSolution &SeedSol); // Start the seed making void SetRandomSeed (int a) -- RandomSeed = a; // Set the seed for the random number stream void SetSeedsize (int a) -- // Set the seed size #ifdef DEBUG assert (a ? Instance().X().p()); #endif SeedSize = a;  private: int SeedSize; long RandomSeed; ; // ==================================================================================================== // Generates just one Seed in the solution vector // ===========================================================...",
            "group": 451,
            "name": "10.1.1.22.7736",
            "keyword": "",
            "title": "C.2 LANDSAT Imaging Project"
        },
        {
            "abstract": "The importance of benchmarking as an essential methodology in evaluating and comparing digital systems synthesis tools is well established. Equally well are the limitations of the majority of current benchmarks recognized. Our primary goal is to lay out the theoretically and statistically sound foundations for addressing the key issues related to the selection of generic benchmarks and the synthesis and analysis of statistical synthetic benchmarks. The methodology for the synthetic benchmark generation uses as an intermediate step the methodology for the generic benchmark selection.  The benefits of our methodology for the generic benchmark selection are: (1) it provides the sound statistical foundations for the generic benchmark selection, (2) it establishes the fact that the development of a generic benchmark is a well-defined optimization problem. The advantages of the statistical synthetic benchmarks over the generic benchmarks include: (1) having an unlimited supply of synthetic design examples, (2) obtaining the real-life like design examples which suit a particular design or analysis need, (3) having a test set which better fits all available real-life design examples.  Due to the fact that the synthetic benchmark generation is based on the generic benchmark selection, the entire process of the generic and synthetic benchmark development is in sequence. It consists of the following five-stages: (1) collecting and analyzing real life design examples, (2) extracting and selecting the relevant properties that are used to specify designs, (3) deriving a generic benchmark set from the real life design examples, (4) developing the synthetic benchmark generator, and (5) constructing a synthetic benchmark set for the current applications or the future applications based ...",
            "group": 452,
            "name": "10.1.1.22.7920",
            "keyword": "1",
            "title": "Design Methodology for Development of Behavioral Synthesis Generic and Synthetic Benchmarks"
        },
        {
            "abstract": "In the above paper1 Chen et al. investigated the capability of uniformly approximating functions in C(Rn) by standard feedforward neural networks. They found that the boundedness condition on the sigmoidal function plays an essential role in the approximation, and conjectured that the boundedness of the sigmoidal function is a necessary and sufficient condition for the validity of the approximation theorem. However, we find that the conjecture is not correct, that is, the boundedness condition is not sufficient or necessary in C(Rn). Instead, boundedness and unequal limits at infinities conditions on the activation functions are sufficient, but not necessary in C(Rn). Index Terms---Activation functions, approximation capability, boundedness, feedforward networks.",
            "group": 453,
            "name": "10.1.1.22.8227",
            "keyword": "",
            "title": "Comments on \u0093Approximation Capability in C(Rn) by Multilayer Feedforward Networks and Related Problems\u0094"
        },
        {
            "abstract": "The need to register data is abundant in applications such as: world modeling, part inspection and manufacturing, object recognition, pose estimation, robotic navigation, and reverse engineering. Registration occurs by aligning the regions that are common to multiple images. The largest difficulty in performing this registration is dealing with outliers and local minima while remaining efficient. A commonly used technique, iterative closest point, is efficient but is unable to deal with outliers or avoid local minima. Another commonly used optimization algorithm, simulated annealing, is effective at dealing with local minima but is very slow. Therefore, the algorithm developed in this paper is a hybrid algorithm that combines the speed of iterative closest point with the robustness of simulated annealing. Additionally, a robust error function is incorporated to deal with outliers. This algorithm is incorporated into a complete modeling system that inputs two sets of range data, registers the sets, and outputs a composite model. ",
            "group": 454,
            "name": "10.1.1.22.9486",
            "keyword": "",
            "title": "Registration of Range Data Using a Hybrid Simulated Annealing and Iterative Closest Point Algorithm"
        },
        {
            "abstract": "this article we're going to spoil your fun by figuring out what the maximum number of heads is for any starting size, and devising a strategy for reaching that number. Before reading on, you might want to try solving the 10-coin puzzle (Blet-10) on your own. (You may prefer using a 2-color counter instead of coins. Or you can use pencil and paper. An electronic version, with 28 \"coins\" labeled 0 or 1, is available at http://www.ma.utexas.edu/users/voloch/blet.html).  2. Matrices and polygonal paths  It's inconvenient to work with circular sequences, so we will pick a starting point, once and for all. Our configuration is then a word w in two symbols H and T , such as the example  w = HTHHTTTH . (1)  If the k-1-st, k-th and k+1-st letters are THT , we can convert them to HTH. We call this a \"type-I\" move, ",
            "group": 455,
            "name": "10.1.1.22.9881",
            "keyword": "",
            "title": "Blet: A Mathematical Puzzle"
        },
        {
            "abstract": "Most of digital synthesis tasks are known to belong to NP-complexity class. Practical tasks often have hidden properties which allow to reduce originally enormous solution space. Combining calculus power of computers and designer 's experience it is possible to #nd near-optimal solution to synthesis problem within restricted time limit.  Two of VLSI synthesis subtasks are considered in the thesis. The #rst part of the thesis is devoted to HW#SW codesign, based on one and half year studies and research period at the Department of Electronic System Design of Royal Institute of Technology, Sweden. An overview of contemporary codesign techniques is provided and development of one research codesign toolkit AKKA is considered and evaluated. Main emphasis is on multi-level simulation methodology and elaboration of prototyping architectures on Xilinx FPGA chips. Also, future vision of improvements of the toolkit properties is given.  The second part of the thesis is devoted to one of register-transfer level synthesis subtask - the controller decomposition using iterative methods. This work was done with several interrupts in Tallinn Technical University, partially in Darmstadt Technical University, during the years 1984-1994. First, the method using of bulk data sorting for partitioning and clustering of FSM descriptions is investigated. Second, stochastic optimization algorithms like simulated annealing and Kohonen's self-organization method are elaborated and evaluated. This part is strongly in#uenced by the partition pair algebra.  The following information is presented in the appendix : the de#nitions of used textual formats, table of used Logic Synthesis FSM benchmarks and examples of FSM synthesis from VHDL.  v  Res#umee  Valdavalt on digitaals#usteemide s#unteesi # ulesa...",
            "group": 456,
            "name": "10.1.1.23.644",
            "keyword": "",
            "title": "Control Intensive Digital System Synthesis"
        },
        {
            "abstract": "FPGA-based ASIC development systems have become important tools in contemporary ASIC design. Existing systems exhibit low per-FPGA gate utilization (10 to 20 percent) due to limited inter-chip communication. Attempts at overcoming this limitation through the use of high dimensional interconnection topologies have met with limited success.  This paper focuses on the prototype hardware and software interfaces that have been developed for an FPGA-based ASIC emulation system based on a new technique for overcoming inter-chip communication limitations. This technique, referred to as virtual wires, intelligently multiplexes each physical FPGA wire among a number of logical wires. The Virtual Wires Emulation System exhibits high FPGA gate utilization while achieving system speeds comparable to existing logic emulators. A two-dimensional mesh interconnection topology of FPGAs is used to eliminate the cost of signal switching elements and to facilitate scalability.  A system capable of emulating 20,000 gates has been constructed for under $3000. This system includes both prototype emulation hardware and a Virtual Wires netlist compiler. Currently, this system is being used as both a simulation accelerator for the LSI Logic LSIM and Cadence Verilog simulators and as an in-circuit emulator. Results from mapping netlists, such as the 18K gate Sparcle microprocessor [2], to this system for simulation acceleration and in-circuit emulation indicate that virtual wires can substantially increase FPGA utilization without adversely affecting emulation speed.   ",
            "group": 457,
            "name": "10.1.1.23.1577",
            "keyword": "FPGAlogic emulationprototypingmesh topologystatic routingvirtual wires",
            "title": "The Virtual Wires Emulation System: A Gate-Efficient ASIC Prototyping Environment"
        },
        {
            "abstract": "In this work we introduce the multiagent metaphor as a framework for metaheuristic algorithms. The first step is the definition of agents which search on a fitness landscape. In this framework it is possible to implement the classical metaheuristic algorithms and introduce cooperative search in a well-defined way. The second step extends the model in a multi-level system, where agents at each level have different computational capabilities and tasks: solution construction, solution improvement, extracting solutions building blocks, analyzing regions of the search space and perform meta-reasoning on the behavior of lower level agents. We propose this perspective with the aim to achieve a better and clearer understanding of metaheuristics, obtain new algorithms and suggest directions for a software engineering-oriented implementation.",
            "group": 458,
            "name": "10.1.1.23.1869",
            "keyword": "metaheuristicscombinatorial optimizationtness landscapeagents Contents",
            "title": "Metaheuristics: a Multiagent Perspective"
        },
        {
            "abstract": "We investigate a topological design and routing problem for Low-Earth Orbit (LEO) satellite communication networks where each satellite can have a limited number of direct inter-satellite links (ISL's) to a subset of satellites within its line-of-sight. First, we model LEO satellite network as a FSA (Finite State Automaton) using satellite constellation information. Second, we solve a combined topological design and routing problem for each configuration corresponding to a state in the FSA. The topological design (or link assignment) problem deals with the selection of ISL's, and the routing problem handles the traffic distribution over the selected links to maximize the number of carried calls. In this paper, this NP-complete mixed integer optimization problem is solved by a two-step heuristic algorithm that first solves the topological design problem, and then finds the optimal routing. The algorithm is iterated using the simulated annealing technique until the near-optimal solution ...",
            "group": 459,
            "name": "10.1.1.23.3755",
            "keyword": "LEO satellite networktopological designroutingsimulated annealing",
            "title": "Topological Design and Routing for Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "Parallel systems are often employed for time-critical applications. This concerns especially the area of digital signal processing, where real-time constraints have to be met. In this paper, we present a tool for mapping and scheduling tasks of real-time applications onto parallel hardware systems. The tool consists of (i) a flexible model for the application software and the parallel hardware system and (ii) an optimization solver based on the Simulated Annealing algorithm. The model describes real-time applications, parallel processor hardware and communication structures with strong emphasis on applicability. Hardware related system characteristics, like direct memory access mechanisms or hardware-buffered links, are included in the model. A cost function is used to control the optimization process for different objectives. Several examples show the suitability and applicability of our tool to technical systems.  1 ",
            "group": 460,
            "name": "10.1.1.23.3961",
            "keyword": "",
            "title": "A Flexible Tool for Mapping and Scheduling Real-Time Applications onto Parallel Systems"
        },
        {
            "abstract": "FDA (the Factorized Distribution Algorithm) is an evolutionary algorithm that combines mutation and recombination by using a distribution. The distribution is estimated from a set of selected points. It is then used to generate new points for the next generation. In general a distribution defined for n binary variables has 2^n  parameters. Therefore it is too expensive to compute. For additively decomposed discrete functions (ADFs) there exists an algorithm that factors the distribution into conditional and marginal distributions, each of which can be computed in polynomial time. Previously, we have shown a convergence theorem for FDA . But it is only valid using Boltzmann selection. Boltzmann selection was not used in practice because a good annealing schedule was lacking. Using a Taylor expansion of the average fitness of the Boltzmann distribution, we have developed an adaptive annealing schedule called SDS (standard deviation schedule) that is introduced in this work. The inverse temperature   is changed inversely proportional to the standard deviation. ",
            "group": 461,
            "name": "10.1.1.23.4104",
            "keyword": "genetic algorithmssimulated annealingBoltzmann distributionBoltzmann selection",
            "title": "A New Adaptive Boltzmann Selection Schedule SDS"
        },
        {
            "abstract": "Deep dyslexia is an acquired reading disorder marked by the occurrence of semantic errors (e.g., reading RIVER as \"ocean\"). In addition, patients exhibit a number of other symptoms, including visual and morphological effects in their errors, a part-of-speech effect, and an advantage for concrete over abstract words. Deep dyslexia poses a distinct challenge for cognitive neuropsychology because there is little understanding of why such a variety of symptoms should co-occur in virtually all known patients. Hinton and Shallice (1991) replicated the co-occurrence of visual and semantic errors by lesioning a recurrent connectionist network trained to map from orthography to semantics. While the success of their simulations is encouraging, there is little understanding of what underlying principles are responsible for them. In this paper we evaluate and, where possible, improve on the most important design decisions made by Hinton and Shallice, relating to the task, the network architecture, the training procedure, and the testing procedure. We identify four properties of networks that underly their ability to reproduce the deep dyslexic symptom-complex: distributed orthographic and semantic representations, gradient descent learning, attractors for word meanings, and greater richness of concrete vs. abstract semantics. The first three of these are general connectionist principles and the last is based on earlier theorizing. Taken together, the results demonstrate the usefulness of a connectionist approach to understanding deep dyslexia in particular, and the viability of connectionist neuropsychology in general.  ",
            "group": 462,
            "name": "10.1.1.23.4257",
            "keyword": "",
            "title": "Deep Dyslexia: A Case Study of Connectionist Neuropsychology"
        },
        {
            "abstract": "Scheduling is probably the most often addressed optimization problem in DSP compilation, behavioral synthesis, and system-level synthesis research. With the rapid pace of changes in modern DSP applications requirements and implementation technologies, however, new types of scheduling challenges arise. This paper is concerned with the problem of scheduling blocks of computations in order to optimize the efficiency of their execution on programmable embedded systems under a realistic timing model of their processors. We describe an effective scheme for scheduling the blocks of any computation on a given system architecture and with a specified algorithm implementing each block. We also present algorithmic techniques for performing optimal block scheduling simultaneously with optimal architecture and algorithm selection. Our techniques address the block scheduling problem for both single- and multiple-processor system platforms and for a variety of optimization objectives including throug...",
            "group": 463,
            "name": "10.1.1.23.4566",
            "keyword": "",
            "title": "Efficient Block Scheduling for Programmable Embedded Processors"
        },
        {
            "abstract": "AND KEYWORDS PAGE  Title: A Voxel Based Representation for Evolutionary Shape  Optimisation  Abstract  A voxel-based shape representation when integrated with an evolutionary algorithm offers a number of potential advantage for shape optimisation. Topology need not be predefined, geometric constraints are easily imposed and, with adequate resolution, any shape can be approximated to arbitrary accuracy. However, lack of boundary smoothness, length of chromosome and inclusion of small holes in the final shape have been stated as problems with this representation. This paper describes two experiments performed in an attempt to address some of these problems. Firstly, a design problem with only a small computational cost of evaluating candidate shapes was used as a testbed for designing genetic operators for this shape representation. Secondly, these operators were refined for a design problem using a more costly finite element evaluation. It was concluded that the voxel representation can, with careful design of genetic operators, be useful in shape optimisation.  Keywords: shape optimisation, evolutionary algorithms, voxel representation.  1. ",
            "group": 464,
            "name": "10.1.1.23.4604",
            "keyword": "shape optimisationevolutionary algorithmsvoxel representation",
            "title": "A Voxel Based Representation for Evolutionary Shape Optimisation"
        },
        {
            "abstract": "This paper studies the problem of automatically selecting a suitable system architecture for implementing a real-time application. Given a library of hardware components, it is shown how an architecture can be synthesized with the goal of fulfilling the real-time constraints stated in the system's specification. In case the selected architecture contains several processing units, the specification is partitioned by assigning tasks to these. The use of three heuristic search techniques is investigated: genetic algorithms, simulated annealing, and tabu search; and it is described how these can be adapted to the architecture synthesis problem. It is concluded that tabu search is the most promising technique, but that simulated annealing is also applicable.",
            "group": 465,
            "name": "10.1.1.23.4844",
            "keyword": "",
            "title": "Architecture Synthesis and Partitioning of Real-Time Systems: A Comparison of Three Heuristic Search Strategies"
        },
        {
            "abstract": "Free-form deformations (FFDs) are a popular tool for modeling and keyframe animation. This paper extends the use of FFDs to a dynamic setting. Our goal is to enable normally inanimate graphics objects, such as teapots and tables, to become animated, and learn to move about in a charming, cartoon-like manner. To achieve this goal, we implement a system that can transform a wide class of objects into dynamic characters. Our formulation is based on parameterized hierarchical FFDs augmented with Lagrangian dynamics, and provides an efficient way to animate and control the simulated characters. Objects are assigned mass distributions and elastic deformation properties, which allow them to translate, rotate, and deform according to internal and external forces. In addition, we implement an automated optimization process that searches for suitable control strategies. The primary contributions of the work are threefold. First, we formulate a dynamic generalization of conventional, geometric FFDs. The formulation employs deformation modes which are tailored by the user and are expressed in terms of FFDs. Second, the formulation accommodates a hierarchy of dynamic FFDs that can be used to model local as well as global deformations. Third, the deformation modes can be active, thereby producing locomotion. Index Terms---Physically based animation, free-form deformations, control synthesis, deformation models, Lagrangian dynamics.",
            "group": 466,
            "name": "10.1.1.23.4993",
            "keyword": "",
            "title": "Dynamic Free-Form Deformations for Animation Synthesis"
        },
        {
            "abstract": "Since the beginning of AI, mind games have been studied as relevant  application fields. Nowadays, some programs are better than human  players in most classical games. Their results highlight the efficiency of  AI methods that are now quite standard. Such methods are very useful  to Go programs, but they do not enable a strong Go program to be built. The problems",
            "group": 467,
            "name": "10.1.1.23.5093",
            "keyword": "",
            "title": "Computer Go: an AI Oriented Survey"
        },
        {
            "abstract": "This technical contribution is discussed in detail in Section 6.1",
            "group": 468,
            "name": "10.1.1.23.5888",
            "keyword": "",
            "title": "A Generic Software Framework For Finite Domain Constraint Programming"
        },
        {
            "abstract": "After more than a decade of research, there now exist several neural-network techniques for solving NP-hard combinatorial optimization problems. Hopfield networks and selforganizing maps are the two main categories into which most of the approaches can be divided. Criticism of these approaches includes the tendency of the Hopfield network to produce infeasible solutions, and the lack of generalizability of the self-organizing approaches (being only applicable to Euclidean problems). This paper proposes two new techniques which have overcome these pitfalls: a Hopfield network which enables feasibility of the solutions to be ensured and improved solution quality through escape from local minima, and a self-organizing neural network which generalizes to solve a broad class of combinatorial optimization problems. Two sample practical optimization problems from Australian industry are then used to test the performances of the neural techniques against more traditional heuristic solutions.",
            "group": 469,
            "name": "10.1.1.23.6373",
            "keyword": "traveling",
            "title": "Neural Techniques for Combinatorial Optimization with Applications"
        },
        {
            "abstract": "Design of modern portable application specific systems is often defined by their demand for low power, flexibility, cost sensitivity, and by their DSP nature. Therefore, for economic realization of portable applications effective low power compilation and architectural methodologies are of prime importance. In this paper we introduce an approach for power minimization using a set of compilation and architectural techniques. The key technical innovation is a compilation technique for minimization of the number of operations. We have developed a new divide and conquer technique which combines and coordinates optimization and enabling effects of several transformations so that the number of operations in each logical partition is minimized. On a set of benchmark examples the number of operations is reduced by average of 42.3%, which results in reduction in power consumption by factor of 3.48 on single processor platform with no cost overhead. Even further power reduction, more than an ord...",
            "group": 470,
            "name": "10.1.1.23.6525",
            "keyword": "",
            "title": "Power Optimization using Minimization of the Number of Operations"
        },
        {
            "abstract": "We describe a linear-time algorithm that recovers absolute camera positions for networks of thousands of terrestrial images spanning hundreds of meters, in outdoor urban scenes, under varying lighting conditions. The algorithm requires no human input or interaction. It is robust to up to 80% outliers for synthetic data. For real data, it recovers camera pose which is globally consistent on average to roughly 0.1 #  and five centimeters, or about four pixels of epipolar alignment, expending a few CPUhours of computation on a 250MHz processor. This paper's principal contributions include an extension of Monte Carlo Markov Chain estimation techniques to the case of unknown numbers of feature points, unknown occlusion and deocclusion, and large scale (thousands of images, and hundreds of thousands of point features) and dimensional extent (tens of meters of inter-camera baseline, and hundreds of meters of baseline overall). Also, a principled method is given to manage uncertainty on the sphere of directions; a new use of the Hough Transform is proposed; and a method for aggregating local baseline constraints into a globally consistent constraint set is described. The algorithm takes intrinsic calibration information, and a connected, rotationally registered image network as input. It then assembles local, purely translational motion estimates into a global constraint set, and determines camera positions with respect to a single scene-wide coordinate system. The algorithm's output is an assignment of metric, accurate 6-DOF camera pose, along with its uncertainty, to every image. We assume that the scene exhibits local point features for probabilistic matching, and that adjacent cameras observe overlapping portions of the scene; no further assumptions are made about scene str...",
            "group": 471,
            "name": "10.1.1.23.7177",
            "keyword": "",
            "title": "Scalable, Absolute Position Recovery for Omni-Directional Image Networks"
        },
        {
            "abstract": "Advances in storage and high-speed broadband communication network technologies are making it feasible to design and implement distributed scalable Video-onDemand server networks. In this paper, a hierarchical structure for a server network is proposed to support a metropolitan TV-Anytime service. A key issue, how to map the media assets onto such a hierarchical server network, is addressed in this paper. The definition and heuristic methods for the solution of this new combinatorial optimization problem are presented. The mapping problem studied here addresses the management of media assets taking the storage capacity, communication bandwidth limitation and access pattern into account. The methods presented in this paper are based on parallel simulated annealing and are verified by the use of a set of benchmark instances. The results show that the methods provide a near to optimal performance in terms of the Quality of Service (QoS).",
            "group": 472,
            "name": "10.1.1.23.7538",
            "keyword": "",
            "title": "Solving a Media Mapping Problem in a Hierarchical Server Network with Parallel Simulated Annealing"
        },
        {
            "abstract": "In this paper, we present a basic model for describing hard- and software of heterogeneous multi-DSP systems. The software part consists of a task model that has been tuned to suit typical, cyclic DSP applications. Hardware is described by a processor model adapted to multi-DSP systems with point-to-point communication structure, which also includes an exact model of communication timing behavior. Additionally a special cost function allows to rate each mapping of tasks onto processors. By using the heuristic optimization algorithm Simulated Annealing, different optimization goals can be achieved here. Several examples shows the suitability and applicability or our model to these multi-DSP systems.",
            "group": 473,
            "name": "10.1.1.23.7697",
            "keyword": "",
            "title": "A Model for Scheduling and Mapping DSP Applications onto Multi-DSP Platforms"
        },
        {
            "abstract": "Effective elastic properties of the graphite-epoxy unidirectional fiber composite with \ffibers randomly distributed within the transverse plane section are found. To enhance the efficiency of numerical analysis the complicated real microstructure is replaced by a material representative volume element consisting of a small number of particles, which statistically resembles the real microstructure. First, various statistical descriptors suitable for the microstructure characterization of a\r\nrandom media are considered. Several methods for the determination of these descriptors are proposed and tested for some simple theoretical models of microstructures. Moreover, a validity of various statistical hypotheses usually accepted for a random heterogenous media is checked for the present material as well. Successively, the unit cell is derived from the optimization procedure formulated in terms of these statistical descriptors. A variety of deterministic as well as stochastic optimization\r\nalgorithms is examined to solve this problem. It is shown that for this particular application stochastic method based on genetic algorithm combined with the simulated annealing method is superior to other approaches. Finally, the estimates of elastic properties are found for the resultant unit cells using the Finite Element Method. Results suggest that the proposed approach, which effectively exploits the knowledge of material's statistics of the composite, is more reliable then various averaging techniques or simple unit cell models based on the regular \ffiber distribution.",
            "group": 474,
            "name": "10.1.1.23.7699",
            "keyword": "",
            "title": "Analysis of mechanical properties of fiber-reinforced composites with random microstructure"
        },
        {
            "abstract": "We propose a novel methodology for designing fault-tolerant real-time system to achieve optimal productivity on a single-chip multi-processor platform using the heterogeneous built-in-self-repair(BISR) based graceful degradation and yield enhancement technique as an embedded optimization engine which exploits task-level scheduling and algorithm selection flexibility. We also developed a hardware fault model for modern superscalar processors and multi-processors which enables an efficient treatment of the synthesis and compilation goals. The extensive experimental results indicate that several orders of magnitude improvement in productivity can be achieved by the method. For the first time heterogeneous BISR is used at the task level. The key idea is to adapt scheduling and algorithm selection to the available non-faulty resources. If there is a fault in memory, the algorithms that use less memory are selected and the scheduler exploits the other abundant resource, namely the processors...",
            "group": 475,
            "name": "10.1.1.23.8785",
            "keyword": "",
            "title": "Heterogeneous BISR-approach using System Level Synthesis Flexibility"
        },
        {
            "abstract": "The complex and dynamic feature of the Internet requires scalable and effective network control. In this paper, a collaborative on-line simulation scheme is proposed to provide the automated and pro-active control functions for networks. This scheme introduces autonomous on-line simulators into local networks, which continuously monitor the surrounding network conditions, collect the relevant information, communicate with other simulators and execute collaborative online simulation. Based on the simulation results, the on-line simulators keep tuning the network parameters to the better operation point to fit the current network conditions. In this paper, we describe the basic concepts and investigate the solutions to the challenges faced in the realization of this scheme, particularly in the areas of network modeling, on-line simulation and parameter search. We also discuss the applicability of this scheme, and present the simulation results under ns and the test results of a preliminary implementation on a real network.",
            "group": 476,
            "name": "10.1.1.23.8992",
            "keyword": "",
            "title": "Network Management and Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": "We study two problems in the field of machine learning. First, we propose a novel theoretical framework for understanding learning and generalization which we call the bin model. Using the bin model, a closed form is derived for the generalization error that estimates the out-of-sample performance in terms of the in-sample performance. We address the problem of overfitting, and show that using a simple exhaustive learning algorithm it does not arise. This is independent of the target function, input distribution and learning model, and remains true even with noisy data sets.",
            "group": 477,
            "name": "10.1.1.23.9186",
            "keyword": "",
            "title": "A Generalization Model and Learning in Hardware"
        },
        {
            "abstract": "The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.",
            "group": 478,
            "name": "10.1.1.23.9298",
            "keyword": "",
            "title": "Outlier Detection for High Dimensional Data"
        },
        {
            "abstract": " ",
            "group": 479,
            "name": "10.1.1.24.19",
            "keyword": "",
            "title": "Timing Analysis, Scheduling, and Allocation of Periodic Hard Real-Time Tasks"
        },
        {
            "abstract": "The complexity and dynamics of the Internet is driving the demand for scalable and effective network control. This paper proposes a collaborative on-line simulation architecture to provide pro-active and automated control functions for networks. The general model includes autonomous on-line simulators which continuously monitor/model the network conditions and execute a search in the parameter state space for better settings of protocol parameters. The protocol parameters are then tuned by the online simulation system. In this paper, we describe the building blocks of this architecture and investigate the implementation challenges in the areas of network modeling, on-line simulation and parameter search. We also discuss the applicability of this system and present the simulation and test results of a preliminary implementation.",
            "group": 480,
            "name": "10.1.1.24.43",
            "keyword": "",
            "title": "Traffic Management and Network Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": "Recent work (Yedidia, Freeman, Weiss [22]) has shown that stable points of belief propagation (BP) algorithms [12] for graphs with loops correspond to extrema of the Bethe free energy [3]. These BP algorithms have been used to obtain good solutions to problems for which alternative algorithms fail to work [4], [5], [10] [11]. In this paper we rst obtain the dual energy of the Bethe free energy which throws light on the BP algorithm. Next we introduce a discrete iterative algorithm which we prove is guaranteed to converge to a minimum of the Bethe free energy. We call this the double-loop algorithm because it contains an inner and an outer loop. It extends a class of mean eld theory algorithms developed by [7],[8] and, in particular, [13]. Moreover, the double-loop algorithm is formally very similar to BP which may help understand when BP converges. Finally, we extend all our results to the Kikuchi approximation which includes the Bethe free energy as a special case [3]. (Yedidia et al [22] showed that a \\generalized belief propagation\" algorithm also has its xed points at extrema of the Kikuchi free energy). We are able both to obtain a dual formulation for Kikuchi but also obtain a double-loop discrete iterative algorithm that is guaranteed to converge to a minimum of the Kikuchi free energy. It is anticipated that these double-loop algorithms will be useful for solving optimization problems in computer vision and other applications.",
            "group": 481,
            "name": "10.1.1.24.307",
            "keyword": "",
            "title": "A Double-Loop Algorithm to Minimize the Bethe and Kikuchi Free Energies"
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN), and propose a general paradigm for multi-way circuit partitioning based on dual net transformation. Given a netlist we first compute a K-way partitioning of nets based on the HDN representation, and then transform the K-way net partition into a K-way module partitioning solution. The main contribution of our work is in the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding factor between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present an efficient solution to ...",
            "group": 482,
            "name": "10.1.1.24.1570",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "In this paper we describe a methodology and accompanied tool support for the development of parallel and distributed embedded real--time system software. The presented approach comprises the complete design flow from the modeling of a distributed controller system by means of a high--level graphical language down to the synthesis of executable code for a given target hardware, whereby the implementation is verified to meet hard real--time constraints. The methodology is mainly based upon the tools SEA (System Engineering and Animation) and CHaRy (The C--LAB Hard Real--Time System).",
            "group": 483,
            "name": "10.1.1.24.1716",
            "keyword": "C--LAB Hard Real--Time System",
            "title": "From High-Level Specifications down to Software Implementations of Parallel Embedded Real-Time Systems"
        },
        {
            "abstract": "We introduce a novel approach to Parallel Computational Geometry by using networks  of analog components, referred to as analog networks or analog circuits. The  analog network we study here is the Analog Hopfield Net which was origially introduced  by Hopfield (1983) as a simplified electronic model of human brain cells. Massively  parallel Analog Hopfield Nets with large numbers of processing elements (neurons) exist  in hardware and have proven to be efficient architectures for important problems  (e.g. for constructing an associative memory). We demonstrate how Computational  Geometry problems can be solved by exploiting the features of such analog parallel  architectures. Using massively parallel analog networks requires a radically different  approach to traditional parallel geometric problem solving because (i) time is continuous  instead of the discretized time step used for traditional parallel (or sequential)  processing, and (ii) geometric data is represented by analog components (e.g. voltages  at certain positions of the circuit) instead of the usual digital representation.",
            "group": 484,
            "name": "10.1.1.24.3399",
            "keyword": "",
            "title": "Analog Parallel Computational Geometry"
        },
        {
            "abstract": "Parallel systems are often employed for time-critical applications. This concerns especially the area of digital signal processing, where real-time constraints havetobe met. In this paper, we present a tool for mapping and scheduling tasks of real-time applications onto parallel hardware systems. The tool consists of (i) a flexible model for the application software and the parallel hardware system and (ii) an optimization solver based on the Simulated Annealing algorithm. The model describes real-time applications, parallel processor hardware and communication structures with strong emphasis on applicability. Hardware related system characteristics, like direct memory access mechanisms or hardware-buffered links, are included in the model. A cost function is used to control the optimization process for different objectives. Several examples show the suitability and applicability of our tool to technical systems.",
            "group": 485,
            "name": "10.1.1.24.3763",
            "keyword": "",
            "title": "A Flexible Tool for Mapping and Scheduling Real-Time Applications onto Parallel Systems"
        },
        {
            "abstract": "This paper addresses the general time-route assignment problem :  One considers an air transportation network in a 2 dimensional space with asymmetric non-separable  link cost and a fleet of aircraft with their associated route and slot of departure. For each flight a set  of alternative routes and a set of possible slots of departure are defined. One must find \"optimal\"  route and slot allocation for each aircraft in a way that significaly reduces the peak of workload in  the most congested sectors and in the most congested airports, during one day of traffic.",
            "group": 486,
            "name": "10.1.1.24.4143",
            "keyword": "TopicsGenetic AlgorithmTime of Departure and Route Optimization. Domain AreaAir Traffic Control StatusOperational mock-up. Effort1 man/year",
            "title": "Airspace Congestion Smoothing by Stochastic Optimization"
        },
        {
            "abstract": "Imagine yourself standing in front of an exquisite buffet filled with numerous delicacies. Your goal is to try them all out, but you need to decide in what order. What exchange of tastes will maximize the overall pleasure of your palate? Although much less pleasurable and subjective, that is the type of problem that query optimizers are called to solve. Given a query, there are many plans that a database management system (DBMS) can follow to process it and produce its answer. All plans are equivalent in terms of their final output but vary in their cost, i.e., the amount of time that they need to run. What is the plan that needs the least amount of time? Such query optimization is absolutely necessary in a DBMS. The cost difference between two alternatives can be enormous. For example, consider the following database schema, which will be...",
            "group": 487,
            "name": "10.1.1.24.4154",
            "keyword": "",
            "title": "Query Optimization"
        },
        {
            "abstract": "The difficulties of learning in multilayered networks of computational units has limited the use of connectionist systems in complex domains. This dissertation elucidates the issues of learning in a network's hidden units, and reviews methods for addressing these issues that have been developed through the years. Issues of learning in hidden units are shown to be analogous to learning issues for multilayer systems employing symbolic representations. Comparisons of",
            "group": 488,
            "name": "10.1.1.24.5307",
            "keyword": "",
            "title": "Learning and Problem Solving with Multilayer Connectionist Systems"
        },
        {
            "abstract": "Learning systems that learn from previous experiences and/or provided examples of appropriate behaviors, allow people to specify what the systems should do for each case, not how systems should act for each step. That eases system users' burdens to a great extent. It is essential in efficient and accurate learning for supervised learning systems such as neural networks to be able to utilize knowledge in the forms of such as logical expressions, probability distributions, and constraint on differential data along with provided desirable input and output pairs. Neural networks, which can learn constraint on differential data, have already been applied to pattern recognition and differential equations. Other applications such as robotics have been suggested as applications of neural networks learning differential data. In this dissertation, we investigate the extended framework introduce constraints on differential data into neural networks' learning. We also investigate other items form the foundations for the application of neural networks learning differential data. First new and very general architecture and an algorithm are introduced for multilayer perceptrons to learn differential data. The algorithm is applicable to learning differential data of orders not only first but also higher than first and completely localized to each unit in the multilayer perceptron like the back propagation algorithm. Then the architecture and the algorithm are implemented as computer programs. This required high programming skills and great amount of care. The main module is programmed in C++. The implementation is used to conduct experiments among others to show convergence of neural networks with differential data of up to third order. Along with the architecture and the algorithm we give analyses of neural networks learning differential data such as comparison witf extra pattern scheme, how learnings work, sample compl...",
            "group": 489,
            "name": "10.1.1.24.6268",
            "keyword": "",
            "title": "Neural Network Learning Differential Data"
        },
        {
            "abstract": "This paper deals with application of the cross-entropy (CE) method for solving the buffer allocation problem (BAP), i.e., with determination of optimal buffer allocation in a serial production line with the objective of maximizing the throughput. The cross-entropy method for BAP presents an adaptive algorithm equipped with an auxiliary random mechanism, which transforms the original deterministic problem into an associated stochastic one. The random mechanism in BAP presents an (m 1) x (n + 1) probability matrix P = (P ij), such that P n j=0 P ij = 1; 8i = 1; : : : ; m 1, where m 1 and n denote the number of niches (the number of machines is the number of niches +1) and the buer spaces, respectively, and of the P ij-th element of P represents the probability of niche i receiving j buer spaces (note that allocation of 0 buers in some niche is feasible). Once the matrix P is dened, each iteration of the CE algorithm comprises of the following two phases:",
            "group": 490,
            "name": "10.1.1.24.6724",
            "keyword": "",
            "title": " Application of the Cross Entropy Method for Buffer Allocation Problem in Simulation Based Environment"
        },
        {
            "abstract": "This paper presents a new approach to solve the problem of job  scheduling for parallel processing in heterogeneous systems. The optimization  goals are: (i) minimum total execution time including communication costs  and (ii) shortest response time for all jobs. We introduce a classification for  the given scheduling problem by the heterogeneity of the systems, from the  view of the schedulers' eyes. Then, according to this analysis, a new  scheduling strategy for so-called \"Strictly-Heterogeneous\" systems is  proposed. The key idea of the new approach is the use of the Hungarian  method, which provides a quick and objective-oriented search for the best  schedule by the given optimization criteria. In addition, by modifying this  method into so-called Objective-Oriented Algorithm (OOA), the time  complexity for scheduling is decreased to O(n(E+nlogn)). The simulation  results show us that OOA provides better solution quality while scheduling  time is less than the existing methods.",
            "group": 491,
            "name": "10.1.1.24.6999",
            "keyword": "",
            "title": "Objective-Oriented Algorithm for Job Scheduling in Parallel Heterogeneous Systems"
        },
        {
            "abstract": "Free-form deformations (FFDs) have long been a popular tool in modeling and keyframe animation. This paper extends the use of FFDs to a dynamic setting. A goal of this work is to enable normally rigid objects, such as teapots and tables, to come alive and learn to move about. Objects are assigned mass distributions and deformation properties, which allow them to translate, rotate, and deform according to internal and external forces. The primary contributions are threefold. First, a dynamic formulation of FFDs is presented. It is based on deformation modes which are tailored by the user and are expressed in terms of FFDs. Second, the formulation allows for a hierarchy of dynamic FFDs which can be used to model local as well as global deformations. Third, the deformation modes can be active and used as a basis for locomotion.",
            "group": 492,
            "name": "10.1.1.24.7133",
            "keyword": "CategoryRegular paper 2",
            "title": "Dynamic Animation Synthesis with Free-Form Deformations"
        },
        {
            "abstract": "Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required an ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches - CI tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying Bayesian network structure using a non CI test based method. Results of the evaluation of the algorithm on a number of databases (e.g., ALARM, LED and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems.",
            "group": 493,
            "name": "10.1.1.24.7833",
            "keyword": "Bayesian NetworksProbabilistic NetworksProbabilistic Model ConstructionConditional Independence",
            "title": "Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm"
        },
        {
            "abstract": "The optimization of network protocol parameters based on network simulation can be considered a  \"black-box\" optimization problem with unknown, multi-modal and noisy objective functions. In this  paper, an adaptive random search algorithm is proposed to perform efficient and robust optimization for  the concerned problems. Specifically, the algorithm is designed for use by the on-line simulation scheme  which attempts to automate network management with protocol parameter tuning. The new algorithm  takes advantage of the favorable statistical properties of pure random search and achieves high efficiency  without imposing extra restriction, e.g., differentiability, on the objective function. It is also robust to  noises in the evaluation of objective function since no traditional local search technique is involved. The  proposed algorithm is tested on some classical benchmark functions and its performance compared with  some other stochastic algorithms. Finally, a real case test is presented, in which the parameters of some  RED queues are optimized with our algorithm.",
            "group": 494,
            "name": "10.1.1.24.8138",
            "keyword": "",
            "title": "An Adaptive Random Search Alogrithm for Optimizing Network Protocol Parameters"
        },
        {
            "abstract": "FPGA-based ASIC development systems have become important tools in contemporary ASIC design. Existing systems exhibit low per-FPGA gate utilization (10 to 20 percent) due to limited inter-chip communication. Attempts at overcoming this limitation through the use of high dimensional interconnection topologies have met with limited success.",
            "group": 495,
            "name": "10.1.1.24.9836",
            "keyword": "FPGAlogic emulationprototypingmesh topologystatic routingvirtual wires",
            "title": "The Virtual Wires Emulation System: A Gate-Efficient ASIC Prototyping Environment"
        },
        {
            "abstract": "this paper we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that in most cases multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms",
            "group": 496,
            "name": "10.1.1.25.239",
            "keyword": "General TermsALGORITHMS Additional Key Words and Phrasesspatial joinsmultiway joinsquery processing",
            "title": "Multiway Spatial Joins"
        },
        {
            "abstract": "Fuzzy logic and fuzzy set theory provide an important framework for representing and managing imprecision and uncertainty in medical expert systems, but the need remains to optimise such systems to enhance performance. This paper presents a general technique for optimizing fuzzy models in fuzzy expert systems by simulated annealing and N-dimensional hill climbing simplex method. The application of the technique to a fuzzy expert system for the interpretation of the acid-base balance of blood in the umbilical cord of new born infants is presented. The Spearman Rank Order Correlation statistic was used to assess and to compare the performance of a commercially available crisp expert system, an initial fuzzy expert system and a tuned fuzzy expert system with experienced clinicians.",
            "group": 497,
            "name": "10.1.1.25.432",
            "keyword": "Simulated AnnealingSimplex MethodNeonatal OutcomeAcid-base Balance",
            "title": "Application of Simulated Annealing Fuzzy Model Tuning to Umbilical Cord Acid-Base Interpretation"
        },
        {
            "abstract": "We apply the simulated annealing algorithm to the combinatorial optimization problem of typewriter keyboard design, yielding nearly optimal key-placements using a figure of merit based on English letter pair frequencies and finger travel-times. Our keyboards are demonstrably superior to both the ubiquitous QWERTY keyboard and the less common Dvorak keyboard. The paper is constructed as follows: first we discuss the historical background of keyboard design; this includes August Dvorak's work, and a \"figure-of-merit\" (scalar) metric for keyboards. We discuss a theory of keyboard designs: why keyboard design is a combinatorial problem, how combinatorial problems are typically solved, what is simulated annealing, and why it is especially suitable for the problem at hand. Next we discussed the results, and compare the keyboards produced by simulated annealing to QWERTY and Dvorak 's keyboard. Finally, we suggest some future lines of inquiry. Keyboard Design  Since the invention of the typew...",
            "group": 498,
            "name": "10.1.1.25.1087",
            "keyword": "",
            "title": "Typewriter Keyboards via Simulated Annealing"
        },
        {
            "abstract": "We present a robust generalised queuing network algorithm as an evaluative procedure for optimising production line configurations using simulated annealing. We compare the results obtained with our algorithm to those of other studies and find some interesting similarities but also striking differences between them in the allocation of buffers, numbers of servers, and their service rates. While context dependent, these patterns of allocation are one of the most important insights which emerge in solving very long production lines. The patterns, however, are often counter-intuitive, which underscores the difficulty of the problem we address. The most interesting feature of our optimisation procedure is its bounded execution time, which makes it viable for optimising very long production line configurations. Based on the bounded execution time property, we have optimised configurations of up to 60 stations with 120 buffers and servers in less than five hours of CPU time.",
            "group": 499,
            "name": "10.1.1.25.1439",
            "keyword": "Buffer AllocationNon-linearStochasticIntegerNetwork DesignSimulated Annealing",
            "title": "Large Production Line Optimisation Using Simulated Annealing"
        },
        {
            "abstract": "FDA (the Factorized Distribution Algorithm) is an evolutionary algorithm that combines mutation and recombination by using a distribution. The distribution is estimated from a set of selected points. It is then used to generate new points for the next generation. In general a distribution defined for n binary variables has 2^n  parameters. Therefore it is too expensive to compute. For additively decomposed discrete functions (ADFs) there exists an algorithm that factors the distribution into conditional and marginal distributions, each of which can be computed in polynomial time. Previously, we have shown a convergence theorem for FDA . But it is only valid using Boltzmann selection. Boltzmann selection was not used in practice because a good annealing schedule was lacking. Using a Taylor expansion of the average fitness of the Boltzmann distribution, we have developed an adaptive annealing schedule called SDS (standard deviation schedule) that is introduced in this work. The inverse temperature   is changed inversely proportional to the standard deviation.  ",
            "group": 500,
            "name": "10.1.1.25.1999",
            "keyword": "genetic algorithmssimulated annealingBoltzmann distributionBoltzmann selection",
            "title": "A New Adaptive Boltzmann Selection Schedule SDS"
        },
        {
            "abstract": "A Probabilistic Cooperative-Competitive Hierarchical Search Model  by  Wong Yin Bun, Terence  Master of Philosophy  The Chinese UniversityofHongKong  Stochastic searching methods have been widely applied to areas such as global optimization and combinatorial optimization problems in a vast number of disciplines. To name a few, science, engineering, and operations researches. Representatives of these methods are Simulated annealing (SA),Evolution type algorithms like Genetic algorithms /programming (GA/GP), Evolution programming/strategy (EP/ES), and so on. Their developments are all inspired from nature: physical annealing process, genetic, evolution, and ecology. Interestingly enough, they put little emphasis on the importance of the past searching information and the property of the landscape at the time of searching.",
            "group": 501,
            "name": "10.1.1.25.2366",
            "keyword": "",
            "title": "A Probabilistic Cooperative-Competitive Hierarchical Search Model"
        },
        {
            "abstract": "The optimization of network protocol parameters based on network simulation can be considered a  \"black-box\" optimization problem with unknown, multi-modal and noisy objective functions. In this  paper, an adaptive random search algorithm is proposed to perform efficient and robust optimization for  the concerned problems. Specifically, the algorithm is designed for use by the on-line simulation scheme  which attempts to automate network management with protocol parameter tuning. The new algorithm  takes advantage of the favorable statistical properties of pure random search and achieves high efficiency  without imposing extra restriction, e.g., differentiability, on the objective function. It is also robust to  noises in the evaluation of objective function since no traditional local search technique is involved. The  proposed algorithm is tested on some classical benchmark functions and its performance compared with  some other stochastic algorithms. Finally, a real case test is presented, in which the parameters of some  RED queues are optimized with our algorithm.",
            "group": 502,
            "name": "10.1.1.25.2788",
            "keyword": "",
            "title": "An Adaptive Random Search Alogrithm for Optimizing Network Protocol Parameters"
        },
        {
            "abstract": "In this paper, we present a concept of a CPU kernel with  hardware support for local-search based optimization algorithms like  Simulated Annealing (SA) and Tabu-Search (TS). The special hardware  modules are: (i) A linked-list memory representing the problem space.",
            "group": 503,
            "name": "10.1.1.25.3941",
            "keyword": "",
            "title": "Hardware Support for Simulated Annealing and Tabu Search"
        },
        {
            "abstract": "ms (we show that exact minimization in NP-hard in these cases). These algorithms produce a local minimum in interesting large move spaces. Furthermore, one of them nds a solution within a known factor from the optimum. The algorithms are iterative and compute several graph cuts at each iteration. The running time at each iteration is eectively linear due to the special graph structure. In practice it takes just a few iterations to converge. Moreover most of the progress happens during the rst iteration.  For a certain piecewise constant prior we adapt the algorithms developed for the piecewise smooth prior. One of them nds a solution within a factor of two from the optimum. In addition we develop a third algorithm which nds a local minimum in yet another move space.  We demonstrate the eectiveness of our approach on image restoration, stereo, and motion. For the data with ground truth, our methods signicantly outperform standard methods.  Biographical Sketch  Olga",
            "group": 504,
            "name": "10.1.1.25.4110",
            "keyword": "",
            "title": "Efficient Graph-Based Energy Minimization Methods In Computer Vision"
        },
        {
            "abstract": "Traditionally, CAD has focused on the design needs of integrated circuits and, more recently, electronic systems. However, the emerging and rapid growth of several system-level technologies and application domains has provided design, and particularly computer-aided design, with new and important domain-specific synthesis challenges. Today, perhaps the most popular and widely used system application and technology are the World Wide Web (WWW) and multiple-disk platforms. The explosion of interest in the WWW has made popular Web servers so heavily accessed that many are unable to achieve acceptable throughput and response time. The immense data storage and transfer requirements of a Web server to support a growing volume of service requests requires that such servers be founded on multiple disks. The focus of this paper is on the methodology, algorithms and tools for proper design and operation of multiple-disk Web servers that achieve high throughput and low response time. We identify ...",
            "group": 505,
            "name": "10.1.1.25.4347",
            "keyword": "",
            "title": "Design Methodology and Tools for Throughput Optimization of Web Servers"
        },
        {
            "abstract": "We present an optimal solution to the problem of allocating communicating periodic tasks to heterogeneous processing  nodes (PNs) in a distributed real-time system. The solution is optimal in the sense of minimizing the maximum normalized task  response time, called the system hazard, subject to the precedence constraints resulting from intercommunication among the tasks  to be allocated. Minimization of the system hazard ensures that the solution algorithm will allocate tasks so as to meet all task  deadlines under an optimal schedule, whenever such an allocation exists. The task system is modeled with a task graph (TG), in  which computation and communication modules, communication delays, and intertask precedence constraints are clearly described.",
            "group": 506,
            "name": "10.1.1.25.4714",
            "keyword": "",
            "title": "Assignment and Scheduling Communicating Periodic Tasks in Distributed Real-Time Systems"
        },
        {
            "abstract": "this paper is organized as follows. Section 2 defines the computational environment parameters that were varied in the simulations. Descriptions of the 11 mapping heuristics are found in Section 3. Section 4 examines selected results from the simulation study. A list of implementation parameters and procedures that could be varied for each heuristic is presented in Section 5",
            "group": 507,
            "name": "10.1.1.25.4721",
            "keyword": "",
            "title": "A Comparison of Eleven Static Heuristics for Mapping a Class of Independent Tasks onto Heterogeneous Distributed Computing Systems"
        },
        {
            "abstract": "This report studies the problem of automatically selecting a suitable system architecture for implementing a real-time application. Given a library of hardware components, it is shown how an architecture can be synthesized with the goal of ful#lling the real-time constraints stated in the system's speci#cation. In case the selected architecture contains several processing units, the speci#cation is partitioned by assigning tasks to processing units. We investigate the use of three meta-heuristic search algorithms to solve the problem: genetic algorithms, simulated annealing, and tabu search; and it is described in detail how these can be adapted to the architecture synthesis problem. Their relative merits are discussed at length, as is the importance of scheduling to the solution quality.",
            "group": 508,
            "name": "10.1.1.25.4916",
            "keyword": "",
            "title": "Three Search Strategies for Architecture Synthesis and Partitioning of Real-Time Systems"
        },
        {
            "abstract": "This paper develops a general integer programming model for strategic, area-wide contingency planning of oil spill cleanup operations. Model inputs include the set of risk points and the likely spill scenarios and response requirements for each, the sites of existing storage locations and the inventory of components at each, and potential sites for new storage locations. It prescribes a minimum total cost plan to build new storage locations and/or expand existing ones and to purchase new components and pre-position them, and a contingency plan which determines what response systems should be composed to enable an effective time-phased response for each likely spill scenario. A family of heuristics based on linear programming (LP) is devised to resolve this strategic problem, providing an area-wide contingency plan. The heuristics are evaluated on a set of 10 test problems which involve some 1869 general integer variables and 3264 constraints. Computational tests indicate that four heuristics are quite effective, prescribing solutions for each of 10 test cases within 1.41 % of optimum and within a few minutes runtime. This study focused on modeling the Galveston Bay Area, and the test problems represent application in that area. A sensitivity analysis is demonstrated by assessing the impacts of component availability and the degradation of cleanup capability over time. Use of the model as a decision support aid by responsible parties, contractors, governmental organizations and others is described.",
            "group": 509,
            "name": "10.1.1.25.5248",
            "keyword": "",
            "title": "Strategic, Area-wide Contingency Planning Model for Oil Spill Cleanup Operations with Application Demonstrated to the Galveston Bay Area"
        },
        {
            "abstract": "The problem of finding paths in networks is general and many faceted with a wide range of engineering applications in communication networks. Finding the optimal path or combination of paths usually leads to NP-hard combinatorial optimization problems. A recent and promising method, the cross-entropy method proposed by Rubinstein, manages to produce optimal solutions to such problems in polynomial time. However this algorithm is centralized and batch oriented. In this paper we show how the cross-entropy method can be reformulated to govern the behaviour of multiple mobile agents which act independently and asynchronously of each other. The new algorithm is evaluate on a set of well known Travelling Salesman Problems. A simulator, based on the Network Simulator package, has been implemented which provide realistic simulation environments. Results show good performance and stable convergence towards near optimal solution of the problems tested.",
            "group": 510,
            "name": "10.1.1.25.5345",
            "keyword": "",
            "title": "Using the Cross-Entropy Method to Guide/Govern Mobile Agent's Path Finding in Networks"
        },
        {
            "abstract": "We introduce a quantitative approach to development and validation of synthetic benchmarks in general and of synthetic benchmarks for behavioral synthesis systems in particular. The first step of the approach is numerical parameterization of characteristics of a representative set of designs at the behavioral level. Experimental results on quantitative selection and validation of benchmarks are briefly presented. Based on the quantitative benchmark selection, we lay out theoretically and statistically sound foundations for addressing the key issues related to the synthesis and analysis of synthetic benchmarks for behavioral synthesis systems. The key component of the new approach is the synthetic design example generator which composes the behavioral level specification of a design in such a way that the design possesses the properties specified by a given set of numerical parameters. It is used to produce a set of designs which have the same selected set of properties as the available...",
            "group": 511,
            "name": "10.1.1.25.5775",
            "keyword": "",
            "title": "A Quantitative Approach to Development and Validation of Synthetic Benchmarks for Behavioral Synthesis"
        },
        {
            "abstract": "This paper presents a modified robust M-estimator referred to as annealing M-estimator  (AM-estimator) to avoid problems with M-estimator. The AM-estimator combines the annealing technique into the M-estimator. It has the following advantages: It gives the global solution regardless of the initialization. It involves no scale estimator nor free parameters, avoiding the unreliability therein. Nor does it need order statistics such as the median and hence no sorting. Experimental results show that the AM-estimator is very stable and has an elegant behavior w.r.t. percentage of outliers and noise variance. Key words --- Annealing, M-estimator, pattern recognition, robust statistics.  1 Introduction  Robust statistics methods provide tools for statistics problems in which underlying assumptions are inexact. A robust procedure should be insensitive to departures from underlying assumptions, that is, it should have good performance under the underlying assumptions and the performance deteri...",
            "group": 512,
            "name": "10.1.1.25.6278",
            "keyword": "",
            "title": "Robustizing Robust M-Estimation Using Deterministic Annealing"
        },
        {
            "abstract": "We derive real-time global optimization methods for several clustering optimization problems used in unsupervised texture segmentation. Speed is achieved by exploiting the topological relation of features to design a multiscale optimization technique, while accuracy and global optimization properties are gained using a deterministic annealing method. Coarse grained cost functions are derived for both central and sparse pairwise clustering, where the problem of coarsening sparse random graphs is solved by the concept of structured randomization. Annealing schedules and coarse-to-fine optimization are tightly coupled by a statistical convergence criterion derived from computational learning theory. The algorithms are benchmarked on Brodatz-like micro-texture mixtures. Results are presented for an autonomous robotics application. ",
            "group": 513,
            "name": "10.1.1.25.6760",
            "keyword": "",
            "title": "Multiscale Annealing for Real-time Unsupervised Texture Segmentation"
        },
        {
            "abstract": "In this paper we try to describe the main characters of Heuristics \"derived\" from Nature, a border area between Operations Research and Artificial Intelligence, with applications to graph optimization problems. These algorithms take inspiration from physics, biology, social sciences, and use a certain amount of repeated trials, given by one or more \"agents\" operating with a mechanism of competition-cooperation. Two introductory sections, devoted respectively to a presentation of some general concepts and to a tentative classification of Heuristics from Nature open the work. The paper is then composed of six review sections: each of them concerns a heuristic and its application to an NP-hard combinatorial optimization problem. We consider the following topics: genetic algorithms with timetable problems, simulated annealing with dial-a-ride problems, sampling & clustering with communication spanning tree problems, tabu search with job-shop-scheduling problems, neural nets with ...",
            "group": 514,
            "name": "10.1.1.25.6964",
            "keyword": "",
            "title": "Heuristics From Nature For Hard Combinatorial Optimization Problems"
        },
        {
            "abstract": "Algorithms based on force-directed placement and virtual physical models have become one of the most effective techniques for drawing undirected graphs. Spring-based algorithms that are the subject of this thesis are one type of force-directed algorithms. Spring algorithms are simple. They produce graphs with approximately uniform edge lengths, distribute nodes reasonably well, and preserve graph symmetries. A problem with these algorithms is that depending on their initial layout, it is possible that they find undesirable drawings associated with some local minimum criteria. In addition, it has always been a challenge to determine when a layout is stable in order to stop the algorithm.  In this thesis, we develop a simple but effective cost function that can determine a node layout quality as well as the quality of the entire graph layout during the execution of a Spring algorithm. We use this cost function for producing the initial layout of the algorithm, for helping nodes move out ...",
            "group": 515,
            "name": "10.1.1.25.7458",
            "keyword": "",
            "title": "An Improved Spring-based Graph Embedding Algorithm and LayoutShow: a Java Environment for Graph Drawing"
        },
        {
            "abstract": "Evolutionary algorithms are general, randomized search heuristics that are influenced by many parameters. Though evolutionary algorithms are assumed to be robust, it is well-known that choosing the parameters appropriately is crucial for success and efficiency of the search. It has been shown in many experiments, that non-static parameter settings can be by far superior to static ones but theoretical verifications are hard to find. We investigate a very simple evolutionary algorithm and rigorously prove that employing dynamic parameter control can greatly speed-up optimization.  ",
            "group": 516,
            "name": "10.1.1.25.8198",
            "keyword": "",
            "title": "Dynamic Parameter Control in Simple Evolutionary Algorithms"
        },
        {
            "abstract": "Reasoning with time is a natural activity for instance in the context of scheduling. We are interested in finding interfacing principles to combine several constraint solvers to allow users, for instance reasoning agents, to have a uniform view of the combined constraint store of heterogenous solvers, while allowing efficiently traceable interactions. We have designed and implemented a distributed model for planning and scheduling of transports in a railway company which is usable also for other production planning problems. Our model emphasises the use of an agent for the coordination of distinct subproblems occurring in railway scheduling such as for instance the allocation of track resources to transports, the allocation of vehicles to transports, and the allocation of staff to perform the transportation tasks. Abstraction of constraint stores is an important technique used to enable translations between heterogenous solvers and to improve performance of the overall co...",
            "group": 517,
            "name": "10.1.1.25.8239",
            "keyword": "rosteringabstract constraintsheterogenous constraints 1",
            "title": "Heterogenous Scheduling and Rostering (Extended Abstract)"
        },
        {
            "abstract": "In current change management tools, the actual changes occur outside the tool. In contrast, Infuse concentrates on the actual change process and provides facilities for both managing and coordinating source changes. Infuse provides facilities for automatically structuring the cooperation among programmers, propagating changes, and determining the consistency of changes, and provides a basis for negotiating the resolution of conflicting changes and for iterating over a set of changes.  1. Introduction  A number of tools address the problems of managing changes to large software systems. Most such tools provide a framework in which programmers can reserve modules  1  for change and in which the changes themselves occur outside of the tool. Examples include SCCS [Rochkind 75], Cedar's System Modeller [Lampson 83], Darwin [Minsky 85] and DSEE [Leblang 84]. When a change is to be made to a module, a programmer reserves the module, obtains an official copy of the module, and then proceeds b...",
            "group": 518,
            "name": "10.1.1.25.8630",
            "keyword": "",
            "title": "Infuse: A Tool for Automatically Managing and Coordinating Source Changes in Large Systems"
        },
        {
            "abstract": ". A simple mechanism is presented, based on ant-like agents, for routing and load balancing in telecommunications networks, following the initial works of Appleby and Stewart (1994) and Schoonderwoerd et al. (1997). In the present work, agents are very similar to those proposed by Schoonderwoerd et al. (1997), but a r e supplemented with a simplified dynamic programming capability, initially experimented by Gurin (1997) with more complex agents, which is shown to significantly improve the network's relaxation and its response to perturbations.  Topic area: Intelligent agents and network management  2  1. Introduction  1.1 Routing in telecommunications networks  Routing is a mechanism that allows calls to be transmitted from a source to a destination through a sequence of intermediate switching stations or nodes, because not all points are directly connected: the cost of completely connecting a network becomes prohibitive for more than a few nodes. Routing selects routes that meet the o...",
            "group": 519,
            "name": "10.1.1.25.9060",
            "keyword": "",
            "title": "Routing in Telecommunications Networks With \"smart\" Ant-Like Agents"
        },
        {
            "abstract": "",
            "group": 520,
            "name": "10.1.1.25.9524",
            "keyword": "",
            "title": "Design and Implementation of a Resource Management System for Network Computing using Java"
        },
        {
            "abstract": "We propose a new framework for the layout of graphs, which both unifies and generalizes many approaches known from the literature. Doing so, we briefly survey a number of models, particularly including force directed placement approaches. Our model is based on the stochastic concept of random fields, which have applications in various other areas such as physics, biology, economics, or medical imaging. By allowing, yet not enforcing, the decoupling of model and computation, it integrates algorithmic and declarative approaches to graph layout.  ",
            "group": 521,
            "name": "10.1.1.26.293",
            "keyword": "",
            "title": "Random Field Models for Graph Layout"
        },
        {
            "abstract": "The effectiveness of viscous elements in introducing damping in a structure is a function of several variables, including their number, their location in the structure, and their physical properties. This paper addresses the questions of the placement of these elements and the selection of their physical parameters via optimization techniques. The paper investigates various metrics to define these optimization problems, and compares the damping profiles that are obtained. Both discrete and continuous optimization problems are formulated and solved, respectively, to the problems of placement of damping elements and to the tuning of their parameters. Particular emphasis is placed on techniques to make feasible the scale problems resulting from the optimization formulations. Numerical results involving a lightly damped testbed structure are presented. 1. Introduction  A problem of considerable importance in the development of technology for future space structures is the analysis and opti...",
            "group": 522,
            "name": "10.1.1.26.607",
            "keyword": "",
            "title": "Optimization Methods for Passive Damper Placement and Tuning"
        },
        {
            "abstract": "",
            "group": 523,
            "name": "10.1.1.26.1001",
            "keyword": "ACKNOWLEDGEMENTS............................................... iii ABSTRACT........................",
            "title": "A New Clustering Algorithm For Segmentation Of Magnetic Resonance Images"
        },
        {
            "abstract": "Evolutionary computation (EC) consists of the design and analysis of probabilistic algorithms inspired by the principles of natural selection and variation. Genetic Programming (GP) is one subfield of EC that emphasizes desirable features such as the use of procedural representations, the capability to discover and exploit intrinsic characteristics of the application domain, and the flexibility to adapt the shape and complexity of learned models.  Approaches that learn monolithic representations are considerably less likely to be effective for complex problems, and standard GP is no exception. The main goal of this dissertation is to extend GP capabilities with automatic mechanisms to cope with problems of increasing complexity. Humans succeed here by skillfully using hierarchical decomposition and abstraction mechanisms. The translation of such mechanisms into a general computer implementation is a tremendous challenge, which requires a firm understanding of the interplay between repr...",
            "group": 524,
            "name": "10.1.1.26.1154",
            "keyword": "",
            "title": "Hierarchical Learning with Procedural Abstraction Mechanisms"
        },
        {
            "abstract": "Genetic algorithms have been used for neural networks in two main ways: to optimize the network architecture and to train the weights of a fixed architecture. While most previous work focuses on only one of these two options, this paper investigates an alternative evolutionary  approach called Breeder Genetic Programming (BGP) in which the architecture and the weights are optimized simultaneously. The genotype of each network is represented as a tree whose depth  and width are dynamically adapted to the particular application by specifically defined genetic operators. The weights are trained by a next-ascent hillclimbing search. A new fitness function is proposed  that quantifies the principle of Occam's razor. It makes an optimal trade-off between the error fitting ability and the parsimony of the network. Simulation results on two benchmark problems of differing complexity suggest that the method finds minimal size networks on clean data. The experiments on noisy data show...",
            "group": 525,
            "name": "10.1.1.26.1673",
            "keyword": "",
            "title": "Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor"
        },
        {
            "abstract": " ",
            "group": 526,
            "name": "10.1.1.26.2163",
            "keyword": "",
            "title": "Genetic Algorithms as an Approach to Configuration and Topology Design"
        },
        {
            "abstract": "Procedural representations of control policies have two advantages when facing the scale-up problem in learning tasks. First they are implicit, with potential for inductive generalization over a very large set of situations. Second they facilitate modularization. In this paper we compare several randomized algorithms for learning modular procedural representations. The main algorithm, called Adaptive Representation through Learning (ARL) is a genetic programming extension that relies on the discovery of subroutines. ARL is suitable for learning hierarchies of subroutines and for constructing policies to complex tasks. ARL was successfully tested on a typical reinforcement learning problem of controlling an agent in a dynamic and nondeterministic environment where the discovered subroutines correspond to agent behaviors.  Introduction  The interaction of a learning system with a complex environment represents an opportunity to discover features and invariant properties of the problem th...",
            "group": 527,
            "name": "10.1.1.26.2196",
            "keyword": "",
            "title": "Evolution-based Discovery of Hierarchical Behaviors"
        },
        {
            "abstract": "Bayesian networks, which provide a compact graphical way to express complex probabilistic relationships among several random variables, are rapidly becoming the tool of choice for dealing with uncertainty in knowledge based systems. However, approaches based on Bayesian networks have often been dismissed as unfit for many real-world applications since probabilistic inference is intractable for most problems of realistic size, and algorithms for learning Bayesian networks impose the unrealistic requirement of datasets being complete. In this thesis, I present practical solutions to these two problems, and demonstrate their effectiveness on several real-world problems. The solution proposed to the first problem is to learn selective Bayesian networks, i.e., ones that use only a subset of the given attributes to model a domain. The aim is to learn networks that are smaller, and henc...",
            "group": 528,
            "name": "10.1.1.26.3700",
            "keyword": "",
            "title": "Learning Bayesian Networks for Solving Real-World Problems"
        },
        {
            "abstract": "This paper describes algorithms that learn to improve search performance on largescale optimization tasks. The main algorithm, Stage, works by learning an evaluation function that predicts the outcome of a local search algorithm, such as hillclimbing or Walksat, from features of states visited during search. The learned evaluation function is then used to bias future search trajectories toward better optima on the same problem. Another algorithm, X-Stage, transfers previously learned evaluation functions to new, similar optimization problems. Empirical results are provided on seven large-scale optimization domains: bin-packing, channel routing, Bayesian network structure-finding, radiotherapy treatment planning, cartogram design, Boolean satisfiability, and Boggle board setup.",
            "group": 529,
            "name": "10.1.1.26.3829",
            "keyword": "",
            "title": "Learning Evaluation Functions to Improve Optimization by Local Search"
        },
        {
            "abstract": "The typical planning, design or operations problem has multiple objectives and constraints. Such problems can be solved using only autonomous agents, each specializing in a small and distinct subset of the overall objectives and constraints. No centralized control is necessary. Instead, agents collaborate by observing and modifying one another's work. Convergence to good solutions for a variety of real and academic problems has been obtained by embedding a few simple rules in each agent. The paper develops these rules and illustrates their use.  Keywords: A-Teams, Agent, Asynchronous, Autonomous, Collaboration, Constraint, Cooperation, Nonlinear, Optimization, Specialized 1. INTRODUCTION  Most real world problems require that multiple conflicting objectives and constraints be satisfied. There might be tradeoffs between the different constraints that are not known a-priori and require the consultation of specialists in some aspect of the problem, but might not have any knowledge about o...",
            "group": 530,
            "name": "10.1.1.26.4488",
            "keyword": "A-TeamsAgentAsynchronousAutonomousCollaborationConstraintCooperationNonlinearOptimizationSpecialized",
            "title": "A Collaboration Strategy for Autonomous, Highly Specialized Agents"
        },
        {
            "abstract": "Partitioning a system's functionality among interacting hardware and software  components is an important part of system design. We introduce a new  partitioning algorithm that caters to the main objective of the hardware/software  partitioning problem, i.e. minimizing hardware for given performance constraints.  We demonstrate results superior to those of previously published algorithms intended  for hardware/software partitioning.  Contents  1 Introduction 2 2 Problem Definition 4 3 Solving the hardware/software partitioning problem 5 3.1 Greedy algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 3.2 Hill-climbing algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 A new algorithm based on constraint-search : : : : : : : : : : : : : : : : : : 8 3.3.1 Foundation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8 3.3.2 Algorithm : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 3.3.3 Reducing runtime in p...",
            "group": 531,
            "name": "10.1.1.26.4750",
            "keyword": "Contents",
            "title": "A Hardware-Software Partitioning Algorithm for Minimizing Hardware"
        },
        {
            "abstract": "This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies' foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the ACO metaheuristic are defined. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO metaheuristic. 1 Introduction Ant algorithms were first proposed by Dorigo and colleagues [33, 40] as a multi-agent approach to difficult combinatorial optimization problems such as the traveling salesman problem (TSP) and the quadratic assignment problem (QAP). There is currently much ongoing activi...",
            "group": 532,
            "name": "10.1.1.26.4870",
            "keyword": "",
            "title": "Ant Algorithms for Discrete Optimization"
        },
        {
            "abstract": " ",
            "group": 533,
            "name": "10.1.1.26.5150",
            "keyword": "",
            "title": "Iterated Local Search"
        },
        {
            "abstract": "Inth? a er, a tabu search algorith with a newneigh borh od techp][[ is ro osed for solving flowsh schp?;3Op roblems. An idea obtained from a simulation study on th ada tive beh vior of afish sch ol is used to determineth neigh borh od search techcpB?2 Th coo eration and th diversity of job data are defined for describingth data structure by usingth given information. According to th definitions, job data are classified into four kinds, and two kinds of thp are treated inth] a er. A large number of comutational ex eriments are carried out for investigating h w to assignth local e#ort and th global e#ort under a limited search time.Th resultsh wsthB th allocation of th local and th global e#ort de ends on th job data chO3Wpfi23]?4ph such as coo eration and diversity. Th validity ofth tabu search algorith is examined by com aring itwith th geneticalgorithB It is foundthn th ro osed tabu search algorith hg better convergence and much sh??34 com utational time thm th geneticalgorith2 1 Intr...",
            "group": 534,
            "name": "10.1.1.26.5202",
            "keyword": "",
            "title": "A Tabu Search with a New Neighborhood Search Technique Applied to Flow Shop Scheduling Problems"
        },
        {
            "abstract": "Corpus-based approaches to speech synthesis have been advocated to overcome the limitations of concatenative synthesis from a xed acoustic unit inventory. The frequency of unit concatenations in, e.g., diphone synthesis has been argued to contribute to the perceived lack of naturalness of synthetic speech. The key idea of corpus-based synthesis, or unit selection, is to use an entire speech corpus as the acoustic inventory and to select at run-time from this corpus the longest available strings of phonetic segments that match a sequence of target speech sounds in the utterance to be synthesized, thereby minimizing the number of concatenations and reducing the need for signal processing.  This paper reviews the assumptions underlying this synthesis strategy and the dierent approaches to unit selection, as well as the major challenges encountered by corpus-based methods. One of the biggest problems to date is the relative weighting of acoustic distance measures. We further argue agains...",
            "group": 535,
            "name": "10.1.1.26.6338",
            "keyword": "",
            "title": "Corpus-Based Speech Synthesis: Methods and Challenges"
        },
        {
            "abstract": "This paper presents a novel approach to assist the user in exploring appropriate transfer functions for the visualization of volumetric datasets. The search for a transfer function is treated as a parameter optimization problem and addressed with stochastic search techniques. Starting from an initial population of (random or pre-defined) transfer functions, the evolution of the stochastic algorithms is controlled by either direct user selection of intermediate images or automatic fitness evaluation using user-specified objective functions. This approach essentially shields the user from the complex and tedious \"trial and error\" approach, and demonstrates effective and convenient generation of transfer functions. 1 Introduction  Direct volume rendering is a key technology for the visualization of large sampled, simulated, or synthesized 3D datasets from scientific, engineering, and medical applications. However, an important problem that inhibits its widespread use is the complex interr...",
            "group": 536,
            "name": "10.1.1.26.6953",
            "keyword": "",
            "title": "Generation of Transfer Functions with Stochastic Search Techniques"
        },
        {
            "abstract": "The compact and harmonious layout of ads and text is a fundamental and  costly step in the production of commercial telephone directories (\"Yellow  Pages\"). We formulate a canonical version of Yellow-Pages pagination and  layout (YPPL) as an optimization problem in which the task is to position  ads and text-stream segments on sequential pages so as to minimize total page  length and maximize certain layout aesthetics, subject to constraints derived  from page-format requirements and positional relations between ads and text.  We present a heuristic-search approach to the YPPL problem. Our algorithm  has been applied to a sample of real telephone-directory data, and produces  solutions that are significantly shorter and better than the published ones.  Keywords: directory pagination, page layout, heuristic search, stochastic optimization,  simulated annealing.  This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to  copy in whole or in p...",
            "group": 537,
            "name": "10.1.1.26.7565",
            "keyword": "directory paginationpage layoutheuristic searchstochastic optimizationsimulated",
            "title": "Automatic Yellow-Pages Pagination and Layout"
        },
        {
            "abstract": "We describe a fully-automatic 3Dsegmentation technique for brain MR images. By means of Markov random fields the segmentation algorithm captures three features that are of special importance for MR images: nonparametric distributions of tissue intensities, neighborhood correlations and signal inhomogeneities. Detailed simulations and real MR images demonstrate the performance of the segmentation algorithm. In particular the impact of noise, inhomogeneity, smoothing and structure thickness is analyzed quantitatively. Even singleecho MR images are well classified into grey matter, white matter, cerebrospinal fluid, scalpbone, and background. A simulated annealing and an iterated conditional modes implementation are presented.  Index Terms  Magnetic Resonance Imaging, Segmentation, Markov Random Fields  I. INTRODUCTION  Excellent soft-tissue contrast and high spatial resolution make magnetic resonance imaging the method for anatomical imaging in brain research. Segmentation of the MR imag...",
            "group": 538,
            "name": "10.1.1.26.7888",
            "keyword": "",
            "title": "Markov Random Field Segmentation of Brain MR Images"
        },
        {
            "abstract": "Neural networks are composed of basic units somewhat analogous to neurons. These units  are linked to each other by connections whose strength is modifiable as a result of a learning  process or algorithm. Each of these units integrates independently (in parallel) the  information provided by its synapses in order to evaluate its state of activation. The unit  response is then a linear or nonlinear function of its activation. Linear algebra concepts are  used, in general, to analyze linear units, with eigenvectors and eigenvalues being the core  concepts involved. This analysis makes clear the strong similarity between linear neural networks  and the general linear model developed by statisticians. The linear models presented  here are the perceptron, and the linear associator. The behavior of nonlinear networks  can be described within the framework of optimization and approximation techniques with  dynamical systems (e.g., like those used to model spin glasses). One of the main notio...",
            "group": 539,
            "name": "10.1.1.26.8488",
            "keyword": "neural networksgeneral linear modelperceptronradial basis functionHopfield networkBoltzmann machineback-propagation networkeigenvectoreigenvalueprincipal component analysis",
            "title": "A Neural Network Primer"
        },
        {
            "abstract": "This paper presents a new derivative-free search method for finding models of acceptable data fit in a multidimensional parameter space. It falls into the same class of method as simulated annealing and genetic algorithms, which are commonly used for global optimization problems. The objective here is to find an ensemble of models that preferentially sample the good datafitting regions of parameter space, rather than seeking a single optimal model. (A related paper deals with the quantitative appraisal of the ensemble.) The new search algorithm makes use of the geometrical constructs known as Voronoi cells to drive the search in parameter space. These are nearest neighbour regions defined under a suitable distance norm. The algorithm is conceptually simple, requires just two `tuning parameters', and makes use of only the rank of a data fit criterion rather than the numerical value. In this way all difficulties associated with the scaling of a data misfit function are avoided, and any combination of data fit criteria can be used. It is also shown how Voronoi cells can be used to enhance any existing direct search algorithm, by intermittently replacing the forward modelling calculations with nearest neighbour calculations. The new direct search algorithm is illustrated with an application to a synthetic problem involving the inversion of receiver functions for crustal seismic structure. This is known to be a non-linear problem, where linearized inversion techniques suffer from a strong dependence on the starting solution. It is shown that the new algorithm produces a sophisticated type of `selfadaptive ' search behaviour, which to our knowledge has not been demonstrated in any previous technique of this kind.  ",
            "group": 540,
            "name": "10.1.1.26.8932",
            "keyword": "",
            "title": "Geophysical inversion with a neighbourhood algorithm -- I. Searching a parameter Space"
        },
        {
            "abstract": "Simulated annealing has proven to be a good technique for solving hard combinatorial optimization problems. Some attempts at speeding up annealing algorithms have been based on shared memory multiprocessor systems. Also parallelizations for certain problems on distributed memory multiprocessor systems are known. In this paper, we present a problem independent general purpose parallel implementation of simulated annealing on large distributed memory message-passing multiprocessor systems. The sequential algorithm is studied and we give a classification of combinatorial optimization problems together with their neighborhood structures. Several parallelization approaches are examined considering their suitability for problems of the various classes. For typical representatives of the different classes good parallel simulated annealing implementations are presented. We describe in detail several 'tricks' increasing efficiency and attained solution quality of the different parallel implemen...",
            "group": 541,
            "name": "10.1.1.26.9380",
            "keyword": "network",
            "title": "Problem Independent Distributed Simulated Annealing and its Applications"
        },
        {
            "abstract": "In this paper we develop a model for small scale examination scheduling. Beginning with a formulation of a quadratic assignment problem we show the transformation into a quadratic semi assignment problem and its application to a real world situation. Using simulated annealing we demonstrate the model's ability to generate schedules that satisfy student as well as university expectations. 1 Introduction The examination scheduling problem is to assign a number of exams to a number of potential time periods or slots within the examination period taking into account that no student can take two or more exams at the same time, as well as several other side constraints (cf. [5], p.4). The most common way of modelling the basic examination scheduling problem is as a graph coloring problem (cf. [16]). In such approach the nodes of the graph represent the exams. Two exam-nodes of the graph are connected by an edge if there is at least one student taking both exams. The chromatic number ...",
            "group": 542,
            "name": "10.1.1.26.9838",
            "keyword": "",
            "title": "An Examination Scheduling Model to Maximize Students' Study Time"
        },
        {
            "abstract": "Incorporating functional partitioning into a synthesis methodology leads to several important advantages. In functional partitioning, we first partition a functional specification into smaller sub-specifications and then synthesize structure for each, in contrast to the current approach of first synthesizing structure for the entire specification and then partitioning that structure. One advantage is the improvement in I/O, performance and package count when partitioning among hardware blocks with size and I/O constraints, such as among FPGA's or blocks within an ASIC. A second advantage is the reduction of synthesis runtimes. We describe experiments demonstrating these important advantages, concluding that further research on functional partitioning can lead to improved results from synthesis environments.  ",
            "group": 543,
            "name": "10.1.1.26.9942",
            "keyword": "",
            "title": "Functional Partitioning Improvements Over Structural Partitioning for Packaging Constraints and Synthesis-Tool Performance"
        },
        {
            "abstract": "Combinatorial optimization problems involve the selection of an arrangement of discrete objects (a state) from a discrete, finite space of mutually exclusive states. The goal is to select the particular state that represents the extremization (maximization or minimization) of an objective function. Often, these problems are simple to state, but quite difficult to solve. Exact solution methods, while guaranteed to find the optimal solution to any instance of a problem, may not be practical to implement because their solution time is often an exponential function of problem size.  Thus, heuristic methods are often applied to these problems. Approaches include local search techniques, simulated annealing, and genetic algorithms. The success of these techniques in a particular problem domain can depend heavily on selection of the appropriate parameters for the particular algorithm, the problem domain, and the input set. Further, it is often possible to take advantage of problem-specific in...",
            "group": 544,
            "name": "10.1.1.26.9969",
            "keyword": "",
            "title": "Visualization and Interactive Steering of Simulated Annealing"
        },
        {
            "abstract": "We offer three algorithms for the generation of topographic mappings to the practitioner of unsupervised data analysis. The algorithms are each based on the minimization of a cost function which is performed using an EM algorithm and deterministic annealing. The soft topographic vector quantization algorithm (STVQ) -- like the original Self-Organizing Map (SOM) -- provides a tool for the creation of self-organizing maps of Euclidean data. Its optimization scheme, however, offers an alternative to the heuristic stepwise shrinking of the neighborhood width in the SOM and makes it possible to use a fixed neighborhood function solely to encode desired neighborhood relations between nodes. The kernel-based soft topographic mapping (STMK) is a generalization of STVQ and introduces new distance measures in data space based on kernel functions. Using the new distance measures corresponds to performing the STVQ in a highdimensional feature space, which is related to data space by a nonlinear ma...",
            "group": 545,
            "name": "10.1.1.27.298",
            "keyword": "1",
            "title": "Self-Organizing Maps: Generalizations and New Optimization Techniques"
        },
        {
            "abstract": "Fitness landscapes have proven to be a valuable concept in evolutionary biology, combinatorial optimization, and the physics of disordered systems. A fitness landscape is a mapping from a configuration space into the real numbers. The configuration space is equipped with some notion of adjacency, nearness, distance or accessibility. Landscape theory has emerged as an attempt to devise suitable mathematical structures for describing the \"static\" properties of landscapes as well as their influence on the dynamics of adaptation. In this review we focus on the connections of landscape theory with algebraic combinatorics and random graph theory, where exact results are available.",
            "group": 546,
            "name": "10.1.1.27.629",
            "keyword": "Key words. fitness landscapegenotype phenotype maprandom graphs",
            "title": "Combinatorial Landscapes"
        },
        {
            "abstract": "this paper, a relatively new curvature based criterion, the",
            "group": 547,
            "name": "10.1.1.27.668",
            "keyword": "",
            "title": "Optimal Triangulation by Means of Evolutionary Algorithms"
        },
        {
            "abstract": "This paper describes a new technique for minimising power dissipation  in full scan sequential circuits during test application. The  technique increases the correlation between successive states during  shifting in test vectors and shifting out test responses by reducing spurious  transitions during test application. The reduction is achieved by  freezing the primary input part of the test vector until the smallest  transition count is obtained which leads to lower power dissipation.  This paper presents a new algorithm which determines the primary  input change time such that maximum saving in transition count is  achieved with respect to a given test vector and scan latch order. It  is shown how combining the proposed technique with the recently  reported scan latch and test vector ordering yields further reductions  in power dissipation during test application. Exhaustive experimental  results using compact and non compact test sets demonstrate substantial  savings in power dissipa...",
            "group": 548,
            "name": "10.1.1.27.854",
            "keyword": "",
            "title": "Minimisation of Power Dissipation During Test Application in Full Scan Sequential Circuits Using Primary Input Freezing"
        },
        {
            "abstract": "The Constraint Satisfaction Problem is a type of combinatorial search problem of much interest in Artificial Intelligence and Operations Research. The simplest algorithm for solving such a problem is chronological backtracking, but this method suffers from a malady known as \"thrashing,\" in which essentially the same subproblems end up being solved repeatedly. Intelligent backtracking algorithms, such as backjumping and dependency-directed backtracking, were designed to address this difficulty, but the exact utility and range of applicability of these techniques have not been fully explored. This dissertation describes an experimental and theoretical investigation into the power of these intelligent backtracking algorithms. We compare the empirical performance of several such algorithms on a range of problem distributions. We show that the more sophisticated algorithms are especially useful on those problems with a small number of constraints that happen to be difficult for chronologica...",
            "group": 549,
            "name": "10.1.1.27.1021",
            "keyword": "",
            "title": "Intelligent Backtracking On Constraint Satisfaction Problems: Experimental And Theoretical Results"
        },
        {
            "abstract": "Contents 1 Introduction 3  1.0.1 Minimal cost feeding : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1 General approach : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.2 Algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 1.3 Notation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.4 Some simple unconstrained problems : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.4.1 Positive deniteness : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 1.4.2 Taylor's Theorem : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8 1.4.3 Square root of a matrix : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 1.4.4 Completing the square for quadratics on R  n  : : ",
            "group": 550,
            "name": "10.1.1.27.1461",
            "keyword": "",
            "title": "Static Optimization"
        },
        {
            "abstract": "This paper presents heuristic approximation algorithms and methods to find lower bounds to approximate the Minimum Linear Arrangement problem and evaluates and compares experimentaly their behaviour when they are applied to sparse graphs. The low number of theoretical results for this problem motivates its experimental study. The algorithms presented and analyzed belong to the families of Successive Augmentation algorithms (also called greedy algorithms), local search algorithms (Hillclimbing, Metropolis and Simulated Annealing) and Spectral Sequencing. The empirical results are based on two random models and \"real life\" graphs. The conclusion is that the best approximations are obtained using Simulated Annealing, which involves a large amount of computation time. However, solutions found by Spectral Sequencing are also good and can be found in radically less time. We remark that the performance of the algorithms heavily depends on the kind of graph.",
            "group": 551,
            "name": "10.1.1.27.2100",
            "keyword": "",
            "title": "Approximation Heuristics and Benchmarkings for the MinLA Problem"
        },
        {
            "abstract": "Forest treatment planning and scheduling is an important part of forest resource management. It is a complex task requiring expertise and integration of multi-disciplinary fields. In this paper, we outline a real life problem from the ECOPLAN project called the Long Term Forest Treatment Scheduling Problem (LTFTSP). A review of optimization techniques applicable to forest treatment problems in general is presented, and contrasted with our case. The review suggests that long term scheduling is difficult because of the prohibitive size and complexity inherent to the problem. Based on experience from the successful resolution of a simplified problem, we advocate the use of iterative improvement techniques as a solution strategy. Iterative improvement techniques will in general benefit from high quality initial solutions. We show how a Constraint Satisfaction Problem formulation of the LTFTSP can be used to generate initial solutions. A key element to success is the use of a forest simulat...",
            "group": 552,
            "name": "10.1.1.27.3179",
            "keyword": "",
            "title": "Constraint Technology Applied to Forest Treatment Scheduling"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We pr...",
            "group": 553,
            "name": "10.1.1.27.3566",
            "keyword": "Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealing. Submitted to ACM Trans",
            "title": "Mitsubishi Electric Research Laboratories Cambridge Research Center"
        },
        {
            "abstract": "Graph partitioning is used in high-level synthesis systems in at least two contexts: (1) To facilitate performance estimation; (2) To partition the design into multiple components as is the case in MCM synthesis. In both cases, the graph being partitioned represents a design at the register level or a higher-level such as the behavior level or process level. Graph partitioning has been studied extensively in many application areas as well as in its application-independent form. Most of the work focuses on partitioning the graph into two subsets no larger than a given maximum size, so as to minimize the number of edges between the two subsets. In addition to concentrating on multi-way partitioning, we have introduced additional constraints to the problem. While partitioning, the maximum area constraint is specified and also a new thermal constraint is specified. This paper presents comparative performance results among two graph partitioning algorithms, namely simulated annealing and ...",
            "group": 554,
            "name": "10.1.1.27.3612",
            "keyword": "",
            "title": "Two Randomized Algorithms for Multichip Partitioning Under Multiple Constraints"
        },
        {
            "abstract": "We introduce a meta-heuristic to combine simulated annealing with local search methods for CO problems. This new class of Markov chains leads to significantly more powerful optimization methods than either simulated annealing or local search. The main idea is to embed deterministic local search techniques into simulated annealing so that the chain explores only local optima. It makes large, global changes, even at low temperatures, thus overcoming large barriers in configuration space. We have tested this meta-heuristic for the traveling salesman and graph partitioning problems. Tests on instances from public libraries and random ensembles quantify the power of the method. Our algorithm is able to solve large instances to optimality, improving upon state of the art local search methods very significantly. For the traveling salesman problem with randomly distributed cities in a square, the procedure improves on 3-opt by 1.6%, and on Lin-Kernighan local search by 1.3%. For the partitioni...",
            "group": 555,
            "name": "10.1.1.27.4014",
            "keyword": "",
            "title": "Combining Simulated Annealing with Local Search Heuristics"
        },
        {
            "abstract": "There has been substantial recent interest in two new families of search techniques. One family consists of nonsystematic methods such as gsat; the other contains systematic approaches that use a polynomial amount of justification information to prune the search space. This paper introduces a new technique that combines these two approaches. The algorithm allows substantial freedom of movement in the search space but enough information is retained to ensure the systematicity of the resulting analysis. Bounds are given for the size of the justification database and conditions are presented that guarantee that this database will be polynomial in the size of the problem in question. 1 INTRODUCTION  The past few years have seen rapid progress in the development of algorithms for solving constraintsatisfaction problems, or csps. Csps arise naturally in subfields of AI from planning to vision, and examples include propositional theorem proving, map coloring and scheduling problems. The probl...",
            "group": 556,
            "name": "10.1.1.27.4126",
            "keyword": "",
            "title": "GSAT and Dynamic Backtracking"
        },
        {
            "abstract": "this paper is to review some of these aspects, show possible solutions in the framework of Radial Basis Functions, and point out some open problems.",
            "group": 557,
            "name": "10.1.1.27.5492",
            "keyword": "",
            "title": "On some extensions of Radial Basis Functions and their applications in Artificial Intelligence"
        },
        {
            "abstract": " We propose to design, implement, and evaluate a software framework, called the Adaptware, that consists of architectural support, resource-management mechanisms, and programming abstractions for adapting Quality-of-Service (QoS) to dynamically-fluctuating resource capacity and demands. This framework is to reduce the cost and time of real-time software development by providing the infrastructure necessary for building reusable multi-purpose real-time software components. In much the same way as today's consumers can buy software and hardware components from different vendors and construct a computing environment tailored to their needs, the proposed framework will provide the means of building and integrating real-time system components so as to preserve their temporal correctness while making it possible to dynamically compute predictable end-to-end temporal guarantees commensurate with available resour...",
            "group": 558,
            "name": "10.1.1.27.6629",
            "keyword": "",
            "title": "QoS Adaptation In Real-Time Systems"
        },
        {
            "abstract": "We address the problem of maximizing the speedup of an individual parallel job through the selection  of an appropriate number of processors on which to run it. If a parallel job exhibits speedup that increases  monotonically in the number of processors, the solution is clearly to make use of all available processors.  However, many applications do not have this characteristic: they reach a point beyond which the use of  additional processors degrades performance. For these applications, it is important to choose a processor  allocation carefully.  Our approach to this problem is to provide a runtime system that adjusts the number of processors  used by the application based on dynamic measurements of performance gathered during its execution.  Our runtime system has a number of advantages over user specified fixed allocations, the currently most  common approach to this problem: (1) we are resilient to changes in an application's speedup behavior due  to the input data; and (2) we are...",
            "group": 559,
            "name": "10.1.1.27.6975",
            "keyword": "",
            "title": "Maximizing Speedup Through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "In this work we apply statistical learning methods in the context of  combinatorial optimization, which is understood as nding a binary string  minimizing a given cost function. We rst consider probability densities  over binary strings and we dene two dierent statistical criteria. Then we  recast the initial problem as the problem of nding a density minimizing  one of the two criteria. We restrict ourselves to densities described by  a small number of parameters and solve the new problem by means of  gradient techniques. This results in stochastic algorithms which iteratively  update density parameters. We apply these algorithms to two families of  densities, the Bernoulli model and the Gaussian model. The algorithms  have been implemented and some experiments are reported.  1 Introduction  In this work, we apply statistical learning methods in the context of combinatorial optimization, which is understood as nding a binary string minimizing a given cost function. We transform t...",
            "group": 560,
            "name": "10.1.1.27.7415",
            "keyword": "",
            "title": "Statistical Machine Learning and Combinatorial Optimization"
        },
        {
            "abstract": "this paper, we address these shortcomings with a new datapath allocation method that consists of a new binding model construction scheme, and an optimization technique based on simulated annealing. It meets all three criteria: handling complex models of datapath units, using direct objective functions, and optimizing multiple objective functions simultaneously.",
            "group": 561,
            "name": "10.1.1.27.7791",
            "keyword": "",
            "title": "A Flexible Datapath Allocation Method for Architectural Synthesis"
        },
        {
            "abstract": "Forest harvest planning and scheduling problem is an important part of forest resource management. It is a complex task requiring expertise and integration of multi-disciplinary fields. We review mathematical programming as applied to forest harvesting problem. The review suggests that long term scheduling is difficult because of the prohibitive size of the model. Such long term scheduling allows one to see if sustainable forestry is actually being practiced. Thus, this method alone is not sufficient for sustainable forest harvesting. In this paper we propose how constraint reasoning techniques from Artificial Intelligence may be used along with simulation modelling to solve forest harvesting problem effectively. We show how these techniques can be used to better model forest harvest plans and describe our work in progress towards building better forest harvest scheduling system. 1 Introduction  Amount of forest area that is managed worldwide is huge. In United States alone, United Sta...",
            "group": 562,
            "name": "10.1.1.27.7836",
            "keyword": "",
            "title": "Integration of Constraint Reasoning, and Simulation models in Forest Harvest Scheduling"
        },
        {
            "abstract": "A common framework for 3D image registration consists in minimizing a cost (or energy) function  that expresses the pixel or voxel similarity of the images to be aligned. Standard cost functions, based  on voxel similarity measures, are highly non-linear, non-convex, exhibit many local minima and thus  yield hard optimization problems. Local, deterministic optimization algorithms are known to be very  sensitive to local minima. Global optimization methods (like simulated annealing or evolutionay  algorithms), yield better, often close to the optimal solutions, but are time consuming.  In this paper we consider the parallelization of a general purpose global optimization algorithm  based on random sampling and evolutionary principles: the dierential evolution algorithm. The  inherent parallelism of evolutionary algorithms is used to devise a data-parallel implementation of  dierential evolution. The performances of the parallel version is assessed on a 3D medical image  registration p...",
            "group": 563,
            "name": "10.1.1.27.8646",
            "keyword": "",
            "title": "Parallelizing differential evolution for 3D medical image registration"
        },
        {
            "abstract": "This paper presents the architecture of a software dedicated to real-time manufacturing systems control. It is built on three main subsystems: a scheduler which implements a huge library of algorithms, an universal simulator which tests the solutions on an accurate model of the manufacturing system and a complete interface which helps the user to manage the system and to choose the best solution, according to the current objectives. The multi-model approach used here combines the advantages of several methods coming from operational research, artificial intelligence, human-computer interface and software engineering to build a flexible, portable and user-friendly tool.",
            "group": 564,
            "name": "10.1.1.27.9311",
            "keyword": "",
            "title": "Simulation Based Smart Scheduler for Manufacturing Systems"
        },
        {
            "abstract": "Clustering is the search for those partitions that reflect the structure of an object set. Traditional clustering algorithms search only a small sub-set of all possible clusterings (the solution space) and consequently, there is no guarantee that the solution found will be optimal. We report here on the application of Genetic Algorithms (GAs) --- stochastic search algorithms touted as effective search methods for large and complex spaces --- to the problem of clustering. GAs which have been made applicable to the problem of clustering (by adapting the representation, fitness function, and developing suitable evolutionary operators) are known as Genetic Clustering Algorithms (GCAs). There are two parts to our investigation of GCAs: first we look at clustering into a given number of clusters. The performance of GCAs on three generated data sets, analysed using 4320 differing combinations of adaptions, establishes their efficacy. Choice of adaptions and parameter settings is data set depe...",
            "group": 565,
            "name": "10.1.1.27.9443",
            "keyword": "",
            "title": "Clustering With Genetic Algorithms"
        },
        {
            "abstract": "Short abstract, isn't it?  P.A.C.S. numbers 05.20, 02.50, 87.10  1 Introduction  Large Numbers  \"...the optimal tour displayed (see Figure 6) is the possible unique tour having one arc fixed from among 10  655  tours that are possible among 318 points and have one arc fixed. Assuming that one could possibly enumerate 10  9  tours per second on a computer it would thus take roughly 10  639  years of computing to establish the optimality of this tour by exhaustive enumeration.\"  This quote shows the real difficulty of a combinatorial optimization problem. The huge number of configurations is the primary difficulty when dealing with one of these problems. The quote belongs to M.W Padberg and M. Grotschel, Chap. 9., \"Polyhedral computations\", from the book The Traveling Salesman Problem: A Guided tour of Combinatorial Optimization [124].  It is interesting to compare the number of configurations of real-world problems in combinatorial optimization with those large numbers arising in Cosmol...",
            "group": 566,
            "name": "10.1.1.27.9474",
            "keyword": "",
            "title": "On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts - Towards Memetic Algorithms"
        },
        {
            "abstract": "The work in this paper is motivated by our effort to build a distributed rendering system that uses  the multiple hardware graphics accelerators available in a network of workstations/PCs to provide realtime   rendering performance that is one or two generations ahead of what can be achieved using only a  single commodity machine. Specifically, we address one important aspect of building such a system,  how to partition and assign the overall rendering work. We propose a novel approach called Image Layer  Decomposition (ILD). ILD has several advantages over previous partitioning algorithms for our targeted  environment, including its compatibility with the use of hardware graphics accelerators, decoupling of  communication bandwidth requirement from scene complexity, and reduced communication bandwidth  growth as the system size increases. Furthermore, ILD tries to optimize the rendering of a sequence of  frames (of an interactive application) instead of only individual frames.  Using ...",
            "group": 567,
            "name": "10.1.1.28.252",
            "keyword": "",
            "title": "Image Layer Decomposition for Distributed Rendering on NOWs"
        },
        {
            "abstract": "This paper presents a novel broadband multimedia service called TV-Anytime.  The basic idea of this service is to store broadcast media assets onto media server  systems and allow clients to access these streams at any time. We propose a hierarchical  structure of a distributed server network to support a high quality TV-anytime  service. A key issue, how to map the media assets onto such a hierarchical server  network is addressed and formalized as a combinatorial optimization problem. In  order to solve this optimization problem, a set of heuristic solutions by use of a  parallel simulated annealing library is proposed and verified by a set of benchmark  instances. Finally, the TV Cache is presented as a prototype of a scalable  TV-Anytime system.  1 Introduction  Distributed multimedia systems are constantly growing in popularity thanks also to the presence of the Internet [1] [2] [3] [4]. Whereas an Internet newspaper can be accessed at any time, the access to high bandwidth audio/...",
            "group": 568,
            "name": "10.1.1.28.367",
            "keyword": "",
            "title": "Heuristic Solutions for a Mapping Problem in a TV-Anytime Server Network"
        },
        {
            "abstract": "We introduce an approach for power optimization using a set of compilation and architectural techniques. The key technical innovation is a novel divide-and-conquer compilation technique to minimize the number of operations for general computations. Our technique optimizes not only a significantly wider set of computations than the previously published techniques, but also outperforms (or performs at least as well as other techniques) on all examples. Anong the architectural dimension, we investigate coordinated impact of compilation techniques on the number of processors which provide optimal trade-off between cost and power. We demonstrate that proper compilation techniques can significantly reduce power with bounded hardware cost. The effectiveness of all techniques and algorithms is documented on numerous real-life designs.",
            "group": 569,
            "name": "10.1.1.28.632",
            "keyword": "ersOptimization General TermsAlgorithms Additional Key Words and PhrasesCode generationTransformations",
            "title": "Power Optimization using Divide-and-Conquer Techniques for Minimization of the Number of Operations"
        },
        {
            "abstract": "We address the problem of maximizing application speedup through runtime, self-selection of an appropriate number of processors on which to run. Automatic, runtime selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data or time dependent. We propose the use of a runtime system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job's processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our runtime system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation.  ",
            "group": 570,
            "name": "10.1.1.28.642",
            "keyword": "",
            "title": "Maximizing Speedup through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "In this paper, on the one hand, we aim to give a review on literature dealing with the problem of supervised learning aided by additional unlabeled data. On the other hand, being a part of the author's first year PhD report, the paper serves as a frame to bundle related work by the author as well as numerous suggestions for potential future work. Therefore, this work contains more speculative and partly subjective material than the reader might expect from a literature review. We give a rigorous definition of the problem and relate it to supervised and unsupervised learning. The crucial role of prior knowledge is put forward, and we discuss the important notion of input-dependent regularization. We postulate a number of baseline methods, being algorithms or algorithmic schemes which can more or less straightforwardly be applied to the problem, without the need for genuinely new concepts. However, some of them might serve as basis for a genuine method. In the literature revi...",
            "group": 571,
            "name": "10.1.1.28.850",
            "keyword": "",
            "title": "Learning with Labeled and Unlabeled Data"
        },
        {
            "abstract": "In this thesis, we deal with the following k-way graph partitioning (GP) problem: given an undirected weighted graph G(V; E), partition the nodes of  G into k parts of almost equal size such that the partition-cost (sum of the weights on edges with nodes in different parts) is minimized. We propose some simple and fast algorithms for this problem for both sequential and distributed computing environments. We give three main algorithms for graph partitioning: direct algorithm Auction; and iterative algorithms GreedyPass and GreedyCycle. In the algorithm  Auction, we introduce the idea of using auction and biddings for the GP problem. This is an inherently distributed algorithm. To the depth of our knowledge this is the first distributed algorithm for this problem. The algorithm  GreedyPass is a greedy iterative algorithm. In each iteration we send a node from the current part to another part in order to get maximum decrease in partition-cost, and iteration can continue taking the destin...",
            "group": 572,
            "name": "10.1.1.28.1913",
            "keyword": "",
            "title": "Sequential And Distributed Algorithms For Fast Graph Partitioning"
        },
        {
            "abstract": "Advances in storage and high-speed broadband communication network technologies are making it feasible to design and implement distributed scalable Video-onDemand server networks. In this paper, a hierarchical structure for a server network is proposed to support a metropolitan TV-Anytime service. A key issue, how to map the media assets onto such a hierarchical server network, is addressed in this paper. The definition and heuristic methods for the solution of this new combinatorial optimization problem are presented. The mapping problem studied here addresses the management of media assets taking the storage capacity, communication bandwidth limitation and access pattern into account. The methods presented in this paper are based on parallel simulated annealing and are verified by the use of a set of benchmark instances. The results show that the methods provide a near to optimal performance in terms of the Quality of Service (QoS).  1 Introduction  1.1 Motivation  Video-on-Demand (V...",
            "group": 573,
            "name": "10.1.1.28.2088",
            "keyword": "",
            "title": "Solving a Media Mapping Problem in a Hierarchical Server Network with Parallel Simulated Annealing"
        },
        {
            "abstract": "The Imprecise Computation technique has been proposed as an approach to the construction of real-time systems that are able to provide both guarantee and flexibility. This paper analyzes the use of Imprecise Computation in the scheduling of distributed real-time applications. Initially it is presented an approach to the scheduling of distributed imprecise tasks. Then we discuss the main problems associated with that goal and some possible solutions.  Keywords: Real-time systems, scheduling, imprecise computation, distributed systems.  Correspondence should be sent to:  Rmulo Silva Oliveira  Instituto de Informtica  Univ. Fed. do Rio Grande do Sul  Caixa Postal 15064  Porto Alegre-RS, 91501-970, Brasil  romulo@inf.ufrgs.br  Phone: +55 (51) 316-6828  Fax: +55 (51) 319-1576  On Scheduling Imprecise Tasks in Real-Time Distributed Systems  Rmulo Silva de Oliveira Joni da Silva Fraga  II - Univ. Fed. do Rio Grande do Sul LCMI-DAS - Univ. Fed. de Santa Catarina  Caixa Postal 15064 Caixa Posta...",
            "group": 574,
            "name": "10.1.1.28.2131",
            "keyword": "",
            "title": "On Scheduling Imprecise Tasks in Real-Time Distributed Systems"
        },
        {
            "abstract": "A fast simulated annealing algorithm is developed for automatic object recognition. The object recognition problem is addressed as the problem of best describing a match between a hypothesized object and an image. The normalized correlation coefficient is used as a measure of the match. Templates are generated on-line during the search by transforming model images. Simulated annealing reduces the search time by orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of how landmarks, e.g., traffic signs, can be recognized by a navigating robot. We illustrate the performance of our algorithm with real-world images of complicated scenes with traffic signs. False positive matches occur only for templates with very small information content. To avoid false positive matches, we propose a method to select model images for robust object recognition by measuring the information content of the model images. The algorithm works well in noisy images for m...",
            "group": 575,
            "name": "10.1.1.28.2882",
            "keyword": "",
            "title": "Fast Object Recognition in Noisy Images Using Simulated Annealing"
        },
        {
            "abstract": "Simulated Annealing is a powerful algorithm for global optimization. Applied to nite games, it yields a simple adaptive procedure leading to Nash equilibrium, whose execution can be fully decentralized. Furthermore, the procedure makes for an eective computational technique, which compares well with state-of-the-art methods.  ",
            "group": 576,
            "name": "10.1.1.28.3892",
            "keyword": "",
            "title": "Simulated annealing of game equilibria: A simple adaptive procedure leading to nash equilibrium "
        },
        {
            "abstract": "The simulated annealing paradigm is a general-purpose stochastic optimization technique that has proven to be an effective tool for approximating globally optimal solutions to many types of NP-hard combinatorial optimization problems. Simulated annealing is based on an analogy with the physical annealing process---a technique in the field of condensed matter physics for obtaining the minimum-energy state of a solid. The paradigm has proven to be especially effective in the field of VLSI design automation. The major drawback of the paradigm is its typically high and sometimes prohibitive computational cost. Accelerating the paradigm has been an active area of research since its introduction in 1983. This dissertation explores two methods of acceleration---a uni-processor approach called two-stage simulated annealing and a multi-processor approach called population-oriented simulated annealing. In a traditional",
            "group": 577,
            "name": "10.1.1.28.4113",
            "keyword": "",
            "title": "On the Acceleration of Simulated Annealing"
        },
        {
            "abstract": "In this thesis, we study optimal anytime stochastic search algorithms (SSAs) for solving general constrained nonlinear programming problems (NLPs) in discrete, continuous and mixed-integer space. The algorithms are general in the sense that they do not assume differentiability or convexity of functions. Based on the search algorithms, we develop the",
            "group": 578,
            "name": "10.1.1.28.5218",
            "keyword": "",
            "title": "Optimal Anytime Search For Constrained Nonlinear Programming"
        },
        {
            "abstract": "The authors have developed a lossy coding scheme using a 3D discrete cosine transform, for use with integral 3D image data. Correct selection of the step sizes of the uniform symmetric scalar quantisers used in the scheme is crucial to attaining good rate-distortion performance. Imageorientated optimisations have been performed to obtain the quantisation parameters leading to minimum objective distortion of the reconstructed image at given bit rates. Simulated annealing, an optimisation method suitable for complex non-linear problems whose objective functions typically possess many local extrema, is used for this purpose. A model for optimal quantisation is then constructed for general use across an arbitrary bit rate range. This paper describes the integral 3D imaging system which generates the image data used, the coding scheme and the simulated annealing technique. The results of simulations using optimal model-derived quantisation arrays are presented and compared with previous use of fixed quantisation parameters.",
            "group": 579,
            "name": "10.1.1.28.5853",
            "keyword": "",
            "title": "Simulated Annealing for Optimisation and Characterisation of Quantisation Parameters in Integral 3D Image Compression"
        },
        {
            "abstract": "",
            "group": 580,
            "name": "10.1.1.28.6222",
            "keyword": "",
            "title": "Combinatorial Optimization"
        },
        {
            "abstract": "This paper briefly reviews some properties of Monte Carlo simulation and emphasizes the link to evolutionary computation. It shows how this connection can help to study evolutionary algorithms within a unified framework. It also gives some practical examples of implementation inspired from MOSES (the mutation-or-selection evolution strategy).",
            "group": 581,
            "name": "10.1.1.28.6541",
            "keyword": "",
            "title": "Monte Carlo Simulation and Population-Based Optimization"
        },
        {
            "abstract": "Optimization of netlists subject to area, delay or resource constraints is a common problem used in almost every stage of CAD tools. Most forms of this problem have been proven to be NP-complete. Instead of taking a global approach to optimize a netlist, this thesis suggests using static hardware profiling to identify important sub-circuits, which may then be given priorityby the tools or human designers. Such profiling helps focus optimization efforts on the portions of the netlist that have the greatest impact on the final result. A static",
            "group": 582,
            "name": "10.1.1.28.6606",
            "keyword": "",
            "title": "Static Profile Driven Optimization Of Digital Circuits"
        },
        {
            "abstract": "In this work we focus on efficient heuristics for",
            "group": 583,
            "name": "10.1.1.28.7981",
            "keyword": "",
            "title": "A Clustering Approach to Solving Large Stochastic Matching Problems"
        },
        {
            "abstract": "In most database systems, the values of many important run-time parameters of the system, the data, or the query are unknown at query optimization time. Parametric query optimization attempts to identify several execution plans, each one of which is optimal for a subset of all possible values of the run-time parameters. We present a general formulation of this problem and study it primarily for the buffer size parameter. We adopt randomized algorithms as the main approach to this style of optimization and enhance them with a sideways information passing feature that increases their effectiveness in the new task. Experimental results of these enhanced algorithms show that they optimize queries for large numbers of buffer sizes in the same time needed by their conventional versions Partially supported by NSF under PYI Grant IRI-9157368 and by grants from DEC, HP, and AT&T. y Supported by a University of Maryland Dissertation Fellowship and by NSF under Grants IRI-8719458 and IRI-910...",
            "group": 584,
            "name": "10.1.1.28.8449",
            "keyword": "",
            "title": "Parametric Query Optimization"
        },
        {
            "abstract": "Most of the work on randomized query optimization has relied heavily on the use of transformations rules for the generation of execution plans. Recently, however, we gave evidence that for the problem of choosing a join evaluation order, generating alternatives uniformly at random from the space yields solutions comparable to those obtained with transformation-intensive methods, and requires generating fewer candidate plans. This paper presents a thorough empirical study of the impact of catalogs and join methods on the relative performance of transformation-free and transformation-based randomized optimization. Basically, our previous results remain valid for a wide variety of catalogs and relational profiles. But in contrast with the problem of selecting a join order, selecting join algorithms (e. g. hash, merge, nested-loops) seems better handled by transformations than random picking. We then propose a two-phase approach that combines the speed of random picking with the quality of solutions of transformation-based optimization, and verify experimentally its superiority over the other algorithms, in all the search spaces considered.",
            "group": 585,
            "name": "10.1.1.28.8618",
            "keyword": "planscounting of query evaluation plans",
            "title": "The impact of catalogs and join algorithms on probabilistic query optimization "
        },
        {
            "abstract": "A new method for on-substrate fine positioning of microscale/mesoscale discrete components is presented [1]--[3], where component positions are finely adjusted using microlinear sliders and fixtures on the substrate. Each microlinear slider is actuated by vibratory impacts exerted by two pairs of microcantilever impacters. These microcantilever impacters are selectively resonated by shaking the entire substrate with a piezoelectric vibrator, requiring no need for built-in driving mechanisms such as electrostatic comb actuators, as reported previously [4], [5]. This selective resonance of the microcantilever impacters via an external vibration energy field [6] provides with a very simple means of controlling forward and backward motion of the microlinear slider, facilitating assembly and disassembly of a microcomponent on a substrate. An analytical model of the device is derived in order to obtain, through the simulated annealing algorithm, an optimal design, which maximizes translation speed of the linear slider at desired external input frequencies. Prototypes of the externally resonated linear microvibromotor are fabricated using the three-layer polysilicon surface micromachining process provided by the Microelectronics Center of North Carolina, Research Triangle Park, NC, multiuser microelectromechanical processes service. These prototypes are tested for forward and backward motion via external vibration applied by an piezoelectric flexure vibrator, as well as the horizontal positioning and release of 500- m-square polysilicon chips against a reference fixture element anchored to the substrate. [457]  Index Terms---Microactuators, micromechanical resonators, vibrations. I. ",
            "group": 586,
            "name": "10.1.1.28.9443",
            "keyword": "",
            "title": "Externally Resonated Linear Microvibromotor for Microassembly"
        },
        {
            "abstract": "Deterministic global optimization plays an essential role in the solution of many difficult problems with applications ranging from economics and operations research to computational chemistry and molecular biology. In this chapter we explore the application of deterministic global optimization approaches to problems related to protein structure prediction. Due to the complex nature of protein interactions, energy landscapes which model these systems display huge numbers of local minima often separated by high energy barriers. Since the number of local minima is vast, the corresponding formulation has earned the simple yet suggestive title of \"multiple-minima\" problem. Based on the complexity of the energy hypersurface, there is an obvious need for the development of effective global optimization techniques. In this work, we have focused on the development of such global optimization methods through the foundations of the ffBB deterministic global optimization approach.  Keywords: Deterministic global optimization, Protein folding, Structure prediction, Energy modeling  1  2  1. ",
            "group": 587,
            "name": "10.1.1.28.9515",
            "keyword": "Deterministic global optimizationProtein foldingStructure predictionEnergy modeling 1",
            "title": "Deterministic Global Optimization For Protein Structure Prediction"
        },
        {
            "abstract": ". Genetic algorithms are now among the most promising optimization techniques. They are based on the following reasonable idea. Suppose that we want to maximize an objective function J(x). We somehow choose the first generation of \"individuals\" x 1 ; x 2 ; :::; x n (i.e., possible values of x) and compute the \"fitness\" J(x i ) of all these individuals. To each individual x i , we assign a survival probability p i that is proportional to its fitness. In order to get the next generation we then repeat the following procedure k times: take two individuals at random (i.e., x i with probability p i ) and \"combine\" them according to some rule. For each individual of this new generation, we also compute its fitness (and survival probability), \"combine\" them to get the third generation, etc. Under certain reasonable conditions, the value of the objective function increases from generation to generation and converges to a maximal value.  The performance of genetic algorithms can be essentially improved if we use fitness scaling, i.e., use f(J(x i )) instead of J(x i ) as a fitness value, where f(x) is some fixed function that is called a scaling function. The efficiency of fitness scaling essentially depends on the choice of f . So what f should we choose?  In the present paper we formulate the problem of choosing f as a mathematical optimization problem and solve it under different optimality criteria. As a result, we get a list of functions f that are optimal under these criteria. This list includes both the functions that were empirically proved to be the best for some problems, and some new functions that may be worth trying.  1  1. ",
            "group": 588,
            "name": "10.1.1.29.29",
            "keyword": "1. INTRODUCTION TO THE PROBLEM",
            "title": "Genetic Algorithms: What Fitness Scaling Is Optimal?"
        },
        {
            "abstract": "This application, developed at France Telecom R&D in Lannion (France), takes place in an ATM network administration context. The problem consists in planning connection demands over a period of one year. A new connection demand can be admitted if both bandwidth requirements and quality of service parameters are satisfied. If not, certain already accepted connections can be rerouted in order to accept the new one. This paper presents the time modeling and the hybrid approach that we used to solve the problem within the allowed computing time. Our method mixes shortest path algorithms, constraint propagation and repairing principles. The results of our experiments upon realistic built problems show that the difficult problem of rerouting can be solved with such a method: our rerouting leads to accept over 38% of connections that are rejected with a greedy algorithm without rerouting, like the already existing one.",
            "group": 589,
            "name": "10.1.1.29.336",
            "keyword": "",
            "title": "Resource Allocation In ATM Networks: A Hybrid Approach"
        },
        {
            "abstract": "In this paper we investigate the performance of the threshold accepting heuristic for the index tracking problem. The index tracking problem consists in minimizing the tracking error between a portfolio and a benchmark. The objective is to replicate the performance of a given index upon the condition that the number of stocks allowed in the portfolio is smaller than the number of stocks in the benchmark index. Transaction costs are incurred each time that the portfolio is rebalanced. We find the composition of a portfolio that tracks the performance of the benchmark during a given period in the past and compare it with the performance of the portfolio in a subsequent period. We report computational results in the cases where the benchmarks are market indices tracked by a small number of assets. We find that the threshold accepting heuristic is an e#cient optimization technique for this problem. Keywords: Threshold Accepting, Heuristic Optimization, Index Tracking, Passive Fund Management.  JEL codes: C61, C63 1  Preliminary and incomplete version (July 20, 2001). Future updates will be available at http:/www.unige.ch/ses/metri/gilli. We thank Peter Winker, Nick Webber and Agim Xhaja for insightful comments. Financial support from the Swiss National Science Foundation (project 12--5248.97) is gratefully acknowledged. 1  1 ",
            "group": 590,
            "name": "10.1.1.29.653",
            "keyword": "Threshold AcceptingHeuristic OptimizationIndex TrackingPassive Fund Management",
            "title": "Threshold Accepting for Index Tracking"
        },
        {
            "abstract": "The Due-Date Bargainer is a useful tool to support negotiation on due-dates between a  manufacturer and its customers. To improve the computational performance of an earlier  version of the Due-Date Bargainer, we present a new soft computing approach. It uses a  genetic algorithm to #nd the best priority sequence of customer orders for resource allocation  and fuzzy logic operations to allocate the resources and determine the order completion times,  following the priority sequence of orders. To extend the Due-Date Bargainer to accomodate  bargaining with several customers at the same time, we propose a method to distribute  the total penalty using marginal penalties for the individual bargainers. A demonstration  software package implementing the improved Due Date Bargainer has been developed. It  is oriented at apparel manufacturing enterprises. Experiments using realistic resource data  and randomly generated orders haveachieved satisfactory results.  Manuscript received at  #  D. Wang is with the Department of Systems Engineering, College of Information Science and Engineering, Northeastern University, Shenyang, 110006, P. R. of China. He was a visiting professor with North Carolina State University while this researchwas being conducted.  ##  S.-C. Fang and H.L.W. Nuttle are with the Department of Industrial Engineering and Operations Research, North Carolina State University, Raleigh, NC 27695, U.S.A.  +  This researchwas supported, in part, by the National Textile Center of the United States of America #Grant Number: I95S-02#, the National Natural Science Foundation #No. 69684005# and National High-Tech Program #No. 863-511-9609-003# of the People's Republic of China.  Dingwei Wang is currently Professor in the Department of Systems Engineering, the College of In...",
            "group": 591,
            "name": "10.1.1.29.749",
            "keyword": "",
            "title": "Soft Computing for Multi-Customer Due-Date Bargaining"
        },
        {
            "abstract": "Introduction  In the last two decades, wireless communication systems such as cordless phones, paging systems, wireless data networks, satellite-based and cellular mobile systems have been steadily increasing in both popular importance and technological sophistication. The first-generation wireless systems were developed in the late 1970's and 1980's and were based on analog technology (such as the Advance Mobile Phone Service (AMPS) by AT&T and Nordic Mobile Telephone (NMT) by Ericsson). As demand increased and digital technology matured in the 1980's and 1990's the second-generation digital wireless systems were designed (such as Global System for Mobile Communications (GSM) in Europe and Digital AMPS in North America). These systems, currently in use, offer higher system capacity and improved quality of service. Third-generation systems, referred to as personal communication systems (PCS), are currently under development and expected to be deployed at the beginning of the 21s",
            "group": 592,
            "name": "10.1.1.29.1560",
            "keyword": "",
            "title": "Global Search Techniques for Problems in Mobile Communications"
        },
        {
            "abstract": "Motivation: The alignment of biological sequences obtained from an algorithm  will in general contain both correct and wrong parts. Hence, to allow for a valid  interpretation of the alignment, the local trustworthiness of the alignment has to  be quantied.  Results: We present a novel approach that attributes a reliability index to every  pair, including gapped regions, in the optimal alignment of two protein sequences.  The method is based on a fuzzy recast of the dynamic programming algorithm for  sequence alignment in terms of mean eld annealing. An extensive evaluation with  structural reference alignments not only shows that the probability for a pair to be  correctly aligned grows consistently with increasing reliability index, but moreover  demonstrates that the value of the reliability index can directly be translated into  an estimate of the probability for a correct alignment.  Running Head: Sequence Alignment Reliability  Keywords: sequence alignment; alignment reliability; fuzzy alignment; mean eld annealing 1  fmaxl,mattiasg@thep.lu.se  ",
            "group": 593,
            "name": "10.1.1.29.1804",
            "keyword": "sequence alignmentalignment reliabilityfuzzy alignmentmean eld annealing 1",
            "title": "A Novel Approach to Local Reliability of Sequence Alignments"
        },
        {
            "abstract": ". This paper deals with Automated Test Pattern Generation (ATPG) for large synchronous sequential circuits and describes a new approach based on Simulated Annea ling. Simulation-based ATPG tools have several advantages with respect to deterministic and symbolic ones, especially because they can deal with large circuits. A prototypical system named SAARA is used to assess the effectiveness of the Simulated Annealing approach in terms of test quality and CPU time requirements. Results are reported, showing that SAARA is able to deal with large sequential circuits. A comparison with a state-of-the-art ATPG tool based on a Genetic Algorithm shows that SAARA generally improves the attained results in terms of fault co verage.  1. ",
            "group": 594,
            "name": "10.1.1.29.2217",
            "keyword": "Simulated AnnealingTest Pattern Generation",
            "title": "SAARA: a Simulated Annealing Algorithm for Test Pattern Generation for Digital Circuits"
        },
        {
            "abstract": "Many experimental results are reported on all types of Evolutionary Algorithms but only few results have been proved. A step towards a theory on Evolutionary Algorithms, in particular, the so-called (1 + 1) Evolutionary Algorithm, is performed. Linear functions are proved to be optimized in expected time O(n ln n) but only mutation rates of size #(1/n) can ensure this behavior. For some polynomial of degree 2 the optimization needs exponential time. The same is proved for a unimodal function. Both results were not expected by several other authors. Finally, a hierarchy result is proved. Moreover, methods are presented to analyze the behavior of the (1 + 1) Evolutionary Algorithm. ",
            "group": 595,
            "name": "10.1.1.29.2470",
            "keyword": "1619Evolutionary Programming [5Genetic Algorithms [87and Genetic",
            "title": "On the Analysis of the (1+1) Evolutionary Algorithm "
        },
        {
            "abstract": "This paper presents a generic approach for surface modelling of 3D objects from volume data. Our strategy is to shape a closed surface of connected triangles to match the edge of the object. The adaptive strategy we suggest invokes minimisation of an energy function containing a set of individually weighted terms reflecting a set of desired goals. From the closed surface of triangles we are able to do efficient 3D visualisation as well as perform quantitative measurements within the object. The framework is very general and applicable in many research areas, and we show the viability of the approach in the context of medical imaging. 1. Introduction  Identification and segmentation of a 3D object measured indirectly by volumetric data, i.e., without a physical signal, like reflections etc. from the surface itself, is a recurrent inversion problem in many areas of science. While it is relatively easy for humans to spot and identify an object from, e.g., images of the individual slices o...",
            "group": 596,
            "name": "10.1.1.29.2705",
            "keyword": "",
            "title": "Active Surface Models for Brain Imaging"
        },
        {
            "abstract": "Supervised lvisedG in  neural  networks based on the populI backpropagation method can be often trapped in  al ocal  minimum of the error function. TheclRq of backpropagation-type trainingalningGOR inclGOE l ocal  minimization methods that have no mechanism that alt ws them to escape the influence of a  lcal  minimum. The existence  ofl ocal  minima is due to the fact that the error function is the superposition of nonlOOjG  activation functions that may have minima at di#erent points, which  sometimesresulm  in a nonconvex error function. This work investigates the use ofglRFq search methods for batch-mode training of feedforward mulROO yer  perceptrons.Glcep  search methods are expected to lOE  to\"optimal or\"near-optimal  weight configurations byalO wing the  neural  network to escape  lcal  minima during training and, in that sense, they improve the e#ciency of  thel  earning process. The paper reviews the fundamental of simulXRq  annealG(--  genetic and  evolOqXOj  ary  alR--P----G(  as wel as some  recentl  proposed deflection procedures. SimulR--Ejj  and comparisons are presented.  Key ords:  Local  minima, simulRjj  annealOG(  genetic  aljEqFG(--I evolXIFjG(-- al  gorithms, deflection procedure,  feedforwardneural  networks, supervised training.  3  4 ADVANCES IN CONVEX ANALYSIS AND GLOBAL OPTIMIZATION ",
            "group": 597,
            "name": "10.1.1.29.3863",
            "keyword": "Local minimasimulated annealinggenetic algorithms",
            "title": "Supervised Training Using Global Search Methods"
        },
        {
            "abstract": "This paper describes work in progress. We are designing a distributed model for planning and scheduling of goods transports in a railway company. Our model emphasises the use of an agent for the coordination of distinct subproblems occurring in railway scheduling such as e.g. the allocation of track resources to transports, the allocation of vehicles to transports, and the allocation of staff to perform the transportation tasks. The intention is that by establishing formal connections between the constraint spaces of agents handling different subproblems, a coordinating agent can be designed which is capable of guiding the subsystems towards a common solution. We thus obtain a coherent view on heterogeneous spaces and focus on the essential parameters of a problem. We outline the railway scheduling problem and the rostering problem and discuss techniques which are relevant in a distributed planning support system.",
            "group": 598,
            "name": "10.1.1.29.3869",
            "keyword": "",
            "title": "Coordination of Scheduling and Allocation Agents (Extended Abstract)"
        },
        {
            "abstract": "In this paper we present and analyze new sequential and parallel heuristics to  approximate the Minimum Linear Arrangement problem (MinLA). The heuristics  consist in obtaining a first global solution using Spectral Sequencing and improving  it locally through Simulated Annealing. In order to accelerate the annealing process,  we present a special neighborhood distribution that tends to favor moves with high  probability to be accepted. We show how to make use of this neighborhood to parallelize  the Metropolis stage on distributed memory machines by mapping partitions  of the input graph to processors and performing moves concurrently. The paper  reports the results obtained with this new heuristic when applied to a set of large  graphs, including graphs arising from finite elements methods and graphs arising  from VLSI applications. Compared to other heuristics, the measurements obtained  show that the new heuristic improves the solution quality, decreases the running  time and o#ers an excellent speedup when ran on a commodity network made of  nine personal computers.  1 ",
            "group": 599,
            "name": "10.1.1.29.4021",
            "keyword": "",
            "title": "Combining Spectral Sequencing and Parallel Simulated Annealing for the MinLA Problem"
        },
        {
            "abstract": "The allocation of scarce spectral resources to support as many user applications as possible while maintaining reasonable quality of service is a fundamental problem in wireless communication. We argue that the problem is best formulated in terms of decision theory. We propose a scheme that takes decision-theoretic concerns (like preferences) into account and discuss the difficulties and subtleties involved in applying standard techniques from the theory of Markov Decision Processes (MDPs) in constructing an algorithm that is decision-theoretically optimal. As an example of the proposed framework, we construct such an algorithm under some simplifying assumptions. Additionally, we present analysis and simulation results that show that our algorithm meets its design goals. Finally, we investigate how far from optimal one well-known heuristic is. The main contribution of our results is in providing insight and guidance for the design of near-optimal admission-control policies.  I. INTRODU...",
            "group": 600,
            "name": "10.1.1.29.4332",
            "keyword": "",
            "title": "A Decision-Theoretic Approach to Resource Allocation in Wireless Multimedia Networks"
        },
        {
            "abstract": "We examine the Concave Topology Capacity and Flow Assignment (TCFA) problem. Only two algorithms in the literature are appropriate for solving TCFA problems with concave link cost functions: Kleinrock and Gerla's Concave Branch Elimination (CBE) procedure, [26][20], and a greedy link elimination procedure developed by Gersht [21]. However, neither works well in practice. The CBE procedure does not perform well in the context of strongly concave link cost functions. While Gersht's algorithm performs well, its processing requirements are such that it is applicable for small network design problems only. We present a Concave Link Elimination (CLE) procedure, based on Gersht's greedy link elimination procedure. Our algorithm is shown to perform at least as well as Gersht's procedure and to be significantly faster than both the CBE and Gersht procedures. In addition, we formulate a lower bounding problem which we solve using a continuous branch-and-bound procedure to assess the quality of t...",
            "group": 601,
            "name": "10.1.1.29.4631",
            "keyword": "",
            "title": "A Concave Link Elimination (CLE) Procedure and Lower Bound for Concave Topology, Capacity and Flow Assignment Network Design Problems"
        },
        {
            "abstract": "this paper a new greedy heuristic for",
            "group": 602,
            "name": "10.1.1.29.5502",
            "keyword": "",
            "title": "A Mapping Heuristic for Minimizing Network Contention"
        },
        {
            "abstract": "Experiments over a variety of optimization problems indicate that scale-effective convergence is an emergent behavior of certain computer-based agents, provided these agents are organized into an asynchronous team (A-Team). An A-Team is a problem-solving architecture in which the agents are autonomous and cooperate by modifying one another's trial-solutions. These solutions circulate continually. Convergence is said to occur if and when a persistent solution appears. Convergence is said to be scale-effective if the quality of the persistent solution increases with the number of agents, and the speed of its appearance increases with the number of computers. This paper uses a traveling salesman problem to illustrate scale-effective behavior and develops Markov models that explain its occurrence in A-Teams, particularly, how autonomous agents, without strategic planning or centralized coordination, can converge to solutions of arbitrarily high quality. The models also predict two properti...",
            "group": 603,
            "name": "10.1.1.29.6218",
            "keyword": "",
            "title": "Asynchronous Teams: Cooperation Schemes For Autonomous Agents"
        },
        {
            "abstract": "Research on potential interactions between connectionist learning systems, i.e., artificial neural networks (ANNs), and evolutionary search procedures, like genetic algorithms (GAs), has attracted a lot of attention recently. Evolutionary ANNs (EANNs) can be considered as the combination of ANNs and evolutionary search procedures. This paper first distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures and of learning rules. Then it reviews each kind of evolution in detail and analyses critical issues related to different evolutions. The review shows that although a lot of work has been done on the evolution of connection weights and of architectures, few attempts have been made to understand the evolution of learning rules. Interactions among different evolutions are seldom mentioned in current research. However, the evolution of learning rules and its interactions with other kinds of evolution play a vital role in EANNs. As t...",
            "group": 604,
            "name": "10.1.1.29.9546",
            "keyword": "LearningEvolutionGenetic Algorithms",
            "title": "A Review of Evolutionary Artificial Neural Networks"
        },
        {
            "abstract": "We propose parametric geons as a volumetric description of object components for qualitative object recognition.  Parametric geons are seven qualitative shape types defined by parameterized equations which control the size and  degree of tapering and bending. The models provide global shape constraints which make model recovery procedures  robust against noise and minor variations in object shape. The surface characteristics of parametric geons are  discussed. The properties of parametric geons and conventional geon models are compared. Experiments fitting parametric  geons to multiview data using stochastic optimization were performed. Results show that unique descriptions  of single-part objects with minor shape variations can be obtained with the parametric geon models.  1. INTRODUCTION  A major problem of interest in computer vision is the derivation of geometrical models as shape descriptions of 3D objects from input images. For object recognition, we usually require a manageable ...",
            "group": 605,
            "name": "10.1.1.30.580",
            "keyword": "",
            "title": "Parametric Geons: A Discrete Set of Shapes with Parameterized Attributes"
        },
        {
            "abstract": "We address the problem of maximizing the speedup of an individual parallel job through the selection  of an appropriate number of processors on which to run it. If a parallel job exhibits speedup that increases  monotonically in the number of processors, the solution is clearly to make use of all available processors.  However, many applications do not have this characteristic: they reach a point beyond which the use of  additional processors degrades performance. For these applications, it is important to choose a processor  allocation carefully.  Our approach to this problem is to provide a runtime system that adjusts the number of processors  used by the application based on dynamic measurements of performance gathered during its execution.  Our runtime system has a number of advantages over user specified fixed allocations, the currently most  common approach to this problem: (1) we are resilient to changes in an application's speedup behavior due  to the input data; and (2) we are...",
            "group": 606,
            "name": "10.1.1.30.731",
            "keyword": "",
            "title": "Maximizing Speedup Through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": ". The Traveling Salesman Problem is a standard test-bed for algorithmic  ideas. Currently, there exists a large number of nature-inspired algorithms for the  TSP and for some of these approaches very good performance is reported. In particular,  the best performing approaches combine solution modification or construction  with the subsequent application of a fast and effective local search algorithm. Yet,  comparisons between these algorithms with respect to performance are often difficult  due to different implementation choices of which the one of the local search  algorithm is particularly critical. In this article we experimentally compare some  of the best performing recently proposed nature-inspired algorithms which improve  solutions by using a same local search algorithm and investigate their performance  on a large set of benchmark instances.  1 Introduction  The task in the Traveling Salesman Problem (TSP) is to find a shorted closed tour through a given set of n cities with ...",
            "group": 607,
            "name": "10.1.1.30.791",
            "keyword": "",
            "title": "A Comparison of Nature Inspired Heuristics on the Traveling Salesman Problem"
        },
        {
            "abstract": "We are considering computations associated with data parallel iterative solvers used for the numerical  solution of Partial Differential Equations (PDEs). The mapping of such computations into  load balanced tasks requiring minimum synchronization and communication is a difficult combinatorial  optimization problem. Its optimal solution is essential for the parallel processing of PDE  computations. Determining data mappings that optimize a number of criteria, like workload balance,  synchronization and local communication, often involves the solution of an NP-Complete problem.  Although data mapping algorithms have been known for a few years there is lack of qualitative  and quantitative comparisons based on the actual performance of the parallel computation. In this  paper we present two new data mapping algorithms and evaluate them together with a large number  of existing ones using the actual performance of data parallel iterative PDE solvers on the nCUBE  II. PDE data mappings can...",
            "group": 608,
            "name": "10.1.1.30.1203",
            "keyword": "",
            "title": "Mapping Algorithms and Software Environment for Data Parallel PDE Iterative Solvers"
        },
        {
            "abstract": "Acoustic design is a difficult problem, because the human perception of sound depends on such things as decibel level, direction of propagation, and attenuation over time, none of which are tangible or visible. The advent of computer simulation and visualization techniques for acoustic design and analysis has yielded a variety of approaches for modeling acoustic performance. However, current computer-aided design and simulation tools suffer from two major drawbacks. First, obtaining the desired acoustic effects may require a long, tedious sequence of modeling and/or simulation steps. Second, current techniques for modeling the propagation of sound in an environment are prohibitively slow and do not support interactive design.  This thesis presents a new approach to computer-aided acoustic design. It is based on the inverse problem of determining material and geometric settings for an environment from a description of the desired performance. The user interactively indicates a range of ...",
            "group": 609,
            "name": "10.1.1.30.1271",
            "keyword": "by",
            "title": "Audioptimization: Goal-Based Acoustic Design"
        },
        {
            "abstract": "This report summarizes research on algorithms for finding particularly good solutions to instances of the NP-complete number-partitioning problem.  1  Our approach is based on stochastic search algorithms, which iteratively improve randomly chosen initial solutions. Instead of searching the space of all 2  n\\Gamma1  possible partitionings, however, we use these algorithms to manipulate indirect encodings of candidate solutions. An encoded solution is evaluated by a decoder, which interprets the encoding as instructions for constructing a partitioning of a given problem instance. We present several different solution encodings, including bit strings, permutations, and rule sets, and describe decoding algorithms for them. Our empirical results show that many of these encodings restrict and reshape the solution space in ways that allow relatively generic search methods, such as hill climbing, simulated annealing, and the genetic algorithm, to find solutions that are often as good as those...",
            "group": 610,
            "name": "10.1.1.30.1299",
            "keyword": "",
            "title": "Stochastic Approximation Algorithms for Number Partitioning"
        },
        {
            "abstract": "Stochastic search has recently been shown to be successful for solving large boolean satisfiability problems. However, systematic methods tend to be more effective in problem domains with a large number of dependent variables: that is, variables whose truth values are directly determined by a smaller set of independent variables. In systematic search, truth values can be efficiently propagated from the independent to the dependent variables by unit propagation. Such propagation is more expensive in traditional stochastic procedures. In this paper we propose a mechanism for effectively dealing with dependent variables in stochastic search. We also provide empirical data showing the procedure outperforms the best previous stochastic and systematic search procedures on large formulas with a high ratio of dependent to independent variables. 1 Introduction  Recent years have seen significant progress in our ability to solve large propositional satisfiability problems. Randomly generated pro...",
            "group": 611,
            "name": "10.1.1.30.1681",
            "keyword": "",
            "title": "Exploiting Variable Dependency in Local Search"
        },
        {
            "abstract": ": The Gibbs random field (GRF) has become a popular image model with applications in restoration, segmentation, reconstruction, edge detection, compression, and motion estimation. Its synthesis of natural-looking texture using only a small number of parameters is a key motivation for its widespread use. However, its wide use belies a number of difficulties inherent in the application of the model. In particular, it has proven difficult to control scale and patterning within the GRF framework, and to estimate parameters for a given pattern. The image processing literature has largely ignored the role of the temperature in the GRF, a parameter that appears in the original statistical mechanics formulation of the GRF. In applications such as simulated annealing, temperature is known to control scale, and in nature, temperature plays a critical role in multiresolution pattern formation, e.g., crystallization. Consequently, examination of GRF temperature parameters provides important insigh...",
            "group": 612,
            "name": "10.1.1.30.2103",
            "keyword": "",
            "title": "Temperature and Gibbs Image Modeling"
        },
        {
            "abstract": "Parallel I/O systems typically consist of individual processors, communication networks, and a large number of disks. Managing and utilizing these resources to meet performance, portability and usability goals of applications has become a significant challenge. We believe that a parallel I/O system that automatically selects efficient I/O plans for user applications is a solution to this problem. In this paper, we present such an automatic performance optimization approach for scientific applications performing collective I/O requests on multidimensional arrays. Under our approach, an optimization engine in a parallel I/O system selects optimal I/O plans automatically without human intervention based on a description of the application I/O requests and the system configuration. To validate our hypothesis, we have built an optimizer that uses a rule-based and randomized search-based algorithms to select optimal parameter settings in Panda, a parallel I/O library for multidimensional arr...",
            "group": 613,
            "name": "10.1.1.30.2444",
            "keyword": "",
            "title": "Automatic Parallel I/O Performance Optimization in Panda"
        },
        {
            "abstract": "In this paper, a connection between Multilayer Feedforward Networks, propositional first order well formed formulas and pseudolinear functions is established. Due to this link, it is possible to formulate optimization problems in first order propositional metalanguage, obtaining the objective function and MFN structure with relative ease. This general technique is applied to solving two 6  P  2 -complete problems, finding a stable model of logic program and a stable answer set of CN-program. A connectionist model for minimization of the objective functions based on gradient descent in the space of inputs of a multilayer feedforward network is proposed. The model's massive parallelism gives raise to a hope that even real world size instances of the two problems might be solved. It is also pointed out that due to mutual relationship between stable semantics of logic programs and extensions of default theories another important problem in nonmonotonic reasoning can be solved with a relati...",
            "group": 614,
            "name": "10.1.1.30.3765",
            "keyword": "",
            "title": "Connectionist Approach to Finding Stable Models and Other Structures in Nonmonotonic Reasoning"
        },
        {
            "abstract": "The maturity of schedulabilty analysis techniques for fixed-priority preemptive scheduling has enabled the consideration of timing issues at design time using a specification of the tasking architecture and estimates of execution times for tasks. While successful, this approach has limitations since the preemptive multi-tasking model does not scale well for a large number of tasks, and the fixed priority scheduling theory does not work well with many object-oriented design methods. In this paper we present an approach that uses a scalable implementation architecture where design level tasks are grouped into a smaller number of run-time threads during implementation. The schedulability analysis for this implementation architecture is based on the preemption threshold scheduling model. We show that our approach provides significant advantages over one using fixed-priority preemptive scheduling architecture. The benefits include higher schedulability for small number of tasks, and lower r...",
            "group": 615,
            "name": "10.1.1.30.4420",
            "keyword": "",
            "title": "Scalable Real-Time System Design using Preemption Thresholds"
        },
        {
            "abstract": "We present an overview of the main problem--independent sequential and parallel  amelioration techniques of the Simulated Annealing algorithm.  We begin by briefly exposing the theoretical framework encompassing the standard  markovian model, the notion of cycle and the optimal temperature schedules.  Theory of cycles yields explicit relationships between the geometry of the energy  landscape and the expected behavior of the algorithm. It leads to the design of efficient  temperature schedules, as well as to improvements of the algorithm behavior  by distorting the cost function.  Next, we present a survey of parallelization techniques, focusing on problem--  independent synchronous strategies. They provide flexible and general tools, allowing  operational research practitioners to take advantage of the computational  power of parallel architectures.  We conclude with an application. It concerns the search for Hamiltonian paths  in cubic graphs. It brings to the fore the efficiency of ...",
            "group": 616,
            "name": "10.1.1.30.4470",
            "keyword": "Simulated AnnealingHeuristicsOptimizationTemperature ScheduleCost Function DistortionParallelizationCubic GraphHamiltonian Path. R'esum'e",
            "title": "Simulated Annealing Algorithm: Amelioration Techniques"
        },
        {
            "abstract": ". We investigate the statistical properties of cut sizes generated by heuristic algorithms which solve the graph bisection problem approximately. On an ensemble of sparse random graphs, we find empirically that the distribution of the cut sizes found by \"local\" algorithms becomes peaked as the number of vertices in the graphs becomes large. Evidence is given that this distribution tends toward a Gaussian whose mean and variance scales linearly with the number of vertices of the graphs. Given the distribution of cut sizes associated with each heuristic, we provide a ranking procedure that takes into account both the quality of the solutions and the speed of the algorithms. This procedure is demonstrated for a selection of local graph bisection heuristics.  Key words. graph partitioning, heuristics, self-averaging, ranking  AMS subject classifications. 90C27, 82B44, 82B30  PII. S1052623497321523  1. Introduction. Algorithms for tackling combinatorial optimization problems [27] may be div...",
            "group": 617,
            "name": "10.1.1.30.5272",
            "keyword": "Key words. graph partitioningheuristicsself-averagingranking",
            "title": "Cut Size Statistics Of Graph Bisection Heuristics"
        },
        {
            "abstract": "We study on-line learning processes in artificial neural networks from a general point of view. On-line learning means that a learning step takes place at each presentation of a randomly drawn training pattern. It can be viewed as a stochastic process governed by a continuous-time master equation. On-line learning is necessary if not all training patterns are available all the time. This occurs in many applications when the training patterns are drawn from a time-dependent environmental distribution. Studying learning in a changing environment, we encounter a conflict between the adaptability and the confidence of the network's representation. Minimization of a criterion incorporating both effects yields an algorithm for on-line adaptation of the learning parameter. The inherent noise of on-line learning makes it possible to escape from undesired local minima of the error potential on which the learning rule performs (stochastic) gradient descent. We try to quantify these often made cl...",
            "group": 618,
            "name": "10.1.1.30.5295",
            "keyword": "",
            "title": "On-Line Learning Processes in Artificial Neural Networks"
        },
        {
            "abstract": "The use of small sensor arrays in modern signal processing systems has recently become more common due to the increase in computational processing power and interest in intelligent sensing and surveillance. However, not much information is available on the design of small sensor arrays having arbitrary geometry, that eectively can accomplish these tasks. In this paper we address the problem of designing such small sensor array systems for angle of arrival (AOA) estimation algorithms. Two dierent cost functions are derived and their applicability is demonstrated in simulation. The accuracy of the AOA estimates is also studied for two dierent array congurations.  1 INTRODUCTION  Angle of arrival estimation has been the interest of research for quite a long time. It is an important eld of study for both military and civilian applications such as acoustic detection, surveillance and tracking. Timedelay based AOA estimation has been addressed in several papers. Also some robust methods...",
            "group": 619,
            "name": "10.1.1.30.5557",
            "keyword": "",
            "title": "A Design Method For Small Sensor Arrays In Angle Of Arrival Estimation"
        },
        {
            "abstract": " ",
            "group": 620,
            "name": "10.1.1.30.5934",
            "keyword": "",
            "title": "Dimensionality Reduction for Agent-Based Learning"
        },
        {
            "abstract": "A new approach for computing qualitative part-based descriptions of 3D objects  is presented. The object descriptions are obtained in two steps: Object segmentation  into parts and part model identification. Beginning with single- or multi-view  range data of a 3D object, we simulate the charge density distribution over an  object's surface which has been tessellated by a triangular mesh. We detect the  deep surface concavities by tracing local charge density minima and then decompose  the object into parts at these points. The individual parts are then modelled  by parametric geons. The latter are seven qualitative shapes, each of which is  formulated by a restricted globally deformed superellipsoid. Model recovery is  performed by fitting all parametric geons to a part and selecting the best model  for the part, based on the minimum fitting residual. A newly defined objective  function and a fast global optimisation technique are employed to obtain robust  model fitting results. Expe...",
            "group": 621,
            "name": "10.1.1.30.6032",
            "keyword": "distribution",
            "title": "Signal-To-Symbol Mapping For Laser Rangefinders"
        },
        {
            "abstract": "This paper presents a novel approach to matching boundaries in  images. Carefully chosen attributes of boundaries are used to build  a parameter space. Potential matches are searched for in the parameter  space, rather than in the topological space. The `goodness' of  each potential match is measured by means of an affinity function.  Final matches are obtained using Simulated Annealing, in which the  topology constraint is integrated to drive a global optimisation.  Our method delivers pairings which are topologically correct and does  not require any of the commonly used constraints, such as the maximum   velocity, constant disparity gradient or the epipolarity constraint.  This characteristic makes the method applicable for motion and tracking,  as well as for stereopsis. The current implementation is fully  presented together with the results obtained, which are most satisfactory.  Keywords: Boundaries, Parameter space, Correspondence computation,  Topology constraint, Simulated an...",
            "group": 622,
            "name": "10.1.1.30.6271",
            "keyword": "BoundariesParameter spaceCorrespondence computationTopology constraintSimulated annealing",
            "title": "Boundary-based Correspondence Computation Using the Topology Constraint"
        },
        {
            "abstract": "The paper presents exact schedulability analyses for real-time systems scheduled at run-time with a static priority pre-emptive dispatcher. The tasks to be scheduled are allowed to experience internal blocking (from other tasks with which they share resources) and (with certain restrictions) release jitter --- such as waiting for a message to arrive. The analysis presented is more general than that previously published, and subsumes, for example, techniques based on the Rate Monotonic approach. In addition to presenting the theory, an existing avionics case study is described and analysed. The predictions that follow from this analysis are seen to be in close agreement with the behaviour exhibited during simulation studies. 1. INTRODUCTION One proposed method of building a hard real time system is from a number of periodic and sporadic tasks, and a common way of scheduling such tasks is by using a static priority preemptive scheduler --- at run-time the highest priority runnable task...",
            "group": 623,
            "name": "10.1.1.30.6436",
            "keyword": "",
            "title": "Applying New Scheduling Theory to Static Priority Pre-emptive Scheduling"
        },
        {
            "abstract": "In most commercial Field-Programmable Gate Arrays (FPGAs) the number of wiring tracks in each channel is the same across the entire chip. A long-standing open question for both FPGAs and channelled gate arrays is whether or not some uneven distribution of routing tracks across the chip would lead to an area benefit. For example, many circuit designers intuitively believe that most congestion occurs near the center of a chip, and hence expect that having wider routing channels near the chip center would be beneficial. In this paper we determine the relative area-efficiency of several different routing track distributions. We first investigate FPGAs in which horizontal and vertical channels contain different numbers of tracks in order to determine if such a directional bias provides a density advantage. Secondly, we examine routing track distributions in which the track capacities vary from channel to channel. We compare the area-efficiency of these non-uniform routing architectures to t...",
            "group": 624,
            "name": "10.1.1.30.6506",
            "keyword": "",
            "title": "Effect of the Prefabricated Routing Track Distribution on FPGA Area-Efficiency"
        },
        {
            "abstract": "In this thesis we present new methods for the automated design of new heuristics in knowledge-lean applications and for finding heuristics that can be generalized to unlearned test cases. These applications lack domain knowledge for credit assignment; hence, operators for composing new heuristics are generally model free, domain independent, and syntactic in nature. The operators we have used are genetics based; examples of which include mutation and crossover. Learning is based on a generate-and-test paradigm that maintains a pool of competing heuristics, tests them to a limited extent, creates new ones from those that perform well in the past, and prunes poor ones from the pool. We have studied four important issues in learning better heuristics: (a) partitioning of a problem domain into smaller subsets, called subdomains, so that performance values within each subdomain can be evaluated statistically, (b) anomalies in performance evaluation within a subdomain, (c) rational scheduling of limited computational resources in testing candidate heuristics in single-objective as well as multi-objective learning, and (d) finding heuristics that can be generalized to unlearned subdomains. We show experimental results in learning better heuristics for (a) process placement for distributed-memory multicomputers, (b) node decomposition in a branch-and-bound search, (c) generation of test patterns in VLSI circuit testing, (d) VLSI cell placement and routing, and (e) blind equalization. ",
            "group": 625,
            "name": "10.1.1.30.6562",
            "keyword": "",
            "title": "Automated Design of Knowledge-Lean Heuristics: Learning, Resource Scheduling, and Generalization"
        },
        {
            "abstract": "We use the Hybrid Simulated Annealing[1] (HSA), a variant of the Simulated Annealing method for optimization of multivariate functions, for the search of the equilibrium structures for quantum systems, for instance  Na small clusters, using ab-initio calculations based in the Car-Parrinello method. The HSA method uses global actualization via the Hybrid Monte Carlo algorithm for the proposal of new configurations but without the systematic erros of the usual Molecular Dynamics methods, so allowing a more effective searching scheme. 1  Hybrid Simulated Annealing This is a stochastic method and is designed to finding the value of the  N--dimensional vector x = (x 1 ; x 2 ; :::; xN  ), absolute minimum of the real function E(x). Is based on an analogy with Statistical Physics[2]: a system with N degrees of freedom (x 1 ; : : : ; xN  ) at temperature T has a probability of being on the state with energy E(x) given by the Gibbs factor:  P (x)  exp(\\GammaE(x)=T ) From this relation we can s...",
            "group": 626,
            "name": "10.1.1.30.7526",
            "keyword": "",
            "title": "Ab-Initio Calculation of Equilibrium Structures for Na Clusters using the Hybrid Simulated Annealing"
        },
        {
            "abstract": "Introduction  The resource constrained project scheduling problem (RCPSP) can be given as follows. A single project consists of a set J = f0; 1; : : : ; n; n+1g of activities which have to be processed. Fictitious activities 0 and n + 1 correspond to the \"project start\" and to the \"project end\", respectively. The activities are interrelated by two kinds of constraints. First, precedence constraints force activity j not to be started before all its immediate predecessor activities comprised in the set P j have been finished. Second, performing the activities requires resources with limited capacities. We have K resource types, given by the set K = f1; : : : ; Kg. While being processed, activity j requires r j;k units of resource type<F",
            "group": 627,
            "name": "10.1.1.30.8099",
            "keyword": "",
            "title": "Heuristic Algorithms for Solving the Resource-Constrained Project Scheduling Problem: Classification and Computational Analysis"
        },
        {
            "abstract": "A new Mean Field Annealing (MFA) formulation is proposed for the mapping problem for mesh-connected architectures. The proposed MFA heuristic exploits the conventional routing scheme used in mesh interconnection topologies to introduce an efficient encoding scheme. An efficient implementation scheme which decreases the complexity of the proposed algorithm by asymptotical factors is also developed. Experimental results also show that the proposed MFA heuristic approaches the speed performance of the fast Kernighan-Lin heuristic while approaching the solution quality of the powerful simulated annealing heuristic. ",
            "group": 628,
            "name": "10.1.1.30.9027",
            "keyword": "",
            "title": "An Efficient Mapping Heuristic for Mesh-Connected Parallel Architectures Based on Mean Field Annealing"
        },
        {
            "abstract": "In this paper, we formulate neural-network training as a constrained optimization problem instead of the traditional formulation based on unconstrained optimization. We show that constraints violated during a search provide additional force to help escape from local minima using our newly developed constrained simulated annealing (CSA) algorithm. We demonstrate the merits of our approach by training neural networks to solve the two-spiral problem. To enhance the search, we have developed a strategy to adjust the gain factor of the activation function. We show converged training results for networks with 4, 5, and 6 hidden units, respectively. Our work is the first successful attempt to solve the two-spiral problem with 19 weights. ",
            "group": 629,
            "name": "10.1.1.30.9539",
            "keyword": "",
            "title": "Constrained Formulations for Neural Network Training and Their Applications to Solve the Two-Spiral Problem"
        },
        {
            "abstract": "This paper presents a new approach to 3D shape representation -- approximating  the shapes of object parts by a set of prescribed volumetric models using  single- and multi-view range data. We define a new set of volumetric part models,  called parametric geons. These are seven qualitative shapes, each of which  is formulated by a restricted globally-deformed superellipsoid. Model recovery is  performed by fitting all parametric geons to a part and selecting the best model  for the part based on the minimum fitting residual. A newly-defined objective  function and a fast global optimisation technique are employed to obtain robust  model fitting results. Parametric geons provide a global shape constraint that  allows model recovery to explicitly verify the resultant part descriptions. Through  systematic experiments, we examine the efficiency of the objective function, the  discriminative ability of parametric geons, the effects of object shape imperfection  to model recovery, and the i...",
            "group": 630,
            "name": "10.1.1.31.102",
            "keyword": "computer vision3D object representationobject descriptionmultiview range data",
            "title": "3-D Shape Approximation Using Parametric Geons"
        },
        {
            "abstract": "This work addresses the data--fusion paradigm of multiple targets detected by multiple sensors in the presence of uncertainty. The integrating--information method is developed in a Bayesian framework which allows to embed the data fusion requirements naturally. The merging task is formulated as the minimization of the energy function of a Potts--spin glass where the interactions are given by the degree of consistency of two measurements performed by different sensors. The estimates of the probabilities on which the interactions rely are provided by of a feed--forward neural network. The information is then combined by a self--organizing system which produces a world image by resolving inconsistencies and integrates prior knowledge, such as geometrical constraints and world assumptions. The method is developed for a generic application, but a potential use, based on a specific problem posed by the aircraft industry is presented. This example allows to identify the main features of the m...",
            "group": 631,
            "name": "10.1.1.31.270",
            "keyword": "Potts Spinsannealinglabel relaxationregularization",
            "title": "Maximal a-Posteriori Multi-Sensor Multi-Target Neural Data Fusion"
        },
        {
            "abstract": "This paper presents a novel approach to the detection and  recognition of qualitative parts like geons from real 2D intensity images.  Previous works relied on semi-local properties of either line drawings or  good region segmentation. Here, in the framework of Model-Based Optimisation,  whole geons or substantial sub-parts are recognised by fitting  parametric deformable contour models to the edge image by means of  a Maximum A Posteriori estimation performed by Adaptive Simulated  Annealing, accounting for image clutter and limited occlusions. A number  of experiments, carried out both on synthetic and real edge images,  are presented. ",
            "group": 632,
            "name": "10.1.1.31.1060",
            "keyword": "",
            "title": "Recognition of Geons by Parametric Deformable Contour Models"
        },
        {
            "abstract": "A finite element method for computational fluid dynamics has been implemented on the Connection Machine systems CM-2 and CM-200. An implicit iterative solution strategy, based on the preconditioned matrix-free GMRES algorithm, is employed. Parallel data structures built on both nodal and elemental sets are used to achieve maximum parallelization. Communication primitives provided through the Connection Machine Scientific Software Library substantially improved the overall performance of the program. Computations of three-dimensional compressible flows using unstructured meshes having close to one million elements, such as a complete airplane, demonstrate that the Connection Machine systems are suitable for these applications. Performance comparisons are also carried out with the vector computers Cray Y-MP and Convex C-1.  ii  Contents  Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2. T...",
            "group": 633,
            "name": "10.1.1.31.1081",
            "keyword": "",
            "title": "A Data Parallel Finite Element Method for Computational Fluid Dynamics on the Connection Machine System"
        },
        {
            "abstract": "The biotechnology revolution stems from rapid advances in the biological sciences. One important product of these advances is a large and rapidly growing data base of biopolymer (DNA, RNA, and protein) sequences, which has attracted much attention from researchers in different fields. The great majority of the techniques generated for studying these data have been designed to analyze a single sequence or for the comparison of a pair of sequences. Multiple sequence analysis has remained a difficult challenge. In recent years, formal statistical models have shown potential in one such problem, multiple sequence alignment. In this article we describe a general statistical paradigm, the unified Gibbs method, for the conversion of nearly any existing method for the analysis of a single sequence or for the comparison of a pair of sequences into a multiple sequence analysis method. Our previous successful experiences with the unified Gibbs include the development of the site sampler, the moti...",
            "group": 634,
            "name": "10.1.1.31.1138",
            "keyword": "Key WordsAlignmentGibbs samplerHidden Markov modelsMarkov chainProtein sequences",
            "title": "Unified Gibbs Method For Biological Sequence Analysis"
        },
        {
            "abstract": "This paper presents a new framework aimed at compile--time determination of satisfactory suboptimal solutions to the mapping problem onto modern massively parallel computing systems. The approach incorporates realistic assumptions on the models both for parallel programs and target architectures. It is refined for the k-ary n-cube family of processor networks that use a deterministic routing algorithm and the wormhole flow control strategy. The proposed mapping heuristic is guided by an evaluation function that approximates the total completion time of a given assignment by taking into account communication delays caused by network contention which are the major contributors to message latencies when network traffic is heavy or unevenly distributed. Results achieved on several program-derived graphs with up to 784 tasks prove the effectiveness of this approach.  ",
            "group": 635,
            "name": "10.1.1.31.1349",
            "keyword": "",
            "title": "Minimizing Network Contention for Mapping Tasks onto Massively Parallel Computers"
        },
        {
            "abstract": "We propose a unified approach to solve low, intermediate and high level computer vision problems for 3D object recognition from range images. All three levels of computation are cast in an optimization framework and can be implemented on neural network style architecture. In the low level computation, the tasks are to estimate curvature images from the input range data. Subsequent processing at the intermediate level is concerned with segmenting these curvature images into coherent curvature sign maps. In the high level, image features are matched against model features based on an object description called  attributed relational graph (ARG). We show that the above computational tasks at each of the three different levels can all be formulated as optimizing a two-term energy function. The first term encodes unary constraints while the second term binary ones. These energy functions are minimized using parallel and distributed relaxation-based algorithms which are well suited for neural...",
            "group": 636,
            "name": "10.1.1.31.1530",
            "keyword": "",
            "title": "Toward 3D Vision from Range Images: An Optimization Framework and Parallel Networks"
        },
        {
            "abstract": "This paper focuses on approximating object part shapes by distinctive types of volumetric primitives. Shape approximation is accomplished by fitting volumetric models called parametric geons to multiview range data of single-part objects and classifying the fitting residuals. Parametric geons are seven qualitative shape types defined by parameterized equations which control the size and degree of tapering and bending. Model fitting is performed by minimizing an objective function which measures the similarity in both size and shape between models and objects. Multiple view data, global shape constraints and global optimization are employed to obtain unique models and to compensate for noise and minor variations in object shape. This approach has been studied in experiments with both synthetic 3D data and actual rangefinder data of perfect and imperfect geon-like objects.  1 Introduction  The interest in the derivation of part-based descriptions of 3D objects arises in part because such...",
            "group": 637,
            "name": "10.1.1.31.1851",
            "keyword": "",
            "title": "Recovering Parametric Geons from Multiview Range Data"
        },
        {
            "abstract": "",
            "group": 638,
            "name": "10.1.1.31.1961",
            "keyword": "",
            "title": "Planning and Scheduling"
        },
        {
            "abstract": "We use the Genetic Algorithm (GA), a heuristic search and optimization technique inspired by biological evolution, to search for or \"evolve\" models of partially known nonlinear dynamical systems. We use certain assumptions about the class of \"goal\" systems (those being modeled), to build constraints into our \"model\" systems, which consist of functions represented by tables of numbers. Further knowledge is incorporated into our error metric, which is defined (only) for autonomous dynamical systems. Because we assume that both model and goal systems are autonomous (invariant with respect to translation in time), it is possible to compare them on the basis of the geometry of their respective phase portraits. Thus we formulate a measure, based on phase portrait geometry, of the error or \"distance\" between dynamical systems. By minimizing the distance separating the model from the goal system, the Genetic Algorithm is usually able to find an approximation of the goal system.  We have used G...",
            "group": 639,
            "name": "10.1.1.31.2190",
            "keyword": "",
            "title": "Evolving Dynamical Systems with the Genetic Algorithm"
        },
        {
            "abstract": "In this paper a group of participants of the 12th European Summer  Institute which took place in Tenerife, Spain in June 1995 present  their views on the state of the art and the future trends in Locational  Analysis. The issues discussed include modelling aspects in  discrete Location Theory, the influence of the distance function, the  relation between discrete, network and continuous location, heuristic  techniques, the state of technology and undesirable facility location.  Some general questions are stated regarding the applicability of location  models, promising research directions and the way technology  affects the development of solution techniques.   ",
            "group": 640,
            "name": "10.1.1.31.3281",
            "keyword": "",
            "title": "Some Personal Views on the Current State and the Future of Locational Analysis"
        },
        {
            "abstract": " ",
            "group": 641,
            "name": "10.1.1.31.3361",
            "keyword": "",
            "title": "The Hybrid Monte Carlo method and Global Optimization problems"
        },
        {
            "abstract": "Multilevel algorithms are a successful class of optimisation techniques which address the mesh partitioning problem for mapping meshes onto parallel computers. They usually combine a graph contraction algorithm together with a local optimisation method which refines the partition at each graph level. To date these algorithms have been used almost exclusively to minimise the cut-edge weight in the graph with the aim of minimising the parallel communication overhead. However it has been shown that for certain classes of problem, the convergence of the underlying solution algorithm is strongly influenced by the shape or aspect ratio of the subdomains. In this paper therefore, we modify the multilevel algorithms in order to optimise a cost function based on aspect ratio. Several variants of the algorithms are tested and shown to provide excellent results.  1 Introduction  The need for mesh partitioning arises naturally in many finite element (FE) and finite volume (FV) applications. Meshes...",
            "group": 642,
            "name": "10.1.1.31.3416",
            "keyword": "2",
            "title": "Multilevel Mesh Partitioning for Optimising Domain Shape"
        },
        {
            "abstract": "The paper introduces duty measure for optimization methods. Duty expresses the  relationship between the quality of the result and the time required to obtain the result. The usefulness of the duty measure is demonstrated on a case study involving a local optimization of a large traveling salesman problem. Using duty, a deterministic method  and a probabilistic method are combined into a hybrid method. The hybrid method  exhibits the best quality-time tradeoff of the three methods. The performance of the hybrid method is analyzed and some future research questions are addressed.",
            "group": 643,
            "name": "10.1.1.31.4227",
            "keyword": "combinatorial optimizationlocal searchtraveling salesman problemdutyquality-time tradeoff",
            "title": "Using the Quality-Time Tradeoff in Local Optimization"
        },
        {
            "abstract": "This paper introduces a mobility tracking mechanism that combines a movement-based location update policy with a selective paging scheme. Movement-based location update is selected for its simplicity. It does not require each mobile terminal to store information about the arrangement and the distance relationship of all cells. In fact, each mobile terminal only keeps a counter of the number of cells visited. A location update is performed when this counter exceeds a predefined threshold value. This scheme allows the dynamic selection of the movement threshold on a per-user basis. This is desirable as different users may have very different mobility patterns. Selective paging reduces the cost for locating a mobile terminal in the expense of an increase in the paging delay. In this paper, we propose a selective paging scheme which significantly decreases the location tracking cost under a small increase in the allowable paging delay. We will introduce an analytical model for the proposed...",
            "group": 644,
            "name": "10.1.1.31.4351",
            "keyword": "Key WordsPersonal Communication NetworksLocation UpdateTerminal PagingMobile Terminal. 1",
            "title": "Movement-Based Location Update and Selective Paging for PCS Networks"
        },
        {
            "abstract": "of a Thesis  Presented to  the Faculty of the Department of Computer Science  University of Houston  In Partial Fulfillment  of the Requirements for the Degree  Masters of Science  By  Blaz Zupan  December, 1993  iv  Abstract  Embedded rule-based expert systems must satisfy stringent timing constraints when applied to real-time environments. This thesis describes a novel approach to reduce the response time of rule-based expert systems. Our optimization method is twofold: the first phase constructs the reduced cycle-free finite state transition system corresponding to the input rule-based system, and the second phase further refines the constructed transition system using the simulated annealing approach. The method makes use of rule-base system decomposition, concurrency and stateequivalency. The new and optimized system is synthesized from the derived transition system. Compared with the original system, the synthesized system has (1) fewer number of rule firings to reach the fixed p...",
            "group": 645,
            "name": "10.1.1.31.4364",
            "keyword": "ChrisAndrzejNicolaAlessandro... Special thanks goes to my special friends",
            "title": "Optimization Of Real-Time Rule-Based Expert Systems"
        },
        {
            "abstract": "Optimization is concerned with the finding of global optima (hence the name) of problems that can be cast in the form of a function of several variables and constraints thereof. Among the searching methods, Evolutionary Algorithms have been shown to be adaptable and general tools that have often outperformed traditional ad hoc methods. The Breeder Genetic Algorithm (BGA) combines a direct representation with a nice conceptual simplicity. This work contains a general description of the algorithm and a detailed study on a collection of function optimization tasks. The results show that the BGA is a powerful and reliable searching algorithm. The main discussion concerns the choice of genetic operators and their parameters, among which the family of Extended Intermediate Recombination (EIR) is shown to stand out. In addition, a simple method to dynamically adjust the operator is outlined and found to greatly improve on the already excellent overall performance of the algorithm.  ",
            "group": 646,
            "name": "10.1.1.31.4593",
            "keyword": "",
            "title": "A Study in Function Optimization with the Breeder Genetic Algorithm"
        },
        {
            "abstract": "Determining the most appropriate network architecture for a data generating process (DGP) is a fundamental aspect of modeling relationships via artificial neural networks. Cross-validatory techniques rank among the most popular approaches toward architecture-selection. Cross-validation is used to estimate the expected squared prediction error of a model. Architecture-selection via cross-validation proceeds from the fact that the true but unknown DGP will minimize expected squared prediction error, and selects that model which minimizes the cross-validatory estimate of expected squared prediction error. Conventional wisdom holds that cross-validation is an unbiased but highly variable method for determining the expected squared prediction error of a network architecture. This paper begins by demonstrating that the conventional wisdom may not hold in applied nonlinear settings, and then proceeds to an analysis of the bias and variance of network complexity which can often occur when netw...",
            "group": 647,
            "name": "10.1.1.31.4987",
            "keyword": "Model complexitydata-driven methodsmean square error. 2",
            "title": "On Architecture Selection, Cross-Validation, and Complexity Bias"
        },
        {
            "abstract": "Imagine yourself standing in front of an exquisite buffet filled with numerous delicacies. Your goal is to try them all out, but you need to decide in what order. What exchange of tastes will maximize the overall pleasure of your palate? Although much less pleasurable and subjective, that is the type of problem that query optimizers are called to solve. Given a query, there are many plans that a database management system (DBMS) can follow to process it and produce its answer. All plans are equivalent in terms of their final output but vary in their cost, i.e., the amount of time that they need to run. What is the plan that needs the least amount of time?",
            "group": 648,
            "name": "10.1.1.31.5178",
            "keyword": "",
            "title": "Query Optimization"
        },
        {
            "abstract": "In a distributed hard real time system communication between tasks on different  processor must occur in bounded time. The inevitable communication delay  (termed the end-to-end delay) is composed from the delay in transmitting a  message on the communications media, and also from the delay in delivering the  data to the destination task. This paper gives schedulability analysis bounding the  media access delay and the delivery delay, and hence allows the end-to-end delay  for a message to be bounded. Two access protocols are considered, TDMA and a  802.5-style token ring. Two approaches are also considered for delivery: an ondemand  protocol in which each packet is delivered to the host when it arrives, and  a periodic server approach in which the host polls for incoming messages. The  schedulability analysis covers all combinations of these access and delivery  methods. In addition the effect of incoming messages on the destination processor  and its workload is addressed.   ",
            "group": 649,
            "name": "10.1.1.31.5335",
            "keyword": "",
            "title": "Guaranteeing Hard Real Time End-to-End Communications Deadlines"
        },
        {
            "abstract": "Contents  Table of Contents v List of Tables x List of Figures xi 1. Introduction 1 1.1 Detecting Instability In Numerical Algorithms : : : : : : : : : : 1 1.2 Overview of Functional Stability Analysis : : : : : : : : : : : : : 2 1.3 Results : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.4 Organization : : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 2. Theoretical Background 7 2.1 Problems and Conditioning : : : : : : : : : : : : : : : : : : : : 8 2.1.1 Definitions : : : : : : : : : : : : : : : : : : : : : : : : : : 8 2.1.2 Problems and Conditioning : : : : : : : : : : : : : : : : 9 2.1.3 Alternative Treatments and Descriptions : : : : : : : : : 12 2.2 Approximations and Stability : : : : : : : : : : : : : : : : : : : 12 2.2.1 Definitions : : : : : : : : : : : : : : : : : : : : : : : : :",
            "group": 650,
            "name": "10.1.1.31.5708",
            "keyword": "",
            "title": "Functional Stability Analysis Of Numerical Algorithms"
        },
        {
            "abstract": "A NEW HIGHER-ORDER BINARY-INPUT NEURAL UNIT:  LEARNING AND GENERALIZING EFFECTIVELY  VIA USING MINIMAL NUMBER OF MONOMIALS  Erol Sahin  M.S. in Computer Engineering  Supervisor: Asst. Prof. Dr. Marifi Guler  Cosupervisor: Prof. Dr. Nese Yalabik  February 1994, 69 pages  Higher-Order Neuron (HON) models can be expressed as a finite sum of monomials, if the inputs are binary. Although they represent an interesting and powerful class of computational models, combinatorial explosion in the number of monomials, has been a major bottleneck. Claiming that for a particular problem, only a small number of monomials are relevant, while the remaining ones with relatively smaller weights can be ignored (and must be ignored!), a new model, which is expressed as a finite sum of, so-called product terms, is proposed to determine the most relevant monomials, with their weights also, is proposed. A learning algorithm based on gradient-descent is derived. Also a novel \"Generalization Hypothesis\" is prop...",
            "group": 651,
            "name": "10.1.1.31.5826",
            "keyword": "",
            "title": "A New Higher-order Binary-input Neural Unit: Learning and Generalizing Effectively via Using Minimal Number of Monomials"
        },
        {
            "abstract": "The Hopfield-style network, a variant of the popular Hopfield neural network, has earlier been shown to have fixed points (stable states) that correspond 1-1 with the maximal cliques of the underlying graph. The network sequentially transforms an initial state (set of vertices) to a final state (maximal clique) via certain greedy operations. It has also been noted that this network can be used to store simple, undirected graphs. In the following paper, we exploit these properties to view the Hopfield-style Network as a Maximal Clique Graph Machine. We show that certain problems can be reduced to finding Maximal Cliques on graphs in such a way that the network computations lead to the desired solutions. The theory of NP-Completeness shows us one such problem, SAT, that can be reduced to the Clique problem. In this paper, we show how this reduction allows us to answer certain questions about a CNF formula, via network computations on the corresponding maximal cliques. We also pr...",
            "group": 652,
            "name": "10.1.1.31.6673",
            "keyword": "",
            "title": "The Hopfield-style Network as a Maximal-Clique Graph Machine"
        },
        {
            "abstract": ". This talk describes how techniques developed by Computer  Scientists have helped our understanding of certain problems in statistical  physics which involve randomness and \"frustration\". Examples will  be given from two problems that have been widely studied: the \"spin  glass\" and the \"random field model\".  1 Introduction  An important part of the area of physics known as \"statistical physics\" is the study of phase transitions, at which the system converts from one state to another. Most interest has centered on \"second order\" or \"continuous\" transitions, in which the property which distinguishes the two phases vanishes continuously as the transition is approached. The disappearance of the magnetization of a ferromagnet, such as iron, as the temperature is increased is generally continuous. At the other type of transition, known as \"first order\" or \"discontinuous\", there is a jump in the properties of the system as the transition is crossed, and also a latent heat. An everyday exampl...",
            "group": 653,
            "name": "10.1.1.31.7187",
            "keyword": "",
            "title": "Computer Science in Physics"
        },
        {
            "abstract": "Key words  Classification system and/or index terms (if any)  Supplementary bibliographical information Language  ISSN and key title ISBN  Recipient's notes Number of pages Price  Security classification  Distribution by (name and address)  I, the undersigned, being the copyright owner of the abstract of the above-mentioned dissertation, hereby grant to all reference  sources the permission to publish and disseminate the abstract of the above-mentioned dissertation.  Signature Date  LUND UNIVERSITY  Department of Theoretical Physics  Slvegatan 14A  223 62 LUND  September 2000  Erik Sandelin  Thermodynamics of Protein Folding and Design  The protein folding and protein design problems are addressed,  using coarse-grained models with only two types of amino acids,  hydrophobic and hydrophilic. In addition to hydrophobicity forces,  the models contain sequence-independent local interactions which  are found to strongly influence the thermodynamics of these models.  The models are studied ...",
            "group": 654,
            "name": "10.1.1.31.7536",
            "keyword": "",
            "title": "Thermodynamics of Protein Folding and Design"
        },
        {
            "abstract": "In the past two decades, the simulated annealing technique has been considered as a powerful approach to handle many NP-hard optimization problems in VLSI designs. Recently, a new Monte Carlo and optimization technique, named simulated tempering, was invented and has been successfully applied to many scientific problems, from random field Ising modeling to the traveling salesman problem. It is designed to overcome the drawback in simulated annealing  when the problem has a rough energy landscape with many local minima separated by high energy barriers. In this paper, we have successfully applied a version of relaxed  simulated tempering to slicing floorplan design with consideration of both area and wirelength optimization. Good experimental results were obtained. 1 Introduction  Since its introduction in early 1980s, simulated annealing  has been one of the most popular optimization algorithms used in the VLSI CAD field in the past two decades. It has been applied to almost every step...",
            "group": 655,
            "name": "10.1.1.31.7824",
            "keyword": "",
            "title": "Relaxed Simulated Tempering for VLSI Floorplan Designs"
        },
        {
            "abstract": "We propose a discrete variant of the Bak-Sneppen model for self-organized criticality. In this process, a configuration is an n-bit word, and at each step one chooses a random bit of minimum value (usually a zero) and replaces it and its two neighbors by independent Bernoulli variables with parameter p. We prove bounds on the average number of ones in the stationary distribution and present experimental results.  1 Introduction 1.1 Background How does one model rare catastrophic events such as avalanches, volcanic eruptions, and extinctions of species? Self-organizing criticality is a name common to such models. It refers to the tendency of slowlydriven dissipative systems with many degrees of freedom to evolve intermittently in terms of bursts spanning all scales up to system size. These systems traverse \"rugged landscapes\" in the space of configurations in search of their optimal configuration, with extremely slow relaxation dynamics. One of the paradigms of self-organizing criticali...",
            "group": 656,
            "name": "10.1.1.31.8100",
            "keyword": "",
            "title": "On the discrete Bak-Sneppen model of self-organized criticality"
        },
        {
            "abstract": "We derive real-time global optimization methods for several clustering optimization problems used in unsupervised texture segmentation. Speed is achieved by exploiting the topological relation of features to design a multiscale optimization technique, while accuracy and global optimization properties are gained using a deterministic annealing method. Coarse grained cost functions are derived for both central and sparse pairwise clustering, where the problem of coarsening sparse random graphs is solved by the concept of structured randomization. Annealing schedules and coarse-to-fine optimization are tightly coupled by a statistical convergence criterion derived from computational learning theory. The algorithms are benchmarked on Brodatz-like micro-texture mixtures. Results are presented for an autonomous robotics application.",
            "group": 657,
            "name": "10.1.1.31.8178",
            "keyword": "",
            "title": "Multiscale Annealing for Real-Time Unsupervised Texture Segmentation"
        },
        {
            "abstract": "The field of personal computing has begun to make a transition from the desktop to handheld devices, thereby requiring input paradigms that are more suited for single hand entry than a keyboard and recent developments in online handwriting recognition allow for such input modalities. Data entry using a pen forms a natural, convenient interface. The large number of writing styles and the variability between them makes the problem of writer-independent unconstrained handwriting recognition a very challenging pattern recognition problem. The state-of-the-art in online handwriting recognition is such that it has found practical success in very constrained problems. In this thesis, a method of identifying different writing styles, referred to as lexemes, is described. Approaches for constructing both non-parametric and parametric classifiers are described that take advantage of the identified lexemes to f...",
            "group": 658,
            "name": "10.1.1.31.8554",
            "keyword": "",
            "title": "Online Handwriting Recognition Using Multiple Pattern Class Models"
        },
        {
            "abstract": "The work described in this thesis began as an inquiry into the nature and use of optimization programs based on \"genetic algorithms.\" That inquiry led, eventually, to three powerful heuristics that are broadly applicable in gradient-ascent programs: First, remember the locations of local maxima and restart the optimization program at a place distant from previously located local maxima. Second, adjust the size of probing steps to suit the local nature of the terrain, shrinking when probes do poorly and growing when probes do well. And third, keep track of the directions of recent successes, so as to probe preferentially in the direction of most rapid ascent. These algorithms lie at the core of a novel optimization program that illustrates the power to be had from deploying them together. The efficacy of this program is demonstrated on several test problems selected from a variety of fields, including De Jong's famous testproblem suite, the traveling salesman problem, the problem of coo...",
            "group": 659,
            "name": "10.1.1.31.8556",
            "keyword": "",
            "title": "From Genetic Algorithms To Efficient Optimization"
        },
        {
            "abstract": ". There are many problems in statistics that need some powerful  global optimization methods. This paper reviews two efficient methods:  SNTO (sequential number-theoretic methods for optimization) and  TA (the threshold accepting algorithm). A discussion is given of the  applications of these methods to various statistics problems: maximum  likelihood estimation, regression analysis, model selection, experimental  design, projection pursuit, etc.  Key Words and Phrases: Experimental design, global optimization, numbertheoretic methods, nonlinear regression model, projection pursuit, simulated annealing, threshold accepting.  Mathematical Subject Classifications 1991: 65K10.  1 Introduction  There are many problems in statistics that need powerful algorithms for optimization, for example, maximum likelihood estimation, nonlinear regression, projection pursuit and design of experiments. Let f be a function over a domain G, a subset of R  s  . We are required to find the global maximum (m...",
            "group": 660,
            "name": "10.1.1.31.9106",
            "keyword": "threshold",
            "title": "Some Global Optimization Algorithms in Statistics"
        },
        {
            "abstract": "We propose an algorithm for maintaining a partition of dynamic planar graphs motivated  by applications in load balancing for solving partial dierential equations on a  shared memory multiprocessor. We consider planar graphs of bounded face sizes that  can be modied by local insertions or deletions of vertices or edges so that planarity is  preserved. In our paper we describe a data structure that can be updated in O(log n)  time after any such modication of the graph, where n is the current size of the graph,  and allows an almost optimal partition of a required size to be maintained. More precisely,  the size of the separator is within an O(n  ) factor of the optimal for the class  of planar graphs, where  is any positive constant, and can be listed in time proportional  to its size. The dynamic data structure occupies O(n) space and can initially be  constructed in time linear to the size of the original graph.  1 Introduction  Separator theorems are ecient and widely used tool f...",
            "group": 661,
            "name": "10.1.1.31.9812",
            "keyword": "",
            "title": "A Dynamic Algorithm for Maintaining Graph Partitions"
        },
        {
            "abstract": "A novel artificial neural network heuristic (INN) for general constraint satisfaction problems is presented, extending a recently suggested method for binary problems. It employs a particular non-polynomial cost function, based on the information balance between multi-state Potts variables and constraints. Implemented as an annealing algorithm, the method is numerically explored on a testbed of Graph Coloring problems. The performance is comparable to that of dedicated heuristics, and clearly superior to that of a conventional mean-field ANN approach. An appealing feature of the method is its versatility, inherited from ANN; it is applicable to a wide range of discrete constraint satisfaction problems.  1  henrik@thep.lu.se  2  bs@thep.lu.se  1 Introduction  Artificial Neural Networks (ANN) have provided a versatile heuristic approach to combinatorial optimization and constraint satisfaction [7, 14, 4].  In a recent paper [9], an improved ANN approach (INN) to binary constraint satisfa...",
            "group": 662,
            "name": "10.1.1.31.9847",
            "keyword": "",
            "title": "An Information-Based Neural Approach to Generic Constraint Satisfaction"
        },
        {
            "abstract": "There has been substantial recent interest in two new families of search techniques. One family consists of nonsystematic methods such as gsat; the other contains systematic approaches that use a polynomial amount of justification information to prune the search space. This paper introduces a new technique that combines these two approaches. The algorithm allows substantial freedom of movement in the search space but enough information is retained to ensure the systematicity of the resulting analysis. Bounds are given for the size of the justification database and conditions are presented that guarantee that this database will be polynomial in the size of the problem in question. 1 INTRODUCTION  The past few years have seen rapid progress in the development of algorithms for solving constraintsatisfaction problems, or csps. Csps arise naturally in subfields of AI from planning to vision, and examples include propositional theorem proving, map coloring and scheduling problems. The probl...",
            "group": 663,
            "name": "10.1.1.31.9942",
            "keyword": "",
            "title": "GSAT and Dynamic Backtracking"
        },
        {
            "abstract": "We consider the following map labelling problem: given distinct points p  1  , p  2  , . . . , p  n  in the plane, and given #, find a maximum cardinality set of pairwise disjoint axis-parallel # # squares Q 1 , Q 2 , . . . , Q r . This problem reduces to that of finding a maximum cardinality independent set in an associated graph called the conflict graph. We describe several heuristics for the maximum cardinality independent set problem, some of which use an LP solution as input. Also, we describe a branch-and-cut algorithm to solve it to optimality. The standard independent set formulation has an inequality for each edge in the conflict graph which ensures that only one of its endpoints can belong to an independent set. To obtain good starting points for our LP-based heuristics and good upper bounds on the optimal value for our branch-and-cut algorithm we replace this set of inequalities by the set of inequalities describing all maximal cliques in the conflict graph. For this streng...",
            "group": 664,
            "name": "10.1.1.32.242",
            "keyword": "",
            "title": "Algorithms for Maximum Independent Set Applied to Map Labelling"
        },
        {
            "abstract": "Deep dyslexia is an acquired reading disorder marked by the occurrence of semantic errors  (e.g. reading RIVER as \"ocean\"). In addition, patients exhibit a number of other symptoms,  including visual and morphological effects in their errors, a part-of-speech effect, and an  advantage for concrete over abstract words. Deep dyslexia poses a distinct challenge for  cognitive neuropsychology because there is little understanding of why such a variety of  symptomsshould co-occur in virtually all known patients. Hinton & Shallice (1991) replicated  the co-occurrence of visual and semantic errors by lesioning a recurrent connectionist network  trained to map from orthography to semantics. While the success of their simulations is quite  encouraging, there is little understanding of what underlying principles are responsible for  them. In this paper we evaluate and, where possible, improve on the most important design  decisions made by Hinton & Shallice, relating to the task, the network arc...",
            "group": 665,
            "name": "10.1.1.32.416",
            "keyword": "",
            "title": "Deep Dyslexia: A Case Study of Connectionist Neuropsychology"
        },
        {
            "abstract": "Mean field annealing (MFA) algorithm, proposed for solving combinatorial optimization  problems, combines the characteristics of neural networks and simulated  annealing. Previous works on MFA resulted with successful mapping of the algorithm  to some classic optimization problems such as traveling salesperson problem,  scheduling problem, knapsack problem and graph partitioning problem. In this  paper, MFA is formulated for the circuit partitioning problem using the so called  net-cut model. Hence, the deficiencies of using the graph representation for electrical  circuits are avoided. An efficient implementation scheme, which decreases  the complexity of the proposed algorithm by asymptotical factors is also developed.  Comparative performance analysis of the proposed algorithm with two well-known  heuristics, simulated annealing and Kernighan-Lin, indicates that MFA is a successful  alternative heuristic for the circuit partitioning problem.  ",
            "group": 666,
            "name": "10.1.1.32.500",
            "keyword": "Mean field annealingcircuit partitioningnet-cut model",
            "title": "Circuit Partitioning Using Mean Field Annealing"
        },
        {
            "abstract": "In order to efficiently design complex microelectromechanical systems (MEMS) having large numbers of multi-domain components, a hierarchically structured design approach that is compatible with standard IC design is needed. A graphical-based schematic, or structural, view is presented as a geometrically intuitive way to represent MEMS as a set of interconnected lumped-parameter elements. An initial library focuses on suspended-MEMS technology from which inertial sensors and other mechanical mechanisms can be designed. The schematic representation has a simulation interface enabling the designer to simulate the design at the component level. Synthesis of MEMS cells for common topologies provides the system designer with rapid, optimized component layout and associated macro-models. A synthesis module is developed for the popular folded-flexure micromechanical resonator topology. The algorithm minimizes a combination of total layout area and voltage applied to the electromechanical actuato...",
            "group": 667,
            "name": "10.1.1.32.550",
            "keyword": "",
            "title": "Structured Design Of Microelectromechanical Systems"
        },
        {
            "abstract": "Successive, well organized application of transformations has been widely recognized as an exceptionally effective, but complex and difficult CAD task. We introduce a new potential-driven statistical approach for ordering transformations. Two new synthesis ideas are the backbone of the approach. The first idea is to quantify the characteristics of all transformations and the relationship between them based on their potential to reorganize a computation such that the complexity of the corresponding implementation is reduced. The second one is based on the observation that transformations may disable each other not only because they prevent the application of the other transformation, but also because both transformations target the same potential of the computation. These two observations drastically reduce the search space to find efficient and effective scripts for ordering transformations. A key algorithmic novelty is that both conceptual and optimization insights as well as all opti...",
            "group": 668,
            "name": "10.1.1.32.555",
            "keyword": "",
            "title": "Potential-Driven Statistical Ordering of Transformations"
        },
        {
            "abstract": "Cell placement is an important phase of current VLSI circuit design styles as standard cell, gate array, and Field Programmable Gate Array (FPGA). Although nondeterministic algorithms such as Simulated Annealing (SA) have been successful in solving this problem, they are known to be slow. In this paper, we propose a neural network algorithm that produces solutions as good as SA in substantially less time. Our algorithm is based on Mean Field Annealing (MFA) technique, which has been successfully applied to various combinatorial optimization problems. We derive a MFA formulation for the cell placement problem that can easily be applied to all VLSI design styles. To demonstrate that the proposed algorithm is applicable to real world problems, we derive a detailed formulation for the FPGA design style, and generate the layouts of several benchmark circuits. The performance of the proposed cell placement algorithm is evaluated in comparison with commercial automated circuit design software...",
            "group": 669,
            "name": "10.1.1.32.702",
            "keyword": "2",
            "title": "A Fast Neural-Network Algorithm for Cell Placement"
        },
        {
            "abstract": "In this paper, we present a bottom-up clustering algorithm based on recursive collapsing of small cliques in a graph. The sizes of the small cliques are derived using random graph theory. This clustering algorithm leads to a natural parallel implementation in which multiple processors are used to identify clusters simultaneously. We also present a cluster-based partitioning method in which our clustering algorithm is used as a preprocessing step to both the bisection algorithm by Fiduccia and Mattheyses and a ratio-cut algorithm by Wei and Cheng. Our results show that cluster-based partitioning obtains cut sizes up to 49.6% smaller than the bisection algorithm, and obtains ratio cut sizes up to 66.8% smaller than the ratio-cut algorithm. Moreover, we show that cluster-based partitioning produces much stabler results than direct partitioning.",
            "group": 670,
            "name": "10.1.1.32.735",
            "keyword": "",
            "title": "A Parallel Bottom-up Clustering Algorithm with Applications to Circuit Partitioning in VLSI Design"
        },
        {
            "abstract": "System identification using infinite-impulse-response (IIR) model is considered. Because the error surface of IIR filters is generally multi-modal, global optimisation techniques are preferred in order to avoid local minima. An efficient global optimisation method, called the adaptive simulated annealing (ASA), is adopted, and a new batch-recursive ASA algorithm is developed for on-line identification. Simulation study shows that the proposed approach is accurate and has a fast convergence rate, and the results obtained demonstrate that the ASA offers a viable tool to IIR model identification. Keywords -- System identification, IIR filter, global optimisation, adaptive simulated annealing, genetic algorithms.  1. INTRODUCTION  Adaptive IIR filtering has been an active area of research for many years [1, 2]. A major concern in IIR filtering applications is that the cost function is generally multi-modal with respect to the filter coefficients, and the gradient-based algorithm can easily...",
            "group": 671,
            "name": "10.1.1.32.894",
            "keyword": "IIR filterglobal optimisationadaptive simulated annealinggenetic algorithms",
            "title": "IIR Model Identification Using Batch-Recursive Adaptive Simulated Annealing Algorithm"
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN), and propose a general paradigm for multi-way circuit partitioning based on dual net transformation. Given a netlist we first compute a K-way partitioning of nets based on the HDN representation, and then transform the K-way net partition into a K-way module partitioning solution. The main contribution of our work is in the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding function between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present an efficient solution t...",
            "group": 672,
            "name": "10.1.1.32.1233",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "A computer-aided-design (CAD) environment was designed and implemented for the initial design and analysis of large dynamic structures. The design of these types of structures represents a difficult and time consuming task with little support provided by existing CAD packages. The results of this work include:  ffl Algorithmic synthesis of complex, dynamic three-dimensional (3-D) structural space frame geometry from initial design specifications.  ffl Methodology for dynamic analysis of open kinematic chains which is independent of specific joint trajectories.  ffl Dynamic analysis of the complete structure throughout the workspace, including computation of \"worst case\" loading conditions.  ffl Complete algorithmic construction of a Finite Element Analysis (FEA) model for each component of the structure, including conversion of the dynamic loads into a useful form.  ffl Discrete optimization of each link of the structure, including catalog lookup of existing beam sizes and heuristics t...",
            "group": 673,
            "name": "10.1.1.32.1299",
            "keyword": "",
            "title": "An Integrated Environment For Conceptual Design, Synthesis And Analysis Of Dynamic Frame Structures"
        },
        {
            "abstract": "We investigate how to tune a generalized simulated annealing algorithm with piecewise constant cooling schedule to get an optimal convergence exponent. The optimal convergence exponent of generalized simulated annealing algorithms has been computed in [6] and [25]. It is reached only with triangular sequences of temperatures, meaning that different finite sequences are used depending on the time resource available for computations (expressed by an overall number of iterations). We show first that it is possible to get close to the optimal convergence exponent uniformly over suitably bounded families of energy landscapes using a fixed number of temperature steps. Then we show that letting the number of steps increase with the time resource, we can build a cooling schedule which is universally robust with respect to the convergence exponent: a fixed triangular sequence of temperatures gives an optimal convergence exponent for any energy landscape. Piecewise constant temperature sequences...",
            "group": 674,
            "name": "10.1.1.32.1474",
            "keyword": "key wordsLarge deviationsMetropolis dynamicscycle decomposition",
            "title": "Piecewise Constant Triangular Cooling Schedules for Generalized Simulated Annealing Algorithms"
        },
        {
            "abstract": "Genetic Algorithms (GA's) are powerful combinatorial optimizers that are able to find close to optimal solutions for difficult problems by applying the paradigm of evolution with natural selection. We describe a framework for GA's capable of solving certain optimization problems encountered in Geographical Information Systems (GIS's). The framework is especially suited for geographical problems since it is able to exploit their geometrical structure with a novel operator called the Geometrically Local Optimizer. Three such problems are presented as case studies: map labeling, generalization while preserving structure and line simplification. Experiments show that the GA's give good results and are flexible as well.",
            "group": 675,
            "name": "10.1.1.32.1589",
            "keyword": "",
            "title": "Using Genetic Algorithms for Solving Hard Problems in GIS"
        },
        {
            "abstract": "this paper, we aim at:",
            "group": 676,
            "name": "10.1.1.32.2067",
            "keyword": "stochastic optimizationquality controldesign of experiment",
            "title": "Simulated Annealing, Weighted Simulated Annealing and Genetic Algorithm At Work"
        },
        {
            "abstract": " ",
            "group": 677,
            "name": "10.1.1.32.2239",
            "keyword": "",
            "title": "Simulated Annealing Algorithms and Markov chains with Rare Transitions"
        },
        {
            "abstract": "Current and future Internet services will provide a large, rapidly evolving, highly accessed, yet autonomously managed information space. Internet news, perhaps, is the closest existing precursor to such services. It permits autonomous updates, is replicated at thousands of autonomously managed sites, and manages a large database. It gets its performance through massive replication. This paper proposes a scalable mechanism for replicating wide-area, autonomously managed services. We target replication degrees of tens of thousands of weakly consistent replicas. For efficiency, our mechanism probes the network and computes a good logical topology over which to send updates. For scalability, we organize replicas into hierarchical replication groups, analogous to the Internet's autonomous routing domains. We argue that efficient, massive replication does not have to rely on internet multicast. ",
            "group": 678,
            "name": "10.1.1.32.2253",
            "keyword": "",
            "title": "Massively Replicating Services in Wide-Area Internetworks"
        },
        {
            "abstract": "A vast amount of work has been done in recent years on the design, analysis, implementation and verification of special purpose parallel computing systems. This paper presents a survey of various aspects of this work. A long, but by no means complete, bibliography is given.  1. Introduction  Turing [365] demonstrated that, in principle, a single general purpose sequential machine could be designed which would be capable of efficiently performing any computation which could be performed by a special purpose sequential machine. The importance of this universality result for subsequent practical developments in computing cannot be overstated. It showed that, for a given computational problem, the additional efficiency advantages which could be gained by designing a special purpose sequential machine for that problem would not be great. Around 1944, von Neumann produced a proposal [66, 389] for a general purpose storedprogram sequential computer which captured the fundamental principles of...",
            "group": 679,
            "name": "10.1.1.32.3081",
            "keyword": "",
            "title": "Special Purpose Parallel Computing"
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN). Given a netlist, we first compute a K-way partition of the nets based on the HDN representation, and then transform a K-way net partition into a K-way module partitioning solution. The main contribution of our work is the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions, while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding factor between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present efficient solutions to the K-MC problem based on network flow computation. Extensive experimental results show that our algorit...",
            "group": 680,
            "name": "10.1.1.32.3145",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "We propose a declarative implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [3, 5]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. The design of PCCP does not require any additional structure on the underlying constraint system (e.g. fuzzy or belief systems) and therefore also allows a straight forward implementation. We demonstrate the use of this language for implementing randomised algorithms. In particular, we give an extensive treatment of two popular (generic) randomised algorithms, namely Simulated Annealing and Randomised Rounding, and we discuss some instantiations of these algorithms for solving two well-known opti...",
            "group": 681,
            "name": "10.1.1.32.3544",
            "keyword": "",
            "title": "Randomised Algorithms and Probabilistic Constraint Programming"
        },
        {
            "abstract": ". We explore a new general-purpose heuristic for finding highquality  solutions to hard optimization problems. The method, called extremal   optimization, is inspired by \"self-organized criticality,\" a concept  introduced to describe emergent complexity in many physical systems.  In contrast to Genetic Algorithms which operate on an entire \"genepool  \" of possible solutions, extremal optimization successively replaces  extremely undesirable elements of a sub-optimal solution with new, random  ones. Large fluctuations, called \"avalanches,\" ensue that efficiently  explore many local optima. Drawing upon models used to simulate farfrom  -equilibrium dynamics, extremal optimization complements approximation  methods inspired by equilibrium statistical physics, such as simulated  annealing. With only one adjustable parameter, its performance has  proved competitive with more elaborate methods, especially near phase  transitions. Those phase transitions are found in the parameter space of  m...",
            "group": 682,
            "name": "10.1.1.32.3908",
            "keyword": "",
            "title": "Optimizing through Co-Evolutionary Avalanches"
        },
        {
            "abstract": "Acyclic partitioning on combinational boolean networks has wide range of applications, from multiple FPGA chip partitioning to parallel circuit simulation. In this paper, we present two efficient algorithms for the acyclic multi-way partitioning. One is a generalized FMbased algorithm. The other is based on the theory of maximum fanout-free cone (MFFC) decomposition. The acyclic FM-algorithm usually results in larger cut-size, as expected, compared to the undirected FM-algorithm due to the acyclic constraint. To our surprise, however, the MFFC-based acyclic partitioning algorithm consistently produces smaller (50% on average) cut-sized solutions than the conventional FM-algorithm. This result suggests that considering signal directions during the process can lead to very natural circuit decomposition and clustering, which in turn results in better partitioning solutions. We have also implemented parallel gate level simulators in Maisie and applied our partitioning algorithms to evaluat...",
            "group": 683,
            "name": "10.1.1.32.4263",
            "keyword": "",
            "title": "Acyclic Multi-Way Partitioning of Boolean Networks"
        },
        {
            "abstract": "Important layout properties of electronic circuits include space requirements and interconnection lengths. In the process of designing these circuits, a reliable pre-layout interconnection length estimation is essential for improving placement and routing techniques. Donath found an upper bound for the average interconnection length that follows the trends of experimentally observed average lengths. Yet, this upper bound deviates from the experimental value by a factor ffi  2, which is not sufficiently accurate for some applications. We show that we obtain a significantly more accurate estimate by taking into account the inherent features of the optimal placement process.  Keywords: Interconnection length estimates, Donath's hierarchical placement, Rent's rule, Occupancy probability. 1 INTRODUCTION  The production of VLSI and ULSI computer chips requires the layout (placement and routing) of the (logical) chip design onto a physical carrier. With the advent of high level description la...",
            "group": 684,
            "name": "10.1.1.32.4937",
            "keyword": "ruleOccupancy probability",
            "title": "Accurate Interconnection Length Estimations for Predictions Early in the Design Cycle"
        },
        {
            "abstract": " Reinforcement learning algorithms based on the principles of Dynamic Programming (DP) have enjoyed a great deal of recent attention both empirically and theoretically. These algorithms have been referred to generically as Incremental Dynamic Programming (IDP) algorithms. IDP algorithms are intended for use in situations where the information or computational resources needed by traditional dynamic programming algorithms are not available. IDP algorithms attempt to find a global solution to a DP problem by incrementally improving local constraint satisfaction properties as experience is gained through interaction with the environment. This class of algorithms is not new, going back at least as far as Samuel's adaptive checkers-playing programs,...",
            "group": 685,
            "name": "10.1.1.32.5131",
            "keyword": "ACKNOWLEDGEMENTS I wish to thank Dr. Andrew G. Barto for the many contributions he made to",
            "title": "Incremental Dynamic Programming for On-Line Adaptive Optimal Control"
        },
        {
            "abstract": "In a graph, a clique is a set of vertices such that every pair is connected by an edge. MAX-CLIQUE is the optimization problem of finding the largest clique in a given graph, and is NP-hard, even to approximate well. Several real-world and theory problems can be modeled as MAX-CLIQUE. In this paper, we efficiently approximate MAX-CLIQUE in a special case of the Hopfield Network whose stable states are maximal cliques. We present several energy-descent optimizing dynamics; both discrete (deterministic and stochastic) and continuous. One of these emulates, as special cases, two well known greedy algorithms for approximating MAX-CLIQUE. We report on detailed empirical comparisons on random graphs. Mean-Field Annealing---an efficient approximation to Simulated Annealing---and a stochastic dynamics are the narrow but clear winners. All dynamics approximate much better than one which emulates a \"naive\" greedy heuristic. 1 Cliques and Maximum Clique In a graph with undirected edges, a cliq...",
            "group": 686,
            "name": "10.1.1.32.5434",
            "keyword": "",
            "title": "Approximating Maximum Clique with a Hopfield Network"
        },
        {
            "abstract": "In recent years Image Fractal Compression techniques  (IFS) have gained more interest because of their capability  to achieve high compression ratios while maintaining very  good quality of the reconstructed image. The main  drawback of such techniques is the very high computing  time needed to determine the compressed code.  In this work, after a brief description of IFS theory, we  introduce the coefficient quantization problem, presenting  two algorithms for its solution: the first one is based on  Simulated Annealing while the second refers to a fast  iterative algorithm. We discuss IFS parallel implementation at different level of granularity and we show that Massively Parallel Processing on SIMD machines is the best way to  use all the large granularity parallelism offered by the  problem. The results we present are achieved implementing the proposed algorithms for IFS compression and  coefficient quantization on the MPP APE100/Quadrics  machine.  Keywords: IFS coding, Simulated ...",
            "group": 687,
            "name": "10.1.1.32.6418",
            "keyword": "IFS codingSimulated AnnealingQuantizationIterative optimization algorithmMassively Parallel Processing",
            "title": "Massively Parallel Processing Approach To Fractal Image Compression With Near-Optimal Coefficient Quantization"
        },
        {
            "abstract": "We study the average performance of several neural-net heuristics applied to the problem of finding the size of the largest clique in an undirected graph. This function is NP-hard even to approximate within a constant factor in the worst case [ALM  +  92], but the heuristics we study are known to do quite well on average for instances drawn from the uniform distribution on graphs of size n. We extend a theorem of M. Li and P. Vitanyi [LV92] to show that for instances drawn from the universal distribution m(x), the average-case performance of any approximation algorithm has the same order as its worst-case performance. The universal distribution is not computable or samplable. However, we give a realistic analogue q(x)  which lends itself to efficient empirical testing. Our results so far are: out of nine heuristics we tested, three did markedly worse under q(x) than under uniform distribution, but six others revealed little change. Keywords Neural network, Hopfield network, heuristics,...",
            "group": 688,
            "name": "10.1.1.32.6486",
            "keyword": "Hopfield networkheuristicstesting",
            "title": "Performance of MAX-CLIQUE Approximation Heuristics Under Description-Length Weighted Distributions"
        },
        {
            "abstract": "Constraint satisfaction is the core of a large number of problems, notably scheduling. Because of their potential for containing the combinatorial explosion problem in constraint satisfaction, local search methods have received a lot of attention in the last few years. The problem with these methods is that they can be trapped in local minima. GENET is a connectionist approach to constraint satisfaction. It escapes local minima by means of a weight adjustment scheme, which has been demonstrated to be highly effective. The tunneling algorithm described in this paper is an extension of GENET for optimization. The main idea is to introduce modifications to the function which is to be optimized by the network (this function mirrors the objective function which is specified in the problem). We demonstrate the outstanding performance of this algorithm on constraint satisfaction problems, constraint satisfaction optimization problems, partial constraint satisfaction problems, radio frequency ...",
            "group": 689,
            "name": "10.1.1.32.6571",
            "keyword": "",
            "title": "The Tunneling Algorithm for Partial CSPs and Combinatorial Optimization Problems"
        },
        {
            "abstract": ". This paper concerns with automatic pipeline implementation of a program subject to some real time (RT) constraints; the program is described through a Control Data Flow Graph (CDFG). We have developed a mapping methodology which assigns to each instruction of CDFG a time step and a HW resource for its execution. We have defined the space W of all the possible feasible mappings, as well an adjacency criterion on it and a cost function evaluating the quality of the mappings. We have minimized the cost function through a Simulated Annealing algorithm. The minimization process returns a mapping which satisfies all RT constraints, has minimal schedule length and minimal HW resource requirement. In order to show the capabilities of the proposed mapping methodology, we apply it to a graph with 50 nodes and several RT constraints: the obtained mapping gives a pipelined execution modality of the graph which satisfies all the given RT constraints.  Keywords: allocation table, real time, pipeli...",
            "group": 690,
            "name": "10.1.1.32.6806",
            "keyword": "allocation tablereal timepipeline systemsimulated annealing",
            "title": "Real Time Pipelined System Design Through Simulated Annealing"
        },
        {
            "abstract": "The complex and dynamic feature of the Internet requires a scalable and effective network control. In this paper, a collaborative on-line simulation scheme is proposed to provide the automated and pro-active control functions for the networks. This scheme introduces autonomous on-line simulators into local networks, which continuously monitor the surrounding network conditions, collect the relevant information, communicate with other simulators and execute collaborative on-line simulation. Based on the simulation results, the on-line simulators keep tuning up the network parameters to the better operation point to fit the current network situation. In this paper, we describe the basic concept of this scheme, investigate the solutions to the challenges, faced in the realization of this scheme, in areas such as network modeling, on-line simulation and parameter search. We also discuss the applicability of this scheme, and present the simulation results under ns and the test results of a pr...",
            "group": 691,
            "name": "10.1.1.32.6830",
            "keyword": "",
            "title": "Network Management and Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": ": A strategy for finding approximate solutions to discrete optimization problems with inequality constraints using mean field neural networks is presented. The constraints x 0 are encoded by x\\Theta(x) terms in the energy function. A careful treatment of the mean field approximation for the self-coupling parts of the energy is crucial, and results in an essentially parameter-free algorithm. This methodology is extensively tested on the knapsack problem of size up to 10 3 items. The algorithm scales like NM for problems with N items and M constraints. Comparisons are made with an exact branch and bound algorithm when this is computationally possible (N 30). The quality of the neural network solutions consistently lies above 95% of the optimal ones at a significantly lower CPU expense. For the larger problem sizes the algorithm is compared with simulated annealing and a modified linear programming approach. For \"non-homogeneous\" problems these produce good solutions, whereas for the...",
            "group": 692,
            "name": "10.1.1.32.7069",
            "keyword": "",
            "title": "Neural Networks for Optimization Problems with Inequality Constraints - the Knapsack Problem"
        },
        {
            "abstract": "Multilevel algorithms are a successful class of optimisation techniques which address the mesh partitioning problem for mapping meshes onto parallel computers. They usually combine a graph contraction algorithm together with a local optimisation method which refines the partition at each graph level. To date these algorithms have been used almost exclusively to minimise the cut-edge weight in the graph with the aim of minimising the parallel communication overhead. However it has been shown that for certain classes of problem, the convergence of the underlying solution algorithm is strongly influenced by the shape or aspect ratio of the subdomains. In this paper therefore, we modify the multilevel algorithms in order to optimise a cost function based on aspect ratio. Several variants of the algorithms are tested and shown to provide excellent results.  1 Introduction  The need for mesh partitioning arises naturally in many finite element (FE) and finite volume (FV) applications. Meshes...",
            "group": 693,
            "name": "10.1.1.32.8354",
            "keyword": "2",
            "title": "Multilevel Mesh Partitioning for Optimising Subdomain Aspect Ratio"
        },
        {
            "abstract": "Optimization is one of the most promising geometrical reconstruction approaches. In  this approach, the 2D vertices of the given figure maintain their plane coordinates  (X,Y), while a set of Z coordinates (orthogonal to the plane) is computed to obtain a  3D configuration that matches the \"implicit spatial information\" contained in the  departure drawing. In other words, Z coordinates are the variables, and image  regularities are used to define both the Objective Function and the Constraints. Some  authors have introduced and tested the approach. Nevertheless, further improvements  are needed. Mainly because in this problem only global optimum is acceptable in order  to ensure the \"psychologically plausible\" model is always the one to be obtained. In  this paper, some key aspects of the strategy proposed by the authors to convey the  optimization process towards the psychologically plausible solution are discussed.  Keywords: geometrical reconstruction, optimization.  1  This work wa...",
            "group": 694,
            "name": "10.1.1.32.9130",
            "keyword": "geometrical reconstructionoptimization. 1",
            "title": "Geometrical Reconstruction From Single Line Drawings Using Optimization-Based Approaches"
        },
        {
            "abstract": "this technical complication for the time being; details of this kind will be attended to when we address a more realistic example in Section 12.4. With the simplifying assumption of zero bias, the expectation of an individual trial is # i , and its variance, since it is a 0,1-variable, is",
            "group": 695,
            "name": "10.1.1.32.9553",
            "keyword": "",
            "title": "The Markov Chain Monte Carlo Method: An Approach To Approximate Counting And Integration"
        },
        {
            "abstract": "A brief review is given for using feedback artificial neural networks (ANN) to obtain good approximate solutions to combinatorial optimization problems. The key element is the mean field approximation (MFT) The methodology, which is illustrated for the graph bisection and knapsack problems, is easily generalized to Potts systems. The latter is related to the deformable templates method, which is illustrated with the track finding problem. MFT is based on a variational principle, which also can be generalized to non-integer problems. Introduction  Many combinatorial optimization problems are NP-complete, which require state space explorations leading to O(a  N  ) computations for a system with N  degrees of freedom. Different kinds of heuristic methods are therefore often used to find reasonably good solutions. The ANN approach falls within this category. Whereas the use of ANN for pattern recognition and prediction problems is a non-linear extension of conventional linear interpolation...",
            "group": 696,
            "name": "10.1.1.32.9581",
            "keyword": "",
            "title": "Combinatorial Optimization with Feedback Artificial Neural Networks"
        },
        {
            "abstract": "\"Standard\" information theory says nothing about the semantic content of information. Nevertheless, applications such as evolutionary theory demand consideration of precisely this aspect of information, a need that has motivated a largely unsuccessful search for a suitable measure of an \"amount of meaning\". This paper represents an attempt to move beyond this impasse, based on the observation that the meaning of a message can only be understood relative to its receiver. Positing that the semantic value of information is its usefulness in making an informed decision, we define pragmatic information as the information gain in the probability distributions of the receiver's actions, both before and after receipt of a message in some pre-defined ensemble. We then prove rigorously that our definition is the only one that satisfies obvious desiderata, such as the additivity of information from logically independent messages. This definition, when applied to the information \"learned\" by the time evolution of a process, defies the intuitions of the few previous researchers thinking along these lines by being monotonic in the uncertainty that remains after receipt of the message, but non-monotonic in the Shannon entropy of the input ensemble. It then follows that the pragmatic information of the genetic \"messages\" in an evolving population is a global Lyapunov function for Eigen's quasi-species model of biological evolution. A concluding section argues that a theory such as ours must explicitly acknowledge purposeful action, or \"agency\", in such diverse fields as evolutionary theory and finance.",
            "group": 697,
            "name": "10.1.1.32.9672",
            "keyword": "",
            "title": "A Theory of Pragmatic Information and Its Application to the Quasispecies Model of Biological Evolution"
        },
        {
            "abstract": ". We explore a new general-purpose heuristic for nding highquality  solutions to hard optimization problems. The method, called extremal   optimization, is inspired by \\self-organized criticality,\" a concept  introduced to describe emergent complexity in many physical systems.  In contrast to Genetic Algorithms which operate on an entire \\genepool  \" of possible solutions, extremal optimization successively replaces  extremely undesirable elements of a sub-optimal solution with new, random  ones. Large uctuations, called \\avalanches,\" ensue that eciently  explore many local optima. Drawing upon models used to simulate farfrom  -equilibrium dynamics, extremal optimization complements approximation  methods inspired by equilibrium statistical physics, such as simulated  annealing. With only one adjustable parameter, its performance has  proved competitive with more elaborate methods, especially near phase  transitions. Those phase transitions are found in the parameter space of  most o...",
            "group": 698,
            "name": "10.1.1.32.9858",
            "keyword": "",
            "title": "Optimizing through Co-Evolutionary Avalanches"
        },
        {
            "abstract": "We advocate a new methodology for empirically analysing the behaviour of Las  Vegas Algorithms, a large class of probabilistic algorithms comprising prominent  methods such as local search algorithms for SAT and CSPs, like WalkSAT and the  Min-Conflicts Heuristic, as well as more general metaheuristics like Genetic Algorithms,  Simulated Annealing, Iterated Local Search, and Ant Colony Optimization.  Our method is based on measuring and analysing run-time distributions (RTDs) for  individual problem instances. We discuss this empirical methodology and its application  to Las Vegas Algorithms for various problem domains. Our experience so far  strongly suggests that using this approach for studying the behaviour of Las Vegas  Algorithms can provide a basis for improving the understanding of these algorithms  and thus facilitate further successes in their development and application.  ",
            "group": 699,
            "name": "10.1.1.33.132",
            "keyword": "Tabu Search [2Simulated Annealing [11Genetic Algorithms [3Evolution Strategies [1415Ant Colony Optimisation [1or I",
            "title": "On the Empirical Evaluation of Las Vegas Algorithms  "
        },
        {
            "abstract": ": A novel algorithm for particle tracking is presented and evaluated. It is based on deformable templates that converge using a deterministic annealing algorithm. These deformable templates are initialized by Hough transforms. The algorithm, which effectively represents a merger between neuronic decision making and parameter fitting, naturally lends itself to parallel execution. Very good performance is obtained for both non-magnetic and magnetic tracks. For the latter simulated TPC tracks from the CERN DELPHI detector are used. 1 mattias@thep.lu.se 2 carsten@thep.lu.se 3 yuille%gramian@das.harvard.edu 1 Motivation and Results Particle physics contains many challenging feature recognition problems ranging from off-line data analysis to low-level experimental triggers. In particular for the next generation of accelerators (LHC, SSC) the availability of efficient pattern recognition algorithms that can be executed in real-time will be crucial. The event rate at these machines is ...",
            "group": 700,
            "name": "10.1.1.33.248",
            "keyword": "",
            "title": "Track Finding with Deformable Templates - The Elastic Arms Approach"
        },
        {
            "abstract": "In this paper, we investigate bifurcation processes for the mean field theory (MFT) annealing applied to traveling salesman problems (TSPs). Due to the symmetries of the TSP free energy function, some special bifurcations occur: cyclic symmetry breaking bifurcations and reverse symmetry breaking bifurcations. Saddle-node bifurcations also occur. Which type of bifurcations occurs depends on the symmetry of the eigenvector that corresponds to the zero eigenvalue mode of the free energy curvature matrix at the bifurcation point. In the MFT annealing process, a sequence of bifurcations occurs and the bifurcation structure affects the quality of the annealing solution. It is shown that the annealing solution in this process is not unique in general, and it is not always the optimal solution. Our approach can also be applied to the Potts spin model and its bifurcation structure is almost the same as that of the MFT. The practical implications of our results are also discussed. Acknowledgment...",
            "group": 701,
            "name": "10.1.1.33.477",
            "keyword": "",
            "title": "Bifurcations in Mean-Field-Theory Annealing"
        },
        {
            "abstract": "In most database systems, the values of many important run-time parameters of the system, the data, or the query are unknown at query optimization time. Parametric query optimization attempts to identify several execution plans, each one of which is optimal for a subset of all possible values of the run-time parameters. We present a general formulation of this problem and study it primarily for the buffer size parameter. We adopt randomized algorithms as the main approach to this style of optimization and enhance them with a sideways information passing feature that increases their effectiveness in the new task. Experimental results of these enhanced algorithms show that they optimize queries for large numbers of buffer sizes in the same time needed by their conventional versions for a single buffer size, without much sacrifice in the output quality.",
            "group": 702,
            "name": "10.1.1.33.696",
            "keyword": "",
            "title": "Parametric Query Optimization"
        },
        {
            "abstract": "Recent studies (Alizadeh et al, [1]; Bittner et al,[5]; Golub et al, [11]) demonstrate the discovery of putative disease subtypes from gene expression data. The underlying computational problem is to partition the set of sample tissues into statistically meaningful classes. In this paper we present a novel approach to class discovery and develop automatic analysis methods. Our approach is based on statistically scoring candidate partitions according to the overabundance of genes that separate the different classes. Indeed, in biological datasets, an overabundance of genes separating known classes is typically observed. we measure overabundance against a stochastic null model. This allows for highlighting subtle, yet meaningful, partitions that are supported on a small subset of the genes.  Using simulated annealing we explore the space of all possible partitions of the set of samples, seeking partitions with statistically significant overabundance of differentially expressed genes. We ...",
            "group": 703,
            "name": "10.1.1.33.797",
            "keyword": "",
            "title": "Class Discovery in Gene Expression Data"
        },
        {
            "abstract": ".  This account surveys some of the major developments in complexity research and  proposes a number of questions and issues for future research. The topics are organized  into three areas. Under theory, we consider self-organization, emergence, criticality,  connectivity, as well as the paradigm of natural computation. Applications include several  areas of science and technology where complexity plays a prominent part such as  development, evolution, and global information systems. Finally, under practice, we  consider some of the issues involved in trying to develop a coherent methodology for  dealing with complexity.  1. Introduction  At the beginning of the 20  th  Century, there was a sense in many areas of science that the key discoveries had already been made. These impressions were later shaken by discoveries that opened up vast new areas of knowledge. In contrast, we are struck at the start of the 21  st  Century by the enormity of what we do not know. Prominent amongst these...",
            "group": 704,
            "name": "10.1.1.33.799",
            "keyword": "",
            "title": "Towards a theory of everything? - Grand challenges in complexity and informatics"
        },
        {
            "abstract": "When one wishes to solve optimization problems by simulated annealing, the naive mean-eld approximation provides a practical way of doing it. Extensions of the naive approximation by including higher-order terms have been proposed in order to improve accuracy of approximation. It has been reported, however, that higher-order approximations do not work well in low temperature regimes. We present an analytical argument and a geometrical view on this contradictory observation based on information-geometry, and give an intuitive explanation as to why the naive approximation does work well when it is applied to solving optimization problems.  1. INTRODUCTION  Simulated annealing [1] has been recognized as a tool for solving optimization problems in a stochastic manner. It includes the so-called Markov-chain-Monte-Carlo (MCMC) procedure, and therefore, it suers from the same diculty as the one for the MCMC that in general it requires a huge amount of computation for sampling states with re...",
            "group": 705,
            "name": "10.1.1.33.899",
            "keyword": "",
            "title": "Geometrical View On The Effectiveness Of Naive Mean-Field Approximation To Optimization Problems"
        },
        {
            "abstract": ": A novel method is presented and explored within the framework of Potts neural networks for solving optimization problems with a non-trivial topology, with the airline crew scheduling problem as a target application. The key ingredient to handle the topological complications is a propagator defined in terms of Potts neurons. The approach is tested on artificial problems generated with two real-world problems as templates. The results are compared against the properties of the corresponding unrestricted problems. The latter are subject to a detailed analysis in a companion paper [1]. Very good results are obtained for a variety of problem sizes. The computer time demand for the approach only grows like (number of flights)  3  . A realistic problem typically is solved within minutes, partly due to a prior reduction of the problem size, based on an analysis of the local arrival/departure structure at the single airports. To facilitate the reading for audiences not familiar with Potts neu...",
            "group": 706,
            "name": "10.1.1.33.953",
            "keyword": "",
            "title": "Airline Crew Scheduling Using Potts Mean Field Techniques"
        },
        {
            "abstract": ":  A variational method for computing conformational properties of molecules with LennardJones potentials for the monomer-monomer interactions is presented. The approach is tailored to deal with angular degrees of freedom, rotors, and consists in the iterative solution of a set of deterministic equations with annealing in temperature. The singular shortdistance behaviour of the Lennard-Jones potential is adiabatically switched on in order to obtain stable convergence. As testbeds for the approach two distinct ensembles of molecules are used, characterized by a roughly dense-packed ore a more elongated ground state. For the latter, problems are generated from natural frequencies of occurrence of amino acids and phenomenologically determined potential parameters; they seem to represent less disorder than was previously assumed in synthetic protein studies. For the dense-packed problems in particular, the variational algorithm clearly outperforms a gradient descent method in terms of mini...",
            "group": 707,
            "name": "10.1.1.33.1101",
            "keyword": "",
            "title": "A Variational Approach for Minimizing Lennard-Jones Energies"
        },
        {
            "abstract": "We explore a new general-purpose heuristic for finding high-quality solutions to hard optimization problems. The method, called extremal optimization, is inspired by \"self-organized criticality\", a concept introduced to describe emergent complexity in physical systems. In contrast to genetic algorithms, which operate on an entire \"gene-pool\" of possible solutions, extremal optimization successively replaces extremely undesirable elements of a single sub-optimal solution with new, random ones. Large fluctuations, or \"avalanches\", ensue that efficiently explore many local optima. Drawing upon models used to simulate far-from-equilibrium dynamics, extremal optimization complements heuristics inspired by equilibrium statistical physics, such as simulated annealing. With only one adjustable parameter, its performance has proved competitive with more elaborate methods, especially near phase transitions. Phase transitions are found in many combinatorial optimization problems, and have been co...",
            "group": 708,
            "name": "10.1.1.33.1252",
            "keyword": "",
            "title": "Combining Local Search with Co-Evolution in a Remarkably Simple Way"
        },
        {
            "abstract": ". The Traveling Salesman Problem is a standard test-bed for  algorithmic ideas. Currently, there exist a large number of nature-inspired  algorithm applications to the TSP and for some of these approaches  very good performance is reported. In particular, the best performing  approaches combine solution modification or construction with the subsequent  application of a fast but powerful local search algorithm. Yet,  comparisons between these algorithms with respect to their performance  are often difficult due to different implementation choices of which the  choice of the local search algorithm and differing computing environments.  In this article we experimentally compare some of the best performing  recently proposed nature-inspired algorithms and investigate the  differences of their performance on large set of benchmark instances. Surprisingly,  one of the conceptually most simple algorithms shows the best  performance on most of the instances.  1 Introduction  The Traveling Sale...",
            "group": 709,
            "name": "10.1.1.33.1327",
            "keyword": "",
            "title": "A Comparison of Nature Inspired Heuristics on the Traveling Salesman Problem"
        },
        {
            "abstract": "The prior model or penalizing term in Bayesian image analysis is typically a Markov random field parametrized by one or more smoothing parameters. For many commonly applied Markov random field penalizing terms there do not exist both objective and practically applicable methods for choosing the smoothing parameters. In this paper we discuss approaches to analysis of residuals in a simple case of image segmentation, and study to which extent such an analysis can provide information on whether a suitable value of the smoothing parameter has been used.  Keywords: Bayesian image analysis, oversmoothing, penalized likelihood, residuals.  1 Introduction  Segmentation of a digitized image corrupted by noise is an intensively studied problem of statistical inference. The research in this area was in the statistical community initiated by the papers by Geman & Geman (1984) and Besag (1986). A common feature of the methods introduced in these papers is that the segmentation is obtained by maximi...",
            "group": 710,
            "name": "10.1.1.33.1480",
            "keyword": "Bayesian image analysisoversmoothingpenalized likelihoodresiduals",
            "title": "Analysis of Residuals From Segmentation of Noisy Images"
        },
        {
            "abstract": "We advocate a new methodology for empirically analysing the behaviour of Las  Vegas Algorithms, a large class of probabilistic algorithms comprising prominent  methods such as local search algorithms for SAT and CSPs, like WalkSAT and the  Min-Conflicts Heuristic, as well as more general metaheuristics like Genetic Algorithms,  Simulated Annealing, Iterated Local Search, and Ant Colony Optimization.  Our method is based on measuring and analysing run-time distributions (RTDs) for  individual problem instances. We discuss this empirical methodology and its application  to Las Vegas Algorithms for various problem domains. Our experience so far  strongly suggests that using this approach for studying the behaviour of Las Vegas  Algorithms can provide a basis for improving the understanding of these algorithms  and thus facilitate further successes in their development and application.  ",
            "group": 711,
            "name": "10.1.1.33.1873",
            "keyword": "Tabu Search [2Simulated Annealing [11Genetic Algorithms [3Evolution Strategies [1415Ant Colony Optimisation [1or",
            "title": "On the Empirical Evaluation of Las Vegas Algorithms  "
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks -- specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability, and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars, and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-t...",
            "group": 712,
            "name": "10.1.1.33.2032",
            "keyword": "recurrent neural networksnatural language processing",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "This paper presents a new heuristic for graph partitioning called Path Optimization  (PO), and the results of an extensive set of empirical comparisons of the new algorithm with two very well-known algorithms for partitioning: the Kernighan-Lin algorithm and simulated annealing. Our experiments are described in detail, and the results are presented in such a way as to reveal performance trends based on several variables. Sufficient trials are run to obtain 99% confidence intervals small enough to lead to a statistical ranking of the implementations for various circumstances. The results for geometric graphs, which have become a frequently-used benchmark in the evaluation of partitioning algorithms, show that PO holds an advantage over the others. In addition to the main test suite described above, comparisons of PO to more recent partitioning approaches are also given. We present the results of comparisons of PO with a parallelized implementation of Goemans' and Williamson's 0.878 appr...",
            "group": 713,
            "name": "10.1.1.33.2159",
            "keyword": "",
            "title": "Path Optimization for Graph Partitioning Problems"
        },
        {
            "abstract": " We explore a new general-purpose heuristic for nding highquality  solutions to hard optimization problems. The method, called extremal   optimization, is inspired by \"self-organized criticality,\" a concept  introduced to describe emergent complexity in many physical systems.  In contrast to Genetic Algorithms which operate on an entire \"gene-pool\" of possible solutions, extremal optimization successively replaces  extremely undesirable elements of a sub-optimal solution with new, random  ones. Large uctuations, called \"avalanches,\" ensue that eciently  explore many local optima. Drawing upon models used to simulate far-from-equilibrium dynamics, extremal optimization complements approximation  methods inspired by equilibrium statistical physics, such as simulated  annealing. With only one adjustable parameter, its performance has  proved competitive with more elaborate methods, especially near phase  transitions. Those phase transitions are found in the parameter space of  most o...",
            "group": 714,
            "name": "10.1.1.33.2359",
            "keyword": "",
            "title": "Optimizing through Co-Evolutionary Avalanches"
        },
        {
            "abstract": "There exists a need for manipulators that are more flexible and reliable than the current fixed configuration manipulators. Indeed, robot manipulators can be easily reprogrammed to perform different tasks, yet the range of tasks that can be performed by a manipulator is limited by its mechanical structure. In remote and hazardous environments, such as a nuclear facility or a space station, the range of tasks that may need to be performed often exceeds the capabilities of a single manipulator. Moreover, it is essential that critical tasks be executed reliably in these environments. To address this need for a more flexible and reliable manipulator, we propose the concept of a rapidly deployable fault tolerant manipulator system. Such a system combines a Reconfigurable Modular Manipulator System (RMMS) with support software for rapid programming, trajectory planning, and control. This allows the user to rapidly configure a fault tolerant manipulator custom-tailored for a given task. This ...",
            "group": 715,
            "name": "10.1.1.33.2509",
            "keyword": "",
            "title": "An Agent-Based Approach to the Design of Rapidly Deployable Fault Tolerant Manipulators"
        },
        {
            "abstract": "this paper. Let us place it within the neural network perspective, and particularly that of learning. The area of neural networks has greatly benefited from its unique position at the crossroads of several diverse scientific and engineering disciplines including statistics and probability theory, physics, biology, control and signal processing, information theory, complexity theory, and psychology (see [45]). Neural networks have provided a fertile soil for the infusion (and occasionally confusion) of ideas, as well as a meeting ground for comparing viewpoints, sharing tools, and renovating approaches. It is within the ill-defined boundaries of the field of neural networks that researchers in traditionally distant fields have come to the realization that they have been attacking fundamentally similar optimization problems.",
            "group": 716,
            "name": "10.1.1.33.3047",
            "keyword": "",
            "title": "Deterministic Annealing for Clustering, Compression, Classification, Regression, and Related Optimization Problems"
        },
        {
            "abstract": "For global optimization, it has been mathematically proven that Metropolis-type  annealing algorithms, in the form of recursive stochastic algorithms, convergence under  certain conditions to a global minimum. To these recursive algorithms, stochastic  differential equations of the Langevin-type can be associated. In this paper the  Fokker-Planck equation, related to these stochastic differential equations, is studied  for solving global optimization problems. A parameterization of the transition density  is made by radial basis function networks, leading to a learning rule in the unknown  parameter vector of the RBF network. In this way new optimization algorithms are  derived, that work with populations of points, like e.g. genetic algorithms.  At a certain generation, points are generated in search space according to the  density, which is updated according to the Fokker-Planck equation. The update  basically involves the solution to a constrained linear least squares problem. The  ...",
            "group": 717,
            "name": "10.1.1.33.3066",
            "keyword": "",
            "title": "A Fokker-Planck Learning Machine for Global Optimization"
        },
        {
            "abstract": "Voxel coloring methods reconstruct a three-dimensional volumetric surface model from a set of calibrated twodimensional photographs taken of a scene. In this paper, we recast voxel coloring as an optimization problem, the solution of which strives to minimize reprojection error, which measures how well projections of the reconstructed scene reproduce the photographs. The reprojection error, defined in image space, guides the refinement of the scene reconstruction in object space. Unlike previous voxel coloring methods, ours makes better use of all color information from all viewpoints, and thereby produces higher quality reconstructions. In addition, it allows voxels to be added to, not just removed from, the scene at any time during reconstruction. We examine methods to minimize the reprojection error, including greedy and simulated annealing techniques. Reconstructions of both synthetic and real scenes are presented and analyzed.  1 Introduction  Voxel coloring methods [1] [7] [11] r...",
            "group": 718,
            "name": "10.1.1.33.3224",
            "keyword": "",
            "title": "Improved Voxel Coloring Via Volumetric Optimization"
        },
        {
            "abstract": "Back-propagation learning (Rumelhart, Hinton and Williams, 1986) is a useful research tool but it has a number of undesiderable features such as having the experimenter decide from outside what should be learned. We describe a number of simulations of neural networks that internally generate their own teaching input. The networks generate the teaching input by trasforming the network input through connection weights that are evolved using a form of genetic algorithm. What results is an innate (evolved) capacity not to behave efficiently in an environment but to learn to behave efficiently. The analysis of what these networks evolve to learn shows some interesting results.  Introduction  In many supervised learning models of neural networks a vector of activity values is provided as an external teaching input to the network. Each activity value in the vector is compared with the computed value of the corresponding output unit and the resulting error is used to modify the connection weig...",
            "group": 719,
            "name": "10.1.1.33.4356",
            "keyword": "",
            "title": "Auto-Teaching: Networks That Develop Their Own Teaching Input"
        },
        {
            "abstract": "In this paper, first, we show some of the bifurcation properties of Potts mean-fieldtheory annealing applied to traveling salesman problems. Due to these bifurcation properties, this approach, in general, produces non-optimal and non-unique solutions. As an alternative approach, we propose a nonequilibrium version of the Potts spin neural network, called Chaotic Potts Spin (CPS). CPS has several parameters, and bifurcations over each parameter are investigated. Next, experimental results are shown comparing CPS with several related approaches. CPS is good at obtaining optimal solutions for small-scale problems and semi-optimal solutions for relatively large-scale problems. We also describe a couple of CPS modifications: CPS with a heuristic method and CPS with a \"chaotic annealing\" method. These modified algorithms can produce even better CPS solutions.  Chaotic Potts Spin 2 1 Introduction  There have been many studies on artificial neural network models applied to combinatorial optim...",
            "group": 720,
            "name": "10.1.1.33.4816",
            "keyword": "",
            "title": "Chaotic Potts spin model for combinatorial optimization problems"
        },
        {
            "abstract": "This paper is a survey of inductive rule learning algorithms that use a separate-and-conquer strategy. This strategy can be traced back to the AQ learning system and still enjoys popularity as can be seen from its frequent use in inductive logic programming systems. We will put this wide variety of algorithms into a single framework and analyze them along three different dimensions, namely their search, language and overfitting avoidance biases.  ",
            "group": 721,
            "name": "10.1.1.33.4894",
            "keyword": "",
            "title": "Separate-and-conquer rule learning"
        },
        {
            "abstract": "We propose a new learning algorithm for regression modeling. The method is especially suitable for optimizing neural network structures that are amenable to a statistical description as mixture models. These include mixture of experts, hierarchical mixture of experts (HME), and normalized radial basis functions (NRBF). Unlike recent maximum likelihood (ML) approaches, we directly minimize the (squared) regression error. We use the probabilistic framework as means to define an optimization method that avoids many shallow local minima on the complex cost surface. Our method is based on deterministic annealing (DA), where the entropy of the system is gradually reduced, with the expected regression cost (energy) minimized at each entropy level. The corresponding Lagrangian is the system's \"free-energy,\" and this annealing process is controlled by variation of the Lagrange multiplier, which acts as a \"temperature\" parameter. The new method consistently and substantially outperformed the com...",
            "group": 722,
            "name": "10.1.1.33.5073",
            "keyword": "",
            "title": "Mixture of Experts Regression Modeling by Deterministic Annealing"
        },
        {
            "abstract": ": a general purpose implementation of the Tabu Search metaheuristic, called  Universal Tabu Search, is used to optimally design a Locally Recurrent Neural Network  architecture. In fact, generally, the design of a neural network is a tedious and time  consuming trial and error operation that leads to structures whose optimality is not  guaranteed. In this paper, the problem of choosing the number of hidden neurons and the  number of taps and delays in the FIR and IIR network synapses is formalised as an  optimisation problem whose cost function to be minimised is the network error calculated  on a validation data set. The performances of the proposed approach have been tested on  the problem of modelling non isothermal, continuously stirred tank reactor, in which a first  order exothermic reaction is occurring. Comparison with alternative neural approaches are  reported, showing the usefulness of the proposed method.  I. INTRODUCTION  Many engineering problems such as, for instance, ti...",
            "group": 723,
            "name": "10.1.1.33.5363",
            "keyword": "",
            "title": "Automated Neural Model Design For A Cstr Using A Tabu Search Algorithm"
        },
        {
            "abstract": "this paper, an iterative optimization algorithm, called the Comb algorithm, is presented for approximating the global solution to MAP image restoration and segementation. The Comb derives new initial configurations based on the best local minimum found so far and leads a local search towards the global minimum. Experimental comparisons show that the Comb produces solutions of quality comparable to simulated annealing. Key words: Combinatorial optimization, genetic algorithm, image restoration, image segmentation, Markov random fields (MRFs), maximum a posteriori (MAP). 1 Introduction Image restoration is to recover a degraded image and segmentation is to partition an image into regions of similar image properties. Efficient restoration and segmentation are very important for numerous image analysis applications. Both problems can be posed generally as one of image estimation where the underlying image or segmentation map is to be estimated from the degraded image. Due to various uncertainties, an optimal solution is sought. A popular optimality criterion is the maximum a posteriori (MAP) probability principle in which both the prior distribution of the true image class and the conditional (likelihood) distribution of the data are taken into account. Contextual constraints, i.e. constraints between pixels, are important in image analysis. Markov random fields (MRFs) or equivalently Gibbs distributions",
            "group": 724,
            "name": "10.1.1.33.5441",
            "keyword": "maximum a posteriori",
            "title": "Toward Global Solution to MAP Image Restoration and Segementation: Using Common Structure of Local Minima"
        },
        {
            "abstract": "The efficient representation and encoding of signals with limited resources, e.g., finite storage capacity and restricted transmission bandwidth, is a fundamental problem in technical as well as biological information processing systems. Typically, under realistic circumstances, the encoding and communication of messages has to deal with different sources of noise and disturbances. In this paper, we propose a unifying approach to data compression by robust vector quantization, which explicitly deals with channel noise, bandwidth limitations, and random elimination of prototypes. The resulting algorithm is able to limit the detrimental effect of noise in a very general communication scenario. In addition, the presented model allows us to derive a novel competitive neural networks algorithm, which covers topology preserving feature maps, the so-called neural-gas algorithm, and the maximum entropy soft-max rule as special cases. Furthermore, continuation methods based on these noise models impr...",
            "group": 725,
            "name": "10.1.1.33.5473",
            "keyword": "",
            "title": "Competitive Learning Algorithms for Robust Vector Quantization"
        },
        {
            "abstract": "this paper we will use the formalism to describe a closed loop system using the generative method and the evaluative method . The generative models that will be used in this paper are:",
            "group": 726,
            "name": "10.1.1.33.5640",
            "keyword": "Simulated annealingproduction linesbuffer allocationdecomposition method",
            "title": "A Simulated Annealing Approach for Buffer Allocation in Reliable Production Lines"
        },
        {
            "abstract": "In this paper, we explore theoretical models for pose estimation and object matching based on Markov random fields (MRFs) and the maximum a posteriori (MAP) probability principle. The set of pose estimates as well as matching estimates are considered to be MRFs whose prior distributions are used as the prior constraints. The MAP solution is found from these distributions and an assumed observation model. Two statistical models are derived. The first model is aimed at pose clustering from corresponding point data. It estimates possibly multiple poses simultaneously and identifies outlier (false) correspondences. The second model attempts to solve matching (correspondence) and pose of 3D objects simultaneously from a 2D image. It gives a parallel and distributed hypothesis-verification procedure for pose and matching. These models may be used as criteria for evaluating the goodness of matching and pose.  2 To appear in IES Journal, The Institution of Engineers Singapore, February 1997  ...",
            "group": 727,
            "name": "10.1.1.33.5744",
            "keyword": "",
            "title": "Markov Random Field Models for Pose Estimation in Object Recognition"
        },
        {
            "abstract": "Discontinuity adaptive MRF priors have been used for modeling vision problems involving discontinuities and robust statistics models for solving regression problems involving outliers. This paper presents a comparative study of the two kinds of models. We analyze their mechanisms of the adaptation (to discontinuities and the robustness to outliers). This leads to the discovery of a necessary condition for the adaptation. We then give a common definition of the both models. The definition captures the essence of the adaptation ability and gives in theory infinitely many choices of functions suitable for the adaptation. The commonality between the two models suggests that results in the two areas are interchangeable to benefit each other. 1 Introduction  According to the Hammersley-Clifford theorem of MRFGibbs equivalence [2], the joint (prior) probability of an MRF is a Gibbs distribution. This provides a practical way of obtaining prior probabilities by specifying the Gibbs potential f...",
            "group": 728,
            "name": "10.1.1.33.5837",
            "keyword": "",
            "title": "Adaptation to Discontinuities and Robustness to Outliers"
        },
        {
            "abstract": "Recent years have seen growing interest in the problem of super-resolution restoration of  video sequences. Whereas in the traditional single image restoration problem only a single  input image is available for processing, the task of reconstructing super-resolution images  from multiple undersampled and degraded images can take advantage of the additional spatiotemporal  data available in the image sequence. In particular, camera and scene motion lead  to frames in the source video sequence containing similar, but not identical information. The  additional information available in these frames make possible reconstruction of visually superior  frames at higher resolution than that of the original data. In this paper we review the  current state of the art and identify promising directions for future research.  The authors are with the Laboratory for Image and Signal Analysis (LISA), University of Notre Dame, Notre Dame, IN 46556. E-mail: rls@nd.edu .  Executive Summary  This document...",
            "group": 729,
            "name": "10.1.1.33.5918",
            "keyword": "",
            "title": "Spatial Resolution Enhancement of Low-Resolution Image Sequences - A Comprehensive Review with Directions for Future Research"
        },
        {
            "abstract": "A parallel distributed relaxation labeling (RL) method, called the Lagrange-Hopfield (LH) method, is presented. RL is treated as a constrained optimization problem. The LH method solves the problem using the augmented Lagrangian multiplier technique and the graded Hopfield network. The LH method effectively overcomes instabilities that are inherent in the penalty method (e.g. Hopfield network) or the Lagrange multiplier method in constrained optimization. Due to the use of Lagrangian multipliers, the normalization operation in traditional RL methods is dispensed with. This makes the LH algorithm fully parallel and distributed and is suitable for analog implementation. Experiments also show that the method is able to produce good solutions in terms of the optimized objective values.   1. INTRODUCTION  Relaxation labeling (RL) [17] is a class of parallel iterative numerical procedures which use contextual constraints to reduce ambiguities in image analysis. It is widely used to solve ima...",
            "group": 730,
            "name": "10.1.1.33.6423",
            "keyword": "",
            "title": "Parallel Distributed Relaxation Labeling"
        },
        {
            "abstract": "A constrained optimization method, called the Lagrange-Hopfield (LH) method, is presented for solving Markov random field (MRF) based Bayesian image estimation problems for restoration and segmentation. The method combines the augmented Lagrangian multiplier technique with the Hopfield network to solve a constrained optimization problem into which the original Bayesian estimation problem is reformulated. The LH method effectively overcomes instabilities that are inherent in the penalty method (e.g. Hopfield network) or the Lagrange multiplier method in constrained optimization. An additional advantage of the LH method is its suitability for neural-like analog implementation. Experimental results are presented which show that LH yields good quality solutions at reasonable computational costs. 1. INTRODUCTION  Image restoration is to recover a degraded image and segmentation is to partition an image into regions of similar image properties. Both can be posed generally as image estimation...",
            "group": 731,
            "name": "10.1.1.33.6526",
            "keyword": "",
            "title": "Bayesian Image Restoration And Segmentation By Constrained Optimization"
        },
        {
            "abstract": "A Bayesian approach for object matching is presented. An object and a scene are each represented by its features, such as critical points, line segments and surface patches, constrained by unary properties and contextual relations. The matching is posed as a labeling problem where each feature in the scene is assigned (associated with) a feature of the known model objects. The prior distribution of a scene labeling is modeled as a Markov random field (MRF), which encodes the between-object constraints. The conditional distribution of the observed features given the labeling is assumed to be Gaussian, which encodes the within-object constraints. An optimal solution is defined as a maximum a posteriori (MAP) estimate. Relationships with previous work are discussed. Experimental results are shown. 1 Introduction  Object matching and recognition is a high level vision task. For non-simple objects, it is usually performed on object features, such as critical points, line segments and surfac...",
            "group": 732,
            "name": "10.1.1.33.6888",
            "keyword": "",
            "title": "Bayesian Object Matching"
        },
        {
            "abstract": ". Recently, there has been increasing interest in Markov random field (MRF) modeling for solving a variety of computer vision problems formulated in terms of the maximum a posteriori (MAP) probability. When the label set is discrete, such as in image segmentation and matching, the minimization is combinatorial. The objective of this paper is twofold: Firstly, we propose to use the continuous relaxation labeling (RL) as an alternative approach for the minimization. The motivation is that it provides a good compromise between the solution quality and the computational cost. We show how the original combinatorial optimization can be converted into a form suitable for continuous RL. Secondly, we compare various minimization algorithms, namely, the RL algorithms proposed by Rosenfeld et al. and by Hummel and Zucker, the mean field annealing of Peterson and Soderberg, simulated annealing of Kirkpatrick, the iterative conditional modes (ICM) of Besag and an annealing version of ICM proposed ...",
            "group": 733,
            "name": "10.1.1.33.7719",
            "keyword": "Contextual constraintsconstrained optimizationMarkov random field (MRFmaximum a posteriori (MAPrelaxation labeling",
            "title": "Energy Minimization and Relaxation Labeling"
        },
        {
            "abstract": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering  is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean--field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyse dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images.  \"Pai...",
            "group": 734,
            "name": "10.1.1.33.7778",
            "keyword": "",
            "title": "Pairwise Data Clustering by Deterministic Annealing"
        },
        {
            "abstract": "The maturity of schedulabilty analysis techniques for fixed-priority preemptive scheduling has enabled the consideration of timing issues at design time using a specification of the tasking architecture and estimates of execution times for tasks. While successful, this approach has limitations since the preemptive multi-tasking model does not scale well for a large number of tasks, and the fixed priority scheduling theory does not work well with many object-oriented design methods. In this paper we present an approach that uses a scalable implementation architecture where design level tasks are grouped into a smaller number of run-time threads during implementation. The schedulability analysis for this implementation architecture is based on the preemption threshold scheduling model. We show that our approach provides significant advantages over one using fixed-priority preemptive scheduling architecture. The benefits include higher schedulability for small number of tasks, and lower r...",
            "group": 735,
            "name": "10.1.1.33.7872",
            "keyword": "",
            "title": "Scalable Real-Time System Design using Preemption Thresholds"
        },
        {
            "abstract": "The initial weight vector to be used in supervised learning for multilayer feedforward neural networks has a strong influence in the learning speed and in the quality of the solution obtained after convergence. An inadequate initial choice may cause the training process to get stuck in a poor local minimum, or to face abnormal numerical problems. In this paper, we propose a biologically inspired method based on artificial immune systems. This new strategy is applied to several benchmark and real-world problems, and its performance is compared to that produced by other approaches already suggested in the literature.  1. Introduction  The importance of a proper choice for the initial set of weights (weight vector) is stressed by Kolen and Pollak [12]. They showed that it is not feasible to perform a global search to obtain the optimal set of weights. So, for practical purposes, the learning rule should be based on optimization techniques that employ local search to find the solution [19]...",
            "group": 736,
            "name": "10.1.1.33.8215",
            "keyword": "",
            "title": "An Immunological Approach to Initialize Feedforward Neural Network Weights"
        },
        {
            "abstract": "The combinatorial optimization problem of MAP estimation is converted to one of constrained real optimization and then solved by using the proposed augmented Lagrange-Hopeld (ALH) method. The ALH eectively overcomes instabilities that are inherent in the penalty method or the Lagrange multiplier method in constrained optimization. It produces good solutions with reasonable costs. I. Introduction  The aim of image restoration is to recover a degraded image and that of image segmentation is to partition an image into regions of similar image properties. E\u00c6cient restoration and segmentation are very important for numerous image analysis applications. Both problems can be posed generally as one of image estimation where the underlying image or segmentation map is to be estimated from the degraded image. Due to various uncertainties, an optimal solution is sought. A popular optimality criterion is the maximum a posteriori  (MAP) probability principle in which both the prior distribution o...",
            "group": 737,
            "name": "10.1.1.33.8390",
            "keyword": "",
            "title": "MAP Image Restoration and Segmentation By Constrained Optimization"
        },
        {
            "abstract": "INTRODUCTION  Shape matching in computer vision aims to establish correspondences between primitive shape features in the input data such as an image and in model objects. Schemes proposed to date fall into three categories (a) those based on invariant properties, (b) those based on object decomposition into parts and (c) those based on computation of transformation between the shapes in the scene and in a model-base. Two major problems have to be addressed regardless of which scheme is used: how a shape is represented and how matching is carried out. This chapter addresses two issues in shape analysis: (i) invariant-based shape representation and (ii) optimal shape matching. Shape representation is the basis on which shapes can be compared. Shape invariants, that do not change in values under a class of transformations, are the most effective and efficient for matching. This is because they make it possible to compare shapes directly before transformation parameters are found.",
            "group": 738,
            "name": "10.1.1.33.8781",
            "keyword": "",
            "title": "Shape Matching Based on Invariants"
        },
        {
            "abstract": "Using Markov random field (MRF) theory, a variety of computer vision problems can be modeled in terms of optimization based on the maximum a posteriori (MAP) criterion. The MAP configuration minimizes the energy of a posterior (Gibbs) distribution. When the label set is discrete, the minimization is combinatorial. This paper proposes to use the continuous relaxation labeling (RL) method for the minimization. The RL converts the original NP complete problem into one of polynomial complexity. Annealing may be combined into the RL process to improve the quality (globalness) of RL solutions. Performance comparison among four different RL algorithms is given. 1 Introduction  Since 1980's, there has been considerable interest in image and vision modeling using Markov random field (MRF) theory [3]. MRF theory provides us with a tool for modeling a vision problem within the established Bayes framework. In MRF based Bayes modeling, a problem is posed as one of labeling. When the interaction bet...",
            "group": 739,
            "name": "10.1.1.33.8877",
            "keyword": "",
            "title": "Relaxation Labeling of Markov Random Fields"
        },
        {
            "abstract": "of discrete-valued features. Rather, they have a continuum of continuous-valued (i.e. analog) features. The processing elements also form a continuum --- thus there is an uncountable infinity of  1  Poster presentation at Los Alamos National Laboratory Center for Nonlinear Studies 9th Annual International Conference, Emergent Computation, Los Alamos, NM, May 2226, 1989.  processors. Our motto has been, \"If you can count them, you don't have enough.\" 2 Reasoning About Continuous Computation  Of course our postulation of a continuum of processors operating on continuous data structures is an idealization (just as there is a mathematically idealized theory of discrete computation). Actual implementations will often have only a (large) finite number of processors, and (large) finite numbers of elements in the data structures. Even the continuous values of the elements may be represented digitally. However, these are all considered",
            "group": 740,
            "name": "10.1.1.33.9371",
            "keyword": "",
            "title": "Discrete vs. Continuous Computation: Taking Massive Parallelism Seriously"
        },
        {
            "abstract": "Discontinuity adaptive MRF priors have been used for modeling vision problems involving discontinuities and robust statistics models for solving regression problems involving outliers. This paper presents a comparative study of the two kinds of models. We analyze the mechanisms of adaptation (to discontinuities) and robustness (to outliers) and give a necessary condition for the adaptation and the robustness. We then give a common definition of both models. The definition captures the essence of the adaptation ability and gives in theory infinitely many choices of functions suitable for the adaptation in MRF and robust models. The likeness between the two models suggests that results in the two areas are interchangeable to benefit each other. Index terms --- Discontinuities, Markov random fields, robust statistics.  Introduction  Markov random field (MRF) theory provides a theoretic basis for modeling joint prior probabilities to prior contextual constraints by specifying appropriate ...",
            "group": 741,
            "name": "10.1.1.33.9735",
            "keyword": "",
            "title": "Discontinuity-Adaptive MRF Prior and Robust Statistics: A Comparative Study"
        },
        {
            "abstract": "This paper presents a novel approach to assist the user in exploring appropriate transfer functions for the visualization of volumetric datasets. The search for a transfer function is treated as a parameter optimization problem and addressed with stochastic search techniques. Starting from an initial population of (random or pre-defined) transfer functions, the evolution of the stochastic algorithms is controlled by either direct user selection of intermediate images or automatic fitness evaluation using user-specified objective functions. This approach essentially shields the user from the complex and tedious \"trial and error\" approach, and demonstrates effective and convenient generation of transfer functions.",
            "group": 742,
            "name": "10.1.1.33.9776",
            "keyword": "",
            "title": "Generation of Transfer Functions with Stochastic Search Techniques"
        },
        {
            "abstract": ". Assume that some objects are present in an image but can be seen only partially and are overlapping each other. To recognize the objects, we have to firstly separate the objects from one another, and then match them against the modeled objects using partial observation. This paper presents a probabilistic approach for solving this problem. Firstly, the task is formulated as a two-stage optimal estimation process. The first stage, matching, separates different objects and finds feature correspondences between the scene and each potential model object. The second stage, recognition, resolves inconsistencies among the results of matching to different objects and identifies object categories. Both the matching and recognition are formulated in terms of the maximum a posteriori (MAP) principle. Secondly, contextual constraints, which play an important role in solving the problem, are incorporated in the probabilistic formulation. Specifically, between-object constraints are encoded in the...",
            "group": 743,
            "name": "10.1.1.33.9786",
            "keyword": "",
            "title": "A Two-Stage Probabilistic Approach For Object Recognition"
        },
        {
            "abstract": "This paper presents a novel relaxation labeling method called Augmented Lagrangian-Hopfield (ALH) method based on the Augmented Lagrangian multipliers and the graded Hopfield neural network. In the ALH method, RL is formulated as a problem of constrained real optimization. The augmented Lagrange multiplier method [13,14] is used for optimization with the constraints and the Hopfield method [15,16] for bridging the gap between discrete and continuous optimization. The ALH needs no gradient projection nor other normalization operations in its updating equations in keeping the labeling constraints. Therefore, it is more amenable for a neural network implementation than the exiting RL algorithms. Experiments show that the ALH produces good quality solutions in terms of the optimized objective values at a reasonable number of iterations. A recent result shows that the ALH method significantly improves the Hopfield type networks in solving the traveling salesman problem [17]. The ALH has also been used for image restoration and segmentation [18].  The rest of the paper is organized as follows: Section 2 introduces the continuous RL Method. Section 3 poses RL as a constrained optimization problem and presents the ALH method for solving it. Section 4 discusses the constrained optimization methods in connection to RL. Section 5 gives a neural network structure for the ALH computation. Section 6 presents the experimental results.",
            "group": 744,
            "name": "10.1.1.33.9952",
            "keyword": "Augmented Lagrange methodconstrained optimizationgraded Hopfield networksrelaxation",
            "title": "Relaxation Labeling Using Augmented Lagrange-Hopfield Method"
        },
        {
            "abstract": "We present a novel optimization framework for unsupervised texture segmentation that relies on statistical tests as a measure of homogeneity. Texture segmentation is formulated as a data clustering problem based on sparse proximity data. Dissimilarities of pairs of textured regions are computed from a multi-scale Gabor filter image representation. We discuss and compare a class of clustering objective functions which is systematically derived from invariance principles. As a general optimization framework we propose deterministic annealing based on a mean-field approximation. The canonical way to derive clustering algorithms within this framework as well as an efficient implementation of mean-field annealing and the closely related Gibbs sampler are presented. We apply both annealing variants to Brodatz-like micro-texture mixtures and real-word images.",
            "group": 745,
            "name": "10.1.1.34.54",
            "keyword": "",
            "title": "Unsupervised Texture Segmentation in a Deterministic Annealing Framework"
        },
        {
            "abstract": "Most randomized search methods can be regarded as random sampling methods with a (non-uniform) sampling  density function. Differences between the methods are reflected in different shapes of the sampling density  function and in different adaptation mechanisms that update this density function based on the observed  samples. We claim that this observation helps in getting a better understanding of evolutionary optimizers.  An evolutionary algorithm is proposed, that uses an enhanced selection mechanism which uses not only fitness  values but also considers the distribution of samples in the search-space. After a fitness based selection, the  individuals are clustered, and a representative is selected for each cluster. The next generation is created using  only these representatives. The set of representatives is usually small and the efficient incorporation of local  search techniques is possible.  AMS Subject Classification (1991): 68T20  CR Subject Classification (1991): G.1.7, I.2....",
            "group": 746,
            "name": "10.1.1.34.1283",
            "keyword": "evolutionary algorithms",
            "title": "Cluster Evolution Strategies - Enhancing the Sampling Density Function Using Representatives"
        },
        {
            "abstract": "We propose a declarative-based implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [DW97]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. PCCP also includes synchronisation and concurrency aspects. However, for the purpose of this work, the (probabilistic) CLP fragment of PCCP is sufficient. We present a meta-interpreter for this language. This is just a standard prolog metainterpreter, suitably extended so as to deal with probabilistic choice. For the constraint solving, the meta-interpreter exploits existing constraint handling facilities (and in more concrete terms to the SICStus 3.#6 system). This is possible because the design of PCCP d...",
            "group": 747,
            "name": "10.1.1.34.1356",
            "keyword": "",
            "title": "Randomised Algorithms and Constraint Logic Programming"
        },
        {
            "abstract": "We propose a novel person verification system for real-time face identification.  The main features of the system include accurate registration of face  images using a robust form of correlation, a framework for global registration  of a face database using a minimum spanning tree algorithm and a method  for selecting a subset of features optimal for discrimination between clients  and impostors. The results indicate that the image registration is of high accuracy  and the feature selection is successfully improving on the verification  performance.  1 Introduction  Verification of person identity based on biometric information is important for many security applications. Examples include access control to buildings, surveillance and intrusion detection. Furthermore, there are many emerging fields that would benefit from developments in person verification technology such as advanced human-computer interfaces and tele-services including tele-shopping and tele-banking. Comparing verific...",
            "group": 748,
            "name": "10.1.1.34.1638",
            "keyword": "",
            "title": "Saliency-Based Robust Correlation for Real-Time Face Registration and Verification"
        },
        {
            "abstract": "A scalable, high-resolution display may be constructed by tiling many projected images over a single display surface. One fundamental challenge for such a display is to avoid visible seams due to misalignment among the projectors. Traditional methods for avoiding seams involve sophisticated mechanical devices and expensive CRT projectors, coupled with extensive human effort for fine-tuning the projectors. This paper describes an automatic alignment method that relies on an inexpensive, uncalibrated camera to measure the relative mismatches between neighboring projectors, and then correct the projected imagery to avoid seams without significant human effort.  CR Categories and Subject Descriptors: I.4.0 [Image Processing and Computer Vision]: General - Image Displays; I.4.1 [Image Processing And Computer Vision]: Digitization and Image Capture - Imaging Geometry.  Additional Keywords: seamless tiling, automatic alignment, projective mapping, simulated annealing  1 Introduction  Rapid ad...",
            "group": 749,
            "name": "10.1.1.34.1729",
            "keyword": "seamless tilingautomatic alignment",
            "title": "Automatic Alignment Of High-Resolution Multi-Projector Displays Using An Un-Calibrated Camera"
        },
        {
            "abstract": "this paper, we investigate new methods which combine existing algorithms. By combining algorithms, we can obtain drawings which satisfy a combination of aesthetic criteria.",
            "group": 750,
            "name": "10.1.1.34.2241",
            "keyword": "",
            "title": "A Multiagent Approach using A-Teams for Graph Drawing"
        },
        {
            "abstract": "For most computationally intractable problems there exists no heuristic which performs  best on all instances. Usually, a heuristic characterized as best will perform good on the majority of instances  but leave a minority on which other heuristics do better. In priority rule-based scheduling,  attempts to remedy this have been made by combining simple priority rules in a fixed and predetermined  way. We investigate another way, viz. the design of adaptive control schemes which dynamically  combine algorithms as appropriate, taking into account instance-specific knowledge. We scrutinize  recently proposed algorithmic approaches from the open literature. Although these have been  used in various settings (e.g. lotsizing and scheduling, course scheduling), a thorough experimental  investigation, comparing their performance on standard benchmark instances to that of other contemporary  methods, has been lacking. Our research aims to close this gap by validating these approaches  on one ...",
            "group": 751,
            "name": "10.1.1.34.2283",
            "keyword": "",
            "title": "Adaptive Control Schemes for Parameterized Heuristic Scheduling"
        },
        {
            "abstract": " Recent dramatic improvements in integrated circuit fabrication technology have led to Field-Programmable Gate Arrays (FPGAs) capable of implementing entire digital systems, as opposed to the smaller logic circuits that have traditionally been targeted to FPGAs. Unlike the smaller circuits, these large systems often contain memory. Architectural support for the efficient implementation of memory in next-generation FPGAs is therefore crucial.  This dissertation examines the architecture of FPGAs with memory,aswell as algorithms that map circuits into these devices. Three aspects are considered: the analysis of circuits that contain memory as well as the automated random generation of such circuits, the architecture and algorithms for stand-alone con#gurable memory devices, and architect...",
            "group": 752,
            "name": "10.1.1.34.3389",
            "keyword": "",
            "title": "Architectures and Algorithms for Field-Programmable Gate Arrays with Embedded Memory"
        },
        {
            "abstract": "Expert critics have been built to critique human performance in various areas such  as engineering design, decision making, etc. We suggest that critics can also be useful  in building and use of knowledge-based design systems (KBDSs).  Knowledge engineers elicit knowledge from domain experts and build a knowledgebased  design system. The system generates designs. The amount of knowledge the  system possesses and the way it applies the knowledge directly influence the performance  of its designs. Therefore, critics are proposed to assist (1) acquiring sufficient knowledge  for constructing a desirable system, and (2) applying proper knowledge to generating  designs. Methodologies of equipping a KBDS with critics are developed. Our practice  in building and using a KBDS shows the applicability and capability of these critics.  Key Words: Knowledge-based Systems, Design, Critiquing, Knowledge Acquisition  1 Introduction  Engineering design is a domain where a knowledge-based approach has...",
            "group": 753,
            "name": "10.1.1.34.3934",
            "keyword": "Key WordsKnowledge-based SystemsDesignCritiquingKnowledge Acquisition",
            "title": "Critics for Knowledge-Based Design Systems"
        },
        {
            "abstract": " ",
            "group": 754,
            "name": "10.1.1.34.4463",
            "keyword": "",
            "title": "Estimation of Distributed Parameters by Multiresolution Optimization"
        },
        {
            "abstract": "We propose a method for fast face localisation and verification (identification) based on a robust form of correlation. Geometric and photometric normalisation of face images is achieved by direct minimisation. During optimisation, the correlation is estimated from a set of samples drawn from a Sobol sequence. This Monte-Carlo technique speeds the evaluation of correlation approximately twenty five times and makes the optimisation process near-real time. In recognition experiments, the optimised robust correlation outperformed two standard techniques based on the Dynamic Link Architecture [11]. 1 Introduction  Personal identification (authentication, verification of identity) is an important issue in many security applications. In this paper we focus on personal identification from frontal face images. Identification is closely related to recognition, but differs in at least three fundamental aspects. Firstly, a client -- an authorised user of a personal identification system -- is ass...",
            "group": 755,
            "name": "10.1.1.34.5140",
            "keyword": "",
            "title": "Fast Face Localisation and Verification"
        },
        {
            "abstract": "This paper describes a simple method of obtaining longer-term predictions from a nonlinear time-series, assuming one already has a reasonably good short-term predictor. The usefulness of the technique is that it eliminates, to some extent, the systematic errors of the iterated short-term predictor. The technique we describe also provides an indication of the prediction horizon. We consider systems with both observational and dynamic noise and analyse a number of artificial and experimental systems obtaining consistent results. We also compare this method of longer-term prediction with ensemble prediction.",
            "group": 756,
            "name": "10.1.1.34.5577",
            "keyword": "Longer-term predictionsNonlinear time-seriesIterated predictorPrediction horizon",
            "title": "Towards Long-Term Prediction"
        },
        {
            "abstract": "We resolve in the affirmative a question of Boppana and Bui: whether simulated  annealing can, with high probability and in polynomial time, find the optimal bisection  of a random graph in G npr when p \\Gamma r = \\Theta(n  \\Delta\\Gamma2  ) for \\Delta  2. (The random  graph model G npr specifies a \"planted\" bisection of density r, separating two n=2-  vertex subsets of slightly higher density p.) We show that simulated \"annealing\" at  an appropriate fixed temperature (i.e., the Metropolis algorithm) finds the unique  smallest bisection in O(n  2+\"  ) steps with very high probability, provided \\Delta ? 11=6.  (By using a slightly modified neighborhood structure, the number of steps can be  reduced to O(n  1+\"  ).) We leave open the question of whether annealing is effective  for \\Delta in the range 3=2 ! \\Delta  11=6, whose lower limit represents the threshold at  which the planted bisection becomes lost amongst other random small bisections.  It remains open whether hillclimbing (i.e.,...",
            "group": 757,
            "name": "10.1.1.34.6360",
            "keyword": "",
            "title": "Simulated Annealing for Graph Bisection"
        },
        {
            "abstract": "We measure the performance, in the task of apportioning the Congress of the United States, of an algorithm combining a simulated-annealing-driven search with an exact-computation dynamic programming evaluation of the apportionments visited in the search. We compare this with the actual algorithm currently used in the United States to apportion Congress, and with a number of other algorithms that have been proposed. We conclude that on every set of census data in this country's history, the simulated-annealing apportionment provably yields far fairer apportionments than those of any of the other algorithm considered, including the algorithm currently used for Congressional apportionment. 1 Motivation and Overview  How should the seats in the House of Representatives of the United States be allocated among the states? The Constitution stipulates only that \"Representatives shall be apportioned among the several states according to their respective numbers, counting    Research supported i...",
            "group": 758,
            "name": "10.1.1.34.6640",
            "keyword": "",
            "title": "Power Balance and Congressional Apportionment Algorithms"
        },
        {
            "abstract": "This paper presents two simple optimization techniques based on combining the Langevin Equation with the Hopfield Model. Proposed models - referred as Stochastic Model (SM) and Pulsed Noise Model (PNM) - can be viewed as straightforward stochastic extensions of the Hopfield optimization network. Optimization with SM, unlike in previous related models, in which ffi-correlated  Gaussian noises were considered, is based on Gaussian noises with positive autocorrelation times. This is a reasonable assumption from a hardware implementation point of view. In the other model - PNM, Gaussian noises are injected to the system only at certain time instances, as opposite to continuously maintained ffi-correlated noises used in the previous related works. In both models (SM and PNM), intensities of noises added to the model are  independent of neurons' potentials. Moreover, instead of impractically long inverse logarithmic cooling schedules, linear cooling is tested. With the above strong simplific...",
            "group": 759,
            "name": "10.1.1.34.7157",
            "keyword": "",
            "title": "Optimization with the Hopfield network based on correlated noises: an empirical approach"
        },
        {
            "abstract": "Motivation: Aligning protein structures is a highly relevant task. It enables the study of functional and ancestry relationships between proteins and is very important for homology and threading methods in structure prediction. Existing methods typically only partially explore the space of possible alignments and being able to eciently handle permutations eciently is rare.  Results: A novel approach for structure alignment is presented, where the key ingredients are: (1) An error function formulation of the problem simultaneously in terms of binary (Potts) assignment variables and real-valued atomic coordinates. (2) Minimization of the error function by an iterative method, where in each iteration a mean eld method is employed for the assignment variables and exact rotation/translation of atomic coordinates is performed, weighted with the corresponding assignment variables. The approach allows for extensive search of all possible alignments, including those involving arbitrary permuta...",
            "group": 760,
            "name": "10.1.1.34.7231",
            "keyword": "protein structure alignmentpermutationmean eld annealingfuzzy assignmentdatabase searching 1",
            "title": "A Novel Approach to Structure Alignment"
        },
        {
            "abstract": "We investigate a family of algorithms for graph bisection that are  based on a simple local connectivity heuristic, which we call seedgrowth.  We show how the heuristic can be combined with stochastic  search procedures and a postprocess application of the KernighanLin  algorithm. In a series of time-equated comparisons against largesample  runs of pure Kernighan-Lin, the new algorithms find bisections  of the same or superior quality. Their performance is particularly good  on structured graphs representing important industrial applications.  An appendix provides further favorable comparisons to other published  results. Our experimental methodology and extensive empirical results  provide a solid foundation for further empirical investigation of graphbisection  algorithms.  Keywords: Graph bisection, heuristic algorithms, computer-aided  design, graph partitioning.  1 Introduction  Given a graph G = (V; E) with an even number of vertices, the graph bisection problem is to divide V in...",
            "group": 761,
            "name": "10.1.1.34.7870",
            "keyword": "Graph bisectionheuristic algorithmscomputer-aided designgraph partitioning",
            "title": "Seed-Growth Heuristics for Graph Bisection"
        },
        {
            "abstract": "We present a new local optimizer called SOP-3-exchange for the sequential ordering problem that extends a local search for the traveling salesman problem to handle multiple constraints directly without increasing computational complexity. An algorithm that combines the SOP-3-exchange with an Ant Colony Optimization algorithm is described and we present experimental evidence that the resulting algorithm is more effective than existing methods for the problem. The best-known results for many of a standard test set of 22 problems are improved using the SOP-3-exchange with our Ant Colony Optimization algorithm or in combination with the MPO/AI algorithm (Chen and Smith 1996).",
            "group": 762,
            "name": "10.1.1.34.8082",
            "keyword": "Other key wordsAnt Colony OptimizationAnt algorithmsMetaheuristicsSequential ordering problemSwarm intelligence",
            "title": "An Ant Colony System Hybridized With A New Local Search For The Sequential Ordering Problem"
        },
        {
            "abstract": "We present a novel approach to model inter-processor communication in multi-DSP systems. In most multi-DSP systems, inter-processor communication is realized by transferring data over point-to-point links with hardware FIFO buffers. Direct memory access (DMA) is additionally used to concurrently transfer data to the FIFO buffers and perform computation. Our model accounts for the limited size of the communication buffers as well as concurrent DMA transfer. This novel communication model is applied in our rapid prototyping environment for optimizing multi-DSP systems. Given an extended data ow graph of the DSP application and a description of the target multi-processor system, our rapid prototyping environment automatically maps the DSP application onto the multi-processor system and generates a schedule for each processor.",
            "group": 763,
            "name": "10.1.1.34.8231",
            "keyword": "communication modelmapping and schedulingmulti-DSPrapid prototyping",
            "title": "A New Approach to Model Communication for Mapping and Scheduling DSP-Applications"
        },
        {
            "abstract": "The determination of molecular structures is of growing importance in modern chemistry and biology. This thesis presents two practical, systematic algorithms for two structure determination problems. Both algorithms are branch-and-bound techniques adapted to their respective domains. The first problem is the determination of structures of multimers given rigid monomer structures and (potentially ambiguous) intermolecular distance measurements. In other words, we need to find the the transformations to produce the packing interfaces. A substantial diculty results from ambiguities in assigning intermolecular distance measurements (from NMR, for example) to particular intermolecular interfaces in the structure. We present a rapid and efficient method to simultaneously solve the packing and the assignment problems. The algorithm, AmbiPack, uses a hierarchical division of the search space and the branch-and-bound algorithm to eliminate infeasible regions of the space and focus on the remaining ...",
            "group": 764,
            "name": "10.1.1.34.8592",
            "keyword": "",
            "title": "Determining Molecular Conformation from Distance or Density Data"
        },
        {
            "abstract": "We describe two systems: GATE (General Architecture for Text Engineering), an architecture to aid in the production and delivery of language engineering systems which significantly reduces development time and ease of reuse in such systems. We also describe a sense tagger which we implemented within the GATE architecture, and which achieves high accuracy (92% of all words in text to a broad semantic level). We used the implementation of the sense tagger as a real-world task on which to evaluate the usefulness of the GATE architecture and identified strengths and weaknesses in the architecture.",
            "group": 765,
            "name": "10.1.1.34.9259",
            "keyword": "",
            "title": "Implementing a Sense Tagger in a General Architecture for Text Engineering"
        },
        {
            "abstract": "The market demands that software systems be adaptable to changes in requirements. Software must be evolvable to solve slightly different problems over time. The transition from real-world requirements to software is a human-intensive and potentially complex process that provides limited automated support for the analysis of alternative designs with respect to their evolvability. In this paper, we propose an analytical software design approach to localize changes to control flow requirements. We present an analytical and \"heuristically good\" design approach to generate control components that localize change and reduce the computational complexity of an optimal approach. We apply our heuristic to an example and summarize the results. Lastly, we propose future research and summarize our ideas.  1. Introduction  Because the market demands changes in application requirements, there is an increasing need for software systems to adaptively support changes in applicationlevel objectives [3]. ...",
            "group": 766,
            "name": "10.1.1.34.9372",
            "keyword": "",
            "title": "Second Workshop on High Assurance Systems Engineering (Hase'97), IEEE Computer Society Press, Los Alamitos, CA, Aug. 11-12, 1997, pp. 48-55."
        },
        {
            "abstract": "March 1997 y Institute of Management Science, University of Vienna, Bruenner Str. 72, A - 1210 Vienna, Austria email: bullnhei@pom.bwl.univie.ac.at, christine.strauss@univie.ac.at z Department of Applied Computer Science and Information Systems, University of Vienna, Lenaugasse 2/8, A - 1080 Vienna, Austria email: gabi@ani.univie.ac.at 1 Introduction The Ant System is a new member in the class of so-called meta-heuristics which are particularly used to solve hard combinatorial optimization problems. The most well-known of these methods are simulated annealing (cf. e.g. [12]), tabu search (cf. e.g. [8], [9]), neural networks (cf. e.g. [11]), evolution strategies (cf. e.g [14]) and genetic algorithms (cf. e.g. [10]). The structure of the Ant System seems to be highly suitable for parallelization of the algorithm. In this study we develop two parallelization strategies for the Ant System, analyze factors influencing the computational complexity, and discuss design parameters as im...",
            "group": 767,
            "name": "10.1.1.35.185",
            "keyword": "",
            "title": "Parallelization Strategies for the Ant System (Extended Abstract)"
        },
        {
            "abstract": "The problem of logistics and resource management in disease control projects in the developing world can hardly be understated. One problem that arises is the occurance of regional imbalances in supply. Such imbalances can seriously compromise the effectiveness of such programs. A prototype system is described, based on an order-based plan generator and a class of modern combinatorial optimisation techniques, which recommends an implementable  plan for the redistribution of available resources to minimise such shortages for real-world situations.",
            "group": 768,
            "name": "10.1.1.35.395",
            "keyword": "",
            "title": "A Prototype Emergency Resource Redistribution System For Disease Control Programmes"
        },
        {
            "abstract": "The complex, dynamic feature of the current Internet requires a scalable, effective network control. This paper proposes a dynamic network control scheme, which performs the parameter tuning of the underlying network algorithm based on collaborative on-line simulation, thus achieves second order control on the network. We have used ns as the simulation platform and introduced some methods to tackle its drawbacks in trac generation. To efficiently search the concerned parameter space for \"good\" points, we propose an increasingly-improving search procedure and a hybrid search algorithm. We also present some methods to speed up the network simulation. In particular, we find that topology decomposition can achieve very high speed-up if decoupling the execution of different decomposed parts. In addition to the network control, the on-line simulation scheme can also be used in many other areas. The applicability of this scheme to routing algorithms is investigated in this paper. Extensive simulati...",
            "group": 769,
            "name": "10.1.1.35.812",
            "keyword": "",
            "title": "Network Management and Control Using Collaborative On-line Simulation"
        },
        {
            "abstract": "An efficient recursive task allocation scheme, based on the Kernighan-Lin mincut bisection  heuristic, is proposed for the effective mapping of tasks of a parallel program  onto a hypercube parallel computer. It is evaluated by comparison with an adaptive,  scaled simulated annealing method. The recursive allocation scheme is shown to be  effective on a number of large test task graphs -- its solution quality is nearly as good  as that produced by simulated annealing, and its computation time is several orders of  magnitude less.  ",
            "group": 770,
            "name": "10.1.1.35.1050",
            "keyword": "",
            "title": "Task Allocation onto a Hypercube by Recursive Mincut Bipartitioning"
        },
        {
            "abstract": "In this paper we introduce a new Simulated Annealingbased timing-driven placement algorithm for FPGAs. This paper has three main contributions. First, our algorithm employs a novel method of determining source-sink connection delays during placement. Second, we introduce a new cost function that trades off between wire-use and critical path delay, resulting in significant reductions in critical path delay without significant increases in wire-use. Finally, we combine connection-based and path-based timing-analysis to obtain an algorithm that has the low time-complexity of connection-based timing-driven placement, while obtaining the quality of path-based timing-driven placement.  A comparison of our new algorithm to a well known nontiming -driven placement algorithm demonstrates that our algorithm is able to increase the post-place-and-route speed (using a full path-based timing-driven router and a realistic routing architecture) of 20 MCNC benchmark circuits by an average of 42%, whil...",
            "group": 771,
            "name": "10.1.1.35.1130",
            "keyword": "",
            "title": "Timing-Driven Placement for FPGAs"
        },
        {
            "abstract": "Logic emulation enables designers to functionally verify complex integrated circuits prior to chip fabrication. However, traditional FPGA-based logic emulators have poor inter-chip communication bandwidth, commonly limiting gate utilization to less than 20 percent. Global routing contention mandates the use of expensive crossbar and PC-board technology in a system of otherwise low-cost, commodity parts. Even with crossbar technology,current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). Virtual Wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. The resulting increase in bandwidth allows effective use of low dimension, direct interconnect. The size of the FPGA array can be decreased as well, resulting in low cost logic emulation. This pa...",
            "group": 772,
            "name": "10.1.1.35.1990",
            "keyword": "",
            "title": "Logic Emulation with Virtual Wires"
        },
        {
            "abstract": "The goal of this project was to turn the intuitions behind dynamic backtracking into a series of formally verified algorithms, implement the algorithms, and test the results on realistic problems. These goals have been met and exceeded. Dynamic backtracking has been generalized to partial-order dynamic backtracking, and has been formalized, tested on academic benchmarks, and applied (by one of CIRL's industrial partners) to real industrial scheduling problems. Equally importantly, the search for novel search algorithms for scheduling problems has led beyond dynamic backtracking to include new techniques, such as limited discrepancy search and doubleback optimization, that are currently the best known techniques for benchmark scheduling problems of realistic size and character. 4 Chapter 1 Introduction Within the planning community, the general attitude toward search has often been, \"Search and you're dead.\" That is, if the available domain knowledge is so weak that search is neces...",
            "group": 773,
            "name": "10.1.1.35.3725",
            "keyword": "",
            "title": "Dynamic Backtracking"
        },
        {
            "abstract": "A new general framework is presented for implementing complex a priori knowledge, having in mind especially situations where the number of available training data is small compared to the complexity of the learning task. A priori information is hereby decomposed into simple components represented by quadratic building blocks (quadratic concepts) which are then combined by conjunctions and disjunctions to built more complex, problem specific error functionals. While conjunction of quadratic concepts leads to classical quadratic regularization functionals, disjunctions, representing ambiguous priors, result in non-convex error functionals. These go beyond classical quadratic regularization approaches and correspond, in Bayesian interpretation, to non-gaussian processes. Numerical examples show that the resulting stationarity equations, despite being in general nonlinear, inhomogeneous (integro-)differential equations, are not necessarily difficult to solve. Appendix A relates the formalism of statistical",
            "group": 774,
            "name": "10.1.1.35.4066",
            "keyword": "",
            "title": "How to Implement A Priori Information: A Statistical Mechanics Approach"
        },
        {
            "abstract": "We present techniques for generating addresses for memories containing multiple arrays. Because these techniques rely on the inversion or rearrangement of address bits, they are faster and require less hardware to compute than the traditional technique of addition. Use of these techniques can improve performance and cost of application-specific memory subsystems by decreasing effective access time to arrays and by reducing address generation hardware. The primary drawback to this approach is that extra memory space is occasionally required, but in over a million tested cases, this extra memory space is on average only 2% and no worse than 17.4% of the utilized memory space. This amount of wasted address space is significantly less than the amount required by the only known similar technique [7] and rarely necessitates the allocation of additional memory components. These techniques provide a foundation for adderfree address generation for manually and automatically generated applicatio...",
            "group": 775,
            "name": "10.1.1.35.4924",
            "keyword": "transforms",
            "title": "Address Generation for Memories Containing Multiple Arrays"
        },
        {
            "abstract": ": The growing interest in hard multiple objective combinatorial and non-linear problems resulted in a significant number of heuristic methods aiming at generating sets of feasible solutions as approximations to the set of non-dominated solutions. The issue of evaluating these approximations is addressed. Such evaluations are useful when performing experimental comparisons of different multiple objective heuristic algorithms, when defining stopping rules of multiple objective heuristic algorithms, and when adjusting parameters of heuristic algorithms to a given problem. A family of outperformance relations that can be used  to compare approximations under very weak assumptions about a decision-maker's preferences is introduced. These outperformance relations define incomplete orders in the set of all approximations. It is shown that in order to compare approximations, which are incomparable according to the outperformance relations, much stronger assumptions about the decision-maker's p...",
            "group": 776,
            "name": "10.1.1.35.5279",
            "keyword": "Multiple objective optimizationHeuristicsEvaluation",
            "title": "Evaluating the Quality of Approximations to the Non-Dominated Set"
        },
        {
            "abstract": "Introduction 1.1 The Game of Go Go was developed four millennia ago in China; it is one of the oldest and most popular board games in the world. Like chess, it is a deterministic, perfect information, zero-sum game of strategy between two players. They alternate in placing black and white stones on the intersections of a 19x19 grid (smaller for beginners) with the objective of surrounding more board area (territory) with their stones than the opponent. Adjacent stones of the same color form groups; an empty intersection adjacent to a group is called a liberty of that group. A group is captured and removed from the board when its last liberty is occupied by the opponent. To prevent loops, it is illegal to make certain moves which recreate a prior board position. A player may pass at any time; the game ends when both players pass in succession. Each player's score is then calculated as the territory (number",
            "group": 777,
            "name": "10.1.1.35.5335",
            "keyword": "",
            "title": "Learning To Evaluate Go Positions Via Temporal Difference Methods"
        },
        {
            "abstract": "There are several powerful solvers for satisfiability (SAT), such as wsat, Davis-Putnam,  and relsat. However, in practice, the SAT encodings often have so many clauses that  we exceed physical memory resources on attempting to solve them. This excessive size  often arises because conversion to SAT, from a more natural encoding using quantifications  over domains, requires expanding quantifiers.  This suggests that we should \"lift\" successful SAT solvers. That is, adapt the solvers  to use quantified clauses instead of ground clauses. However, it was generally believed  that such lifted solvers would be impractical: Partially, because of the overhead of  handling the predicates and quantifiers, and partially because lifting would not allow  essential indexing and caching schemes.  Here we show that, to the contrary, it is not only practical to handle quantified  clauses directly, but that lifting can give exponential savings. We do this by identifying  certain tasks that are central to...",
            "group": 778,
            "name": "10.1.1.35.6085",
            "keyword": "",
            "title": "Lifted Search Engines for Satisfiability"
        },
        {
            "abstract": "Ordering clones from a genomic library into physical maps of whole chromosomes presents a central computational problem in genetics. Chromosome reconstruction via clone ordering is shown to be isomorphic to the NP-complete Optimal Linear Ordering  problem. Massively parallel algorithms for simulated annealing based on Markov chain distribution are proposed and applied to this problem. Perturbation methods and problem-specific annealing heuristics are proposed and described. Experimental results on a 2048 processor MasPar MP-2 system are presented. Convergence, speedup and scalability characteristics of the various algorithms are analyzed and discussed. 1 Introduction  A central problem in genetics is that of creating maps of entire chromosomes which could then be used to reconstruct the chromosome's DNA sequence. These chromosomal maps fall into two broad categories - genetic maps and physical maps. A physical map is defined as a partial ordering of distinguishable DNA fragments or clo...",
            "group": 779,
            "name": "10.1.1.35.6088",
            "keyword": "",
            "title": "Massively Parallel Algorithms for Chromosome Reconstruction"
        },
        {
            "abstract": "This paper overviews recent work on ant algorithms, that is, algorithms for discrete  optimization which took inspiration from the observation of ant colonies foraging  behavior, and introduces the ant colony optimization (ACO) meta-heuristic. In the  first part of the paper the basic biological findings on real ants are overviewed, and  their artificial counterparts as well as the ACO meta-heuristic are defined. In the  second part of the paper a number of applications to combinatorial optimization and  routing in communications networks are described. We conclude with a discussion of  related work and of some of the most important aspects of the ACO meta-heuristic.  1 Introduction  Ant algorithms were first proposed by Dorigo and colleagues [33, 39] as a multi-agent approach to di#cult combinatorial optimization problems like the traveling salesman problem (TSP) and the quadratic assignment problem (QAP). There is currently a lot of ongoing activity in the scientific community to ext...",
            "group": 780,
            "name": "10.1.1.35.6295",
            "keyword": "",
            "title": "Ant Algorithms for Discrete Optimization"
        },
        {
            "abstract": "A methodology for fusing multiple instances of biometric data to improve the performance of a personal identity verification system is developed. The fusion problem is formulated in the framework of the Bayesian estimation theory. The effect of different fusion strategies on the error probability is analysed theoretically. The proposed methodology is then demonstrated on the problem of personal identity verification using multiple facial images. Experimental studies on the M2VTS database confirm the predicted improvements in performance. A reduction in error rates of up to 40% is achieved. The performance gains are initially monotonic but they tend to saturate after integrating the first few observations. It is also shown that the fusion based on rank order statistic, i.e. the median, is robust to outliers.",
            "group": 781,
            "name": "10.1.1.35.6570",
            "keyword": "Key wordsEvidence combinationFace identificationMultiple observation fusionError sensitivity Preprint submitted to Elsevier Preprint 20 November 1997",
            "title": "Combining Evidence in Personal Identity Verification Systems"
        },
        {
            "abstract": "This paper presents a general diffusion model of a simple genetic algorithm. Unlike the similar previous efforts made for modeling mutation based genetic search, this work includes the effect of crossover by considering the dynamics of spatially averaged observables, under the influence of deterministic and stochastic effects. The analysis shows a possibility of generating Boltzmann distribution in the population. A noise reduction mechanism by means of controlling the distributional bias of the population is conjectured based on observations made on the entropic structure of the representational space.",
            "group": 782,
            "name": "10.1.1.35.6820",
            "keyword": "",
            "title": "Drift, Diffusion and Boltzmann Distribution in Simple Genetic Algorithm"
        },
        {
            "abstract": "Many experimental results are reported on all types of Evolutionary Algorithms but only few results have been proved. A step towards a theory on Evolutionary Algorithms, in particular, the so-called (1 + 1) Evolutionary Algorithm, is performed. Linear functions are proved to be optimized in expected time O(n ln n) but only mutation rates of size #(1/n) can ensure this behavior. For some polynomial of degree 2 the optimization needs exponential time. The same is proved for a unimodal function. Both results were not expected by several other authors. Finally, a hierarchy result is proved. Moreover, methods are presented to analyze the behavior of the (1 + 1) Evolutionary Algorithm.  1 Introduction  Evolutionary Algorithms are a class of search algorithms that are often used as function optimizers for static objective functions. There are a lot of di#erent types of Evolutionary Algorithms, the best known are Evolution Strategies [16,19], Evolutionary Programming [5], Genetic Algorithms [8...",
            "group": 783,
            "name": "10.1.1.35.6970",
            "keyword": "1619Evolutionary Programming [5Genetic Algorithms [87and Genetic",
            "title": "On the Analysis of the (1+1) Evolutionary Algorithm"
        },
        {
            "abstract": "Simulated Annealing (SA) is a powerful stochastic search method applicable to a wide range of problems for which little prior knowledge is available. It can produce very high quality solutions for hard combinatorial optimization problems. However, the computation time required by SA is very large. Various methods have been proposed to reduce the computation time, but they mainly deal with the careful tuning of SA's control parameters. This paper first analyzes the impact of SA's neighbourhood on SA's performance and shows that SA with a larger neighbourhood is better than SA with a smaller one. The paper also gives a general model of SA, which has both dynamic generation probability and acceptance probability, and proves its convergence. All variants of SA can be unified under such a generalization. Finally, a method of extending SA's neighbourhood is proposed, which uses a discrete approximation to some continuous probability function as the generation function in SA, and several impo...",
            "group": 784,
            "name": "10.1.1.35.7879",
            "keyword": "Stochastic SearchAlgorithm AnalysisCombinatorial Optimization",
            "title": "Simulated Annealing with Extended Neighbourhood"
        },
        {
            "abstract": "A method for dynamic scheduling on a network computing system and an approximation algorithm for solving the asymmetric traveling salesman problem (ATSP) are presented in this paper. Dynamic scheduling was implemented to minimize the application program execution time. Our method decomposes the program workload into computationally homogeneous subtasks, which may be of different size, depending on the current load of each machine in a heterogeneous computing system. We present the experimental results of a practical application, the asymmetric traveling salesman problem. All test problems are taken from the literature.  Keywords: heterogeneous computing, dynamic scheduling, optimization method, asymmetric traveling salesman problem 1 Introduction  From the scientific community to the federal government, heterogeneous computing [5] has become an important area of research and interest. Heterogeneous computing includes both parallel [6] and distributed processing. In general, the goal of...",
            "group": 785,
            "name": "10.1.1.35.8080",
            "keyword": "salesman problem",
            "title": "Solving Asymmetric Traveling Salesman Problems Using Dynamic Scheduling on a Heterogeneous Computing System"
        },
        {
            "abstract": "In this paper, we investigate and relate various variants of the greedy satisfiability tester GSAT. We present these algorithms as members of a whole family of algorithms for finding a model for satisfiable propositional logic formulas. In particular, all algorithms can be formulated as instances of the same generic frame GenSAT. Comparing the algorithms, we do not only concentrate on their overall performance, but are also interested in how properties like locality or different kinds of randomness influence the performance. To the end we define a new, theoretically complete instance of GenSAT. This variant can be viewed as a reformulation of simulated annealing (SA) within the GSAT family and thus, defines a link between GSAT and SA. For most of the algorithms experiments have been performed using very hard, randomly generated propositional logic formulas. The results of these experiments are also reported.",
            "group": 786,
            "name": "10.1.1.35.8891",
            "keyword": "",
            "title": "The GSAT/SA-Familiy -- Relating greedy satisifability testing to simulated annealing"
        },
        {
            "abstract": "In the 70's and 80's the success of knowledge-intensive approaches to problem solving eclipsed earlier work on compute-intensive weak methods. However, in recent years, compute-intensive methods have made a surprising comeback. One of the most prominent examples is the success",
            "group": 787,
            "name": "10.1.1.35.9131",
            "keyword": "",
            "title": "Compute-intensive methods in artificial intelligence"
        },
        {
            "abstract": "The interconnection of geographically distributed supercomputers via high-speed  networks allows users to access the needed compute power for large-scale, complex applications.  For efficient use of such systems, the variance in processor performance and  network (i.e., interconnection network versus wide area network) performance must  be considered. In this paper, we present a decomposition tool, called PART, for distributed  systems. PART takes into consideration the variance in performance of the  networks and processors as well as the computational complexity of the application.  This is achieved via the parameters used in the objective function of simulated annealing.  The initial version of PART focuses on finite element based problems. The results  of using PART demonstrate a 30% reduction in execution time as compare to using  conventional schemes that equally partition the problem domain.  ",
            "group": 788,
            "name": "10.1.1.35.9320",
            "keyword": "",
            "title": "PART: A Partitioning Tool for Efficient Use of Distributed Systems"
        },
        {
            "abstract": ". The aerospace industry has been investigating integrated modular avionics (IMA) for some years. IMA offers greater flexibility in the use of computing resources by reconfiguring the software to employ different processors and communications, in order to recover from failure and to redistribute workload. Such reconfiguration offers benefits, but poses difficulties for certification since current certification practice requires assessment of each configuration.  The approach we have adopted is to seek means of clearing a configuration of a system and to identify a number of \"equivalent\" configurations. This requires us to establish \"safe\" reconfigurations for the IMA system. Technically, we have formulated the search for a set of \"equivalent\" configurations as a multiobjective optimisation problem.Pragmatically, the search produces configuration tables which could be used by the IMA operating system to make a \"safe\" change to an \"equivalent\" configuration, when necessary.  INTRODUCTION...",
            "group": 789,
            "name": "10.1.1.35.9573",
            "keyword": "",
            "title": "Approaches to Certification of Reconfigurable IMA Systems"
        },
        {
            "abstract": ". The question of satisfiability for a given propositional formula arises in many areas of AI. Especially finding a model for a satisfiable formula is very important though known to be NP-complete. There exist complete algorithms for satisfiability testing like the Davis-Putnam-Algorithm, but they often do not construct a satisfying assignment for the formula, are not practically applicable for more than 400 or 500 variable problems, or in practice take too much time to find a solution. Recently, a (in practice) very fast, though incomplete, procedure, the model generating algorithm GSAT, has been introduced and several refined variants were created. Another method is Simulated Annealing (SA). Both approaches have already been compared with different results. We clarify these differences and do a more elaborate comparison showing that the performance of an already optimized variant of GSAT and an ordinary SA algorithm are more or less the same and that the attempts to further improve G...",
            "group": 790,
            "name": "10.1.1.35.9830",
            "keyword": "",
            "title": "GSAT versus Simulated Annealing"
        },
        {
            "abstract": "To implement high-density and high-speed FPGA circuits, designers need  tight control over the circuit implementation process. However, current design  tools are unsuited for this purpose as they lack fast turnaround times,  interactiveness, and integration. We present a system for the Xilinx XC6200  FPGA, which addresses these issues. It consists of a suite of tightly integrated  tools for the XC6200 architecture centered around an architectureindependent  tool framework. The system lets the designer easily intervene at  various stages of the design process and features design cycle times (from an  HDL specification to a complete layout) in the order of seconds. ",
            "group": 791,
            "name": "10.1.1.36.76",
            "keyword": "",
            "title": "Fast Integrated Tools for . . "
        },
        {
            "abstract": "The work in this paper is motivated by our effort to build a distributed rendering system that uses  the multiple hardware graphics accelerators available in a network of workstations/PCs to provide realtime   rendering performance that is one or two generations ahead of what can be achieved using only a  single commodity machine. Specifically, we address one important aspect of building such a system,  how to partition and assign the overall rendering work. We propose a novel approach called Image Layer  Decomposition (ILD). ILD has several advantages over previous partitioning algorithms for our targeted  environment, including its compatibility with the use of hardware graphics accelerators, decoupling of  communication bandwidth requirement from scene complexity, and reduced communication bandwidth  growth as the system size increases. Furthermore, ILD tries to optimize the rendering of a sequence of  frames (of an interactive application) instead of only individual frames.  Using ...",
            "group": 792,
            "name": "10.1.1.36.365",
            "keyword": "",
            "title": "Image Layer Decomposition for Distributed Rendering on NOWs"
        },
        {
            "abstract": "Evolutionary algorithms are general, randomized search heuristics that are influenced by many parameters. Though evolutionary algorithms are assumed to be robust, it is well-known that choosing the parameters appropriately is crucial for success and e#ciency of the search. It has been shown in many experiments, that non-static parameter settings can be by far superior to static ones but theoretical verifications are hard to find. We investigate a very simple evolutionary algorithm and rigorously prove that employing dynamic parameter control can greatly speed-up optimization. 1 INTRODUCTION  Evolutionary algorithms are a class of general, randomized search heuristics that can be applied to many di#erent tasks. They are controlled by a number of di#erent parameters which are crucial for success and e#ciency of the search. Though rough guidelines mainly based on empirical experience exist, it remains a di#cult task to find appropriate settings. One way to overcome this problem is to empl...",
            "group": 793,
            "name": "10.1.1.36.678",
            "keyword": "",
            "title": "Dynamic Parameter Control in Simple Evolutionary Algorithms"
        },
        {
            "abstract": "We present in this paper an efficient, flexible, and effective data structure,  B*-trees, for non-slicing floorplans. B*-trees are based on ordered binary trees and the admissible placement presented in [1]. Inheriting from the nice properties of ordered binary trees, B*-trees are very easy for implementation and can perform the respective primitive tree operations search, insertion, and deletion in only O(1), O(1), and O(n) times while existing representations for non-slicing floorplans need at least O(n) time for each of these operations, where n is the number of modules. The correspondence between an admissible placement and its induced B*-tree is 1-to-1 (i.e., no redundancy); further, the transformation between them takes only linear time. Unlike other representations for non-slicing floorplans that need to construct constraint graphs for cost evaluation, in particular, the evaluation can be performed on B*- trees and their corresponding placements directly and incrementally. We fu...",
            "group": 794,
            "name": "10.1.1.36.750",
            "keyword": "",
            "title": "B*-Trees: A New Representation for Non-Slicing Floorplans"
        },
        {
            "abstract": "... This article is a review of numerical methods and the numerical analysis for the computation of crystalline microstructure. ",
            "group": 795,
            "name": "10.1.1.36.1584",
            "keyword": "CONTENTS",
            "title": "On the Computation of Crystalline Microstructure"
        },
        {
            "abstract": " Many tasks in computer vision and image analysis have recently been expressed as the minimization of global energy functions describing the local interactions between the observed data and the images features to be extracted. For the minimization of these global (often nonlinear) energy functions, a variety of stochastic or deterministic non-linear relaxation algorithms have been described in the literature. The major drawback of relaxation algorithms remains the amount of computation required to update the image. For real world applications the computation time quickly becomes prohibitive on workstations. Several efficient approaches have been proposed to alleviate this computational burden. Among them, multigrid techniques [7, 23, 43]. have shown to significantly improve the convergence rate of linear and non-linear relaxation schemes. It is also well known that the computations involved by these algorithms are regular and local, and lead naturally to massive data parallelism, whic...",
            "group": 796,
            "name": "10.1.1.36.1604",
            "keyword": "",
            "title": "Efficient Parallel Non-Linear Multigrid Relaxation Algorithms for Low-level Vision Applications"
        },
        {
            "abstract": "Evolutionary search algorithms (ESAs from now on) are iterative problem solvers developed with inspiration from neo-Darwinian survival of the fittest genes. This thesis looks at the core issues surrounding ESAs and is a step towards building a rational methodology for their effective use. Currently there is no such method of best practice rather the whole process of designing and using ESAs is seen as more of a black art than a tried and tested engineering tool. Consequently, many non-practitioners are sceptical of the worth of ESAs as a useful tool at all.  Therefore the first task of the thesis is to lay out the reasons, from computational theory, why ESAs can be a potentially powerful tool. In this context the theory of NP-completeness is introduced to ground the discussions throughout the thesis. Then a generic framework for describing ESAs is developed to form another cornerstone of these later discussions. As well as arguing for their potential power, the main argument of the the...",
            "group": 797,
            "name": "10.1.1.36.1667",
            "keyword": "",
            "title": "Towards a Rational Methodology for Using Evolutionary Search Algorithms"
        },
        {
            "abstract": "this paper, a Pareto genetic algorithm is developed following existing methodologies",
            "group": 798,
            "name": "10.1.1.36.2408",
            "keyword": "",
            "title": "Low-Thrust Trajectory Optimization Using Stochastic Optimization Methods"
        },
        {
            "abstract": "It is well-known that evolutionary algorithms succeed to optimize some functions efficiently and fail for others. Therefore, one would like to classify fitness functions as more or less hard to optimize for evolutionary algorithms. The aim of this paper is to clarify limitations and possibilities for classifications of fitness functions from a theoretical point of view. We distinguish two different types of classifications, descriptive and analytical ones. We shortly discuss three widely known approaches, namely the NK-model, epistasis variance, and fitness distance correlation. Furthermore, we consider another recent measure, bit-wise epistasis introduced by Fonlupt, Robilliard, and Preux (1998). We discuss shortcomings and counter-examples for all four measures and use this to motivate a discussion of possibilities and limitations of classifications of fitness functions in a broader context. and find out its shortcomings.",
            "group": 799,
            "name": "10.1.1.36.2865",
            "keyword": "",
            "title": "On the Classification of Fitness Functions"
        },
        {
            "abstract": "Clustering is an important unsupervised learning paradigm, but  so far the traditional methodologies are mostly based on the  minimization of the variance between the data and the cluster  means. Here we propose a new evaluation function based on a  recently developed information theoretic measure defined from  Renyi's entropy. We show how to apply Renyi's entropy to  clustering and analyze the resulting staircase nature of the  performance function that can be expected during learning. We  suggest simulated annealing as a possible optimization criterion.  1. INTRODUCTION  Clustering is a difficult task, because normally there is no a-priori information about the structure of the data or about the number of clusters. Hence, the accuracy of clustering entirely depends upon the data and the way the training algorithm is able to capture the structure in the data. We sometimes impose externally such structure to simply algorithm development. For example, we can assume that the data is a mi...",
            "group": 800,
            "name": "10.1.1.36.2938",
            "keyword": "",
            "title": "A New Clustering Evaluation Function Using Renyi's Information Potential"
        },
        {
            "abstract": "Curves with thickness, or thick curves, arise naturally in certain applications such as magnetic resonance imaging of the brain; they can also arise through low-level image processing procedures such as morphological dilation. In this paper we describe and analyze an active contour algorithm for the reconstruction and mapping of the central contour of thick curves. We address two issues of theoretical and practical importance: convergence --- i.e., is a unique solution guaranteed? --- and fidelity --- i.e., will the solution agree with the true central contour? We show that convexity of the active contour model depends strongly on the thick curve itself, but that under certain conditions the problem is convex and a unique globally optimal solution exists. We also find through a frequency domain analysis conditions under which the solution can agree with the true thick curve. Ideally these two conditions prescribe an interval from which the regularization parameter of the problem should...",
            "group": 801,
            "name": "10.1.1.36.2982",
            "keyword": "",
            "title": "An Active Contour Algorithm For Thick Curves"
        },
        {
            "abstract": "INTRODUCTION  Adaptive IIR filtering has been an active area of research for many years, and many properties of IIR filters are well known [1, 2]. A major concern in IIR filtering applications is that the cost function of IIR filters is generally multi-modal with respect to the filter coefficients, and the usual gradient-based algorithm can easily be stuck at local minima. In order to achieve a global minimum solution, global optimisation techniques are needed. Global optimisation methods require extensive computations and are usually batch-type algorithms, as the cost function employed must be evaluated on a block of data. In contrast, gradient-based learning can be implemented recursively to update the filter coefficients as each new data sample is acquired. Despite of these drawbacks, applying global optimisation methods to IIR filter design is attractive,since in many applications a global optimal solution can be much better than local optimal ones. When considering global opt",
            "group": 802,
            "name": "10.1.1.36.3609",
            "keyword": "",
            "title": "Digital IIR Filter Design Using Adaptive Simulated Annealing"
        },
        {
            "abstract": "This paper presents theoretical results derived in the analysis of the model proposed in part I for the olfactory pathway. Some of these results are modelspecific, others are of more generic interest. The latter include the description of the dynamics in the presence of noise as a two-step Markov process: this leads to the derivation of a Boltzmann-type distribution of the steady-state probabilities of attractors for a discrete-time dynamic systems with cycles of maximum length two. This leads to a clear understanding of the phenomena described from simulations in part I, including the emergence of three different noise regimes. More specific of the model is the description of the deterministic dynamics and the mathematical justification of the coding properties emerging from the prevalent lateral inhibition in the model.  2  1. INTRODUCTION In this article, we report a generic result, which was established in the course of the theoretical analysis of the network described in part I o...",
            "group": 803,
            "name": "10.1.1.36.4121",
            "keyword": "",
            "title": "A Dynamic Model Of Key Feature Extraction: The Example Of Olfaction II - Theoretical analysis by a Boltzmann-type distribution of attractors"
        },
        {
            "abstract": "Many experimental results are reported on all types of Evolutionary Algorithms but only few results have been proved. A step towards a theory on Evolutionary Algorithms, in particular, the so-called (1 + 1) Evolutionary Algorithm, is performed. Linear functions are proved to be optimized in expected time O(n ln n) but only mutation rates of size (1=n) can ensure this behavior. For some polynomial of degree 2 the optimization needs exponential time. The same is proved for a unimodal function. Both results were not expected by several other authors. Finally, a hierarchy result is proved. Moreover, methods are presented to analyze the behavior of the (1 + 1) Evolutionary Algorithm.  ",
            "group": 804,
            "name": "10.1.1.36.4208",
            "keyword": "",
            "title": "On the Analysis of the (1+1) Evolutionary Algorithm"
        },
        {
            "abstract": "This paper presents a new approach to handle constraints using evolutionary algorithms. The new technique treats constraints as objectives, and uses a multiobjective optimization approach to solve the re-stated single-objective optimization problem. The new approach is compared against other numerical and evolutionary optimization techniques in several engineering optimization problems with different kinds of constraints. The results obtained show that the new approach can consistently outperform the other techniques using relatively small sub-populations, and without a significant sacrifice in terms of performance.",
            "group": 805,
            "name": "10.1.1.36.5160",
            "keyword": "genetic algorithmsconstraint handlingmultiobjective optimizationevolutionary optimizationnumerical",
            "title": "Treating Constraints As Objectives For Single-Objective Evolutionary Optimization"
        },
        {
            "abstract": "We study the convergence speed of generalized simulated annealing algorithms. Large deviations estimates uniform in the cooling schedule are established for the exit from the cycles in the spirit of Catoni's sequential annealing work [2]. We compute the optimal convergence speed exponent proving a conjectured of R. Azencott [1].  ",
            "group": 806,
            "name": "10.1.1.36.5581",
            "keyword": "",
            "title": "Rough Large Deviation Estimates for the Optimal Convergence Speed Exponent of Generalized Simulated Annealing Algorithms"
        },
        {
            "abstract": "GENET is a heuristic repair algorithm which demonstrates impressive efficiency in solving  some large-scale and hard instances of constraint satisfaction problems (CSPs). In this paper,  we draw a surprising connection between GENET and discrete Lagrange multiplier methods.  Based on the work of Wah and Shang, we propose a discrete Lagrangian-based search scheme  LSDL, defining a class of search algorithms for solving CSPs. We show how GENET can  be reconstructed from LSDL. The dual viewpoint of GENET as a heuristic repair method  and a discrete Lagrange multiplier method allows us to investigate variants of GENET from  both perspectives. Benchmarking results confirm that first, our reconstructed GENET has the  same fast convergence behavior as the original GENET implementation, and has competitive  performance with other local search solvers DLM, WalkSAT, and Wsat(oip), on a set of difficult  benchmark problems. Second, our improved variant, which combines techniques from heuristic  r...",
            "group": 807,
            "name": "10.1.1.36.5744",
            "keyword": "",
            "title": "A Lagrangian Reconstruction of GENET"
        },
        {
            "abstract": "A method for the data mining task of data classification, suitable to be implemented on massively parallel architectures, is proposed. The method combines genetic programming and simulated annealing to evolve a population of decision trees. A cellular automaton is used to realise a fine-grained parallel implementation of genetic programming through the diffusion model and the annealing schedule to decide the acceptance of a new solution. Preliminary experimental results, obtained by simulating the behaviour of the cellular automaton on a sequential machine, show significant better performances with respect to C4.5.",
            "group": 808,
            "name": "10.1.1.36.6517",
            "keyword": "",
            "title": "Genetic Programming and Simulated Annealing: a Hybrid Method to Evolve Decision Trees"
        },
        {
            "abstract": "This work deals with the animation and control of flexible and active characters. These are characters whose rigidity and shape can vary in accordance with the desired aesthetic result and goal of the motion. In our approach characters change shape and learn to move using a set of user-defined deformation models, implemented using free-form deformations. Restricting the possible deformations to those which can be constructed by the set of predefined deformation models allows for both efficient simulation and predictable results. The interaction with the environment is physics-based and it is implemented using Lagrangian dynamics. Lagrangian dynamics and the use of parameterized deformations lead to a compact formulation of the equations of motion. Using this physical framework, the control problem can be addressed using methods that have been developed for controlling the motion of simulated articulated figures. In general, our work combines key-framing, physics-based animation techniq...",
            "group": 809,
            "name": "10.1.1.36.7029",
            "keyword": "",
            "title": "Physics-Based Animation And Control Of Flexible Characters"
        },
        {
            "abstract": "This paper presents a computational approach to developing design space models that are utilized to improve the design process by predicting values of downstream design attributes based on information available at early stages, such as preliminary design specifications. The predictive models are similar in function, though not in form, to the internal (mental) models created by experienced designers; however, the advantages of these models are that it may be possible to construct them in the absence of a designer's internal models, and that they can be passed on to and used by less experienced designers. Once created, the computational models aid designers in exploration of design alternatives and to reduce design costs and product development time.  1 INTRODUCTION  Experienced designers are often able to guide the design process toward promising designs at preliminary stages by predicting downstream design attributes using information available early on in the process. A designer may ...",
            "group": 810,
            "name": "10.1.1.36.7179",
            "keyword": "",
            "title": "Improving The Design Process by . . ."
        },
        {
            "abstract": "Introduction  Many applications which process natural language can be enhanced by incorporating information about the probabilities of word strings; that is, by using statistical language model information (Church et al., 1991; Church and Mercer, 1993; Gale, Church, and Yarowsky, 1992; Liddy and Paik, 1992). For example, speech recognition systems often require some model of the prior likelihood of a given utterance (Jelinek, 1976). For convenience, the quality of these components can be measured by test set perplexity,  PP (Bahl, Jelinek, and Mercer, 1983; Bahl et al., 1989; Jelinek, Mercer, and Roukos, 1990), in spite of some limitations (Ueberla, 1994): PP =   P (w  N  1  )  \\Gamma  1  N  , where there are N  words in the word stream hw  N  1 i and    P is some estima",
            "group": 811,
            "name": "10.1.1.36.7744",
            "keyword": "",
            "title": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies"
        },
        {
            "abstract": "The popularity of simulated annealing for engineering design applications has grown in recent years, increasing the need for new techniques that improve algorithm performance. Simulated annealing is a time-consuming, iteration-intensive algorithm. One area of algorithm enhancement with high potential impact is the development of methods for improving the algorithm by reducing the amount of wasted or non-productive search. This paper presents an approach to detection of productive search based on statistical process control (SPC) concepts. The proposed Detection of Productive Search (DPS) annealing schedule is compared to three other viable schedules using a 100-city traveling salesman problem. The DPS schedule produces results on par with the best from the more traditional schedules but does so with significantly fewer iterations.  1 INTRODUCTION  Simulated annealing is a global stochastic optimization technique enjoying increased use in recent years in mechanical design. Driving the g...",
            "group": 812,
            "name": "10.1.1.36.8383",
            "keyword": "",
            "title": "Improving The Efficiency Of Simulated Annealing Optimization Through Detection Of Productive Search"
        },
        {
            "abstract": "Analog synthesis tools have failed to migrate into mainstream use primarily because of difficulties in reconciling the simplified models required for synthesis with the industrial-strength simulation environments required for validation. MAELSTROM is a new approach that synthesizes a circuit using the same simulation environment created to validate the circuit. We introduce a novel genetic/ annealing optimizer, and leverage network parallelism to achieve efficient simulator-in-the-loop analog synthesis.",
            "group": 813,
            "name": "10.1.1.37.79",
            "keyword": "",
            "title": "MAELSTROM: Efficient Simulation-Based Synthesis for Custom Analog Cells"
        },
        {
            "abstract": "Simulated annealing, a methodology for solving combinatorial optimization problems, is a very computationally expensive algorithm, and as such, numerous researchers have undertaken efforts to parallelize it. In this paper, we investigate three of these parallel simulated annealing strategies when applied to standard cell placement, specifically the TimberWolfSC placement tool. We have examined a parallel moves strategy, as well as two new approaches to parallel cell placement, multiple Markov chains and speculative computation. These algorithms have been implemented in ProperPLACE, our parallel cell placement application, as part of the ProperCAD II project. We have constructed ProperPLACE so that it is portable across a wide range of parallel architectures. Our parallel moves algorithm uses novel approaches to dynamic message sizing, message prioritization, and error control. We show that parallel moves and multiple Markov chains are effective approaches to parallel simulated annealin...",
            "group": 814,
            "name": "10.1.1.37.430",
            "keyword": "",
            "title": "An Evaluation of Parallel Simulated Annealing Strategies with Application to Standard Cell Placement"
        },
        {
            "abstract": "A two-dimensional cellular complex is a partition of a surface into a finite number of elements---faces (open disks), edges (open arcs), and vertices (points). The topology of a cellular complex consists of the abstract incidence and adjacency relations among its elements. Here we describe a program that, given only the topology of a cellular complex, computes a geometric realization of the same---that is, a specific partition of a specific surface in three-space---guided by various aesthetic and presentational criteria.",
            "group": 815,
            "name": "10.1.1.37.871",
            "keyword": "",
            "title": "Automatic Visualization of Two-Dimensional Cellular Complexes"
        },
        {
            "abstract": "this article, we take an evolutionary view in describing how these technologies have been applied to job shop scheduling problems. To do this, we discuss a few of the most important contributions in each of these technology areas and the most recent trends.",
            "group": 816,
            "name": "10.1.1.37.1262",
            "keyword": "",
            "title": "Survey of Job Shop Scheduling Techniques"
        },
        {
            "abstract": ": This paper presents a novel algorithm for temporal partitioning of graphs representing  a behavioral description. The algorithm is based on an extension of  the traditional static-list scheduling that tailors it to resolve both scheduling  and temporal partitioning. The nodes to be mapped into a partition are selected based on a statically computed cost model. The cost for each node integrates  communication effects, the critical path length, and the possibility of the critical  path to hide the delay of parallel nodes. In order to alleviate the runtime  there is no dynamic update of the costs. A comparison of the algorithm to  other schedulers and with close-to-optimum results obtained with a simulated  annealing approach is shown. The presented algorithm has been implemented  and the results show that it is robust, effective, and efficient, and when compared  to other methods finds very good results in small amounts of CPU time.  1. INTRODUCTION  The availability of RPUs (reconfigu...",
            "group": 817,
            "name": "10.1.1.37.1361",
            "keyword": "Key wordsTemporal PartitioningReconfigurable ComputingDynamic Reconfiguration",
            "title": "An Enhanced Static-List Scheduling Algorithm for Temporal Partitioning onto RPUs"
        },
        {
            "abstract": "The Open Assembly Design Environment (OpenADE) project is an initiative at the National Institute of Standards and Technology (NIST) to provide an integrated and augmented CAD environment for assembly design. The goals of the project are: (1) to identify representations and issues for the next generation of assembly-related standards and (2) assist designers with assembly considerations throughout the phases of a product's design--- from conception to final process plan development. OpenADE 's open architecture provides standard interfaces that allow it to link to commercial and non-commercial design tools: parametric design systems, virtual reality environments, assembly analysis tools, and assembly process planners. The OpenADE project has explored issues relating to knowledge representations, virtual reality, assembly -level tolerances, constraint-based specifications, and assembly process management. This article describes the OpenADE architecture and the components that have alrea...",
            "group": 818,
            "name": "10.1.1.37.2986",
            "keyword": "",
            "title": "The Open Assembly Design Environment Project: An Architecture for Design Agent Interoperability"
        },
        {
            "abstract": "An effective partnership between industry and the university resulted in the system of design tools for the layout of HVAC systems presented in this paper and illustrated with the design of a heat pump. The system provides tools to assist in the placement of components and routing of tubes between the components. Traditional tubes, tubes that have minimized length and number of bends, and those that are impossible to route in the traditional manner, are generated. The paper provides insight on both the collaborative research interaction and the resulting set of tools. 1. INTRODUCTION The need for greater competitiveness in a global marketplace has increased pressures on companies to reduce design cycles and timeto -market. Among these companies, Carrier has recognized the importance of developing CAD technology to assist in this design process. The design of HVAC systems, Carrier's product, is a long and tedious process, often requiring weeks to design and refine a single design conc...",
            "group": 819,
            "name": "10.1.1.37.3429",
            "keyword": "",
            "title": "Hvac Cad Layout Tools: A Case Study Of University/Industry Collaboration"
        },
        {
            "abstract": "This paper is concerned with the application of performance prediction techniques to the optimisation of parallel systems, and, in particular, the use of these techniques on-the-y for optimising performance at run-time. In contrast to other performance tools, performance prediction results are made available very rapidly, which allows their use in real-time environments. When applied to program optimisation, this allows consideration of run-time variables such as input data and resource availability that are not, in general, available during the traditional (ahead-of-time) performance tuning stage.  The main contribution of this work is the application of predictive performance data to the scheduling of a number of parallel tasks across a large heterogeneous distributed computing system. This is achieved through use of just-in-time performance prediction coupled with iterative heuristic algorithms for optimisation of the meta-schedule.  The paper describes the main theoretical conside...",
            "group": 820,
            "name": "10.1.1.37.4010",
            "keyword": "",
            "title": "Theory and Operation of the Warwick Multiprocessor Scheduling (MS) System"
        },
        {
            "abstract": "this paper both involve the determination of the structure of clusters of atoms or molecules, but each application uses a di#erent potential energy function. The first potential is given by the sum of the pairwise interactions between atoms described by the Lennard-Jones function, and the second is the empirical water dimer potential energy surface function (RWK2-M) described in [10]. Problems in determining molecular structure lead to optimization problems because the naturally occurring structure usually minimizes the potential energy of the system. These problems become global optimization problems because typically such functions have very many local minimizers.",
            "group": 821,
            "name": "10.1.1.37.5424",
            "keyword": "",
            "title": "A Large-Scale Stochastic-Perturbation Global Optimization Method for Molecular Cluster problems"
        },
        {
            "abstract": "We describe an implementation of dynamic trees with \\in-subtree\" operations. Our implementation follows Sleator and Tarjan's framework of dynamic-tree implementations based on splay trees. We consider the following two examples of \\in-subtree\" operations. (a) For a given node v, nd a node with the minimum key in the subtree rooted at v. (b) For a given node v, nd a random node with key X in the subtree rooted at v (value X is xed throughout the whole computation). The rst operation may provide support for edge deletions in the dynamic minimum spanning tree problem. The second one may be useful in local search methods for degree-constrained minimum spanning tree problems. We conducted experiments with our dynamic-tree implementation within these two contexts, and the results suggest that this implementation may lead to considerably faster codes than straightforward approaches do.  1. INTRODUCTION  A dynamic tree collection (or simply dynamic trees) is a data structure which maintain...",
            "group": 822,
            "name": "10.1.1.37.5743",
            "keyword": "",
            "title": "Implementation of Dynamic Trees with In-Subtree Operations"
        },
        {
            "abstract": "A two-dimensional cellular complex is a partition of a surface into a finite number of elements---faces (open disks), edges (open arcs), and vertices (points). The  topology of a cellular complex is the abstract incidence and adjacency relations among its elements. Here we describe a program that, given only the topology of a cellular complex, computes a geometric realization of the same---that is, a specific partition of a specific surface in three-space---guided by various aesthetic and presentational criteria. ",
            "group": 823,
            "name": "10.1.1.37.5825",
            "keyword": "Computer graphicsvisualizationgraph drawingsolid modelingminimum-energy surfacescomputational topology",
            "title": "Automatic Visualization of Two-Dimensional Cellular Complexes"
        },
        {
            "abstract": "This paper presents an integrated framework for assembly design. The framework allows the designer to represent knowledge about the design process and constraints, as well as information about the artifact being designed, design history and rationale. Because the complexity of assembly design leads to extremely large design spaces, adequately supporting design space exploration is a key issue that must be addressed. This is achieved in part by allowing the designer to use both top-down and bottom-up approaches to assembly design. Exploration of the design space is further enabled by incorporating a simulated annealing-based optimization tool that allows the designer to rapidly complete partial designs, refine complete designs, and generate multiple design alternatives. INTRODUCTION  In order to design and optimize a product, designers must be able to consider di#erent alternatives, perform analysis to guide their own design process and focus in on a \"good\", if not optimal, design. It i...",
            "group": 824,
            "name": "10.1.1.37.5994",
            "keyword": "",
            "title": "Combining Interactive Exploration and Optimization for Assembly Design"
        },
        {
            "abstract": "Stochastic hill-climbing algorithms, particularly simulated annealing (SA) and threshold acceptance  (TA), have become very popular for global optimization applications. Typical implementations  of SA or TA use monotone temperature or threshold schedules, and moreover are  not formulated to accommodate practical time limits. We present a new threshold acceptance  strategy called Old Bachelor Acceptance (OBA) which has three distinguishing features: (i) it  is specifically motivated by the practical requirement of optimization within a prescribed time  bound, (ii) the threshold schedule is self-tuning, and (iii) the threshold schedule is non-monotone,  with threshold values allowed to become negative if necessary. The standard implementation  of the TA method of Dueck and Scheuer is a special case of OBA. Experiments using several  classes of symmetric traveling salesman problem instances show that OBA outperforms previous  hill-climbing methods for time-critical optimizations. A number...",
            "group": 825,
            "name": "10.1.1.37.7186",
            "keyword": "",
            "title": "Old Bachelor Acceptance: A New Class of Non-Monotone Threshold Accepting Methods"
        },
        {
            "abstract": "In an ATM network, topology design and bandwidth allocation are required in order to meet user traffic demands and to guarantee an acceptable performance. However, since the offered traffic pattern constantly varies it is necessary that the network be provided with some form of dynamic topology reconfiguration capability. This capability will also enable the network to recover from facility failures.  In this paper we present two approaches which may be useful for dynamic reconfiguration of ATM networks. In the first approach we minimize the weighted average call blocking probability, in the second, we minimize network utilization. In both cases we guarantee a maximum cell propagation delay, a maximum call blocking probability for each service type between any node pairs, and a maximum cell loss probability for each service type. These parameters characterize the Quality Of Service (QOS) requirements the ATM network will have to meet.  1 Introduction  In ATM networks, the traditional r...",
            "group": 826,
            "name": "10.1.1.37.7334",
            "keyword": "",
            "title": "Topology Design Of Multiservice Atm Networks"
        },
        {
            "abstract": "A practical algorithm for construction of physical maps based on hybridization fingerprint data of short (non-unique) oligonucleotide probes has been developed and extensively tested. Extensive experiments with realistic, high-noise simulated data show that in over 95% of the simulations, the algorithm creates an essentially completely correct map. The influence of specific experimental parameters has also been tested, demonstrating strong robustness to both false positive and false negative experimental errors.  Contents  1 Introduction 2 2 The statistical model 7 3 The Bayesian overlap score 9 3.1 Clone Pairs Overlap Score . . . . . . . . . . . . . . . . . . . . 9 3.1.1 Complexity . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Contig Pairs Overlap Score . . . . . . . . . . . . . . . . . . . 12 3.2.1 Complexity . . . . . . . . . . . . . . . . . . . . . . . . 12 4 The construction algorithm 13 4.1 The Basic Algorithm . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Improvem...",
            "group": 827,
            "name": "10.1.1.37.7495",
            "keyword": "Contents",
            "title": "A Robust Algorithm for Constructing Physical Maps From Noisy Non-Unique Probes Fingerprints"
        },
        {
            "abstract": "Due to the evolution of Geographical Information Systems, large collections of spatial data  representing various thematic contents are currently available. As a result, the interest of users  is not limited to simple spatial selections and joins, but complex query types that implicate  numerous spatial inputs become more common. Although several algorithms have been  proposed for computing the result of pairwise spatial joins, limited work exists on processing  and optimization of multiway spatial joins. In this paper we review pairwise spatial join  algorithms and show how they can be combined for multiple inputs. In addition, we explore  the application of synchronous traversal (ST), a methodology that processes synchronously all  inputs without producing intermediate results. Then, we integrate the two approaches in an  engine that includes ST and pairwise algorithms, using dynamic programming to determine the  optimal execution plan. The results show that in most cases multiway sp...",
            "group": 828,
            "name": "10.1.1.37.8094",
            "keyword": "",
            "title": "Multiway Spatial Joins"
        },
        {
            "abstract": "In this paper, we present an efficient iterative improvement based partitioning algorithm, called FM-LSRb, that combines a signal flow based hierarchical clustering algorithm using Maximum Fanout Free Subgraph (MFFS) technique with a loose and stable net removal partitioning approach (LSR). The MFFS clustering approach generalized existing MFFC decomposition method from combinational circuits to general sequential circuits, which enable us to handle cycles in circuits naturally, and make hierarchical clustering and declustering possible. We also studied the properties of the nets that straddle the cutline carefully, and introduced the concepts of the loose nets and stable nets. Our FM-LSRb algorithm begins with the hierarchical MFFS clustering based on signal direction, logical dependency and connectivity. Then, it focuses on removing loose and stable nets from the cutset of the clustered netlist. A hierarchical declustering process was embedded in the partitioning algorithm, which was...",
            "group": 829,
            "name": "10.1.1.37.8227",
            "keyword": "",
            "title": "Large Scale Circuit Partitioning with Loose/Stable Net Removal and Signal Flow Based Hierarchical Clustering"
        },
        {
            "abstract": "It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies. Introduction  The performance of a stochastic local search procedure critically depends upon the setting of the \"noise\" parameter that determines the likelihood of escaping from local minima by making non-optimal moves. In simulated annealing (Kirkpatrick et al. 1983, Dowsland 1993) this is the temperature; ...",
            "group": 830,
            "name": "10.1.1.37.8559",
            "keyword": "",
            "title": "Evidence for Invariants in Local Search"
        },
        {
            "abstract": "In this paper, an algorithm for cluster generation using tabu search approach with simulated annealing is proposed. The main idea of this algorithm is to use the tabu search approach to generate non-local moves for the clusters and apply the simulated annealing technique to select suitable current best solution so that speed the cluster generation. Experimental results demonstrate the proposed tabu search approach with simulated annealing algorithm for cluster generation is superior to the tabu search approach with Generalised Lloyd algorithm.  1 Clustering  Clustering is the process of grouping patterns into a number of clusters, each of which contains the patterns that are similar to each other in some way. The existing clustering algorithms can be simply classied into the following two categories: hierarchical clustering and partitional clustering [1]. The hierarchical clustering operates by partitioning the patterns into successively fewer structures. This method gives rise to a d...",
            "group": 831,
            "name": "10.1.1.37.8618",
            "keyword": "",
            "title": "A Clustering Algorithm using the Tabu Search Approach with Simulated Annealing"
        },
        {
            "abstract": "In this article, we consider words over f0; 1g. The autodistance of such a word is the lowest among the Hamming distances between the word and its images by circular permutations other than identity; the word's reverse autodistance is the highest among these distances. For each  l  2, we study the words of length l whose autodistance and reverse autodistance are close to  l=2 (we call such words synchronizing sequences).  We establish, for every l  3, an upper bound on the autodistance of words of length l. This upper bound, called up (l), is very close to l=2.  We briefly describe the maximal period linear recurring sequences, a previously known family of words over f0; 1g; such words exist for every length of the form l = 2  n  \\Gamma 1 and their autodistances achieve the upper bound up (l). Examples of words whose autodistance and reverse autodistance are both equal or close to up (l) are discussed; we describe the method (based on simulated annealing) which was used to find the exa...",
            "group": 832,
            "name": "10.1.1.37.8697",
            "keyword": "",
            "title": "Binary Periodic Synchronizing Sequences"
        },
        {
            "abstract": "The exception handling code of a system is in general the least documented, tested and understood part, since exceptions are expected to occur only rarely. This paper presents a technique for automatically generating test-data to test exceptions. The approach is based on the application of a dynamic global optimisation based search for the required test-data. The authors' work has focused on test-data generation for safety-critical systems. Such systems must be free from anomalous and uncontrolled behaviour. Typically, it is easier to prove the absence of any exceptions than it is to prove that the exception handling is safe. A process for integrating automated testing with exception freeness proofs is presented as a way forward for tackling the special needs of safety critical systems. An evaluation shows the application of the technique to a commercial aircraft engine controller system as part of a proof of exception freeness.  1 Introduction  A failure occurs when software is preven...",
            "group": 833,
            "name": "10.1.1.37.8991",
            "keyword": "",
            "title": "Integrating Automated Testing with Exception Freeness Proofs for Safety Critical Systems"
        },
        {
            "abstract": "This paper presents a new active database discrimination network algorithm called Gator, and its implementation in a modified version of the Ariel active DBMS. Gator is a generalization of the widely known Rete and TREAT algorithms, and is designed as a target for a discrimination network optimizer. Ariel now has an optimizer that can choose an efficient Gator discrimination network for testing the conditions of a set of rules, given information about the rules, database size and attribute cardinality, and update frequency distribution. The optimizer uses a randomized strategy similar to one which has been successfully used previously to optimize large join queries. Use of Gator gives large speedups (3 times for one realistic rule tested -- potentially much more) compared with the unoptimized TREAT strategy formerly used in Ariel. 1 Introduction  A crucial component of an active database system is the mechanism it uses to test rule conditions as the database changes. Tools for artifici...",
            "group": 834,
            "name": "10.1.1.37.9073",
            "keyword": "",
            "title": "Optimized Rule Condition Testing in Ariel using Gator Networks"
        },
        {
            "abstract": "Reducing synchronization constraints in parallel simulated annealing algorithms can improve performance. However, this introduces error in the global cost function. Previous work in parallel simulated annealing suggests that if the amount of error in the cost function is controlled, good quality results can still be obtained. In this paper, we present a model of error in asynchronous parallel simulated annealing algorithms to partition graphs and use it to predict the behavior of three different synchronization strategies. These three approaches were implemented on a 20-processor Encore, a shared memory, MIMD multiprocessor, and tested on a variety of graphs. As predicted, the strategy which allows controlled error yields solutions comparable to those of the serial algorithm with greatly improved running time. Speedups from 5 to 11 (depending on the density of the graphs) using 16 processors were obtained. In contrast, the more synchronized version exhibited unacceptably high running t...",
            "group": 835,
            "name": "10.1.1.37.9460",
            "keyword": "",
            "title": "Cost Function Error in Asynchronous Parallel Simulated Annealing Algorithms"
        },
        {
            "abstract": ". The Very Fast Simulated Reannealing algorithm has been  shown to perform well when minimizing difficult and complex functions.  However, because the VFSR generating distribution is aligned along the  coordinate axes, the performance of the algorithm can be sensitive to  the local orientation and shape of the objective function system. Performance  degradation can occur, for example, when the objective function  to be minimized is rotated or contains local interactions. This problem  can be mitigated by rotating the VFSR generating distribution about the  coordinate axes so that new states are generated about a more favorable  set of axes. In a new algorithm, the parameters used to orient the generating  distribution are stochastically generated using the same VFSR  algorithm that is used to minimize the objective function. The capabilities  of the new algorithm are demonstrated on a rotated set of difficult  to minimize objective functions. The performances of this algorithm are  als...",
            "group": 836,
            "name": "10.1.1.37.9533",
            "keyword": "Simulated AnnealingOptimizationRotationVery Fast Simulated Reannealing",
            "title": "Stochastic Orientation of the Generating Distribution in Very Fast Simulated Reannealing"
        },
        {
            "abstract": " ",
            "group": 837,
            "name": "10.1.1.37.9770",
            "keyword": "AlgorithmsTheory and Experiments",
            "title": "Metropolis, Simulated Annealing and I.E.T. Algorithms: Theory and Experiments"
        },
        {
            "abstract": "We compare the performances of two routing schemes for LEO satellite networks through simulation. The two routing schemes represent static and dynamic routing for the case where the LEO satellite network is modeled as a Finite State Automaton (FSA). Each state in this FSA modeling corresponds to an equal-length interval within the period of the LEO satellite network. Modeling the LEO satellite network in this way allows us to consider the LEO satellite network as if it is a fixed topology network within each state. The routing table for the static routing is fixed within each state whereas that for the dynamic routing is updated continuously according to the shortest-path algorithm. The simulation results show that the static routing performs better in terms of newly initiated call blocking than the dynamic one. The results also show that the static routing gives lower ongoing call blocking probabilities than the dynamic counterpart since the former's pre-computed routing table is less...",
            "group": 838,
            "name": "10.1.1.38.6",
            "keyword": "call blockingLEO satellite networkfinite state automatonlink assignmentroutingsimulation",
            "title": "Performance Comparison of Static Routing and Dynamic Routing in Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "A new robust clustering algorithm, called generalized annealing M-estimator (GAM-estimator), is proposed. Initialized with multiple seeds, the GAM-estimator converges to several optimal cluster centers. Neither knowledge about the number of clusters nor scale is needed. The global optimal solution of clustering is achieved by minimization of an objective function. The algorithm is applied to unsupervised texture segmentation and texture-based defect detection .  1. INTRODUCTION  Approaching computer vision problem as feature space clustering has numerous advantages [1]. The feature space can integrate multiple sources of information about the same image region, allowing a more complete analysis [1]. A robust procedure aims to produce a solution insensitive to departures from the underlying assumption. The solution should have good performance under the underlying assumption and the performance should deteriorate gracefully as the situation departs from the assumption [2]. Applications ...",
            "group": 839,
            "name": "10.1.1.38.443",
            "keyword": "",
            "title": "Robust Unsupervised Clustering Using Generalized Annealing M-Estimator"
        },
        {
            "abstract": "These notes provide an introduction to Markov chain Monte Carlo methods that are useful in both Bayesian and frequent...",
            "group": 840,
            "name": "10.1.1.38.515",
            "keyword": "Auto{logistic distributionAuxiliary variablesBayesian computationCompeting risksContingency tablesExact p{valuesGibbs samplerHastings algorithmHidden Markov modelsImportance samplingIsing modelLangevin diusionMarkov chain Monte CarloMarkov random eldsMaximum likelihood estimationMetropolis methodMixture modelsNoisy binary channelPerfect simulationPoint processesRandom graphsRasch modelReversibilityReversible jumpsSimulated annealingSocial networksSpatial statisticsSwendsen{Wang algorithmWeibull distribution 1 The",
            "title": "Markov Chain Monte Carlo for Statistical Inference"
        },
        {
            "abstract": "Neural networks can be regarded as statistical models, and can be analysed in a Bayesian framework. Generalisation is measured by the performance on independent test data drawn from the same distribution as the training data. Such performance can be quantified by the posterior average of the information divergence between the true and the model distributions. Averaging over the Bayesian posterior guarantees internal coherence; Using information divergence guarantees invariance with respect to representation. The theory generalises the least mean squares theory for linear Gaussian models to general problems of statistical estimation. The main results are: (1) the ideal optimal estimate is always given by average over the posterior; (2) the optimal estimate within a computational model is given by the projection of the ideal estimate to the model. This incidentally shows some currently popular methods dealing with hyperpriors are in general unnecessary and misleading. The extension of in...",
            "group": 841,
            "name": "10.1.1.38.881",
            "keyword": "",
            "title": "Information Geometric Measurements of Generalisation"
        },
        {
            "abstract": "Much research has been devoted to path planning during the past decade, i.e. the geometrical problem of finding a collision-free path between two given postures (configurations) of an articulated body (robot) among obstacles. This problem has straightforward applications in robotic automation, computer aided design, and computer graphics animation. Current global techniques compute explicitly the non-colliding zones in configuration space. Thus, they require exponential space and time in the number of Degrees of Freedom (DOF) of the body. These methods are therefore untractable for more than 4 DOF. This report presents a new approach to path planning which does not require construction of an explicit description of the configuration space. The method consists of building and searching a graph connecting the local minima of a potential function defined over the configuration space. The graph is explored by means of a randomization technique that escapes the local minima by executing Bro...",
            "group": 842,
            "name": "10.1.1.38.1429",
            "keyword": "",
            "title": "Automatic Motion Planning for Complex Articulated Bodies"
        },
        {
            "abstract": "In this paper, we present an efficient iterative improvement based partitioning algorithm, called FM-LSRb, that combines a signal flow based hierarchical clustering algorithm using Maximum Fanout Free Subgraph (MFFS) technique with a loose and stable net removal partitioning approach (LSR). The MFFS clustering approach generalized existing MFFC decomposition method from combinational circuits to general sequential circuits, which enable us to handle cycles in circuits naturally, and make hierarchical clustering and declustering possible. We also studied the properties of the nets that straddle the cutline carefully, and introduced the concepts of the loose nets and stable nets. Our FM-LSRb algorithm begins with the hierarchical MFFS clustering based on signal direction, logical dependency and connectivity. Then, it focuses on removing loose and stable nets from the cutset of the clustered netlist. A hierarchical declustering process was embedded in the partitioning algorithm, which was...",
            "group": 843,
            "name": "10.1.1.38.1644",
            "keyword": "1",
            "title": "Large Scale Circuit Partitioning With Loose/Stable Net Removal And Signal Flow Based Hierarchical Clustering"
        },
        {
            "abstract": "In the work of communications network design there are several recurring themes: maximizing flows, finding circuits, and finding shortest paths or minimal cost spanning trees, among others. Some of these problems appear to be harder than others. For some, effective algorithms exist for solving them, for others, tight bounds are known, and for still others, researchers have few clues towards a good approach. One of these latter, nastier problems arises in the design of communications networks: the Optimal Communication Spanning Tree Problem (OCSTP). First posed by Hu in 1974, this problem has been shown to be in the family of NP-complete problems. So far, a good, general-purpose approximation algorithm for it has proven elusive.  This thesis describes the design of a genetic algorithm for finding reliably good solutions to the OCSTP. The genetic algorithm approach was thought to be an appropriate choice since they are computationally simple, provide a powerful parallel search capability...",
            "group": 844,
            "name": "10.1.1.38.1716",
            "keyword": "",
            "title": "An Approach To A Problem In Network Design Using Genetic Algorithms"
        },
        {
            "abstract": "This work has attempted to exploit information sharing to improve the results of Adaptive Simulated Annealing [1] as an optimization algorithm of the high-level synthesis of testable data paths. We have used Messengers [3] as a coordination tool to run several parallel instances of the annealing algorithm on the same design with different probability arrays for the perturbations. When all these instances complete annealing, they exchange information about the best design among them which is given by a cost function [2] based on area, speed and testability costs of the digital design. This best design is then used as a starting point and several instances of annealing are run again in an attempt to further improve the design.   ",
            "group": 845,
            "name": "10.1.1.38.2781",
            "keyword": "",
            "title": "Distributed Adaptive Simulated Annealing for Synthesis Design Space Exploration"
        },
        {
            "abstract": "The problem of optimally approximating a function with a linear expansion over a redundant dictionary of waveforms is NP-hard. The greedy matching pursuit algorithm and its orthogonalized variant produce sub-optimal function expansions by iteratively choosing the dictionary waveforms which best match the function's structures. Matching pursuits provide a means of quickly computing compact, adaptive function approximations.  Numerical experiments show that the approximation errors from matching pursuits initially decrease rapidly, but the asymptotic decay rate of the errors is slow. We explain this behavior by showing that matching pursuits are chaotic, ergodic maps. The statistical properties of the approximation errors of a pursuit can be obtained from the invariant measure of the pursuit. We characterize these measures using group symmetries of dictionaries and using a stochastic differential equation model. These invariant measures define a noise with respect to a given dictionary. ...",
            "group": 846,
            "name": "10.1.1.38.2932",
            "keyword": "",
            "title": "Acknowledgements"
        },
        {
            "abstract": "With several commercial tools becoming available, the high-level synthesis of applicationspecific integrated circuits is finding wide spread acceptance in VLSI industry today. Existing tools for synthesis focus on optimizing cost while meeting performance constraints or vice versa. Yet, verification and testing have emerged as major concerns of IC vendors since the repurcussions of chips being recalled are far-reaching. In this paper, we concentrate on the synthesis of testable RTL designs using techniques from Artificial Intelligence. We present an adaptive version of the well known Simulated Annealing algorithm and describe its application to a combinatorial optimization problem arising in the high-level synthesis of digital systems. The conventional annealing algorithm was conceived with a single perturb operator which applies a small modification to the existing solution to derive a new solution. The Metropolis criterion is then used to accept or reject the new solution. ...",
            "group": 847,
            "name": "10.1.1.38.3040",
            "keyword": "Key Words Test SynthesisAdaptive AlgorithmsSimulated AnnealingRTL Synthesis",
            "title": "Synthesis of Testable RTL Designs using Adaptive Simulated Annealing Algorithm"
        },
        {
            "abstract": ". Simulated annealing is a stochastic algorithm for generating nearly-optimal solutions to discrete optimization problems. To reduce execution time, researchers have parallelized it. A specific parallel technique, which involves a noisy cost function, has shown great promise: by reducing communication we increase speed but introduce errors. Errors also appear in common sequential cost functions: they use approximation to reduce computation costs. Unfortunately, the errors degrade the outcome if the number of annealing steps remains fixed. Prior theoretical results have not adequately related these errors to final annealing quality or run time. This paper proves several results about annealing with an inaccurate cost function: 1) Equilibrium is exponentially affected by fl=T , where fl limits the error range and T is temperature. 2) Equilibrium in Gaussian spaces is exponentially affected by fi 1=T and  fi 2=T , where fi 1 and fi 2 are functions of the variance range. 3) Constraining er...",
            "group": 848,
            "name": "10.1.1.38.4104",
            "keyword": "General TermsAlgorithmsperformance. Additional Key Words and PhrasesParallel simulated annealingGaussian noisecombinatorial optimizationstatistical mechanicsBoltzmann machineconductancemixing speed",
            "title": "Simulated Annealing with Inaccurate Cost Functions"
        },
        {
            "abstract": " ",
            "group": 849,
            "name": "10.1.1.38.4264",
            "keyword": "",
            "title": "Blind Image Deconvolution: An Algorithmic Approach to Practical Image Restoration"
        },
        {
            "abstract": "This paper argues for a computational model of creativity as the unique and favorable combining of past experiences. It is hypothesized that the inner process that combines the experiences is universal and largely syntactic, i.e. it is only the emotional associations that are attached to encoded experience and not the semantic content of experience that is used by the inner creative process. A chess system, \"Morph\", has been developed that can be viewed as exploring this model. Morph is limited to 1-ply of search, little domain knowledge and no supervised training. It is only through playing another program, and encoding its experience (as \"pattern-weight\" pairs) that it can improve. Morph's learning system is a combination of machine learning methods that have been successful in other settings - and is covered in depth. Part of Morph's strength comes from a powerful representation for chess patterns that allows generalizations across positions (experiences). Performance results, inclu...",
            "group": 850,
            "name": "10.1.1.38.4335",
            "keyword": "",
            "title": "Experience-Based Creativity"
        },
        {
            "abstract": "Motivation:  Assembling shotgun sequencing data from repetitive DNA sequences is a non-trivial  task. In existing sequence assembly methods repeats are resolved by either using  statistical analyses to identify and separate fragments corresponding to repeats, or  by using extra information, not contained in the fragments. In this paper we take a  different approach. Using the simulated-tempering Monte Carlo method, we resolve  repeats by performing an extensive search of the solution space.  Results:  The method is tested on two highly repetitive sequences with a two-copy and a threecopy  repeat, respectively. We find that the method is able to correctly assemble these  two sequences, except for a twofold degeneracy for the three-copy repeat sequence.  The alternative solution obtained in this case is related by a simple symmetry to the  correct one. The performance of the method is compared with that of simulated annealing.  We find that simulated tempering is a competitive alternativ...",
            "group": 851,
            "name": "10.1.1.38.4416",
            "keyword": "Contact",
            "title": "A Monte Carlo Approach to Sequence Assembly"
        },
        {
            "abstract": ": Many tasks in computer vision and image analysis have recently been expressed as the minimization of global energy functions describing the local interactions between the observed data and the images features to be extracted. For the minimization of these global (often non-linear) energy functions, a variety of stochastic or deterministic relaxation algorithms have been described in the literature. Besides, multigrid techniques have significantly improved the convergence rate of iterative relaxation schemes. However, the major drawback of (iterative) relaxation algorithms remains the amount of computation required to update the image. For real world applications the computation time quickly becomes prohibitive on workstations. On the other hand, the computations involved by these algorithms are regular and local, and lead naturally to massive data parallelism, which is well suited for parallel processing on array processor architectures.  Standard parallel implementations of relaxati...",
            "group": 852,
            "name": "10.1.1.38.5205",
            "keyword": "",
            "title": "Efficient Parallel Non-Linear Multigrid Relaxation Algorithms for Low-Level Vision Applications"
        },
        {
            "abstract": " ",
            "group": 853,
            "name": "10.1.1.38.5382",
            "keyword": "",
            "title": "Solving Scheduling Problems by Simulated Annealing"
        },
        {
            "abstract": "Simulation of logic designs is a very important part of the VLSI-design  process. The increasing size of the designs requires more efficient simulation  strategies to accelerate the simulation process. Parallel logic simulation seems  to be a promising approach in this direction. This paper describes the basic  principles of parallel logic simulation, discusses different approaches, and  surveys the research done in this field so far.  ",
            "group": 854,
            "name": "10.1.1.38.6268",
            "keyword": "",
            "title": "A Survey on Parallel Logic Simulation"
        },
        {
            "abstract": "Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test-data to achieve 100% coverage of a given structural coverage metric is labour intensive and expensive. This paper presents an approach to automate the generation of such test-data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test-data. The same approach can be generalised to solve other test-data generation problems. Three such applications are discussed -- boundary value analysis, assertion/run-time exception testing and component re-use testing. A prototype tool-set has been developed to facilitate the automatic generation of test-data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the efficiency and effectiveness of this approach.  1 Introduction  Software testing is an expensive and time-consu...",
            "group": 855,
            "name": "10.1.1.38.6416",
            "keyword": "",
            "title": "An Automated Framework for Structural Test-Data Generation"
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks -- specifically,  the task considered is that of training a network to classify natural language sentences as grammatical  or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the  Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks  are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt  to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a  recurrent neural network could possess linguistic capability, and the properties of various common recurrent  neural network architectures are discussed. The problem exhibits training behavior which is often  not present with smaller grammars, and training was initially difficult. However, after implementing several  techniques aimed at improving the convergence of the gradient descent backprop...",
            "group": 856,
            "name": "10.1.1.38.6501",
            "keyword": "frameworkautomata extraction",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "The authors have developed a lossy coding scheme using a 3D discrete cosine transform, for use with integral 3D image data. Correct selection of the step sizes of the uniform symmetric scalar quantisers used in the scheme is crucial to attaining good rate-distortion performance. Imageorientated optimisations have been performed to obtain the quantisation parameters leading to minimum objective distortion of the reconstructed image at given bit rates. Simulated annealing, an optimisation method suitable for complex non-linear problems whose objective functions typically possess many local extrema, is used for this purpose. A model for optimal quantisation is then constructed for general use across an arbitrary bit rate range. This paper describes the integral 3D imaging system which generates the image data used, the coding scheme and the simulated annealing technique. The results of simulations using optimal model-derived quantisation arrays are presented and compared with previous use...",
            "group": 857,
            "name": "10.1.1.38.6597",
            "keyword": "",
            "title": "Simulated Annealing for Optimisation and Characterisation of Quantisation Parameters in Integral 3D Image Compression"
        },
        {
            "abstract": "Learning methods based on dynamic programming (DP) are receiving increasing attention  in artificial intelligence. Researchers have argued that DP provides the appropriate  basis for compiling planning results into reactive strategies for real-time control,  as well as for learning such strategies when the system being controlled is incompletely  known. We introduce an algorithm based on DP, which we call Real-Time DP  (RTDP), by which an embedded system can improve its performance with experience.  RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty.  We invoke results from the theory of asynchronous DP to prove that RTDP  achieves optimal behavior in several different classes of problems. We also use the theory  of asynchronous DP to illuminate aspects of other DP-based reinforcement learning  methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to  provide a bridge between AI research on real-time planning and learni...",
            "group": 858,
            "name": "10.1.1.38.7343",
            "keyword": "",
            "title": "Learning to Act using Real-Time Dynamic Programming"
        },
        {
            "abstract": "A mobile user location management mechanism is introduced that incorporates a distance based location update scheme and a paging mechanism that satisfies predefined delay requirements. An analytical model is developed which captures the mobility and call arrival pattern of a terminal. Given the respective costs for location update and terminal paging, the average total location update and terminal paging cost is determined. An iterative algorithm is then used to determine the optimal location update threshold distance that results in the minimum cost. Analytical results are also obtained to demonstrate the relative cost incurred by the proposed mechanism under various delay requirements.  1 Introduction  Personal communication networks (PCNs) consist of a fixed wireline network and a large number of mobile terminals. These terminals include telephones, portable computers, and other devices that exchange information with remote terminals through the fixed network. The wireline network c...",
            "group": 859,
            "name": "10.1.1.38.7526",
            "keyword": "",
            "title": "A Mobile User Location Update and Paging Mechanism Under Delay Constraints"
        },
        {
            "abstract": "Planning by incomplete stochastic search offers a promising alternative to classical, complete planning methods. The success of this approach is documented by recent performance results obtained by transforming planning tasks into propositional satisfiability problems and using existing efficient local search methods to solve them. On the other hand, we argue that these results can still be improved if local search is employed directly to planning problems, i.e., without making the detour via propositional logic. To this end we introduce a general-purpose planning structure which is amenable to stochastic search methods. The core of this structure is formed by constraints, which represent a planning problem and at the same time determine the objective function to be minimized by local search. Adapting the planning structure on the basis of a pre-processing by domain analysis may furthermore significantly reduce the number of constraints and so speed up the following search.  Introducti...",
            "group": 860,
            "name": "10.1.1.38.7930",
            "keyword": "",
            "title": "Constraint Based Planning by Stochastic Search"
        },
        {
            "abstract": "Product units provide a method of automatically learning the higher-order input combinations required for the efficient synthesis of Boolean logic functions by neural networks. Product units also have a higher information capacity than sigmoidal networks. However, this activation function has not received much attention in the literature. A possible reason for this is that one encounters some problems when using standard backpropagation to train networks containing these units. This report examines these problems, and evaluates the performance of three training algorithms on networks of this type. Empirical results indicate that the error surface of networks containing product units have more local minima than corresponding networks with summation units. For this reason, a combination of local and global training algorithms were found to provide the most reliable convergence. We then investigate how `hints' can be added to the training algorithm. By extracting a common frequency from t...",
            "group": 861,
            "name": "10.1.1.38.7967",
            "keyword": "",
            "title": "Product Unit Learning"
        },
        {
            "abstract": "previously seemed, since they can be successfully applied to only a limited number of problems exhibiting special, amenable properties. Combinatorial optimization, neural networks, mean eld annealing. i optimization networks Key words: Summary  I am greatly indebted to my supervisor, Richard Prager, for initially allowing me the freedom to explore various research areas, and subsequently providing invaluable support as my work progressed. Members of the Speech, Vision and Robotics Group at the Cambridge University Department of Engineering have provided a stimulating and friendly environment to work in: special thanks must go to Patrick Gosling and Tony Robinson for maintaining a superb computing service, and to Sree Aiyer for both setting me on the right course and for numerous helpful discussions since then. I would like to thank the Science and Engineering Research Council of Great Britain, the Cambridge University Department of Engineering and Queen",
            "group": 862,
            "name": "10.1.1.38.8720",
            "keyword": "",
            "title": "Problem Solving with Optimization Networks"
        },
        {
            "abstract": "This paper presents a heuristic for maximizing the throughput of a task graph that is pre-partitioned for a multi-FPGA reconfigurable architecture. The nodes in the graph represent behavioral task segments and the edges denote data dependencies. Given, for each task, is a set of implementation options corresponding to the various area-time trade-off points in its design space. We present a fast and efficient heuristic that selects an implementation for each task such that a near-optimal execution time for the graph can be achieved, while satisfying the resource constraints imposed by the FPGAs. Also part of our approach is a area estimation heuristic that accounts for efficient sharing of resources between tasks that are mutually time exclusive. We compare our approach with a genetic algorithm, a general-purpose combinational search technique, that solves the same problem. We present results that support our heuristic.   This work was supported by the ARPA RASSP program and monitored b...",
            "group": 863,
            "name": "10.1.1.38.9014",
            "keyword": "",
            "title": "Throughput Optimization with Design Space Exploration during Partitioning for Multi-FPGA Architectures"
        },
        {
            "abstract": "Local search is a traditional technique to solve combinatorial search problems which has raised much interest in recent years. The design and implementation of local search algorithms is not an easy task in general and may require considerable experimentation and programming effort. However, contrary to global search, little support is available to assist the design and implementation of local search algorithms. This paper is an attempt to support the implementation of local search. It presents the preliminary design of Localizer, a modeling language which makes it possible to express local search algorithms in a notation close to their informal descriptions in scientific papers. Experimental results on our first implementation show the feasibility of the approach.",
            "group": 864,
            "name": "10.1.1.39.141",
            "keyword": "",
            "title": "Localizer: A Modeling Language for Local Search"
        },
        {
            "abstract": "Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods. To Appear in:  IEEE Transactions on Neural Networks  The Ohio State University July 16, 1993 1  An Evolutio...",
            "group": 865,
            "name": "10.1.1.39.234",
            "keyword": "",
            "title": "An Evolutionary Algorithm that Constructs Recurrent Neural Networks"
        },
        {
            "abstract": "The subject of this thesis is the A-Teams formalism. This formalism facilitates the organization of multiple algorithms, encapsulated as autonomous agents, into cooperating teams to solve difficult problems. The ATeams formalism is one of many agent-based systems, and I start by providing a taxonomy of agent-based systems that allows us to see they how A-Teams relate to other agent-based systems. A-Teams are constructed from memories that store solutions and agents that work on those solutions. ATeams are open to the addition of new memories as well as of new agents. Sets of memories and agents can also be combined in different ways to create a variety of customized A-Teams. As new memories and agents are created, they can be added to existing repositories and reused for future applications. The automatic construction of problem-specific custom A-Teams from repositories of components has been a long standing goal of research in A-Teams. Current guidelines for A-Team construction requir...",
            "group": 866,
            "name": "10.1.1.39.464",
            "keyword": "",
            "title": "Explorations in Asynchronous Teams"
        },
        {
            "abstract": "This paper presents a new segmentation algorithm by fitting active contour models (or snakes) to objects using adaptive splines. The adaptive spline model describes the contour of an object by a set of piecewisely interpolating C^2 polynomial spline patches which are locally controlled. Thus the resulting description of the object contour is continuous and smooth. Polynomial splines provide a fast and efficient way for interpolating the object contour and allow us to compute its internal energy due to bending and elasticity deformations analytically. The adaptive spline model can be represented by its spline control points. The accuracy of the model is gradually increased during the segmentation process by inserting new control points. For estimating the optimal position of the control points, two different relaxation techniques based on Markov Random Fields (MRFs) have been combined and evaluated: Simulated Annealing (SA), which is a stochastic relaxation technique, and Iterated Conditional Modes (ICM), which is a probabilistic relaxation technique. We have studied convergence behaviour and performance on artificial and medical images. The results show that the combination of both relaxation techniques provides very robust and initialization independent segmentation results.",
            "group": 867,
            "name": "10.1.1.39.632",
            "keyword": "",
            "title": "Contour Fitting Using an Adaptive Spline Model"
        },
        {
            "abstract": "We propose a two-stage simulated annealing method. While most previous work has focused on ad hoc experimentally-derived constant starting temperatures for the low temperature annealing phase, this paper instead presents a more formal method for generalized starting temperature determination in two-stage simulated annealing systems. We have tested our method on three NP-hard optimization problems using both classic and logarithmic cooling schedules. The experimental results have been consistently very good---on average the running time is halved when using a logarithmic cooling schedule and reduced by a third in the case of the classic schedule---with no loss in solution quality. We also present results for an alternative stop criterion used with the classic schedule that further reduces the two-stage running time by an additional five to ten percent in our problem suite.   ",
            "group": 868,
            "name": "10.1.1.39.1541",
            "keyword": "combinatorial optimizationoperations researchsimulated annealingtwo-stage simulated annealingVLSI design automation",
            "title": "A Two-Stage Simulated Annealing Methodology"
        },
        {
            "abstract": "Introduction  Lecturer: Gabriel Istrate Scribe: Gabriel Istrate 1 Markov Chains: Classical Theory  S state space. Distribution on S:   = ( i ) i2S . Its support: fi :  i ? 0g.   i  0  P  i2S  i = 1:  Markov Chain (mc) on S:  a sequence (X n ) n0 of r.v. on S. for every (i; j) 2 S \\Theta S a transition probability p i;j memorylessness: 8n  0: P r[X n+1 = jjX n = i] = p i;j . initial distributio",
            "group": 869,
            "name": "10.1.1.39.1627",
            "keyword": "",
            "title": "Markov Chain techniques in Theoretical Computer Science"
        },
        {
            "abstract": "We present a new multiscale contour #tting process which combines information about the image and the contour of the object at di#erent levels of scale. The algorithm is based on energy minimizing deformable models but avoids some of the problems associated with these models. The segmentation algorithm starts by constructing a linear scale-space of an image through convolution of the original image with a Gaussian kernel at di#erent levels of scale, where the scale corresponds to the standard deviation of the Gaussian kernel. At high levels of scale large scale features of the objects are preserved while small scale features, like object details as well as noise, are suppressed. In order to maximize the accuracy of the segmentation, the contour of the object of interest is then tracked in scale-space from coarse to #ne scales. We propose a hybrid Multi-Temperature Simulated Annealing optimization to minimize the energy of the deformable model. At high levels of scale the SA optimizatio...",
            "group": 870,
            "name": "10.1.1.39.1891",
            "keyword": "Image SegmentationContour FittingMultiscale Image AnalysisDeformable ModelsMultiresolution Deformable ModelsSimulated Annealing (SAIterated Conditional Modes (ICM",
            "title": "A Multiscale Approach to Contour Fitting for MR Images"
        },
        {
            "abstract": "Many experimental results are reported on all types of Evolutionary Algorithms but only few results have been proved. A step towards a theory on Evolutionary Algorithms, in particular, the so-called (1 + 1) Evolutionary Algorithm, is performed. Linear functions are proved to be optimized in expected time O(n ln n) but only mutation rates of size (1/n) can ensure this behavior. For some polynomial of degree 2 the optimization needs exponential time. The same is proved for a unimodal function. Both results were not expected by several other authors. Finally, a hierarchy result is proved. Moreover, methods are presented to analyze the behavior of the (1 + 1) Evolutionary Algorithm.  ",
            "group": 871,
            "name": "10.1.1.39.2993",
            "keyword": "",
            "title": "On the Analysis of the (1+1) Evolutionary Algorithm"
        },
        {
            "abstract": "Digital signal processing algorithms with multiple shift-invariant dependence graphs (DGs) can be mapped to FPGA hardware in many different types of systolic processor arrays. Because of the finite amount of hardware resources, the problem is to use a \"right\" amount of hardware in a specific configuration so to maximize the processing speed. In this paper, the problem of finding the right processor array configuration is formulated as a constrained optimization problem where the cost function includes not only the cost of individual processor arrays but also the cost of interfacing circuits. Three heuristic algorithms are presented for the optimization problem. Among them, both the L-th axial neighbor algorithm and the simulated annealing algorithm produce good results on a test case. Simulation results on the test case also indicate that the initial configuration is important in getting a good configuration for both algorithms. The L-th axial neighbor algorithm has the extra advantage...",
            "group": 872,
            "name": "10.1.1.39.3587",
            "keyword": "Dependence GraphFPGAProcessor ArraySystolic Array",
            "title": "Processor Array Design with FPGA Area Constraint"
        },
        {
            "abstract": ". Word sense disambiguation continues to be a di#cult problem in machine  translation #MT#. Current methods either demand large amounts of corpus data and  training or rely on knowledge of hard selectional constraints. In either case, the methods  have been demonstrated only on a small scale and mostly in isolation, where disambiguation  is a task by itself. It is not clear that the methods can be scaled up and integrated  with other components of analysis and generation that constitute an end-to-end MT  system. In this paper, we illustrate how the Mikrokosmos Knowledge-Based MT system  disambiguates word senses in real-world texts with a very high degree of correctness.  Disambiguation in Mikrokosmos is achieved by a combination of #i# a broad-coverage ontology  with many selectional constraints per concept, #ii# a large computational-semantic  lexicon grounded in the ontology, #iii# an optimized search algorithm for checking selectional  constraints in the ontology, and #iv# an e#cie...",
            "group": 873,
            "name": "10.1.1.39.3688",
            "keyword": "",
            "title": "Word Sense Disambiguation: Why Statistics When We Have These Numbers?"
        },
        {
            "abstract": "Spatial layout is the problem of arranging a set of components in an enclosure such that a set of objectives and constraints is satisfied. The constraints may include non-interference of objects, accessibility requirements and connection cost limits. Spatial layout problems are found primarily in the domains of electrical engineering and mechanical engineering in the design of integrated circuits and mechanical or electromechanical artifacts. Traditional approaches include ad-hoc (or specialized) heuristics, Genetic Algorithms and Simulated Annealing. The A-Teams approach provides a way of synergistically combining these approaches in a modular agent based fashion. A-Teams are also open to the addition of new agents. Modifications in the task requirements translate to modifications in the agent mix. In this paper we describe how modular A-Team based optimization can be used to solve 3 dimensional spatial layout problems.  ",
            "group": 874,
            "name": "10.1.1.39.3936",
            "keyword": "",
            "title": "3D Spatial Layouts Using A-Teams"
        },
        {
            "abstract": "Current and future Internet services will provide a large, rapidly evolving, highly accessed, yet autonomously managed information space. Internet news, perhaps, is the closest existing precursor to such services. It permits autonomous updates, is replicated at thousands of autonomously managed sites, and manages a large database. It gets its performance through massive replication. This paper proposes a scalable mechanism for replicating wide-area, autonomously managed services. We target replication degrees of tens of thousands of weaklyconsistent replicas. For efficiency, our mechanismprobes the network and computes a good logical topology over which to send updates. For scalability, we organize replicas into hierarchical replication groups, analogous to the Internet's autonomous routing domains. We argue that efficient, massive replication does not have to rely on internet multicast. 1 Introduction  Future Internet services will manipulate large, rapidly evolving, highly accessed, ...",
            "group": 875,
            "name": "10.1.1.39.4031",
            "keyword": "",
            "title": "Massively Replicating Services in Wide-Area Internetworks"
        },
        {
            "abstract": "This paper deals with synchronization problems arising in the context  of cellular networks. It presents and compares several algorithms that can  be used for solving these problems.  1 Introduction  The Base Transceiver Stations (BTS) are the components of a cellular network that ensure the connection between the Mobile Stations (MS) and the communication network. Every BTS has an internal clock, but all these clocks are not synchronized. In other words, the BTSs do not have access to an absolute reference of time.  We are interested in proposing distributed algorithmic solutions for synchronizing these clocks. The main diculty of our problem comes from the fact that we do not give ourselves any possibility of measuring the dephasing (i.e. the dierence of phase) between a given clock and some time reference. The only informations come from random measurements of local dephasings between two clocks : one can indeed measure such dephasings (with noise) when a Mobile Station makes a han...",
            "group": 876,
            "name": "10.1.1.39.4115",
            "keyword": "",
            "title": "Some Algorithms for Synchronizing Clocks of Base Transceiver Stations in a Cellular Network"
        },
        {
            "abstract": "We study the problem of identification for nonlinear systems in the presence of unknown driving noise, using both feedforward multilayer neural network and radial basis function network models. Our objective is to resolve the difficulty associated with the Persistency of Excitation condition inherent to the standard schemes in the neural identification literature. This difficulty is circumvented here by a novel formulation and by using a new class of identification algorithms recently obtained in [1]. We show how these algorithms can be exploited to successfully identify the nonlinearity in the system using neural network models. By embedding the original problem in one with noise-perturbed state measurements, we present a class of identifiers (under L1 and L2 cost criteria) which secure a good approximant for the system nonlinearity provided that some global optimization technique is used. In this respect, many available learning algorithms in the current neural network literature, e....",
            "group": 877,
            "name": "10.1.1.39.4155",
            "keyword": "Nonlinear systemsH-infinity optimizationFeedforward neural networksBackpropagationRadial basis function networksPersistency of excitation",
            "title": "Robust Nonlinear System Identification Using Neural Network Models"
        },
        {
            "abstract": ". We present a new approach to shape-based segmentation and  tracking of multiple, deformable anatomical structures in cardiac MR images.  We propose to use an energy-minimizing geometrically deformable  template #GDT# which can deform into similar shapes under the in#uence  of image forces. The degree of deformation of the template from  its equilibrium shape is measured by a penalty function associated with  mapping between the two shapes. In 2D, this term corresponds to the  bending energy of an idealized thin-plate of metal. By minimizing this  term along with the image energy terms of the classic deformable model,  the deformable template is attracted towards objects in the image whose  shape is similar to its equilibrium shape. This framework allows for the  simultaneous segmentation of multiple deformable objects using intra- as  well as inter-shape information. The energy minimization problem of the  deformable template is formulated in a Bayesian framework and solved  using re...",
            "group": 878,
            "name": "10.1.1.39.4250",
            "keyword": "",
            "title": "Geometrically Deformable Templates for Shape-based Segmentation and Tracking in Cardiac MR Images"
        },
        {
            "abstract": "Subsurface rock properties are manifested in seismic  records as variations in traveltimes, amplitudes, and  waveforms. It is commonly acknowledged that traveltimes  are sensitive to the long wavelength part of the  velocity, whereas amplitudes are sensitive to the short  wavelength part of the velocity. The inherent sensitivity  of seismic velocity at different wavelengths suggests an  approach that decomposes the waveform data into traveltime  and amplitude components. Therefore we propose  a divide-and-conquer approach to the elastic waveform  inversion problem. We first estimate the smoothly  varying background velocity from the traveltime and the  rapidly changing perturbations from the amplitude by  amplitude variation with offset (AVO) inversion based  on linearized reflection coefficient. Thenwe combine the  perturbation with the background to obtain a starting  model to be used in the final waveform inversion that  models all converted waves and internal multiples assuming  a ...",
            "group": 879,
            "name": "10.1.1.39.4957",
            "keyword": "",
            "title": "1-D elastic waveform inversion: A divide-and-conquer approach"
        },
        {
            "abstract": " ",
            "group": 880,
            "name": "10.1.1.39.4989",
            "keyword": "",
            "title": "Optimal Motion Generation for Hydraulic Robots"
        },
        {
            "abstract": "Back-propagation learning (Rumelhart, Hinton and Williams, 1986) is a useful research tool but it has a number of undesiderable features such as having the experimenter decide from outside what should be learned. We describe a number of simulations of neural networks that internally generate their own teaching input. The networks generate the teaching input by trasforming the network input through connection weights that are evolved using a form of genetic algorithm. What results is an innate (evolved) capacity not to behave efficiently in an environment but to learn to behave efficiently. The analysis of what these networks evolve to learn shows some interesting results. Introduction  In many supervised learning models of neural networks a vector of activity values is provided as an external teaching input to the network. Each activity value in the vector is compared with the computed value of the corresponding output unit and the resulting error is used to modify the connection weigh...",
            "group": 881,
            "name": "10.1.1.39.5252",
            "keyword": "",
            "title": "Auto-Teaching: Networks That Develop Their Own Teaching Input"
        },
        {
            "abstract": "We present a new approach to shape-based segmentation and tracking of multiple, deformable anatomical structures in cardiac MR images. We propose to use an energy-minimizing geometrically deformable template (GDT) which can deform into similar shapes under the influence of image forces. The degree of deformation of the template from its equilibrium shape is measured by a penalty function associated with mapping between the two shapes. In 2D, this term corresponds to the bending energy of an idealized thin-plate of metal. By minimizing this term along with the image energy terms of the classic deformable model, the deformable template is attracted towards objects in the image whose shape is similar to its equilibrium shape. This framework allows the simultaneous segmentation of multiple deformable objects using intra- as well as inter-shape information. The energy minimization problem of the deformable template is formulated in a Bayesian framework and solved using relaxa...",
            "group": 882,
            "name": "10.1.1.39.5981",
            "keyword": "",
            "title": "Shape-based Segmentation and Tracking in 4D Cardiac MR Images"
        },
        {
            "abstract": "Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This paper introduces diffracting trees, novel data structures for shared counting and load balancing in a distributed parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting net...",
            "group": 883,
            "name": "10.1.1.39.7401",
            "keyword": "",
            "title": "Diffracting Trees"
        },
        {
            "abstract": "It has been widely recognised in the computational intelligence and machine learning communities that the key to understanding the behaviour of learning algorithms is to understand what representation is employed to capture and manipulate knowledge acquired during the learning process. However, traditional evolutionary algorithms have tended to employ a fixed representation space (binary strings), in order to allow the use of standardised genetic operators. This approach leads to complications for many problem domains, as it forces a somewhat artificial mapping between the problem variables and the canonical binary representation, especially when there are dependencies between problem variables (e.g. problems naturally defined over permutations). This often obscures the relationship between genetic structure and problem features, making it difficult to understand the actions of the standard genetic operators with reference to problem-specific structures. This thesis instead advocates m...",
            "group": 884,
            "name": "10.1.1.39.7961",
            "keyword": "",
            "title": "A Prescriptive Formalism for Constructing Domain-specific Evolutionary Algorithms"
        },
        {
            "abstract": "We propose a new global routing area estimation approach for high-performance VLSI and MCMs. The objective is to route nets with minimum density of global cells, producing a two-bend routing for each two-terminal net. A solution to this problem can also be used for quick estimation of necessary wiring space (for standard cell array designs) and difficulty of routing (for gate array designs) in the early design planning stage. 1  This work was supported in part by the academic research fund of the Ministry of Education, republic of Korea, through the Inter-University Research Center (ISRC 97-E-2031) at Seoul National Univesity  1 Introduction  As physical feature sizes decreases, the time delay of electrical signals traveling in the interconnect between active devices and gates is approaching the delay through the devices and gates. Thus, physical interconnections delay will overtake gate delays as a design concern by the tear 2000, mandating a shift in the physical design flow for dee...",
            "group": 885,
            "name": "10.1.1.39.8874",
            "keyword": "",
            "title": "Wiring Space Estimation in Two Dimensional Arrays"
        },
        {
            "abstract": "Markov/Gibbs random fields have been used for posing a variety of computer vision and image processing problems. Many of these problems are then solved using a simulated annealing type of method which involves the varying of the \"temperature,\" a scale parameter for the model. In this paper we analyze the effect of temperature on random field texture patterns. We obtain new results relating structure in the texture cooccurrence matrix to temperature. We also show the existence of multiple \"transition temperatures \" which delimit regions of different bandwidth in the co-occurrence matrix, and hence can be used to control pattern formation.  ",
            "group": 886,
            "name": "10.1.1.39.9671",
            "keyword": "",
            "title": "Markov/Gibbs modeling: Texture and Temperature"
        },
        {
            "abstract": "The Sequence Pair (SP), which was introduced by Murata [1], appears to be an efficient and powerful structure for general module placement representation. Unfortunately, most of the known algorithms that apply to the sequence pair still have ###  #  # complexity or worse, which renders the algorithms often impractical for large problem instances. In this paper we propose a method to improve the average run-time complexity of sequence-pair operations using the recently introduced DV algorithm [2]. Additionally, we reduce the computational overhead of some basic sequence-pair operations, exploiting symmetry properties or precomputed information. All operations are embedded in a Very Fast Simulated Annealing algorithm, which is described in detail. The new approach, which has significantly lower complexity components of order #  ####  , allows faster convergence of the annealing algorithm, substantially improving over previous results. Keywords--- Sequence Pair, floorplanning, placement, ...",
            "group": 887,
            "name": "10.1.1.40.102",
            "keyword": "floorplanningplacementlayout",
            "title": "A More Efficient Sequence Pair Perturbation Scheme"
        },
        {
            "abstract": "this paper should be helpful to researchers and practitioners working in the fields of video compression and processing, as well as in computer vision. Although the understanding of issues involved in the computation of motion has significantly increased over the last decade, we are still far from generic, robust, real-time motion estimation algorithms. The selection of the best motion estimator is still highly dependent on the application. Nevertheless, a broad variety of estimation models, criteria and optimization schemes can be treated in a unified framework presented here, thus allowing a direct comparison and leading to a deeper understanding of the properties of the resulting estimators.",
            "group": 888,
            "name": "10.1.1.40.627",
            "keyword": "",
            "title": "Estimating Motion in Image Sequences - A tutorial on modeling and computation of 2D motion"
        },
        {
            "abstract": "The problem of optimally approximating a function with a linear expansion over a redundant dictionary of waveforms is NP-hard. The greedy matching pursuit algorithm and its orthogonalized variant produce sub-optimal function expansions by iteratively choosing the dictionary waveforms which best match the function's structures. Matching pursuits provide a means of quickly computing compact, adaptive function approximations. Numerical experiments show that the approximation errors from matching pursuits initially decrease rapidly, but the asymptotic decay rate of the errors is slow. We explain this behavior by showing that matching pursuits are chaotic, ergodic maps. The statistical properties of the approximation errors of a pursuit can be obtained from the invariant measure of the pursuit. We characterize these measures using group symmetries of dictionaries and using a stochastic differential equation model. These invariant measures define a noise with respect to a given dictionary. T...",
            "group": 889,
            "name": "10.1.1.40.710",
            "keyword": "",
            "title": "Adaptive Nonlinear Approximations"
        },
        {
            "abstract": ". A general problem in model selection is to obtain the right  parameters that make a model fit observed data. If the model selected is  a Multilayer Perceptron (MLP) trained with Backpropagation (BP), it  is necessary to find appropriate initial weights and learning parameters.  This paper proposes a method that combines Simulated Annealing (SimAnn)  and BP to train MLPs with a single hidden layer, termed SA-Prop.  SimAnn selects the initial weights and the learning rate of the network.  SA-Prop combines the advantages of the stochastic search performed by  the Simulated Annealing over the MLP parameter space and the local  search of the BP algorithm.  The application of the proposed method to several real-world benchmark  problems shows that MLPs evolved using SA-Prop achieve a higher  level of generalization than other perceptron training algorithms, such  as QuickPropagation (QP) or RPROP, and other evolutive algorithms,  such as G-LVQ.  1 Introduction  Whatever the application usi...",
            "group": 890,
            "name": "10.1.1.40.1160",
            "keyword": "",
            "title": "SA-Prop: Optimization of Multilayer Perceptron Parameters using Simulated Annealing"
        },
        {
            "abstract": "In this paper, we consider approximating global minima of zero or small residual,  nonlinear least-squares problems. We propose a selective search approach based on  the concept of selective minimization recently introduced in Zhang et al [14]. To test  the viability of the proposed approach, we construct a simple implementation using a  Levenberg-Marquardt type method combined with a multi-start scheme, and compare it  with several existing global optimization techniques. Numerical experiments were performed  on zero residual nonlinear least-squares problems chosen from structural biology  applications and from the literature. On the problems of significant sizes, the performance  of the new approach compared favorably with other tested methods, indicating  that the new approach is promising for the intended class of problems.  Keywords: Global minimization, zero or small residual least-squares problems, selective minimization, Levenberg-Marquardt method, multi-start.  1 Introduction ...",
            "group": 891,
            "name": "10.1.1.40.1992",
            "keyword": "Global minimizationzero or small residual least-squares problemsselective minimizationLevenberg-Marquardt methodmulti-start",
            "title": "Selective Search for Global Optimization of Zero or Small Residual Least-Squares Problems: A Numerical Study"
        },
        {
            "abstract": "In this paper, we present a constraint transformation and topology selection methodology that explores the system level parameter space to compute acceptable regions in the component parameter space. The search process of an underlying circuit synthesis tool could be confined to these regions of valid solutions. Experimental results showing the impact of parameter space exploration at a higher level on analog circuit synthesis are presented, demonstrating the effectiveness of this technique.  1 Introduction  Crucial to a top-down analog design process [10] is an interfacing mechanism to communicate the specifications and constraints on the design elements used at one level with those at the next level. The task of transforming the high-level specifications onto module parameters is called Constraint Transformation [3]. Efficient automation of this task is one of the the most important steps in automating the design of analog and mixed analog-digital systems. There are two important asp...",
            "group": 892,
            "name": "10.1.1.40.2284",
            "keyword": "",
            "title": "Automatic Constraint Transformation with Integrated Parameter Space Exploration in Analog System Synthesis"
        },
        {
            "abstract": "Given a set of samples of an unknown probability distribution, we study the problem of constructing a good approximative Bayesian network model of the probability distribution in question. This task can be viewed as a search problem, where the goal is to find a maximal probability network model, given the data. In this work, we do not make an attempt to learn arbitrarily complex multi-connected Bayesian network structures, since such resulting models can be unsuitable for practical purposes due to the exponential amount of time required for the reasoning task. Instead, we restrict ourselves to a special class of simple tree-structured Bayesian networks called Bayesian prototype trees, for which a polynomial time algorithm for Bayesian reasoning exists. We show how the probability of a given Bayesian prototype tree model can be evaluated, given the data, and how this evaluation criterion can be used in a stochastic simulated annealing algorithm for searching the model space. The simulat...",
            "group": 893,
            "name": "10.1.1.40.2587",
            "keyword": "",
            "title": "Learning Bayesian Prototype Trees by Simulated Annealing"
        },
        {
            "abstract": "Manufacturing scheduling is a difficult problem, particularly when it takes place in an open, dynamic environment. Agent-based technology has recently been used in attempts to resolve this problem. A bidding mechanism based on Contract Net protocol is often proposed as a key solution component. Our approach is to combine a bidding mechanism based on Contract Net protocol with a mediation mechanism based on Mediator architecture, for dynamic manufacturing scheduling and rescheduling. A machine-centered scheduling mechanism and related concepts and mechanisms are also described. Keywords: Agents, scheduling, distributed manufacturing systems, bidding mechanism, Contract Net, mediator. 1. Introduction Manufacturing scheduling is a difficult problem, particularly when it takes place in an open, dynamic environment. In a job shop manufacturing system, rarely do things go as expected. The set of things to do is generally dynamic. The system may be asked to do additional tasks that were no...",
            "group": 894,
            "name": "10.1.1.40.2734",
            "keyword": "Agentsschedulingdistributed manufacturing systemsbidding mechanismContract Netmediator",
            "title": "An Agent-Based Approach for Dynamic Manufacturing Scheduling"
        },
        {
            "abstract": "The Cascade Vulnerability Problem is a potential problem which must be faced when using the interconnected accredited system approach of the Trusted Network Interpretation. It belongs to a subset of the problem set that addresses the issue of whether the interconnection of secure systems via a secure channel results in a secure distributed system. The Cascade Vulnerability problem appears when an adversary can take advantage of network connections to compromise information across a range of sensitivity levels that is greater than the accreditation range of any of the component systems s/he must defeat to do so. In this paper, we present the general Cascade vulnerability problem, describe the basic properties of the most important detection algorithms, conduct a brief comparative analysis, and present a new approach based on simulated annealing for its correction.  Keywords: Cascade Vulnerability Problem, Network & Open Distributed Systems Security, Risk Analysis, Simulated annealing.  ...",
            "group": 895,
            "name": "10.1.1.40.2775",
            "keyword": "Cascade Vulnerability ProblemNetwork & Open Distributed Systems SecurityRisk AnalysisSimulated annealing",
            "title": "The Cascade Vulnerability problem: The Detection problem and a Simulated Annealing Approach for its Correction"
        },
        {
            "abstract": "Introduction  In this chapter we are concerned with the estimation of 2-D motion from timevarying images and with the application of the computed motion to image sequence processing. Our goal for motion estimation is to propose a general formulation that incorporates object acceleration, nonlinear motion trajectories, occlusion effects and multichannel (vector) observations. To achieve this objective we use Gibbs-Markov models linked together by the Maximum A Posteriori  Probability criterion which results in minimization of a multiple-term cost function. The specific applications of motion-compensated processing of image sequences are prediction, noise reduction and spatiotemporal interpolation. Estimation of motion from dynamic images is a very difficult task due to its ill-posedness [4]. Despite this difficulty, however, many approaches to the problem have been proposed in the last dozen years [27],[24],[40]. This activity can certainly be attrib",
            "group": 896,
            "name": "10.1.1.40.2908",
            "keyword": "",
            "title": "Estimation of 2-D Motion Fields from Image Sequences with Application to Motion-Compensated Processing"
        },
        {
            "abstract": "This paper presents an integrated approach to hardware software partitioning and hardware design space exploration. We propose a genetic algorithm which performs hardware software partitioning on a task graph while simultaneously contemplating various design alternatives for tasks mapped to hardware. We primarily deal with data dominated designs typically found in digital signal processing and image processing applications. A detailed description of various genetic operators is presented. We provide results to illustrate the effectiveness of our integrated methodology.  ",
            "group": 897,
            "name": "10.1.1.40.3666",
            "keyword": "",
            "title": "Hardware Software Partitioning with Integrated Hardware Design Space Exploration"
        },
        {
            "abstract": "Given a set of samples of an unknown probability distribution, we study the problem of constructing a good approximative Bayesian network model of the probability distribution in question. This task can be viewed as a search problem, where the goal is to find a maximal probability network model, given the data. In this work, we do not make an attempt to learn arbitrarily complex multi-connected Bayesian network structures, since such resulting models can be unsuitable for practical purposes due to the exponential amount of time required for the reasoning task. Instead, we restrict ourselves to a special class of simple tree-structured Bayesian networks called Bayesian prototype trees, for which a polynomial time algorithm for Bayesian reasoning exists. We show how the probability of a given Bayesian prototype tree model can be evaluated, given the data, and how this evaluation criterion can be used in a stochastic simulated annealing algorithm for searching the model space. The simulat...",
            "group": 898,
            "name": "10.1.1.40.3993",
            "keyword": "",
            "title": "Constructing Computationally Efficient Bayesian Models via Unsupervised Clustering"
        },
        {
            "abstract": " ",
            "group": 899,
            "name": "10.1.1.40.4130",
            "keyword": "",
            "title": "Information theory and Visual Plasticity"
        },
        {
            "abstract": "System specification and design consists of describing a system's desired functionality, and of mapping that functionality for implementation on a set of system components, such as processors, ASIC's, memories, and buses. In this article, we describe the key problems of system specification and design, including specification capture, design exploration, hierarchical modeling, software and hardware synthesis, and cosimulation. We highlight existing tools and methods for solving those problems, and we discuss issues that remain to be solved. 1 Introduction  Embedded systems have become commonplace in recent years. Examples include automobile cruise-control, fuel-injection systems, aircraft autopilots, telecommunication products, interactive television processors, network switches, video focusing units, robot controllers, and numerous medical devices. While there is no widespread agreement of what defines an embedded system, we note that such systems possess a few key characteristics. Th...",
            "group": 900,
            "name": "10.1.1.40.4523",
            "keyword": "",
            "title": "Specification and Design of Embedded Software/Hardware Systems"
        },
        {
            "abstract": ". We determine the asymptotical satisfiability probability of a random at-most-k-Horn formula, via a probabilistic analysis of a simple version, called PUR, of positive unit resolution. We show that for k = k(n) ! 1 the problem can be \"reduced\" to the case k(n) = n, that was solved in [12]. On the other hand, in the case k = constant the behavior of PUR is modeled by a simple queuing chain, leading to a closedform solution when k = 2. Our analysis predicts an \"easy-hard-easy\" pattern in this latter case. Under a rescaled parameter, the graphs of limit probability corresponding to finite vales of k converge to the one for the uniform case, a critical behavior similar to the one found experimentally in [16] for k-SAT. The phenomenon is qualitatively explained by a threshold property for the number of iterations of PUR makes on random satisfiable  Horn formulas. Also, for k = 2 PUR has a peak in average complexity at the critical point.  Key words. sign-nonsingular matrix, LU-factorizatio...",
            "group": 901,
            "name": "10.1.1.40.4527",
            "keyword": "",
            "title": "Critical Behavior In The Satisfiability Of Random K-Horn Formulae"
        },
        {
            "abstract": ". The maximum entropy framework has proved to be expressive and powerful for statistical language modelling, but it suffers from the computational expensiveness of model building. The iterative scaling algorithm that is used for parameter estimation is rather slow while the feature selection process might require parameters for many candidate features to be estimated many times. In this paper we present a novel approach for building maximum entropy models. Our approach uses a feature collocation lattice as a feature generation engine and selects candidate features without resorting to iterative scaling but instead through our own frequency redistribution algorithm. After the candidate features have been selected we use iterative scaling to estimate a fully saturated model for the maximal (factorial) feature space and then start to relax (eliminate) the most specific features. During constraint relaxation we always have a fully fit maximum entropy model, so we rank the constraints on th...",
            "group": 902,
            "name": "10.1.1.40.5003",
            "keyword": "",
            "title": "Feature Lattices and Maximum Entropy Models"
        },
        {
            "abstract": "We use a new \"aura\" framework to rewrite the nonlinear energy function of a homogeneous anisotropic Markov/Gibbs random field (MRF) as a linear sum of aura measures. The new formulation relates MRF's to co-occurrence matrices. It also provides a physical interpretation of MRF textures in terms of the mixing and separation of graylevel sets, and in terms of boundary maximization and minimization. Within this framework, we introduce the use of temperature for texture modeling and show how the parameters of the MRF can be interpreted as temperature annealing rates. In particular, we show evidence for a transition temperature, above which all patterns generated will be visually similar, and below which a pattern evolves down to its ground state. Finally, we describe briefly some new results which characterize the ground state patterns. 1 Introduction  Since the establishment of the equivalence between Gibbs and Markov random fields (MRF's), there has been a flurry of application of MRF's t...",
            "group": 903,
            "name": "10.1.1.40.5180",
            "keyword": "",
            "title": "Markov/Gibbs Texture Modeling: Aura Matrices and Temperature Effects"
        },
        {
            "abstract": "This paper describes how Fractal Coding Theory may be applied to compress video images using an image resampling sequencer (IRS) in a video compression system on a modular image processing system. The first part of the paper describes the background theory of image coding using a form of fractal equation known as Iterated Function System (IFS) codes. The second part deals with the modular image processing system on which to implement these operations. The third part briefly covers how IFS codes may be calculated. Finally, how the IRS and 2  nd  order geometric transformations may be used to describe inter-frame changes to compress motion video. Appendix E The Use of Fractal Theory in a Video Compression System  Introduction  IFS encoding offers a very high compression ratio (CR) but is computationally intensive and thus requires specialised hardware or a lot of time (in the order of a second) for real-time image processing to be realised. To use IFS encoding's ability of high compress...",
            "group": 904,
            "name": "10.1.1.40.5559",
            "keyword": "",
            "title": "Appendix E The Use of Fractal Theory in a Video Compression System"
        },
        {
            "abstract": "Most research on the utilization of standardized application software  packages in large companies is focussed on introducing a certain  package for entire processes or even the entire company. A mix of  existing, mostly individual applications and selected modules of  different standardized application packages, however, may be  superior in certain situations. A cost-based model for information  systems optimization is presented. A cost-minimal allocation of  software packages can be determined for every set of communication  costs associated with information flows between modules and  automation costs associated with process steps. Moreover some  general hypotheses on software package introduction in large  companies are validated by a large number of optimization series  basing on the underlying decision model.  The paper presents the general decision model, its application to  evaluate package allocation, and general simulation results.  440 Part II: Theory Keywords: Business packa...",
            "group": 905,
            "name": "10.1.1.40.5601",
            "keyword": "Business package evaluationcost-based decision modelbusiness package",
            "title": "Optimal Allocation Of Standardized Application Software Packages To Business Process Steps: A Simulation Study Based On Communication And Automation Costs"
        },
        {
            "abstract": "A circuit layout problem requires determining the component pin placement and routing of interconnections for a given circuit schematic on a single or multilayer printed circuit board. In this paper, an integer programming based approach is introduced to solve the layout problem which performs placement and routing simultaneously. Since an integer programming problem is computationally intractable, a heuristic method to solve the placement and routing separately has been developed by utilizing the integer programming formulation. By applying a fixed routing scheme that connects components by direct line segments, the layout problem is transformed into a quadratic cost optimization problem in which the only decision variable is the pin placement, and which is solved by drawing an analogy between the quadratic cost term and the power dissipation term in a purely resistive network. Partitioning is then used to assign components to locations on a grid. Once the placement is determined, rou...",
            "group": 906,
            "name": "10.1.1.40.5790",
            "keyword": "Printed circuit boardplacementroutinginteger programmingSteiner tree",
            "title": "An Integer Programming Approach to Placement and Routing in Circuit Layout"
        },
        {
            "abstract": "The presented technical report is a preliminary English translation of selected revised sections from the first part of the book Theoretical Issues of Neural Networks [75] by the first author which represents a brief introduction to neural networks. This work does not cover a complete survey of the neural network models but the exposition here is focused more on the original motivations and on the clear technical description of several basic type models. It can be understood as an invitation to a deeper study of this field. Thus, the respective background is prepared for those who have not met this phenomenon yet so that they could appreciate the subsequent theoretical parts of the book. In addition, this can also be profitable for those engineers who want to apply the neural networks in the area of their expertise. The introductory part does not require deeper preliminary knowledge, it contains many pictures and the mathematical formalism is reduced to the lowest degree in the first c...",
            "group": 907,
            "name": "10.1.1.40.6664",
            "keyword": "",
            "title": "Introduction to Neural Networks"
        },
        {
            "abstract": "Manufacturing scheduling is a difficult problem, particularly when it takes place in an open, dynamic environment. Agent-based technology has recently been used in attempts to resolve this problem. A bidding mechanism based on Contract Net protocol is often proposed as a key solution component. Our approach is to combine a bidding mechanism based on Contract Net protocol with a mediation mechanism based on Mediator architecture, for dynamic manufacturing scheduling and rescheduling. A machine-centered scheduling mechanism and related concepts and mechanisms are also described. ",
            "group": 908,
            "name": "10.1.1.40.7149",
            "keyword": "Agentsschedulingdistributed manufacturing systemsbidding mechanismContract Netmediator",
            "title": "An Agent-Based Approach for Dynamic Manufacturing Scheduling"
        },
        {
            "abstract": "In this paper, we define the generalization problem, summarize various approaches in generalization, identify the credit assignment problem, and present the problem and some solutions in measuring generalizability. We discuss anomalies in the ordering of hypotheses in a subdomain when performance is normalized and averaged, and show conditions under which anomalies can be eliminated. To generalize performance across subdomains, we present a measure called probability of win that measures the probability whether a hypothesis is better than another. Finally, we discuss some limitations in using probabilities of win and illustrate their application in finding new parameter values for TimberWolf, a package for VLSI cell placement and routing. 1 Introduction  Generalization in psychology is the tendency to respond in the same way to different but similar stimuli [6]. Such transfer of tendency may be based on temporal stimuli, spatial cues, or other physical characteristics. Learning, on the...",
            "group": 909,
            "name": "10.1.1.40.7209",
            "keyword": "",
            "title": "Generalization and Generalizability Measures"
        },
        {
            "abstract": "Simulated annealing and and single trial versions of evolution strategies possess a close relationship when they are designed for optimization over continuous variables. Analytical investigations of their differences and similarities lead to a cross-fertilization of both approaches, resulting in new theoretical results, new parallel population based algorithms, and a better understanding of the interrelationships.",
            "group": 910,
            "name": "10.1.1.40.7534",
            "keyword": "global optimizationparallel simulated annealingparallel evolutionary algorithmsneighborhood algorithms",
            "title": "Massively Parallel Simulated Annealing and its Relation to Evolutionary Algorithms"
        },
        {
            "abstract": "It is generally accepted that a component's cost is an increasing function of its reliability. Most researchers adopt exponentially increasing functions for optimal reliability / redundancy allocation problem formulations. In this paper we address cases where an exact, convex relationship between reliability and cost is not known. Instead, we consider the more realistic assumption of discrete cost-reliability data sets for each component. The resulting integer programming formulations are non-linear and NP hard. We use a nested simulated annealing (SA) approach; one SA for feasibility and one SA for optimality to solve this difficult problem while ensuring feasibility of the final solution. This approach is demonstrated on two example problems with large search spaces.  KEYWORDS  Reliability, Simulated Annealing, Redundancy Allocation, Design Optimization.  INTRODUCTION  Many forms of the optimal reliability / redundancy allocation problem have been studied extensively in the past usin...",
            "group": 911,
            "name": "10.1.1.40.7884",
            "keyword": "Simulated AnnealingRedundancy AllocationDesign Optimization",
            "title": "Optimal Reliability Allocation In Series-Parallel Systems From Components' Discrete Cost - Reliability Data Sets: A Nested Simulated Annealing Approach"
        },
        {
            "abstract": "In general, neural networks are regarded as models for massively parallel computation.  But very often, this parallelism is rather limited, especially when considering  symmetric networks. For instance, Hopfield networks do not really compute in parallel  as their updating algorithm always requires sequential execution. Nevertheless,  Hopfield networks can be used as auto-associative memories, were shown to have an  expressive power equivalent to propositional logic, and can be used to solve several  combinatorial problems. Extensions like the Boltzmann Machine with continuous  activation functions can additionally be used to solve optimization problems. But,  all of these approaches suffer from one disadvantage, namely the impossibility to  perform simultaneous computations of more than one unit, i.e. real parallelism. We  describe a recurrent network corresponding to a symmetric network and introduce a  method of parallel updating multiple units. We show how this may be extended to  ...",
            "group": 912,
            "name": "10.1.1.40.7990",
            "keyword": "Multi-Flip NetworksExtending Symmetric Networks",
            "title": "Multi-Flip Networks: Extending Symmetric Networks to Real Parallelism"
        },
        {
            "abstract": "In this paper, we consider approximating global minima of zero or small residual,  nonlinear least-squares problems. We propose a selective search approach based on  the concept of selective minimization recently introduced in Zhang et al [14]. To test  the viability of the proposed approach, we construct a simple implementation using a  Levenberg-Marquardt type method combined with a multi-start scheme, and compare it  with several existing global optimization techniques. Numerical experiments were performed  on zero residual nonlinear least-squares problems chosen from structural biology  applications and from the literature. On the problems of significant sizes, the performance  of the new approach compared favorably with other tested methods, indicating  that the new approach is promising for the intended class of problems.  Keywords: Global minimization, zero or small residual least-squares problems, selective minimization, Levenberg-Marquardt method, multi-start.  1 Introduction ...",
            "group": 913,
            "name": "10.1.1.40.8880",
            "keyword": "Global minimizationzero or small residual least-squares problemsselective minimizationLevenberg-Marquardt methodmulti-start",
            "title": "Selective Search for Global Optimization of Zero or Small Residual Least-Squares Problems: A Numerical Study"
        },
        {
            "abstract": "Cell placement is an important phase of current VLSI circuit design styles such as standard cell, gate array, and Field Programmable Gate Array (FPGA). Although nondeterministic algorithms such as Simulated Annealing (SA) have been successful in solving this problem, they are known to be slow. In this paper, we propose a neural network algorithm that produces solutions as good as SA in substantially less time. Our algorithm is based on Mean Field Annealing (MFA) technique, which has been successfully applied to various combinatorial optimization problems. We derive a MFA formulation for the cell placement problem that can easily be applied to all VLSI design styles. To demonstrate that the proposed algorithm is applicable in practice, we derive a detailed formulation for the FPGA design style, and generate the layouts of several benchmark circuits. The performance of the proposed cell placement algorithm is evaluated in comparison with commercial automated circuit design software Xilin...",
            "group": 914,
            "name": "10.1.1.40.9289",
            "keyword": "1",
            "title": "A Fast Neural-Network Algorithm for VLSI Cell Placement"
        },
        {
            "abstract": "All the previous Kernighan-Lin based (KLbased) circuit partitioning algorithms employ the locking mechanism, which enforces each cell to move exactly once per pass. In this paper, we propose two novel approaches for multiway circuit partitioning to overcome this limitation. Our approaches allow each cell to move more than once. Our first approach still uses the locking mechanism but in a relaxed way. It introduces the phase concept such that each pass can include more than one phase and a phase can include at most one move of each cell. Our second approach does not use the locking mechanism at all. It introduces the mobility concept such that each cell can move as freely as allowed by its mobility. Each approach leads to KL-based generic algorithms whose parameters can be set to obtain algorithms with different performance characteristics. We generated three versions of each generic algorithm and evaluated them on a subset of common benchmark circuits in comparison with Sanchis' algori...",
            "group": 915,
            "name": "10.1.1.40.9291",
            "keyword": "",
            "title": "Two Novel Multiway Circuit Partitioning Algorithms Using Relaxed Locking"
        },
        {
            "abstract": "This report is based on a presentation to be held at the Fourth International",
            "group": 916,
            "name": "10.1.1.40.9342",
            "keyword": "",
            "title": "An Agent-Based Resource Allocator for Close Air Support"
        },
        {
            "abstract": ". In this paper, we propose an order-independent global routing  algorithm for SRAM type FPGAs based on Mean Field Annealing.  The performance of the proposed global routing algorithm is evaluated in  comparison with LocusRoute global router on ACM/SIGDA Design Automation   benchmarks. Experimental results indicate that the proposed  MFA heuristic performs better than the LocusRoute in terms of the distribution  of the channel densities.  1 Introduction  This paper investigates the routing problem in Static RAM (SRAM) based Field Programmable Gate Arrays (FPGAs) [7]. As the routing in FPGAs is a very complex combinatorial optimization problem, routing process can be carried out in two phases: global routing followed by detailed routing [5]. Global routing determines the course of wires through sequences of channel segments. Detailed routing determines the wire segment allocation for the channel segment routes found in the first phase which enables feasible switch box interconnection co...",
            "group": 917,
            "name": "10.1.1.40.9550",
            "keyword": "",
            "title": "A Global Routing Heuristic for FPGAs Based on Mean Field Annealing"
        },
        {
            "abstract": "this paper we have studied the process of learning from examples with a stochastic training dynamics. The level of noise in the dynamics is denoted by the temperature T . One of the most important results of our analysis is that learning at finite temperature is possible, and sometimes advantageous. For any finite T , as the number of examples increases, the network weights approach their optimal values, namely the values that minimize the generalization error. In Part I we have focused mainly on realizable rules. When trained with a fixed number of examples, our realizable models have average generalization errors that increase with T . Thus from purely equilibrium considerations working at T = 0 is better. On the other hand, the lower the temperature the longer it may take to reach equilibrium. This is particularly true for highly nonlinear models, such as the boolean perceptron with discrete weights. Although the critical number of examples per weight ff c (T ) increases with T",
            "group": 918,
            "name": "10.1.1.40.9669",
            "keyword": "",
            "title": "Statistical Mechanics of Learning From Examples - I. General Formulation and Annealed Approximation"
        },
        {
            "abstract": ". In this paper we present a Lagrange-multiplier formulation of discrete constrained optimization problems, the associated discrete-space first-order necessary and sufficient conditions for saddle points, and an efficient first-order search procedure that looks for saddle points in discrete space. Our new theory provides a strong mathematical foundation for solving general nonlinear discrete optimization problems. Specifically, we propose a new vector-based definition of descent directions in discrete space and show that the new definition does not obey the rules of calculus in continuous space. Starting from the concept of saddle points and using only vector calculus, we then prove the discrete-space first-order necessary and sufficient conditions for saddle points. Using well-defined transformations on the constraint functions, we further prove that the set of discrete-space saddle points is the same as the set of constrained local minima, leading to the first-order necessary and suf...",
            "group": 919,
            "name": "10.1.1.41.357",
            "keyword": "",
            "title": "The Theory of Discrete Lagrange Multipliers for Nonlinear Discrete Optimization"
        },
        {
            "abstract": "Advanced data assimilation becomes extremely complicated and challenging when used with strongly nonlinear models. Several previous works have reported various problems when applying existing popular data assimilation techniques with strongly nonlinear dynamics. Common for these techniques is that they can all be considered as extensions to methods which have proven to work well with linear dynamics. This paper shows that a weak constraint variational formulation for the Lorenz model, where the full model state in space and time is considered as control variables, can be minimized using a gradient descent method. It is further shown that the weak constraint formulation removes some of the previous reported problems associated to the predictability limit of nonlinear models when strong constraint formulations are used. Further, by using a gradient descent method, problems associated to the use of an approximate tangent linear model when solving the Euler-Lagrange equations or when the e...",
            "group": 920,
            "name": "10.1.1.41.606",
            "keyword": "",
            "title": "Solving for the generalized inverse of the Lorenz model"
        },
        {
            "abstract": ".  In this paper, we introduce a rather straightforward but fundamental observation concerning the convergence of the general iteration process  x  k+1  = x  k  \\Gamma ff(x  k  )B(x  k  )  \\Gamma1  rf(x  k  ) for minimizing a function f(x). We give necessary and sufficient conditions for a stationary point of  f(x) to be a point of strong attraction of the iteration process. We will discuss various ramifications of this fundamental result, particularly for nonlinear least squares problems.  Key words. Strong attraction, weak repulsion, selective minimization.  AMS subject classifications. 65K05, 90C30  1. Introduction. We consider the unconstrained minimization problem minf(x); (1.1) where f : !  n  ! ! is assumed to be twice (Frechet) differentiable, and the general iteration:  x  k+1  = x  k  \\Gamma ff  k  (B  k  )  \\Gamma1  rf(x  k  ): (1.2) This iterative framework has been studied extensively and many results are available for various choices of ff  k  and B  k  that guarantee con...",
            "group": 921,
            "name": "10.1.1.41.1058",
            "keyword": "",
            "title": "On Convergence of Minimization Methods: Attraction, Repulsion and Selection"
        },
        {
            "abstract": ". In this paper we propose an optimal anytime version of constrained  simulated annealing (CSA) for solving constrained nonlinear  programming problems (NLPs). One of the goals of the algorithm is  to generate feasible solutions of certain prescribed quality using an average  time of the same order of magnitude as that spent by the original  CSA with an optimal cooling schedule in generating a solution of similar  quality. Here, an optimal cooling schedule is one that leads to the  shortest average total number of probes when the original CSA with the  optimal schedule is run multiple times until it finds a solution. Our second  goal is to design an anytime version of CSA that generates gradually  improving feasible solutions as more time is spent, eventually finding a  constrained global minimum (CGM). In our study, we have observed a  monotonically non-decreasing function relating the success probability  of obtaining a solution and the average completion time of CSA, and an  exponen...",
            "group": 922,
            "name": "10.1.1.41.1592",
            "keyword": "",
            "title": "Optimal Anytime Constrained Simulated Annealing for Constrained Global Optimization"
        },
        {
            "abstract": "This paper studies various strategies in constrained simulated annealing (CSA), a global optimization algorithm that achieves asymptotic convergence to constrained global minima (CGM) with probability one for solving discrete constrained  nonlinear programming problems (NLPs). The algorithm is based on the necessary and sufficient condition for discrete constrained local minima (CLM) in the theory of discrete Lagrange multipliers and its extensions to continuous and mixed-integer constrained NLPs. The strategies studied include adaptive neighborhoods, distributions to control sampling, acceptance probabilities, and cooling schedules. We report much better solutions than the best-known solutions in the literature on two sets of continuous benchmarks and their discretized versions.",
            "group": 923,
            "name": "10.1.1.41.1614",
            "keyword": "Asymptotic convergence with probability oneconstrained global minimumconstrained local minimumconstrained simulated annealingdiscrete Lagrange multipliersMetropolis probabilitynonlinear programmingsaddle points. 1 Problem Definition",
            "title": "Tuning Strategies In Constrained Simulated Annealing For Nonlinear Global Optimization"
        },
        {
            "abstract": "ing with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org.  2 \\Delta L.A. Hemaspaandra et al. 1. MOTIVATION AND OVERVIEW  How should the seats in the House of Representatives of the United States be allocated among the states? The Constitution stipulates only that \"Representatives shall be apportioned among the several states according to their respective numbers, counting the whole numbers of persons in each State : : :.\" The obvious implementation of this requirement would almost always yield fractional numbers of seats. The issue of how to achieve fair integer seat allocations has been controversial in this country virtually since its founding. In fact, many of the apportionment algorit...",
            "group": 924,
            "name": "10.1.1.41.1723",
            "keyword": "Categories and Subject DescriptorsJ.4 [Computer ApplicationsSocial and Behavioral SciencesK.4 [Computing MilieuxComputers and Society General TermsAlgorithmsExperimentation Additional Key Words and Phrasespower indicesapportionment algorithmssimulated-annealing",
            "title": "Power Balance and Apportionment Algorithms for the United States Congress"
        },
        {
            "abstract": "Can stochastic search algorithms outperform existing deterministic heuristics for the NP-hard problem Number Partitioning if given a sufficient, but practically realizable amount of time? In a thorough empirical investigation using a straightforward implementation of one such algorithm, simulated annealing, Johnson et al. (1991) concluded tentatively that the answer is \"no.\" In this paper we show that the answer can be \"yes\" if attention is devoted to the issue of problem representation (encoding). We present results from empirical tests of several encodings of Number Partitioning with problem instances consisting of multiple-precision integers drawn from a uniform probability distribution. With these instances and with an appropriate choice of representation, stochastic and deterministic searches can---routinely and in a practical amount of time---find solutions several orders of magnitude better than those constructed by the best heuristic known (Karmarkar and Karp, 1982), which does...",
            "group": 925,
            "name": "10.1.1.41.1774",
            "keyword": "",
            "title": "Easily Searched Encodings for Number Partitioning"
        },
        {
            "abstract": "The task of determining the globally optimal (minimum-energy) conformation of a protein given its potential-energy function is widely believed to require an amount of computer time that is exponential in the number of soft degrees of freedom in the protein. Conventional reasoning as to the exponential time complexity of this problem is fallacious---it is based solely on the size of the search space---and for some variants of the protein-structure prediction problem the conclusion is likely to be incorrect. Every problem in combinatorial optimization has an exponential number of candidate solutions, but many such problems can be solved by algorithms that do not require exponential time. We present a critical review of efforts to characterize rigorously the computational requirements of global potential-energy minimization for a polypeptide chain that has a unique energy minimum corresponding to the native structure of the protein. An argument by Crippen (1975) demonstrated that an algor...",
            "group": 926,
            "name": "10.1.1.41.2031",
            "keyword": "",
            "title": "Computational Complexity, Protein Structure Prediction, and the Levinthal Paradox"
        },
        {
            "abstract": "In this research we present new results on discrete Lagrangian methods (DLM) and extend our previous (incomplete and highly simplified) theory on the method. Our proposed method forms a strong mathematical foundation for solving general nonlinear discrete optimization problems. Specifically, we show for continuous Lagrangian methods the relationship among local minimal solutions satisfying constraints, solutions found by the first-order necessary and second-order sufficient conditions, and saddle points. Since there is no corresponding definition of gradients in discrete space, we propose a new vector-based definition of gradient, develop first-order conditions similar to those in continuous space, propose a heuristic method to find saddle points, and show the relationship between saddle points and local minimal solutions satisfying constraints. We then show, when all the constraint functions are non-negative, that the set of saddle points is the same as the set of local minimal points...",
            "group": 927,
            "name": "10.1.1.41.2092",
            "keyword": "",
            "title": "The Discrete Lagrangian Theory And Its Application To Solve Nonlinear Discrete Constrained Optimization Problems"
        },
        {
            "abstract": "Mean field annealing (MFA) algorithm, proposed for solving combinatorial optimization problems,  combines the characteristics of neural networks and simulated annealing. Previous works on MFA  resulted with successful mapping of the algorithm to some classic optimization problems such  as traveling salesperson problem, scheduling problem, knapsack problem and graph partitioning  problem. In this paper, MFA is formulated for the circuit partitioning problem using the so called  net-cut model. Hence, the deficiencies of using the graph representation for electrical circuits are  avoided. An efficient implementation scheme, which decreases the complexity of the proposed  algorithm by asymptotical factors is also developed. Comparative performance analysis of the  proposed algorithm with two well-known heuristics, simulated annealing and Kernighan-Lin,  indicates that MFA is a successful alternative heuristic for the circuit partitioning problem.  Keywords : Mean field annealing, circuit p...",
            "group": 928,
            "name": "10.1.1.41.2654",
            "keyword": "Mean field annealingcircuit partitioningnet-cut model",
            "title": "Circuit Partitioning Using Mean Field Annealing"
        },
        {
            "abstract": ". The satisfiability (SAT) problem is a core problem in mathematical logic and computing theory. In practice, SAT is fundamental in solving many problems in automated reasoning, computer-aided design, computeraided manufacturing, machine vision, database, robotics, integrated circuit design, computer architecture design, and computer network design. Traditional methods treat SAT as a discrete, constrained decision problem. In recent years, many optimization methods, parallel algorithms, and practical techniques have been developed for solving SAT. In this survey, we present a general framework (an algorithm space) that integrates existing SAT algorithms into a unified perspective. We describe sequential and parallel SAT algorithms including variable splitting, resolution, local search, global optimization, mathematical programming, and practical SAT algorithms. We give performance evaluation of some existing SAT algorithms. Finally, we provide a set of practical applications of the sat...",
            "group": 929,
            "name": "10.1.1.41.2697",
            "keyword": "",
            "title": "Algorithms for the Satisfiability (SAT) Problem: A Survey"
        },
        {
            "abstract": "Batch distillation processes are widely used in chemical industry. In this work, we consider the optimization of such processes by simulated annealing. Although this method is stochastically in nature, it has two evitable advantages: it can be readily connected to highly sophisticated simulation codes and it converges towards a global optimum. According to the characteristics of batch distillation operation we propose to use a two-step computation approach. A feasible strategy (admissible control) will be searched for in the first step and it will be optimized in the second step. The approach has been applied to three models of batch distillation ranging from a simple test example to a real production system. These results show the potential of the method for developing optimal operation strategies for batch chemical processes.  Keywords: batch distillation, simulated annealing, dynamic optimization.  1 Introduction  The determination of optimal control strategies for chemical processe...",
            "group": 930,
            "name": "10.1.1.41.3098",
            "keyword": "batch distillationsimulated annealingdynamic optimization",
            "title": "Simulated Annealing For The Optimization Of Chemical Batch Production Processes"
        },
        {
            "abstract": "In this paper, we formulate neural-network training as a constrained optimization problem instead of the traditional formulation based on unconstrained optimization. We show that constraints violated during a search provide additional force to help escape from local minima using our newly developed constrained simulated annealing (CSA) algorithm. We demonstrate the merits of our approach by training neural networks to solve the two-spiral problem. To enhance the search, we have developed a strategy to adjust the gain factor of the activation function. We show converged training results for networks with 4, 5, and 6 hidden units, respectively. Our work is the first successful attempt to solve the two-spiral problem with 19 weights.  1 Formulation of Supervised NeuralNetwork Training  Traditional supervised neural-network training is formulated as an unconstrained optimization problem of minimizing the sum of squared errors of the output over all training patterns:  min  w  E(w) =  n  X ...",
            "group": 931,
            "name": "10.1.1.41.3121",
            "keyword": "",
            "title": "Constrained Formulations for Neural Network Training and Their Applications to Solve the Two-Spiral Problem"
        },
        {
            "abstract": "The processing performed by a feed-forward neural network is often interpreted  through use of decision hyperplanes at each layer. The adaptation  process, however, is normally explained using an error landscape picture. In  this paper the actual dynamics of the decision hyperplanes is investigated. As  a result a mechanical analogy is drawn with a system of spins acted upon by  forces. The spin objects have a variable mass, and relaxation in the system  is represented by increased overall system mass, also termed \"cooling\". The  analogy is used to clarify the dynamics of learning, information storage and robustness,  and in particular the functioning of the process of back-propagation.  Learning deadlocks and local minima are explained. The concept of network  \"plasticity\" is introduced, and used to understand destructive relearning, and  how it may thus be better avoided.  Practical benefits from this interpretation are illustrated with hints for optimal  weight initializations, avoi...",
            "group": 932,
            "name": "10.1.1.41.3198",
            "keyword": "",
            "title": "Hyperplane \"Spin\" Dynamics, Network Plasticity and Back-Propagation Learning"
        },
        {
            "abstract": "We survey the computational geometry relevant to finite element mesh  generation. We especially focus on optimal triangulations of geometric domains  in two- and three-dimensions. An optimal triangulation is a partition  of the domain into triangles or tetrahedra, that is best according to some  criterion that measures the size, shape, or number of triangles. We discuss  algorithms both for the optimization of triangulations on a fixed set of vertices  and for the placement of new vertices (Steiner points). We briefly survey the  heuristic algorithms used in some practical mesh generators.  1. Introduction  Computational geometry claims the two aims of solving practical problems and producing beautiful mathematics. There is a natural tension between these goals: the most elegant formulation of a problem rarely occurs in practice. But surprisingly often the aims complement each other. This chapter discusses the interplay between an important practical problem---finite element mesh gener...",
            "group": 933,
            "name": "10.1.1.41.3221",
            "keyword": "",
            "title": "Mesh Generation And Optimal Triangulation"
        },
        {
            "abstract": ":  A strategy for finding approximate solutions to discrete optimization problems with inequality constraints using mean field neural networks is presented. The constraints x  0 are encoded by x\\Theta(x) terms in the energy function. A careful treatment of the mean field approximation for the self-coupling parts of the energy is crucial, and results in an essentially parameter-free algorithm.  This methodology is extensively tested on the knapsack problem of size up to 10  3  items. The algorithm scales like NM for problems with N items and M constraints. Comparisons are made with an exact branch and bound algorithm when this is computationally possible (N  30). The quality of the neural network solutions consistently lies above 95% of the optimal ones at a significantly lower CPU expense. For the larger problem sizes the algorithm is compared with simulated annealing and a modified linear programming approach. For \"non-homogeneous\" problems these produce good solutions, whereas for th...",
            "group": 934,
            "name": "10.1.1.41.3436",
            "keyword": "",
            "title": "Neural Networks for Optimization Problems with Inequality Constraints - the Knapsack Problem"
        },
        {
            "abstract": "The objective for this project was to develop and test a method and software system for estimating the pose of artificial knee implant components from X-ray fluoroscopy images. The topic is of great interest to the field of orthopedics, due to the occurrence of premature failure in today's implant designs. Our approach was to use supervised iterative optimization. Specifically, a simulated annealing algorithm was used to iteratively adjust the pose of the component model to minimize the error, or discrepancy, between the predicted image of the model and the actual image. A software system was developed that allowed an operator to visualize the progress of the model fitting process, and intervene if necessary to correct any errors and guide the system to a correct solution. The resulting system was formally evaluated to determine its accuracy and repeatability. On synthetic images, we measured the mean error translation to be 0.005 mm and the mean error of rotation to be 0.0015 degree. ...",
            "group": 935,
            "name": "10.1.1.41.3722",
            "keyword": "",
            "title": "Measurement of Implant Component Position and Orientation from X-ray Images"
        },
        {
            "abstract": "Neural networks have traditionally been applied to recognition problems, and most learning  algorithms are tailored to those problems. We discuss the requirements of learning for generalization,  where the traditional methods based on gradient descent have limited success. We  present a new stochastic learning algorithm based on simulated annealing in weight space. We  verify the convergence properties and feasibility of the algorithm. We also describe an implementation  of the algorithm and validation experiments.  1. Introduction  Neural networks are being applied to a wide variety of applications from speech generation[1], to handwriting recognition[2]. Last decade has seen great advances in design of neural networks for a class of problems called recognition problems, and in design of learning algorithms[3-5, 5-7]. The learning of weights for neural network for many recognition problem is no longer a difficult task. However, designing a neural network for generalization problem is ...",
            "group": 936,
            "name": "10.1.1.41.3833",
            "keyword": "",
            "title": "Generalization by Neural Networks"
        },
        {
            "abstract": "This paper presents heuristic approximation algorithms and methods to find lower bounds to approximate the Minimum Linear Arrangement problem and evaluates and compares experimentaly their behaviour when they are applied to sparse graphs. The low number of theoretical results for this problem motivates its experimental study. The algorithms presented and analyzed belong to the families of Successive Augmentation algorithms (also called greedy algorithms), local search algorithms (Hillclimbing, Metropolis and Simulated Annealing) and Spectral Sequencing. The empirical results are based on two random models and \"real life\" graphs. The conclusion is that the best approximations are obtained using Simulated Annealing, which involves a large amount of computation time. However, solutions found by Spectral Sequencing are also good and can be found in radically less time. We remark that the performance of the algorithms heavily depends on the kind of graph. 1 Introduction and basic results  G...",
            "group": 937,
            "name": "10.1.1.41.4278",
            "keyword": "",
            "title": "Approximation Heuristics and Benchmarkings for the MinLA Problem"
        },
        {
            "abstract": "We describe the capabilities of and algorithms used in a new FPGA CAD tool,  Versatile Place and Route (VPR). In terms of minimizing routing area, VPR outperforms  all published FPGA place and route tools to which we can compare.  Although the algorithms used are based on previously known approaches, we  present several enhancements that improve run-time and quality. We present placement  and routing results on a new set of large circuits to allow future benchmark  comparisons of FPGA place and route tools on circuit sizes more typical of today's  industrial designs.  VPR is capable of targeting a broad range of FPGA architectures, and the source  code is publicly available. It and the associated netlist translation / clustering tool  VPACK have already been used in a number of research projects worldwide, and  should be useful in many areas of FPGA architecture research.  ",
            "group": 938,
            "name": "10.1.1.41.4505",
            "keyword": "",
            "title": "VPR: A New Packing, Placement and Routing Tool for FPGA Research"
        },
        {
            "abstract": "In this report we show how existing real-time scheduling theory, developed to analyse the scheduling of processors and network communications, can also be applied to the problem of guaranteeing the performance of multi-media information streams read from a disk drive. We develop simple analysis that is independent of disk layout information; a small example is analysed, using a simple tool embodying the analysis. The analysis is then extended to take account of disk layout information; the example is then re-analysed using this less pessimistic model. One of the advantages of the proposed approach is that it gives a precise means of determining the impact of buffer size on disk utilisation. 1. INTRODUCTION  The technological problems associated with multi-media computing has received much attention recently. One of the critical difficulties in multi-media is the real-time scheduling problem of ensuring that multiple multi-media data streams from a disk drive (or a collection of disk dr...",
            "group": 939,
            "name": "10.1.1.41.4933",
            "keyword": "",
            "title": "SCHEDULING Hard REAL-TIME Multi-Media Disk Traffic"
        },
        {
            "abstract": "The benefits of a behavioral synthesis design methodology, including higher designer productivity and shorter time-to-market, are the results of allowing the designer to use more abstract and familiar specifications. One of the most common and familiar abstractions used by hardware and software designers is the array, which allows the specification of a set of values that have a unique index associated with each element. A behavioral synthesis tool that accepts specifications containing arrays must map the storage implied by the arrays to memory in an implementation of that specification. In many data-intensive applications, these memory design decisions have a larger impact on the cost, performance, and power of the implementation than any other design decision made by a behavioral synthesis tool. Most behavioral synthesis tools, however, fail to separate the concepts of array specification and memory implementation, which severely restricts the span of designs that can be explored gi...",
            "group": 940,
            "name": "10.1.1.41.5286",
            "keyword": "",
            "title": "Synthesis of Application-Specific Memory Structures"
        },
        {
            "abstract": "Heterogeneous computing (HC) environments are well suited to meet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (defined as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a difficult problem, because comparisons are often clouded by different underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined are Opportunistic Load Balancing, User-Directed Assignment, Fast Greedy, Min-min, Max-min, Greedy, Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu, and A*. This study provides one even basis for comparison and insights into c...",
            "group": 941,
            "name": "10.1.1.41.5581",
            "keyword": "are Opportunistic Load BalancingUser-Directed AssignmentFast GreedyMin-minMax-minGreedyGenetic Algorithm",
            "title": "A Comparison Study of Static Mapping Heuristics for a Class of Meta-tasks on Heterogeneous Computing Systems"
        },
        {
            "abstract": "The question of satisfiability for a given propositional formula arises in many areas of AI. Especially finding a model for a satisfiable formula is very important though known to be NP-complete. There exist complete algorithms for satisfiability testing like the Davis-Putnam-Algorithm, but they often do not construct a satisfying assignment for the formula. Furthermore, existing complete procedures are not practically applicable for more than 400 or 500 variable problems and in practice take too much time to find a solution. Recently, a (in practice) very fast, though incomplete, procedure, the model generating algorithm GSAT, has been introduced and several refined variants were created. Another method is Simulated Annealing (SA). Both approaches have already been compared with different results. We clarify these differences and do a more elaborate comparison using identical problems for both algorithms. We show that the performance of an already optimized variant of GSAT and an ordi...",
            "group": 942,
            "name": "10.1.1.41.5615",
            "keyword": "",
            "title": "GSAT and Simulated Annealing -- a comparison"
        },
        {
            "abstract": "Heterogeneous computing (HC) environments are well suited to meet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (defined as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a difficult problem, because comparisons are often clouded by different underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined are Opportunistic Load Balancing, User-Directed Assignment, Fast Greedy, Min-min, Max-min, Greedy, Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu, and A*. This study provides one even basis for comparison and insights into c...",
            "group": 943,
            "name": "10.1.1.41.5675",
            "keyword": "are Opportunistic Load BalancingUser-Directed AssignmentFast GreedyMin-minMax-minGreedyGenetic Algorithm",
            "title": "A Comparison Study of Static Mapping Heuristics for a Class of Meta-tasks on Heterogeneous Computing Systems"
        },
        {
            "abstract": "Wavelets can be thought of as a set of well-localized basis functions with very good approximation properties. The difficulty in applying wavelet approximation to high-dimensional data is that the number of basis functions increases exponentially with the number of dimensions, making the application of standard mathematical methods for determining coefficients diffic ult. We propose a modeling methodology that uses multidimensional cubic B-spline wavelets whose co efficients are determined by a nonlinear optimization procedure that combines simulated anneali ng with hill climbing. 1 Introduction  The wavelets can be viewed as an alternative to Fourier analysis for nonlinear modeling. The most important difference between Fourier analysis and wavelet analysis is probably in the nature of basis functions used: the Fourier basis functions are global while the wavelets basis functions are local. The local basis functions with certain other attractive approximation properties have made the ...",
            "group": 944,
            "name": "10.1.1.41.5937",
            "keyword": "",
            "title": "High-Dimensional Wavelet Modeling"
        },
        {
            "abstract": "Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This paper introduces diffracting trees, novel distributed-parallel structures for shared counting and load balancing. Diffracting trees combine a randomized coordination method together with a combinatorial data structure, to yield a logarithmic depth counter that improves on the log  2  depth of counting networks, and overcomes the resiliency drawbacks of combining trees. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message passing architectures, shows that diffracting trees scale better and are more robust then both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters. Diffracting trees have already been used to implement highly efficient producer /consumer queues, and we believ...",
            "group": 945,
            "name": "10.1.1.41.6113",
            "keyword": "",
            "title": "Diffracting Trees"
        },
        {
            "abstract": "Recurrent neural networks can be used to map input sequences to output sequences,  such as for recognition, production or prediction problems. However, practical  difficulties have been reported in training recurrent neural networks to perform  tasks in which the temporal contingencies present in the input/output sequences span  long intervals. We showwhy gradient based learning algorithms face an increasingly  difficult problem as the duration of the dependencies to be captured increases. These  results expose a trade-off between efficient learning by gradient descent and latching  on information for long periods. Based on an understanding of this problem,  alternatives to standard gradient descent are considered. ",
            "group": 946,
            "name": "10.1.1.41.7128",
            "keyword": "",
            "title": "Learning Long-Term Dependencies with Gradient Descent is Difficult"
        },
        {
            "abstract": "In this paper, we propose a new probabilistic sampling procedure and its application in simulated annealing (SA). The new procedure uses Bayesian analysis to evaluate samples made already and draws the next sample based on a density function constructed through Bayesian analysis. After integrating our procedure in SA, we apply it to solve a set of optimization benchmarks. Our results show that our proposed procedure, when used in SA, is very effective in generating highquality samples that are more reliable and robust in leading to global solutions.  1 Introduction  It's well known that simulated annealing (SA) [3, 4] converges asymptotically to a global minimum for any nonlinear, unconstrained nonconvex problem. However, to achieve global convergence, SA normally requires a large number of function evaluations. In order to improve the efficiency of SA, one way is to reduce the number of function evaluations by generating high-quality samples that have high probability of hitting a glo...",
            "group": 947,
            "name": "10.1.1.41.7364",
            "keyword": "",
            "title": "Data Sampling Using Bayesian Analysis and its Applications in Simulated Annealing"
        },
        {
            "abstract": "We describe a simulated annealing approach for solving the buffer allocation  problem in reliable production lines. The problem entails the determination of near  optimal buffer allocation plans in large production lines with the objective of maximising  their average throughput. The latter is calculated utilising a decomposition  method. The allocation plan is calculated subject to a given amount of total buffer  slots in a computationally efficient way.  1 Introduction and Literature Review  Buffer allocation is a major optimisation problem faced by manufacturing systems designers. It has to do with devising an allocation plan for distributing a certain amount of buffer space among the intermediate buffers of a production line. This problem is a very complex task that must account for the random fluctuations in mean production rates of the individual workstations of the lines. To solve this problem there is a need of two different tools. The first is a tool that calculates the perfor...",
            "group": 948,
            "name": "10.1.1.41.7793",
            "keyword": "",
            "title": "A Simulated Annealing Approach for Buffer Allocation in Reliable Production Lines"
        },
        {
            "abstract": "Recurrent neural networks can be used to map input sequences to output sequences,  such as for recognition, production or prediction problems. However, practical  difficulties have been reported in training recurrent neural networks to perform  tasks in which the temporal contingencies present in the input/output sequences span  long intervals. We show why gradient based learning algorithms face an increasingly  difficult problem as the duration of the dependencies to be captured increases. These  results expose a trade-off between efficient learning by gradient descent and latching  on information for long periods. Based on an understanding of this problem,  alternatives to standard gradient descent are considered. ",
            "group": 949,
            "name": "10.1.1.41.7870",
            "keyword": "",
            "title": "Learning Long-Term Dependencies with Gradient Descent is Difficult"
        },
        {
            "abstract": "The problem of logistics and resource management in disease control projects in the developing world can hardly be understated. One example is the occurance of regional imbalances in supply. A prototype system, based upon evolutionary and `meta-heuristic' optimisation techniques is described that recommends a plan for the redistribution of available resources to minimise shortages. Evaluation of the system on data from real world situations indicated that the generation of good, feasible redistribution plans is possible even on large datasets. Comparison of the optimisers showed that evolutionary techniques perform poorly on this problem compared to stochastic hillclimbing. 1 Introduction  Dealing with aid projects in the developing world can be made difficult by the fact that though an effective policy exists, the actual implementation is often more difficult. For instance, situations may occur where regional imbalances arise: an example would be a treatment site with sufficient diagn...",
            "group": 950,
            "name": "10.1.1.41.8313",
            "keyword": "",
            "title": "An Evolutionary/Meta-Heuristic Approach To Emergency Resource Redistribution In The Developing World"
        },
        {
            "abstract": "In this paper, we present new results on the automated generalization of performance-related heuristics learned for knowledge-lean applications. We study methods to statistically generalize new heuristics learned for some small subsets of a problem space (using methods such as genetics-based learning) to unlearned problem subdomains. Our method uses a new statistical metric called probability of win. By assessing the performance of heuristics in a range-independent and distribution-independent manner, we can compare heuristics across problem subdomains in a consistent manner. To illustrate our approach, we show experimental results on generalizing heuristics learned for sequential circuit testing, VLSI cell placement and routing, and branch-and-bound search. We show that generalization can lead to new and robust heuristics that perform better than the original heuristics across problem instances of different characteristics.  1 Introduction  Heuristics or heuristic methods (HMs), in ge...",
            "group": 951,
            "name": "10.1.1.41.8698",
            "keyword": "",
            "title": "Statistical Generalization Of Performance-Related Heuristics for Knowledge-Lean Applications"
        },
        {
            "abstract": "Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called ANFIS (Adaptive-Network-based Fuzzy Inference System), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed.   ",
            "group": 952,
            "name": "10.1.1.42.152",
            "keyword": "",
            "title": "Neuro-Fuzzy Modeling and Control"
        },
        {
            "abstract": "Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called ANFIS (Adaptive-Network-based Fuzzy Inference System), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed.   ",
            "group": 953,
            "name": "10.1.1.42.152",
            "keyword": "",
            "title": "Neuro-Fuzzy Modeling and Control"
        },
        {
            "abstract": "|Most models concerned with real-world applications can be improved in structuring data and incorporating knowledge about the domain. In our problem of radio electrical wave dying down prediction for mobile communication, a geographic database can be divided in contextual subsets, each representing an homogeneous domain where a predictive model performs better. More precisely, by clustering the input space, a predictive model (here a multilayer perceptron) can be trained on each subspace. Various unsupervised algorithms for clustering were evaluated (Kohonen 's maps. Desieno's algorithm, Neural gas, Growing Neural Gas, Buhmann's algorithm) to obtain classes homogeneous enough to decrease the predictive error of the radio electrical wave prediction.  I. Introduction  A modular approach is often chosen when a problem is too complex to be eciently carried out by a single classication. The modules can be built and optimized in one step, like in mixtures of experts [4]. They can also be co...",
            "group": 954,
            "name": "10.1.1.42.288",
            "keyword": "",
            "title": "Unsupervised Connectionist Clustering Algorithms for a better Supervised Prediction: Application to a radio communication problem"
        },
        {
            "abstract": "Existing FPGA-based logic emulators suffer from limited inter-chip communication bandwidth, resulting in low gate utilization (10 to 20 percent). This resource imbalance increases the number of chips needed to emulate a particular logic design and thereby decreases emulation speed, since signals must cross more chip boundaries. Current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). These logical wires are not active simultaneouslyand are only switched at emulation clock speeds. Virtual wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. A virtual wire represents a connection from a logical output on one FPGA to a logical input on another FPGA. Virtual wires not only increase usable bandwidth, but also relax the absolute limits imposed o...",
            "group": 955,
            "name": "10.1.1.42.332",
            "keyword": "FPGAlogic emulationprototypingreconfigurable architecturesstatic routingvirtual wires",
            "title": "Virtual Wires: Overcoming Pin Limitations in FPGA-based Logic Emulators"
        },
        {
            "abstract": "Computers that &quot;program themselves&quot;; science fact or fiction? Genetic Programming uses novel optimisation techniques to &quot;evolve &quot; simple programs; mimicking the way humans construct programs by progressively re-writing them. Trial programs are repeatedly modified in the search for &quot;better/fitter &quot; solutions. The underlying basis is Genetic Algorithms (GAs). Genetic Algorithms, pioneered by Holland [Hol92], Goldberg [Gol89] and others, are evolutionary search techniques inspired by natural selection (i.e survival of the fittest). GAs work with a &quot;population &quot; of trial solutions to a problem, frequently encoded as strings, and repeatedly select the &quot;fitter &quot; solutions, attempting to evolve better ones. The power of GAs is being demonstrated for an increasing range of applications; financial, imaging, VLSI circuit layout, gas pipeline control and production scheduling [Dav91]. But one of the most intriguing uses of GAs- driven by Koza [Koz92]- is automatic program generation. Genetic Programming applies GAs to a &quot;population &quot; of programs- typically encoded as tree-structures. Trial programs are evaluated against a &quot;fitness function &quot; and the best solutions selected for modification and re-evaluation. This modification-evaluation cycle is repeated",
            "group": 956,
            "name": "10.1.1.42.454",
            "keyword": "Machine LearningGenetic AlgorithmsGenetic Programming. 1",
            "title": "Genetic programming -- computers using \"natural selection\" to generate programs"
        },
        {
            "abstract": "In this chapter, we present new results on the automated generalization of performance-related heuristics learned for knowledge-lean applications. By first applying genetics-based learning to learn new heuristics for some small subsets of test cases in a problem space, we study methods to generalize these heuristics to unlearned subdomains of test cases. Our method uses a new statistical metric called probability of win. By assessing the performance of heuristics in a range-independent and distribution-independent manner, we can compare heuristics across problem subdomains in a consistent manner. To illustrate our approach, we show experimental results on generalizing heuristics learned for sequential circuit testing, VLSI cell placement and routing, branch-and-bound search, and blind equalization. We show that generalization can lead to new and robust heuristics that perform better than the original heuristics across test cases of different characteristics.  Keywords: Generalization, ...",
            "group": 957,
            "name": "10.1.1.42.538",
            "keyword": "Generalizationgenetics-based learningheuristicsmachine learningprobability of win",
            "title": "Statistical Generalization Of Performance-Related Heuristics For Knowledge-Lean Applications"
        },
        {
            "abstract": "We address the problem of maximizing application speedup through runtime, self-selection of an appropriate number of processors on which to run. Automatic, runtime selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data or time dependent. We propose the use of a runtime system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job's processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our runtime system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation.  1. Introduction  We consider the problem of maximizing the speedup of an individual para...",
            "group": 958,
            "name": "10.1.1.42.1235",
            "keyword": "",
            "title": "Maximizing Speedup through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "This paper is structured as follows. Section 2 introduces application and machine representations  that are used to model the performance characteristics of parallel static applications on parallel machines. Section 3 gives a detailed study on the structure of the phase space (or landscape) of the TAP. Section 4 is dedicated to the geometrical phase transition occurring in the TAP. In section 5 the following experimental methods are presented: Simulated Annealing (SA) [8], for finding optima, and Weinberger correlation for phase space structure characterisation [21]. In section 6 experimental results are presented, which are discussed in section 7. Finally, some concluding remarks and directions for future work are given in section 8. 2 Application and Machine Models",
            "group": 959,
            "name": "10.1.1.42.1644",
            "keyword": "Contents",
            "title": "Properties of the Task Allocation Problem"
        },
        {
            "abstract": "This paper describes about a new method for reconstructing a shape of a skin surface replica from three shading images taken with three different lightings. Since the shading images include shadows caused by surface height fluctuation, the conventional photometric stereo method is not suitable for reconstructing its surface shape. In the proposed method, the evaluation function of the surface shape is defined in consideration of the effects of shadow, then the shape is reconstructed by optimizing the evaluation using simulated annealing. The experiments to reconstruct the shape from synthesized images and real images demonstrate that the proposed method is effective for shape reconstruction from shading images which include shadows. 1 Introduction",
            "group": 960,
            "name": "10.1.1.42.1711",
            "keyword": "",
            "title": "Shape Reconstruction of Skin Surface from Shading Images Using Simulated Annealing"
        },
        {
            "abstract": "We consider the problem of finding good  finite-horizon policies for POMDPs under the  expected reward metric. The policies considered  are free finite-memory policies with  limited memory; a policy is a mapping from  the space of observation-memory pairs to the  space of action-memory pairs (the policy updates  the memory as it goes), and the number  of possible memory states is a parameter  of the input to the policy-finding algorithms.  The algorithms considered here are preliminary  implementations of three search heuristics:  local search, simulated annealing, and  genetic algorithms. We compare their outcomes  to each other and to the optimal policies  for each instance. We compare run times  of each policy and of a dynamic programming  algorithm for POMDPs developed by Hansen  that iteratively improves a finite-state controller  --- the previous state of the art for  finite memory policies. The value of the best  policy can only improve as the amount of  memory increases, up to ...",
            "group": 961,
            "name": "10.1.1.42.2335",
            "keyword": "",
            "title": "My Brain is Full: When More Memory Helps"
        },
        {
            "abstract": "Gibbs random field (GRF) models work well for synthesizing complex natural-looking image data with a small number of parameters; however, estimation methods for these parameters have a lot of problems. This paper addresses the analysis problem in a new way by examining the role of the temperature parameter of the Gibbs distribution. Studies of the model energy with respect to the temperature are used to indicate pattern equilibrium and regions of different behavior, analogous to the existence of distinct phases in a physical system. The results on equilibrium and regions of different \"phases\" are offered as explanations for some of the peculiar behavior of current estimation algorithms.  1 Gibbs random fields  This paper focuses on the discrete Gibbs random field (GRF), defined as follows. Let an image be represented by a finite rectangular M \\Theta N lattice  S with a neighborhood structure N = fNs ; s 2 Sg where Ns ` S is the set of sites which are neighbors of the site s 2 S. Every ...",
            "group": 962,
            "name": "10.1.1.42.2799",
            "keyword": "",
            "title": "Gibbs Random Fields: Temperature And Parameter Analysis"
        },
        {
            "abstract": "Evolution Strategies apply mutation and recombination operators in order to create their offspring. Both  operators have a different role in the evolution process: recombination should combine information of different  individuals, while mutation performs a kind of random walk to introduce new values. In an ES these operators  are always applied together, but their different roles suggest that it might be better to apply them independently  and at different rates. In order to do so the ES has been split into two levels. The resulting Modular Evolution  Strategy consists of a population of local optimizers and a distributed population manager. Both parts have their  own specific role in the optimization process. As a result of its modularity this method can be adapted more easily  to specific classes of numerical optimization problems, and introduction of adaptive mechanisms is relatively  easy. A further interesting aspect about this algorithm is that it does not need any global commun...",
            "group": 963,
            "name": "10.1.1.42.2896",
            "keyword": "",
            "title": "Cs-R9559 1995"
        },
        {
            "abstract": ".  In this paper, we introduce a rather straightforward but fundamental observation concerning the convergence of the general iteration process  x  k+1  = x  k  \\Gamma ff(x  k  )B(x  k  )  \\Gamma1  rf(x  k  ) for minimizing a function f(x). We give necessary and sufficient conditions for a stationary point of  f(x) to be a point of strong attraction of the iteration process. We will discuss various ramifications of this fundamental result, particularly for nonlinear least squares problems.  Key words. Strong attraction, weak repulsion, selective minimization.  AMS subject classifications. 65K05, 90C30  1. Introduction. We consider the unconstrained minimization problem minf(x); (1.1) where f : !  n  ! ! is assumed to be twice (Frechet) differentiable, and the general iteration:  x  k+1  = x  k  \\Gamma ff  k  (B  k  )  \\Gamma1  rf(x  k  ): (1.2) This iterative framework has been studied extensively and many results are available for various choices of ff  k  and B  k  that guarantee con...",
            "group": 964,
            "name": "10.1.1.42.3216",
            "keyword": "Key words. Strong attractionweak repulsionselective minimization",
            "title": "On Convergence of Minimization Methods: Attraction, Repulsion and Selection"
        },
        {
            "abstract": "We compare the use of stochastic dynamic programming (SDP), Neural Networks and a simple approximation rule for calculating the real option value of a flexible production system. While SDP yields the best solution to the problem, it is computationally prohibitive for larger settings. We test two approximations of the value function and show that the results are comparable to those obtained via SDP. These methods have the advantage of a high computational performance and of no restrictions on the type of process used. Our approach is not only useful for supporting large investment decisions, but it can also be applied in the case of routine decisions like the determination of the production program when stochastic profit margins occur. Keywords  Real Options, Neural Networks, Capital Budgeting, Simulated Annealing, Flexible Manufacturing Systems, Dynamic Programming 1 Motivation  While in finance the importance of option valuation is well established, option valuation techniques have re...",
            "group": 965,
            "name": "10.1.1.42.3736",
            "keyword": "Neural NetworksCapital BudgetingSimulated AnnealingFlexible Manufacturing SystemsDynamic Programming",
            "title": "Neural Networks, Stochastic Dynamic Programming and a Heuristic for Valuing Flexible Manufacturing Systems"
        },
        {
            "abstract": ". This paper shows how parallelism has been integrated into  SCOOP, a C++ class library for solving optimisation problems. After a  description of the modeling and the optimisation parts of SCOOP, two  new classes that permit parallel optimisation are presented: a class whose  only purpose is to handle messages and a class for managing optimiser  and message handler objects. Two of the most interesting aspects of  SCOOP, modularity and generality, are preserved by clearly separating  problem representation, solution techniques and parallelisation scheme.  This allows the user to easily model a problem and construct a parallel  optimiser for solving it by combining existing SCOOP classes.  1 Introduction  SCOOP (SINTEF Constrained Optimisation Package) [2], is a generic, objectoriented C++ class library for modeling and solving optimisation problems. The library has been used for solving large--scale real--life problems in the fields of vehicle routing and forestry management [7]. Origi...",
            "group": 966,
            "name": "10.1.1.42.4203",
            "keyword": "",
            "title": "Parallel Optimisation in the SCOOP Library"
        },
        {
            "abstract": "Manufacturing enterprises are now moving towards open architectures for integrating their activities with those of their suppliers, customers and partners within wide supply chain networks. Agent-based technology provides a natural way to design and implement such environments. This paper presents an agent-based manufacturing enterprise infrastructure. After a brief review of recent advancements in this domain, we describe the main features of the proposed infrastructure and the functions of its components. A machine-centered dynamic scheduling/rescheduling mechanism is then detailed and a prototype implementation is presented. ",
            "group": 967,
            "name": "10.1.1.42.4551",
            "keyword": "Enterprise integrationdistributed manufacturing systemsmanufacturing schedulingagentmulti-agent",
            "title": "An Agent-Based Manufacturing Enterprise Infrastructure for Distributed Integrated Intelligent Manufacturing Systems"
        },
        {
            "abstract": "Standard graphics systems encode pictures by assigning an address and colour attribute for each point of the object resulting in a long list of addresses and attributes. Fractal geometry enables a newer class of geometrical shapes to be used to encode whole objects, thus image compression is achieved. Compression ratios of 10,000:1 have been claimed by researchers  1  in this field. The fractal equations describing these shapes are very simple equations. Specifically, iterated function system (IFS)  codes are investigated. The difficult inverse problem of finding a suitable IFS code whose fractal image is to represent the real image and hence achieve compression is investigated through the use of: a) a library of IFS codes and complex moments,  b) the method of simulated annealing, for solving non-linear equations of many parameters. Image Compression  Image compression is reducing the number of bits required to represent an image in such a way that either an exact replica of the image...",
            "group": 968,
            "name": "10.1.1.42.5000",
            "keyword": "",
            "title": "Fractal Image Compression"
        },
        {
            "abstract": ". Satisfiability is a class of NP-complete problems that model a wide range of realworld applications. These problems are difficult to solve because they have many local minima in their search space, often trapping greedy search methods that utilize some form of descent. In this paper, we propose a new discrete Lagrange-multiplier-based global-search method (DLM) for solving satisfiability problems. We derive new approaches for applying Lagrangian methods in discrete space, show that equilibrium is reached when a feasible assignment to the original problem is found, and present heuristic algorithms to look for equilibrium points. Our method and analysis provides a theoretical foundation and generalization of local search schemes that optimize the objective alone and penalty-based schemes that optimize the constraints alone. In contrast to local search methods that restart from a new starting point when a search reaches a local trap, the Lagrange multipliers in DLM provide a force to le...",
            "group": 969,
            "name": "10.1.1.42.5550",
            "keyword": "",
            "title": "A Discrete Lagrangian-Based Global-Search Method for Solving Satisfiability Problems"
        },
        {
            "abstract": "This thesis explores how recurrent neural networks can be exploited for learning certain high-dimensional mappings. Recurrent networks are shown to be as powerful as Turing machines in terms of the class of functions they can compute. Given this computational power, a natural question to ask is how recurrent networks can be used to simplify the problem of learning from examples. Some researchers have proposed using recurrent networks for learning fixed point mappings that can also be learned on a feedforward network even though learning algorithms for recurrent networks are more complex. An important question is whether recurrent networks provide an advantage over feedforward networks for such learning tasks. The main problem with learning high-dimensional functions is the curse of dimensionality which roughly states that the number of examples needed to learn a function increases exponentially with input dimension. Reducing the dimensionality of the function being learned is therefore...",
            "group": 970,
            "name": "10.1.1.42.6078",
            "keyword": "",
            "title": "Using Recurrent Networks for Dimensionality Reduction"
        },
        {
            "abstract": "In this paper, we present new results on the automated generalization of performance-related heuristics learned for knowledge-lean applications. By first applying genetics-based learning to learn new heuristics for some small subsets of test cases in a problem space, we study methods to generalize these heuristics to unlearned subdomains of test cases. Our method uses a new statistical metric called probability of win. By assessing the performance of heuristics in a range-independent and distribution-independent manner, we can compare heuristics across problem subdomains in a consistent manner. To illustrate our approach, we show experimental results on generalizing heuristics learned for sequential circuit testing, VLSI cell placement and routing, branch-and-bound search, and blind equalization. We show that generalization can lead to new and robust heuristics that perform better than the original heuristics across test cases of different characteristics.  Keywords: Generalization, ge...",
            "group": 971,
            "name": "10.1.1.42.6154",
            "keyword": "Generalizationgenetics-based learningheuristicsmachine learningprobability of win",
            "title": "Statistical Generalization Of Performance-Related Heuristics For Knowledge-Lean Applications"
        },
        {
            "abstract": ". The Traveling Salesman Problem is a standard test-bed for algorithmic  ideas. Currently, there exist a large number of nature-inspired algorithms to the TSP  and for some of these approaches very good performance is reported. In particular,  the best performing approaches combine solution modification or construction with  the subsequent application of a fast but powerful local search algorithm. Yet, comparisons  between these algorithms with respect to their performance are often difficult  due to different implementation choices of which the choice of the local search  algorithm is particularly critical. In this article we experimentally compare some  of the best performing recently proposed nature-inspired algorithms which improve  solutions by using a same local search algorithm and investigate their performance  on a large set of benchmark instances.  1 Introduction  In the Traveling Salesman Problem (TSP) the task is to find a shorted closed tour through a given set of n cities...",
            "group": 972,
            "name": "10.1.1.42.6635",
            "keyword": "",
            "title": "A Comparison of Nature Inspired Heuristics on the Traveling Salesman Problem"
        },
        {
            "abstract": "In this report we describe the conversion of a simple Master-Worker parallel program from global blocking communications to non-blocking communications. The program is MPI-based and has been run on different computer architectures. By moving the communication to the background the processors can use the former waiting time for computation. However we find that the computing time increases by the time the communication time decreases in the used MPICH implementation on a cluster of workstations. Also using non-global communication instead of the global communication slows the algorithm down on computers with optimized global communication routines like the Cray T3D.",
            "group": 973,
            "name": "10.1.1.42.7888",
            "keyword": "MPIMPICHblocking communicationnon-blocking communication",
            "title": "Blocking vs. Non-blocking Communication under MPI on a Master-Worker Problem"
        },
        {
            "abstract": "This paper discusses product variety design under optimization viewpoint. Product variety design means the challenge to simultaneously design multiple products toward higher optimality beyond ordinary design methods for a single product. The paper explores the possibilities of design optimization for product variety under fixed product architecture. Such optimization demands to determine the contents of modules and their combinations under modular architecture. This indicates that product variety optimization includes three classes of optimization problems, attribute assignment, module combination and simultaneous design of both. The paper formulates problem classification, domains and situations of such optimization problems. Further, the paper demonstrates two typical optimization examples through aircraft design for simultaneous attribute optimization and through design of television circuit boards for module combination, respectively. The paper concludes with the roles of problem c...",
            "group": 974,
            "name": "10.1.1.42.7931",
            "keyword": "Design optimizationProduct familyModular architectureMathematical modeling",
            "title": "Product Variety Optimization Under Modular Architecture"
        },
        {
            "abstract": ": For most computationally intractable problems there exists no heuristic which performs  best on all instances. Usually, a heuristic characterized as best will perform good on the majority of instances  but leave a minority on which other heuristics do better. In priority rule-based scheduling,  attempts to remedy this have been made by combining simple priority rules in a fixed and predetermined  way. We investigate another way, viz. the design of adaptive control schemes which dynamically  combine algorithms as appropriate, taking into account instance-specific knowledge. We scrutinize  recently proposed algorithmic approaches from the open literature. Although these have been  used in various settings (e.g. lotsizing and scheduling, course scheduling), a thorough experimental  investigation, comparing their performance on standard benchmark instances to that of other contemporary  methods, has been lacking. Our research aims to close this gap by validating these approaches  on one ...",
            "group": 975,
            "name": "10.1.1.42.9902",
            "keyword": "",
            "title": "Adaptive Control Schemes for Parameterized Heuristic Scheduling"
        },
        {
            "abstract": "Beginning in 1983, simulated annealing was marketed as a global optimization methodology that  mimics the physical annealing process by which molten substances cool to crystalline lattices of  minimal energy. This marketing strategy had a polarizing effect, attracting those who delighted  in metaphor and alienating others who found metaphor insufficient at best and facile at worst.  In fact, the emotional outbursts that accompany many discussions of simulated annealing are an  unfortunate distraction. Whatever its pros and cons, simulated annealing can be grounded in  rigorous mathematics. Here we provide an elementary, self-contained introduction to simulated  annealing in terms of Markov chains.  Contents  1 Introduction 2 2 Motivation 2 3 Markov Chains 3 4 Integration 6 5 Global Optimization 8 6 Simulated Annealing 9 7 Discussion 10 Associate Professor, Department of Mathematics, College of William & Mary, P.O. Box 8795, Williamsburg, VA 23187-8795 (e-mail: trosset@math.wm.edu); and...",
            "group": 976,
            "name": "10.1.1.43.126",
            "keyword": "Contents",
            "title": "What is Simulated Annealing?"
        },
        {
            "abstract": "This paper addresses the development of general purpose meta-heuristic based combinatorial optimisation algorithms. With this system, a user can specify a problem in a high level algebraic language based on linked list data structures, rather than conventional vector notation. The new formulation is much more concise than the vector form, and lends itself to search heuristics based on local neighbourhood operators . The specification is then compiled into C code, and a unique solver is generated for each particular problem. Currently, search engines for simulated annealing, greedy search and Tabu search have been implemented, and good results have been achieved over a wide range of optimisation problems. We have also implemented a special purpose computer that solves one particular optimisation problem formulated using this technique, namely the travelling salesman problem. Solvers have been produced for simulated annealing, greedy and Tabu search, and a speedup up to 16 times has been achieved over a high-end workstation.  1 Introduction",
            "group": 977,
            "name": "10.1.1.43.242",
            "keyword": "",
            "title": "Cities: Performing Combinatorial Optimisation Using Linked Lists On Special Purpose Computers"
        },
        {
            "abstract": ": A novel method is presented for automatically generating quadrilateral meshes on arbitrary two-dimensional domains. Global minimization of a potential function governs mesh formation and characteristics. Comprised of several terms, the potential function distributes the elements throughout the domain and aligns the edges of the elements to form valid connectivities. If there are any remaining unlinked element edges, the local connectivity is examined and a \"hole elimination\" algorithm is applied that successively finds alternative connectivities. Unlinked edges, representing holes in the mesh, are moved to either coalesce, or to a boundary. The components of the potential, the minimization procedure, and the connectivity refinement algorithm are presented. The method shows promise for extension to automatic three-dimensional hexahedral meshing. Initial conditions required to ensure mesh closure include an even number of elements on the boundary and a closed boundary. The desired mesh...",
            "group": 978,
            "name": "10.1.1.43.447",
            "keyword": "Automatic quadrilateral meshingglobal minimization-based meshingmeshcomputational mechanics",
            "title": "A Global Minimization-Based, Automatic Quadrilateral Meshing Algorithm"
        },
        {
            "abstract": "A common approach to parallelizing simulated annealing to generate several perturbations  to the current solution simultaneously, requiring synchronization to guarantee correct evaluation  of the cost function. The cost of this synchronization may be reduced by allowing inaccuracies  in the cost calculations. We provide a framework for understanding the theoretical implications  of this approach based on a model of processor interaction under reduced synchronization that  demonstrates how errors in cost calculations occur and how to estimate them. We show how  bounds on error in the cost calculations in a simulated annealing algorithm can be translated  into worst-case bounds on perturbations in the parameters which describe the behavior of the  algorithm.  Keywords: Parallel simulated annealing; Metropolis algorithm; Boltzmann distribution.  Simulated annealing (SA) is an iterative method for finding approximate solutions to intractable combinatorial optimization problems. For paralle...",
            "group": 979,
            "name": "10.1.1.43.464",
            "keyword": "Parallel simulated annealingMetropolis algorithmBoltzmann distribution",
            "title": "Trading Accuracy for Speed in Parallel Simulated Annealing with Simultaneous Moves"
        },
        {
            "abstract": ". In this paper we investigate benefits of classifier combination (fusion) for a multimodal system for personal identity verification. The system uses frontal face images and speech. We show that a sophisticated fusion strategy enables the system to outperform its facial and vocal modules when taken seperately. We show that both trained linear weighted schemes and fusion by Support Vector Machine classifier leads to a significant reduction of total error rates. The complete system is tested on data from a publicly available audio-visual database (XM2VTS, 295 subjects) according to a published protocol.  2 IDIAP--RR 98-18 1 Introduction  Recognition systems based on biometric features (face, voice, iris, etc ...) have received a lot of attention in recent years Most of the proposed approaches focus on mono-modal identification. The system uses a single modality to find the closest person to the user in a database. Relatively high recognition rates were obtained for different modalities...",
            "group": 980,
            "name": "10.1.1.43.699",
            "keyword": "Josef Kittler",
            "title": "Audio-Visual Person Verification"
        },
        {
            "abstract": "The space layout planning problem is one of the most difficult in architectural design. It is practically important in architectural design because it is the basis of the development of most designs. It is important in a wider context because it maps onto a large class of location-allocation problems including VLSI floorplanning, process layouts and facilities layout problems. We will use the formalization of the space layout problem as a particular case of a combinatorial optimization problem - a quadratic assignment problem [9]. As such it is NP-complete and presents all the difficulties associated with this class of problems. Over the years a number of approximate algorithms based on combinations of global and local search techniques and heuristics have been developed specifically for this class of problems. Although they are reasonably efficient for small-scale problems, the computational cost is still too high for large-scale problems. Another shortcoming of the majo...",
            "group": 981,
            "name": "10.1.1.43.950",
            "keyword": "",
            "title": "Evolving Design Genes in Space Layout Planning Problems"
        },
        {
            "abstract": "There are several powerful solvers for satisfiability (SAT), such as WSAT, Davis-Putnam, and RelSAT. However, in practice, the SAT encodings often have so many clauses that we exceed physical memory resources on attempting to solve them. This excessive size often arises because conversion to SAT, from a more natural encoding using quantifications over domains, requires expanding quantifiers. This suggests that we should \"lift\" successful SAT solvers. That is, adapt the solvers to use quantified clauses instead of ground clauses. However, it was generally believed that such lifted solvers would be impractical: Partially, because of the overhead of handling the predicates and quantifiers, and partially becau...",
            "group": 982,
            "name": "10.1.1.43.2271",
            "keyword": "",
            "title": "Lifted Search Engines For Satisfiability"
        },
        {
            "abstract": ". The ability to obtain multiple distinct solutions in a single run is an important, though often forgotten, practical advantage of genetic algorithms, and therefore methods that enhance this ability are desirable. The approach taken here is inspired by nature: sexually reproducing organisms do not mate indiscriminately --- the choice of mate has a large impact upon the fitness of the organism's offspring, and by balancing exploration and exploitation, mate (or sexual) selection can lead to speciation effects which may enhance the genetic algorithm's ability to locate multiple distinct solutions. The investigation described here shows that sexual selection was able to enhance the genetic algorithm's ability to locate and maintain multiple distinct solutions, although it can interact detrimentally with other techniques designed for the same purpose. 1 Introduction  Standard implementations of the genetic algorithm (GA) [11] select the second parent either completely at random, or on the...",
            "group": 983,
            "name": "10.1.1.43.3060",
            "keyword": "",
            "title": "An Investigation of Sexual Selection as a Mechanism for Obtaining Multiple Distinct Solutions"
        },
        {
            "abstract": "It has recently been shown that local search is surprisingly good at  finding satisfying assignments for certain classes of CNF formulas (Selman   et al. 1992). In this paper we demonstrate that the power of local  search for satisfiability testing can be further enhanced by employing  a new strategy, called \"mixed random walk\", for escaping from local  minima. We present a detailed comparison of this strategy with  simulated annealing, and show that mixed random walk is the superior  strategy on several classes of computationally difficult problem  instances. We also present results demonstrating the effectiveness of  local search with walk for solving circuit synthesis and diagnosis problems.  Finally, we show that mixed random walk improves upon the  results of Hansen and Jaumard on MAX-SAT problems. ",
            "group": 984,
            "name": "10.1.1.43.3322",
            "keyword": "",
            "title": "Local Search Strategies for Satisfiability Testing"
        },
        {
            "abstract": ". This paper presents a heuristic for directing the neighbourhood  (mutation operator) of stochastic optimisers, such as evolutionary  algorithms, so to improve performance for the flowshop sequencing problem.  Based on idle time, the heuristic works on the assumption that jobs  that have to wait a relatively long time between machines are in an  unsuitable position in the schedule and should be moved. The results  presented here show that the heuristic improves performance, especially  for problems with a large number of jobs. In addition the effectiveness  of the heuristic and search in general was found to depend upon the  neighbourhood structure in a consistent fashion across optimisers.  1 Introduction  One problem of interest in the artificial intelligence and operations research scheduling communities is the flowshop sequencing problem. The aim of this problem is, quite simply, to find a sequence of n jobs that minimises the makespan --- the time for all of the jobs to be proces...",
            "group": 985,
            "name": "10.1.1.43.3515",
            "keyword": "",
            "title": "Directing the Search of Evolutionary and Neighbourhood-Search Optimisers for the Flowshop Sequencing Problem with an Idle-Time Heuristic"
        },
        {
            "abstract": "The problem of logistics in aid projects in the developing world can hardly be understated. One problem that arises is the occurance of regional imbalances in supply. Such imbalances can seriously compromise the effectiveness of a relief program. A system is required that can suggest a workable redistribution plan. Initial results for a prototype system based on a heuristic plan generator and evolutionary/meta-heuristic optimisers, on real data, were encouraging; however more work still needs to be undertaken before the system can be deployed in the real-world. This paper describes some ongoing work to address some of these issues. 1 Introduction  Dealing with aid projects in the developing world can be made difficult by the fact that, though an effective policy exists, the actual implementation poses many obstacles. For instance, situations may occur where regional imbalances arise: an example would be a treatment site with sufficient diagnostic kits for a disease, but with no drugs t...",
            "group": 986,
            "name": "10.1.1.43.3776",
            "keyword": "",
            "title": "Emergency Resource Redistribution in the Developing World: Towards a Practical Evolutionary/Meta-Heuristic Scheduling System"
        },
        {
            "abstract": "We propose a declarative-based implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [DW97]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. PCCP also includes synchronisation and concurrency aspects. However, for the purpose of this work, the (probabilistic) CLP fragment of PCCP is sufficient.  We present a meta-interpreter for this language. This is just a standard prolog meta-interpreter, suitably extended so as to deal with probabilistic choice. For the constraint solving, the meta-interpreter exploits existing constraint handling facilities (and in more concrete terms to the SICStus 3.#6 system). This is possible because the design of PCCP...",
            "group": 987,
            "name": "10.1.1.43.4071",
            "keyword": "",
            "title": "Implementing Randomised Algorithms in Constraint Logic Programming"
        },
        {
            "abstract": ". In this paper we present a framework to simultaneously segment  portal images and register them to 3D treatment planning CT data  sets for the purpose of radiotherapy setup verification. Due to the low  resolution and low contrast of the portal image, taken with a high energy  treatment photon beam, registration to the 3D CT data is a di#cult  problem. However, if some structure can be segmented in the portal image,  it can be used to help registration, and if there is an estimate of  the registration parameters, it can help improve the segmention of the  portal image. The minimax entropy algorithm proposed in this paper  evaluates appropriate entropies in order to segment the portal image  and to find the registration parameters iteratively. The proposed algorithm  can be used, in general, for registering a high resolution image to  a low resolution image. Finally, we show the proposed algorithm's relation  to the mutual information [19] metric proposed in the literature for  multi-...",
            "group": 988,
            "name": "10.1.1.43.4136",
            "keyword": "",
            "title": "A Novel Approach for the Registration of 2D Portal and 3D CT Images for Treatment Setup Verification in Radiotherapy"
        },
        {
            "abstract": "this paper.",
            "group": 989,
            "name": "10.1.1.43.4804",
            "keyword": "",
            "title": "MO Mathematical Optimization"
        },
        {
            "abstract": ". This paper shows how parallelism has been integrated into  SCOOP, a C++ class library for solving optimisation problems. After a  description of the modeling and the optimisation parts of SCOOP, two  new classes that permit parallel optimisation are presented: a class whose  only purpose is to handle messages and a class for managing optimiser  and message handler objects. Two of the most interesting aspects of  SCOOP, modularity and generality, are preserved by clearly separating  problem representation, solution techniques and parallelisation scheme.  This allows the user to easily model a problem and construct a parallel  optimiser for solving it by combining existing SCOOP classes.  1 Introduction  SCOOP (SINTEF Constrained Optimisation Package) [2], is a generic, objectoriented C++ class library for modeling and solving optimisation problems. The library has been used for solving largescale reallife problems in the elds of vehicle routing and forestry management [5]. Originally ...",
            "group": 990,
            "name": "10.1.1.43.4922",
            "keyword": "",
            "title": "Parallel Optimisation in the SCOOP Library"
        },
        {
            "abstract": "The presence of a formal basis is of vital importance to realise an extensive  research on a real-life problem. For Sta Scheduling Problems (SSPs), this basis  seems to be missing. Apparently, this is due to the diculty of using approaches  like linear programming or traditional constraint programming techniques as tools  for modelling and solving complex SSPs. Since SSPs appear all over the world, it  would be promising indeed to make them accessible to a large forum and thereby  intensify the research.  This paper presents a wide class of SSPs by means of three fundamental constraint  types. The constraints are based mainly on practical experience with nurse  scheduling problems, and secondarily, profound studies of relevant papers. Algorithms  for reducing search space and for checking consistency are provided too,  together with meta heuristics for solving SSPs. Finally, the perspectives of the work  are emphasised by modelling and solving some very dicult, real-life problems.   ",
            "group": 991,
            "name": "10.1.1.43.4943",
            "keyword": "Sta  scheduling classconstraintsconsistencymeta heuristics",
            "title": "Nurse Scheduling Generalised"
        },
        {
            "abstract": "This paper presents a two-pass algorithm for estimating motion vectors from image sequences. In the proposed algorithm, the motion estimation is formulated as a problem of obtaining the Maximum A Posteriori in the Markov Random Field (MAP-MRF). An optimization method based on the Mean Field Theory (MFT) is opted to conduct the MAP search. The estimation of motion vectors is modeled by only two MRF's, namely, the motion vector field and unpredictable field. Instead of utilizing the line field, a truncation function is introduced to take care of the discontinuity between the motion vectors on neighboring sites. In this algorithm, a \"double threshold\" preprocessing pass is first employed to partition the sites into three regions, whereby the ensuing MFT-based pass for each MRF is conducted on one or two of the three regions. With this algorithm, no significant difference exists between the block-based and pixel-based MAP searches any more. Consequently, a good compromise between precision...",
            "group": 992,
            "name": "10.1.1.43.5576",
            "keyword": "Motion estimationMarkov Random FieldMean Field TheoryVideo processing",
            "title": "An Efficient Two-pass MAP-MRF Algorithm for Motion Estimation Based on Mean Field Theory"
        },
        {
            "abstract": "Learning methods based on dynamic programming (DP) are receiving increasing attention  in artificial intelligence. Researchers have argued that DP provides the appropriate  basis for compiling planning results into reactive strategies for real-time control,  as well as for learning such strategies when the system being controlled is incompletely  known. We introduce an algorithm based on DP, which we call Real-Time DP  (RTDP), by which an embedded system can improve its performance with experience.  RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty.  We invoke results from the theory of asynchronous DP to prove that RTDP  achieves optimal behavior in several different classes of problems. We also use the theory  of asynchronous DP to illuminate aspects of other DP-based reinforcement learning  methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to  provide a bridge between AI research on real-time planning and learni...",
            "group": 993,
            "name": "10.1.1.43.6020",
            "keyword": "",
            "title": "Learning to Act using Real-Time Dynamic Programming"
        },
        {
            "abstract": "We propose a novel person verification system for real-time face identification. The main features of the system include accurate registration of face images using a robust form of correlation, a framework for global registration of a face database using a minimum spanning tree algorithm and a method for selecting a subset of features optimal for discrimination between clients and impostors. We present results obtained through experiments on a large database with 295 subjects and show that the method is performing well in comparison with two standard methods based on elastic graph matching. ",
            "group": 994,
            "name": "10.1.1.43.6924",
            "keyword": "",
            "title": "Learning Salient Features for Real-Time Face Verification"
        },
        {
            "abstract": "There are generally three approaches to constraint satisfaction and optimization: domain-filtering, tree-search labelling and solution repair. The main attractions of repair-based algorithms over domainfiltering and/or tree-search algorithms seem to be their scalability, reactivity and applicability to optimization problems. The main detraction of the repair-based algorithms appear to be their failure to  guarantee optimality. In this paper, a repair-based algorithm, that guarantees to find an optimal solution if one exists, is presented. The search space of the algorithm is controlled by no-good backmarking, a learning process of polynomial complexity that records generic patterns of no-good partial labels  3  . These no-goods serve to avoid the repeated traversing of those failed paths of a search graph and to force the search process to jump out of a local optimum. Unlike some similar repair-based methods which usually work on complete (but possibly inconsistent) labels, the propose...",
            "group": 995,
            "name": "10.1.1.43.7392",
            "keyword": "Constraint Satisfaction and OptimizationBackmarkingLearningBackjumpingRepairbased MethodsSimulated AnnealingTabu SearchNo-good recording and No-good JustificationDynamic BacktrackingGSAT and Breakout",
            "title": "Nogood Backmarking With Min-Conflict Repair in Constraint Satisfaction and Optimization"
        },
        {
            "abstract": "The Commonality-Based Crossover Framework has been presented as a general model for designing problem specific operators. Following this model, the Common Features/Random Sample Climbing operator has been developed for feature subset selection--a binary string optimization problem. Although this problem should be an ideal application for genetic algorithms with standard crossover operators, experiments show that the new operator can find better feature subsets for classifier training. 1 INTRODUCTION  A classification system is used to predict the decision class of an object based on its features. When training a classifier, it is beneficial to use only the features relevant to prediction accuracy, and to ignore the irrelevant features [Koh95]. The benefit arises from an increase in the \"signalto -noise ratio\" of the data, and a reduction in the time required to train the classifier. Thus, the objective of feature subset selection is to identify the (most) relevant features. Feature sub...",
            "group": 996,
            "name": "10.1.1.43.7423",
            "keyword": "",
            "title": "Non-Standard Crossover for a Standard Representation -- Commonality-Based Feature Subset Selection"
        },
        {
            "abstract": "Simulated annealing (SA) is a stochastic technique for solving constraint satisfaction and optimisation problems. Research on SA usually focuses on neighbourhood operators and annealing schedules. While SA can theoretically converge to a global optimum with stationary distributions, this requires an exponential-time execution of the annealing algorithm; in practice only a short annealing schedule can be used. To devise an appropriate annealing schedule one must consider how much time to spend at high temperatures for approximating stationary distributions and how much time to spend at low temperatures for greedily approaching better solutions. Unfortunately these two things are always in conflict. Annealing schedules are problem dependent, and deciding a good one for a specific problem relies on empirical studies. This makes it difficult to balance the conflict. We argue that the process of balancing the conflict should be a dynamic procedure rather than a static one. We present a refi...",
            "group": 997,
            "name": "10.1.1.43.7712",
            "keyword": "Simulated AnnealingConstraint Satisfaction and Optimization",
            "title": "Localized Simulated Annealing in Constraint Satisfaction and Optimization"
        },
        {
            "abstract": "Increasing system complexity imposes high demands on computer aided design (CAD) tools for system synthesis. Especially the layout (placement and routing) of a design has become a problem hard to solve. To find a `good' layout, CAD tools need accurate estimators to predict area requirements, interconnection lengths, power dissipation, etc. In this paper we address the estimation of interconnection lengths. Previous work on this subject is primarily based on a technique introduced by Donath [1] which has been improved by Stroobandt et al [2]. However, this technique only estimates interconnection lengths between two cells within the system or chip. Of equal importance to the CAD tools is the length estimation of interconnections between cells and I/O pads. This paper provides a technique to accurately estimate cell to I/O pad lengths. I. Introduction  The production of VLSI and ULSI computer chips requires the layout (placement and routing) of the chip design on a carrier. With the adve...",
            "group": 998,
            "name": "10.1.1.43.8060",
            "keyword": "",
            "title": "Estimating Logic Cell to I/O Pad Lengths in Computer Systems"
        },
        {
            "abstract": "We introduce four new general optimization algorithms based on the `demon' algorithm from statistical physics and the simulated annealing (SA) optimization method. These algorithms reduce the computation time per trial without significant effect on the quality of solutions found. Any SA annealing schedule or move generation function can be used. The algorithms are tested on traveling salesman problems including Grotschel's 442-city problem with results comparable to SA. Applications to the Boltzmann machine are considered.  Keywords--- Demon algorithm, simulated annealing, optimization, traveling salesman problem, Grotschel's 442-city TSP, Boltzmann machine.  I. Introduction  We present here a number of optimization algorithms based on the simulated annealing (SA) method. These new methods aim to speed up SA by reducing computation time per trial without sacrificing the quality of solutions. The choice of parameters is kept fairly simple, and applicability to other variations of SA is ...",
            "group": 999,
            "name": "10.1.1.43.8308",
            "keyword": "",
            "title": "Demon Algorithms and their Application to Optimization Problems"
        },
        {
            "abstract": ". We present a method for mapping a given Bayesian network to a Boltzmann machine architecture, in the sense that the the updating process of the resulting Boltzmann machine model provably converges to a state which can be mapped back to a maximum a posteriori (MAP) probability state in the probability distribution represented by the Bayesian network. The Boltzmann machine model can be implemented efficiently on massively parallel hardware, since the resulting structure can be divided into two separate clusters where all the nodes in one cluster can be updated simultaneously. This means that the proposed mapping can be used for providing Bayesian network models with a massively parallel probabilistic reasoning module, capable of finding the MAP states in a computationally efficient manner. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can be seen as a method for automatically determining the structure and the connection weights of a B...",
            "group": 1000,
            "name": "10.1.1.43.9259",
            "keyword": "Boltzmann machinesprobabilistic reasoningBayesian networkssimulated annealing",
            "title": "Massively Parallel Probabilistic Reasoning with Boltzmann Machines"
        },
        {
            "abstract": "Despite the extended applicability of evolutionary algorithms to a wide range of domains, the fact that these algorithms are unconstrained optimization techniques leaves open the issue regarding how to incorporate constraints of any kind (linear, non-linear, equality and inequality) into the fitness function as to search efficiently. The main goal of this paper is to provide a detailed and comprehensive survey of the many constraint handling approaches that have been proposed for evolutionary algorithms, analyzing in each case their advantages and disadvantages, and concluding with some of the most promising paths of research.",
            "group": 1001,
            "name": "10.1.1.43.9288",
            "keyword": "evolutionary algorithmsoptimizationgenetic algorithmsevolutionary optimizationconstraint handling",
            "title": "A Survey of Constraint Handling Techniques used with Evolutionary Algorithms"
        },
        {
            "abstract": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean--field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images. Index T...",
            "group": 1002,
            "name": "10.1.1.44.177",
            "keyword": "",
            "title": "Pairwise Data Clustering by Deterministic Annealing"
        },
        {
            "abstract": "The NP-hard graph bisection problem is to partition the nodes of an undirected graph into two equal-sized groups so as to minimize the number of edges that cross the partition. The more general graph l-partition problem is to partition the nodes of an undirected graph into l equal-sized groups so as to minimize the total number of edges that cross between groups. We present a simple, linear-time algorithm for the graph l-partition problem and analyze it on a random \"planted l-partition\" model. In this model, the n nodes of a graph are partitioned into l groups, each of size n=l; two nodes in the same group are connected by an edge with some probability p, and two nodes in different groups are connected by an edge with some probability  r ! p. We show that if p \\Gamma r  n  \\Gamma1=2+ffl  for some constant ffl, then the algorithm finds the optimal partition with probability 1 \\Gamma exp(\\Gamman  \\Theta(ffl)  ). 1 Introduction  The graph l-partition problem is to partition the nodes of a...",
            "group": 1003,
            "name": "10.1.1.44.361",
            "keyword": "",
            "title": "Algorithms for Graph Partitioning on the Planted Partition Model"
        },
        {
            "abstract": "this article, we provide a framework for characterizing planning and scheduling problems that focuses on properties of the underlying dynamical system and the capabilities of the planning system to observe its surroundings. The presentation of specific techniques distinguishes between refinement-based methods that construct plans and schedules piece by piece, and repair-based methods that modify complete plans and schedules. Both refinement- and repair-based methods are generally applied in the context of heuristic search. Most planning and scheduling problems are computationally complex. As a consequence of this complexity, most practical approaches rely on heuristics that exploit knowledge of the planning domain. Current research focuses on improving the efficiency of algorithms based on existing representations and on developing new representations for the underlying dynamics  that account for important features of the domain (e.g., uncertainty) and allow for the encoding of appropriate heuristic knowledge. Given the complexity of most planning and scheduling problems, an important area for future research concerns identifying and quantifying tradeoffs, such as those involving solution quality and algorithmic complexity. Planning and scheduling in artificial intelligence cover a wide range of techniques and issues. We have not attempted to be comprehensive in this relatively short article. Citations in the main text provide attribution for specifically mentioned techniques. These citations are not meant to be exhaustive by any means. General references are provided in the `Further Information' section at the end of this article. 5 Defining Terms",
            "group": 1004,
            "name": "10.1.1.44.930",
            "keyword": "",
            "title": "Planning and Scheduling"
        },
        {
            "abstract": "Can stochastic search algorithms outperform existing deterministic heuristics for the NP-hard problem Number Partitioning if given a sufficient, but practically realizable amount of time? In a thorough empirical investigation using a straightforward implementation of one such algorithm, simulated annealing, Johnson et al. (1991) concluded tentatively that the answer is \"no.\" In this paper we show that the answer can be \"yes\" if attention is devoted to the issue of problem representation (encoding). We present results from empirical tests of several encodings of Number Partitioning with problem instances consisting of multiple-precision integers drawn from a uniform probability distribution. With these instances and with an appropriate choice of representation, stochastic and deterministic searches can---routinely and in a practical amount of time---find solutions several orders of magnitude better than those constructed by the best heuristic known (Karmarkar and Karp, 1982), which does...",
            "group": 1005,
            "name": "10.1.1.44.1132",
            "keyword": "",
            "title": "Easily Searched Encodings for Number Partitioning"
        },
        {
            "abstract": "We present a new heuristic algorithm for graph bisection, based on an implicit notion of clustering. We describe how the heuristic can be combined with stochastic search procedures and a postprocess application of the Kernighan-Lin algorithm. In a series of time-equated comparisons with large-sample runs of pure Kernighan-Lin, the new algorithm demonstrates significant superiority in terms of the best bisections found. 1 Introduction  Given a graph G = (V; E) with an even number of vertices, the graph-bisection problem is to divide  V into two equal-size subsets X and Y such that the number of edges connecting vertices in X to vertices in Y (the size of the cut set , notated cut(X; Y )) is minimized. This problem is NP-complete [7]. Graph bisection and its generalizations  1  have considerable practical significance, especially in the areas of VLSI design and operations research. The benchmark algorithm for graph bisection is due to Kernighan and Lin [13]. (The efficient implementation...",
            "group": 1006,
            "name": "10.1.1.44.1955",
            "keyword": "",
            "title": "A Seed-Growth Heuristic for Graph Bisection"
        },
        {
            "abstract": " The demand for high-speed Field-Programmable Gate Array (FPGA) compilation tools has escalated for three reasons: first, as FPGA device capacity has grown, the computation time devoted to placement and routing of circuits has grown more dramatically than the available computer power. Second, there exists a subset of users who are willing to accept a reduction in the quality of result (using a larger FPGA or more resources on a given FPGA) in exchange for a high-speed compilation. Third, high-speed compile has been a long-standing desire of users of FPGA-based custom computing machines, since their compile time requirements are ideally closer to those of regular computers. This thesis focuses on the placement phase of the compile process, and presents an ultrafast placement algorithm for FPGAs. The algorithm is based on a combinati...",
            "group": 1007,
            "name": "10.1.1.44.2574",
            "keyword": "",
            "title": "Ultra-Fast Automatic Placement for FPGAs"
        },
        {
            "abstract": "The advent of nondestructive sensing equipment (CT, MRI) created an entirely new field of research for image engineers. The equipment generates a point sampling of a true three-dimensional object. Typically, this point sampling is presented as a series of slices through the 3D object. It has become apparent, however, that displaying individual slices does not lend itself to conveying the true three-dimensional structure of the scanned object. Research, therefore, has focused on alternative methods of presenting volume data. This thesis proposes an approach that will generate a topologically closed simple geometric model of an object within a scalar field. A Geometrically Deformed Model, GDM, is created by placing an initial simple model in the data set which is then deformed by minimizing a set of constraints. The constraint functions evaluated at each vertex in the model control the local deformation, the interaction between the model and the data set, and maintain the shape and topol...",
            "group": 1008,
            "name": "10.1.1.44.2749",
            "keyword": "1.3 GDM'sGeometrically Deformed Models2",
            "title": "On GDM's: Geometrically Deformed Models for the Extraction of Closed Shapes from Volume Data"
        },
        {
            "abstract": "This paper presents an integrated framework for assembly design. The framework allows the designer to represent knowledge about the design process and constraints, as well as information about the artifact being designed, design history and rationale. Because the complexity of assembly design leads to extremely large design spaces, adequately supporting design space exploration is a key issue that must be addressed. This is achieved in part by allowing the designer to use both top-down and bottom-up approaches to assembly design. Exploration of the design space is further enabled by incorporating a simulated annealing-based optimization tool that allows the designer to rapidly complete partial designs, refine complete designs, and generate multiple design alternatives.  1 Introduction  In order to design and optimize a product, designers must be able to consider different alternatives, perform analysis to guide their own design process and focus in on a \"good\", if not optimal, design. ...",
            "group": 1009,
            "name": "10.1.1.44.3020",
            "keyword": "",
            "title": "Combining Interactive Exploration and Optimization for Assembly Design"
        },
        {
            "abstract": "This paper presents an integrated framework for conceptual assembly design. Because the complexity of assembly design leads to extremely large design spaces, adequate support of design space exploration is a key issue that must be addressed. CAMF allows the designer to manage the overall design process and explore the design space through explicit representation of design stages and their relationships (history), assembly design constraints, and rationale. The designer is free to use both bottom-up or topdown approaches to explore different assembly configurations. Exploration of the design space is further enabled by incorporating a simulated annealing-based refinement tool that allows the designer to rapidly complete partial designs, refine complete designs, and generate multiple design alternatives. 1 INTRODUCTION  In order to design and optimize a product, designers must be able to consider different alternatives, perform analysis to guide their own design process and focus in on a...",
            "group": 1010,
            "name": "10.1.1.44.3217",
            "keyword": "",
            "title": "Combining Interactive Exploration and Optimization for Assembly Design"
        },
        {
            "abstract": "Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straight-forward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. Then, we experimentally analyze the effectiveness of model regularization, dynamic model modification, and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences using a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite. 1 Introduction  Since their introduction to the computational biology community (Haussler et al., 1993; Krogh et al., 1994a), hidden Markov models (HMMs...",
            "group": 1011,
            "name": "10.1.1.44.3365",
            "keyword": "Running titleHidden Markov models for sequence analysis KeywordsHidden Markov modelparallel computationmultiple sequence alignmentprotein",
            "title": "Hidden Markov models for sequence analysis: extension and analysis of the basic method"
        },
        {
            "abstract": "The inter-wire spacing in a VLSI chip becomes closer as the VLSI fabrication technology rapidly evolves. Accordingly, it becomes important to minimize crosstalk caused by the coupling capacitance between adjacent wires in the layout design for the fast and safe VLSI circuits. We present a simulated annealing approach based on segment rearrangement to crosstalk minimization in an initially gridded channel routing. The proposed technique is compared with previous track-oriented techniques, especially a track permutation technique whose performance is bounded by an exhaustive track permutation algorithm. Experiments showed that the presented technique is more effective than the track permutation technique.  Keywords - VLSI, coupling capacitance, crosstalk, channel routing, track permutation, segment rearrangement. INTRODUCTION  The inter-wire spacing in a VLSI chip becomes closer as the VLSI fabrication technology rapidly evolves. It is known that the coupling capacitance between wires be...",
            "group": 1012,
            "name": "10.1.1.44.3544",
            "keyword": "coupling capacitancecrosstalkchannel routingtrack permutationsegment rearrangement",
            "title": "Simulated Annealing Approach to Crosstalk Minimization in Gridded Channel Routing"
        },
        {
            "abstract": "This paper describes work in progress to enhance the performance of the Ground Processing Scheduling System (GPSS). The GPSS is a constraint-based scheduler that deals with three kinds of constraints: temporal, resource, and configuration. It starts with a complete schedule that may have some constraint violations (conflicts) and gradually improves the schedule by reducing the violations. The objective is to optimize the schedule in terms of constraint violations and resource utilization. The enhancement is accomplished by improving heuristics currently used for constraint satisfaction and adding an extra feature which we call fencing. Fencing allows for obtaining a conflict-free subschedule within a selected period of time. 1 Introduction  The Ground Processing Scheduling System (GPSS) is a constraint-based scheduler. It is used for assigning  times, and resources to all the activities involved in the maintenance, repair and preparation of the fleet of Space Shuttles from the time one...",
            "group": 1013,
            "name": "10.1.1.44.3567",
            "keyword": "",
            "title": "Enhancements to the Ground Processing Scheduling System"
        },
        {
            "abstract": "The problem of recognizing objects imaged in complex real-world scenes is examined from a parametric perspective using the theory of statistical estimation. A scalar measure of an object's complexity, which is invariant under affine transformation and changes in image noise level, is extracted from the object's Fisher information. The volume of Fisher information is shown to provide an overall statistical measure of the object's recognizability in a particular image, while the complexity provides an intrinsically physical measure that characterizes the object in any image. An information-conserving method is then developed for recognizing an object imaged in a complex scene. Here the term \"information-conserving\" means that the method uses all the measured data pertinent to the object's recognizability, attains the theoretical lower bound on estimation error for any unbiased estimate of the parameter vector describing the object, and therefore is statistically optimal. This method is t...",
            "group": 1014,
            "name": "10.1.1.44.4162",
            "keyword": "",
            "title": "Information-Conserving Object Recognition"
        },
        {
            "abstract": "We present a novel optimization framework for unsupervised texture segmentation that relies on statistical tests as a measure of homogeneity. Texture segmentation is formulated as a data clustering problem based on sparse proximity data. Dissimilarities of pairs of textured regions are computed from a multi-scale Gabor filter image representation. We discuss and compare a class of clustering objective functions which is systematically derived from invariance principles. As a general optimization framework we propose deterministic annealing based on a mean-field approximation. The canonical way to derive clustering algorithms within this framework as well as an efficient implementation of mean-field annealing and the closely related Gibbs sampler are presented. We apply both annealing variants to Brodatz-like micro-texture mixtures and real-word images.",
            "group": 1015,
            "name": "10.1.1.44.4351",
            "keyword": "",
            "title": "Unsupervised Texture Segmentation in a Deterministic Annealing Framework"
        },
        {
            "abstract": "This paper deals with the problem of linear gate assignment in two layout styles: onedimensional logic array, and gate matrix layout. The goal is to find the optimal sequencing of gates in order to minimize the required number of tracks, and thus to reduce the overall circuit layout area. This is known to be an NP-Hard optimization problem, for whose solution no absolute approximation algorithm exists. Here we report the use of a new optimization heuristic derived from statistical mechanics - the microcanonical optimization algorithm, \u00b5O - to solve the linear gate assignment problem. Our numerical results show that \u00b5O compares favorably with at least five previously employed heuristics: simulated annealing, the unidirectional and the bidirectional Hong construction methods, and the artificial intelligence heuristics GM_Plan and GM_Learn. Moreover, in a massive set of experiments with circuits whose optimal layout is not known, our algorithm has been able to match and even to improve, b...",
            "group": 1016,
            "name": "10.1.1.44.4667",
            "keyword": "Physical DesignLayout CompactionVLSICircuit 19/01/99 2",
            "title": "Linear Gate Assignment: a Fast Statistical Mechanics Approach"
        },
        {
            "abstract": "We do a computational study of different local optimization methods to solve the job-shop scheduling problem. In this problem we are given a set of jobs to be scheduled on a set of machines. The objective is to schedule the jobs on the machines so that we minimize the time by which all jobs are completed. This problem has been widely studied and various techniques have been used to solve them. We consider several approximation methods to solve the job-shop scheduling problem. We focus on local optimization methods, reviewing the application of the techniques known as local improvement and simulated annealing to this problem. We propose a variant of the above local optimization methods, known as large-step optimization, to solve the job-shop scheduling problem. We present computational results obtained from the application of all these methods to several instances of the problem. From the computational results we can conclude that the local improvement method is clearly inferior, even w...",
            "group": 1017,
            "name": "10.1.1.44.4695",
            "keyword": "",
            "title": "Local Optimization and The Job-Shop Scheduling Problem"
        },
        {
            "abstract": "In this paper we study the minimum congestion routing problem for local and metropolitan light-wave networks, where congestion is defined as the maximum flow carried on any link. We consider algorithms for computing both the upper and lower bounds on the congestion.  For the lower bound, we discuss a new technique based on minimization of the total flow carried in the network and present an efficient algorithm to compute it. This algorithm allows us to have an instance specific bound which can be used in evaluating the quality of heuristic algorithms and also used in termination criteria during minimization. For the upper bound, we develop techniques to allow the application of two known heuristics ---variable depth local search and simulated annealing--- to obtain approximate solutions of this problem. The performance of these two algorithms is analyzed and compared with extensive simulation studies. The simulation results show that our heuristics perform, on the average, within 20% o...",
            "group": 1018,
            "name": "10.1.1.44.4865",
            "keyword": "",
            "title": "A Study of Upper and Lower Bounds for Minimum Congestion Routing in Light-wave Networks"
        },
        {
            "abstract": "Architectures and Algorithms for Field-Programmable Gate Arrays with Embedded Memory Doctor of Philosophy, 1997 Steven J.E. Wilton Department of Electrical and Computer Engineering University of Toronto Recent dramatic improvements in integrated circuit fabrication technology have led to Field-Programmable Gate Arrays (FPGAs) capable of implementing entire digital systems, as opposed to the smaller logic circuits that have traditionally been targeted to FPGAs. Unlike the smaller circuits, these large systems often contain memory. Architectural support for the efficient implementation of memory in next-generation FPGAs is therefore crucial. This dissertation examines the architecture of FPGAs with memory, as well as algorithms that map circuits into these devices. Three aspects are considered: the analysis of circuits that contain memory as well as the automated random generation of such circuits, the architecture and algorithms for stand-alone configurable memory devices, and architect...",
            "group": 1019,
            "name": "10.1.1.44.4925",
            "keyword": "",
            "title": "Architectures and Algorithms for Field-Programmable Gate Arrays with Embedded Memory"
        },
        {
            "abstract": "In this technical note, we show that, for any given combinatorial optimization problem, and under very general conditions, a \"repeated-random-start\" local search algorithm is superior to a generalized version of the so-called \"simulated annealing\" algorithm, in the sense that, beyond some common finite running time threshold, the repeated-random-start local search algorithm's probability of finding an optimal solution is always larger than the corresponding simulated annealing algorithm's probability. Keywords: Randomized Algorithms, Generalized Simulated Annealing, RandomStart Local Search, Combinatorial Optimization   Working paper, CERMA-86-12-3  z  Mathematics Department, ENPC, Paris, France. Supported in part by NSF/CNRS  1 Introduction  Consider a generic combinatorial optimization problem CO, i.e. a finite set\\Omega of feasible solutions and a function value OE  :\\Omega  7! R, so that the problem is to find  ! opt  2\\Omega  such that OE(! opt )  OE(!) for all ! 2 \\Omega\\Gamma W...",
            "group": 1020,
            "name": "10.1.1.44.5011",
            "keyword": "Randomized AlgorithmsGeneralized Simulated AnnealingRandomStart Local SearchCombinatorial Optimization",
            "title": "On the Convergence of Simulated Annealing and Random-Start Local Search Algorithms for Combinatorial Optimization Problems"
        },
        {
            "abstract": "We introduce four new general optimization algorithms based on the `demon' algorithm from statistical physics and the simulated annealing (SA) optimization method. These algorithms use a computationally simpler acceptance function, but can use any SA annealing schedule or move generation function. Computation per trial is significantly reduced. The algorithms are tested on traveling salesman problems including Grotschel's 442-city problem and the results are comparable to those produced using SA. Applications to the Boltzmann machine are considered. 1. Introduction We present here a number of optimization algorithms based on the simulated annealing (SA) method. These new methods aim to speed up SA by reducing computation time per trial without sacrificing the quality of solutions. The choice of parameters is kept fairly simple, and applicability to other variations of SA is maintained. The initial motivation for this study came from an interest in improving the speed of the Boltzmann ...",
            "group": 1021,
            "name": "10.1.1.44.5114",
            "keyword": "",
            "title": "Fast Optimization by Demon Algorithms"
        },
        {
            "abstract": "This paper is about genetic programming -- a way to implement Turing's third way to achieve machine intelligence. Genetic programming is a \"genetical or evolutionary\" technique that automatically creates a computer program from a high-level statement of a problem's requirements. In particular, genetic programming is an extension of the genetic algorithm described in John Holland's pioneering 1975 book Adaptation in Natural and Artificial Systems [7]. Starting with a primordial ooze of thousands of randomly created computer programs, genetic programming progressively breeds a population of computer programs over a series of generations. Genetic programming employs the Darwinian principle of survival of the fittest,  analogs of naturally occurring operations such as sexual recombination (crossover), mutation, gene duplication, and gene deletion, and certain mechanisms of developmental biology [2, 3,",
            "group": 1022,
            "name": "10.1.1.44.5388",
            "keyword": "",
            "title": "Genetic Programming: Turing's Third Way to Achieve Machine Intelligence"
        },
        {
            "abstract": "A fast simulated annealing algorithm is developed for automatic object recognition. The object recognition problem is addressed as the problem of best describing a match between a hypothesized object and an image. The normalized correlation coefficient is used as a measure of the match. Templates are generated on-line during the search by transforming model images. Simulated annealing reduces the search time by orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of how landmarks, for example, traffic signs, can be recognized by an autonomous vehicle or a navigating robot. Images are assumed to be taken while the robot or the vehicle is moving through its environment. It tries to match them with templates created online from models stored in a database. We illustrate the performance of our algorithm with real-world images of complicated scenes with traffic signs. False positive matches occur only for templates with very small information con...",
            "group": 1023,
            "name": "10.1.1.44.5941",
            "keyword": "",
            "title": "Fast Object Recognition in Noisy Images Using Simulated Annealing"
        },
        {
            "abstract": "We investigate a topological design and routing problem for Low-Earth Orbit (LEO) satellite communication networks where each satellite can have a limited number of direct inter-satellite links (ISL's) to a subset of satellites within its line-of-sight. First, we model LEO satellite network as a FSA (Finite State Automaton) using satellite constellation information. Second, we solve a combined topological design and routing problem for each configuration corresponding to a state in the FSA. The topological design (or link assignment) problem deals with the selection of ISL's, and the routing problem handles the traffic distribution over the selected links to maximize the number of carried calls. In this paper, this NP-complete mixed integer optimization problem is solved by a two-step heuristic algorithm that first solves the topological design problem, and then finds the optimal routing. The algorithm is iterated using the simulated annealing technique until the near-optimal solution ...",
            "group": 1024,
            "name": "10.1.1.44.6298",
            "keyword": "LEO satellite networktopological designroutingsimulated annealing",
            "title": "Topological Design and Routing for LEO Satellite Networks"
        },
        {
            "abstract": "This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees. 1. Introduction  Current data collection technology provides a unique challenge and opportunity for automated machine learning techniques. The advent of major scientific projects such as the Human Genome Project, the Hubble Space Telescope, and the human brain mappi...",
            "group": 1025,
            "name": "10.1.1.44.6304",
            "keyword": "",
            "title": "A System for Induction of Oblique Decision Trees"
        },
        {
            "abstract": "Evolution Strategies apply mutation and recombination operators in order to create their offspring. Both operators have a different role in the evolution process: recombination should combine information of different individuals, while mutation performs a kind of random walk to introduce new values. In an ES these operators are always applied together, but their different roles suggest that it might be better to apply them independently and at different rates. In order to do so the ES has been split into two levels. The resulting Modular Evolution Strategy consists of a population of local optimizers and a distributed population manager. Both parts have their own specific role in the optimization process. As a result of its modularity this method can be adapted more easily to specific classes of numerical optimization problems, and introduction of adaptive mechanisms is relatively easy. A further interesting aspect about this algorithm is that it does not need any global communication,...",
            "group": 1026,
            "name": "10.1.1.44.6564",
            "keyword": "Evolution based learning systemssuch as Genetic AlgorithmsGenetic ProgrammingEvolution Programming",
            "title": "A Two-level Evolution Strategy - balancing global and local search"
        },
        {
            "abstract": "This article provides a first theoretical analysis on a new Monte Carlo approach, the dynamic weighting, proposed recently by Wong and Liang. In dynamic weighting, one augments the original state space of interest by a weighting factor, which allows the resulting Markov chain to move more freely and to escape from local modes. It uses a new invariance principle to guide the construction of transition rules. We analyze the behaviors of the weights resulting from such a process and provide detailed recommendations on how to use these weights properly. Our recommendations are supported by a renewal theory-type analysis. Our theoretical investigations are further demonstrated by a simulation study and applications in the neural network training and the Ising model simulations. Keywords: Gibbs Sampling; Importance Sampling; Ising Model, Metropolis algorithm, Neural Network, Renewal Theory, Simulated Annealing, Simulated Tempering, 1  Jun S. Liu is Assistant Professor, Department of Statisti...",
            "group": 1027,
            "name": "10.1.1.44.7140",
            "keyword": "Gibbs SamplingImportance SamplingIsing ModelMetropolis algorithmNeural NetworkRenewal TheorySimulated AnnealingSimulated Tempering1",
            "title": "Dynamic Weighting In Markov Chain Monte Carlo"
        },
        {
            "abstract": "This article introduce an original approach to overcome difficulties of multimodality global elastic registration. A structural description of the volumes and in addition a combinatorial optimization process allows us to solve, in a reasonable time, a non-linear problem presenting numerous degrees of freedom. In an other way, our algorithm could be seen as a generalized stochastic Hough transform using the principles of natural selection. Thus, the great originality of our algorithm resides from the particular use of a genetic algorithm (from encoding to the examination of the genetic space of the solutions). After a non-exhaustive state of the art of medical image registration, we will introduce in section III the geometrical consideration of registration we used for our algorithm, then we will focus (section IV) on genetic algorithms and global optimization problems. In section V we will detail our registration procedure and illustrate it with an example in section VI. Eventually we will conclude and give some open issues and future directions.  II Medical image registration, a state of the art.",
            "group": 1028,
            "name": "10.1.1.44.8455",
            "keyword": "",
            "title": "Robust 3-D Elastic Multimodality Image Registration though Genetic Algorithms"
        },
        {
            "abstract": "This paper investigates the problem of designing finite precision one-dimensional (1-D) infinite impulse response (IIR) filters with prescribed magnitude, phase and stability constraints. The design problem is formulated as the minimization of a cost function incorporating these conflicting requirements. The first two elements of the cost function express magnitude and group delay errors between the desired and the actual frequency responses of a filter, while the third one is related to its stability margin. This cost function is minimized using simulated annealing based on the Metropolis algorithm. Examples of several finite wordlength filters designed by the above method are presented and compared with Chebyshev and elliptic filters with rounded coefficients.  1. INTRODUCTION  Digital filters with finite-precision implementation find application today in a wide range of areas e.g., telecommunications, robotics, control. This diversity calls for a general and flexible method of desig...",
            "group": 1029,
            "name": "10.1.1.44.8514",
            "keyword": "",
            "title": "Design Of Finite Wordlength IIR Filters With Prescribed Magnitude, Group Delay And Stability Properties Using Simulated Annealing"
        },
        {
            "abstract": "We investigate constraint relaxation within a general constraint model. We claim that a key to relaxation is recognition that a constraint can be modified in a variety of ways and that each modification potentially carries a different impact for both the quality of the solution and the problem solving process. Our primary motivation is the application of constraint relaxation as a technique for coordination of multiple agents in a shared environment. We propose a schema for constraint relaxation that is based on the propagation of information through a constraint graph. The schema isolates five heuristic decision points where techniques of varying complexities can be specified. Three algorithms within the schema are declared and shown to perform well on Partial Constraint Sa...",
            "group": 1030,
            "name": "10.1.1.44.8672",
            "keyword": "",
            "title": "A Schema for Constraint Relaxation with Instantiations for Partial Constraint Satisfaction and Schedule Optimization"
        },
        {
            "abstract": "A new approach to optimisation is introduced based on a precise probabilistic statement of what is ideally required of an optimisation method. It is convenient to express the formalism in terms of the control of a stationary environment. This leads to an objective function for the controller which unifies the objectives of exploration and exploitation, thereby providing a quantitative principle for managing this trade-off. This is demonstrated using a variant of the multi-armed bandit problem. This approach opens new possibilities for optimisation algorithms, particularly by using neural network or other adaptive methods for the adaptive controller. It also opens possibilities for deepening understanding of existing methods. The realisation of these possibilities requires research into practical approximations of the exact formalism. 1 Introduction  Optimisation methods can be compared according to various criteria, such as the computation time they require, the accuracy of the solutio...",
            "group": 1031,
            "name": "10.1.1.44.8930",
            "keyword": "",
            "title": "A Bayesian Formulation of Search, Control and the Exploration/Exploitation Trade-off"
        },
        {
            "abstract": "New heuristic algorithms are proposed for the Graph Partitioning problem. A greedy construction scheme with an appropriate tie--breaking rule (MIN-MAX-GREEDY) produces initial assignments in a very fast time. For some classes of graphs, independent repetitions of MIN-MAX-GREEDY are sufficient to reproduce solutions found by more complex techniques. When the method is not competitive, the initial assignments are used as starting points for a prohibition-based scheme, where the prohibition is chosen in a randomized and reactive way, with a bias towards more successful choices in the previous part of the run. The relationship between prohibition-based diversification (Tabu Search) and the variable-depth Kernighan--Lin algorithm is discussed. Detailed experimental results are presented on benchmark suites used in the previous literature, consisting of graphs derived from parametric models (random graphs, geometric graphs, etc.) and of \"realworld \" graphs of large size. On the first series ...",
            "group": 1032,
            "name": "10.1.1.44.9198",
            "keyword": "",
            "title": "Greedy, Prohibition, and Reactive Heuristics for Graph Partitioning"
        },
        {
            "abstract": "Neural Network Learning algorithms based on Conjugate Gradient Techniques and Quasi Newton Techniques such as Broyden, DFP, BFGS, and SSVM algorithms require exact or inexact line searches in order to satisfy their convergence criteria. Line searches are very costly and slow down the learning process. This paper will present new Neural Network learning algorithms based on Hoshino's weak line search technique and Davidon's Optimally Conditioned line search free technique. Also, a practical method of using these optimization algorithms is presented such that they will avoid getting trapped in local minima for the most part. The global minimization problem is a serious one when quadratically convergent techniques such as Quasi Newton methods are used. Furthermore, to display the performance of the proposed learning algorithms, the more practical algorithm based on Davidon's minimization technique is used in conjunction with a cursive handwriting recognition problem. For comparison with ot...",
            "group": 1033,
            "name": "10.1.1.44.9499",
            "keyword": "LearningNeural NetworksQuasi Newton MethodsOCR Unconstrained OptimizationHandwriting Recognition",
            "title": "Neural Network Learning Through Optimally Conditioned Quadratically Convergent Methods Requiring NO LINE SEARCH"
        },
        {
            "abstract": "This paper is a survey of inductive rule learning algorithms that use a separate-andconquer strategy. This strategy can be traced back to the AQ learning system and still enjoys popularity as can be seen from its frequent use in inductive logic programming systems. We will put this wide variety of algorithms into a single framework and analyze them along three different dimensions, namely their search, language and overfitting avoidance biases. 1. Introduction In this paper we will give an overview of a large family of symbolic rule learning algorithms, the so-called separate-and-conquer or covering algorithms. All members of this family share the same top-level loop: basically a separate-and-conquer algorithm searches for a rule that explains a part of its training instances, separates these examples, and recursively conquers the remaining examples by learning more rules until no examples remain. This ensures that each instance of the original training set is covered by at least one...",
            "group": 1034,
            "name": "10.1.1.45.557",
            "keyword": "",
            "title": "Separate-and-Conquer Rule Learning"
        },
        {
            "abstract": "The NP complete problem of the graph bisection is a mayor problem occurring in the design of VLSI chips. A simulated annealing algorithm is used to obtain solutions to the graph partitioning problem. As stated in may publications one of the major problems with the simulated annealing approach is the huge amount of time one has to spend in the annealing process. To increase the speed the structure of the graph is used before and while the annealing process is performed. This is done by reducing the graph and applying the annealing process after the reduction step. Nodes which have neighbors to the other partition are preferred for a possible interchange. The project has the following purpose:  ffl Investigation of simulated annealing for the uniform graph partitioning problem. Different annealing schedules are compared.  ffl Investigation of the influence of the reduction algorithm on the speed and the quality of the solutions obtained.  ffl Investigation of the use of the Cauchy rule i...",
            "group": 1035,
            "name": "10.1.1.45.1084",
            "keyword": "1 Graph Partitioning 3",
            "title": "A Collection of Graph Partitioning Algorithms: Simulated Annealing, Simulated . . ."
        },
        {
            "abstract": "viii 1 Introduction 1  1.1 Combinatorial Optimization Problem and Neural Network : : : : 1 1.2 Related Works : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3 1.3 Approach of the Thesis : : : : : : : : : : : : : : : : : : : : : : : : 7 1.4 Organization of the Thesis : : : : : : : : : : : : : : : : : : : : : : 9  2 Dual-Mode Dynamics Neural Networks 10  2.1 Network Configuration Space and Equilibrium Manifold : : : : : : : : : : : : : : : : : : : : : : : : : 10 2.2 Network Structure : : : : : : : : : : : : : : : : : : : : : : : : : : 13 2.3 Dual-Mode Dynamics : : : : : : : : : : : : : : : : : : : : : : : : : 16 2.3.1 Discrete Model Dual-Mode Dynamics : : : : : : : : : : : : 16 2.3.2 Continuous Model Dual-Mode Dynamics : : : : : : : : : : 19 2.3.3 Symmetry Preserving Recurrent Backpropagation : : : : : 27 2.4 Binary Value Solution vs. Continuous State Variable : : : : : : : 31 2.5 Asymmetric Weight vs. Symmetric Weight : : : : : : : : : : : : : 36  3 Problem Solving with Dual-M...",
            "group": 1036,
            "name": "10.1.1.45.2191",
            "keyword": "Contents Acknowledgments ii",
            "title": "Dual-Mode Dynamics Neural Networks For Combinatorial Optimization"
        },
        {
            "abstract": "Genetic Programming (GP), an extension of the Genetic Algorithm (GA), is a powerful optimization technique wherein tree objects representing programs are manipulated in order to evolve programmatic solutions to given problems. A number of GP implementations have been forthcoming -- the original being in LISP -- all of which have been deficient in one or more areas. This author's implementation of a Genetic Programming Environment in Smal ltalk (GPEIST) attempts to provide an environment with a rich set of inspectors, utilities and instrumentation that is extensible enough to provide a test bed for GP problems.",
            "group": 1037,
            "name": "10.1.1.45.2349",
            "keyword": "",
            "title": "A Genetic Programming Environment in Smalltalk"
        },
        {
            "abstract": "Evolutionary computation courses have been offered by a wide range of departments/schools to students with many different backgrounds. This paper describes three postgraduate courses with significant evolutionary computation components offered by the University College of the University of New South Wales at the Australian Defence Force Academy. The courses have been offered as part of the Master of Science in Information Technology programme and Master of Science in Operations Research and Statistics programme. This paper also summarises the result of a recent survey on evolutionary computation teaching conducted over the Internet.  1 Introduction  Although the history of evolutionary computation (EC) can be traced back to 1950's [1], EC courses have only been offered in universities in the last decade or so. There has been a dramatic increase in the number of universities offering EC or EC-related courses in recent years [2]. However, there have been no standard teaching methods or t...",
            "group": 1038,
            "name": "10.1.1.45.3253",
            "keyword": "",
            "title": "How Does Evolutionary Computation Fit Into IT Postgraduate Teaching"
        },
        {
            "abstract": " ",
            "group": 1039,
            "name": "10.1.1.45.3348",
            "keyword": "Acknowledgements",
            "title": "A corpus-based approach to Language learning"
        },
        {
            "abstract": "The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning. Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by selfplay alone. A network with less than 500 weights learned within 3,000 games of 9x9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level. 1 INTRODUCTION Go was developed three to four millenia ag...",
            "group": 1040,
            "name": "10.1.1.45.4261",
            "keyword": "",
            "title": "Temporal Difference Learning of Position Evaluation in the Game of Go"
        },
        {
            "abstract": "In this paper we compare the performance of six heuristics with suboptimal solutions for the data distribution of two dimensional meshes that are used for the numerical solution of Partial Differential Equations (PDEs) on multicomputers. The data mapping heuristics are evaluated with respect to seven criteria covering load balancing, interprocessor communication, flexibility and ease of use for a class of single-phase iterative PDE solvers. Our evaluation suggests that the simple and fast block distribution heuristic can be as effective as the other five complex and computational expensive algorithms.",
            "group": 1041,
            "name": "10.1.1.45.4712",
            "keyword": "",
            "title": "A Comparison of Optimization Heuristics for the Data Mapping Problem"
        },
        {
            "abstract": "We propose a novel person verification system for real-time face identification. The main features of the system include accurate registration of face images using a robust form of correlation, a framework for global registration of a face database using a minimum spanning tree algorithm and a method for selecting a subset of features optimal for discrimination between clients and impostors. We present results obtained through experiments on a large database with 295 subjects and show that the method is performing well in comparison with two standard methods based on elastic graph matching.  1 Introduction  Verification of person identity based on biometric information is important for many security applications. Examples include access control to buildings, surveillance and intrusion detection. Furthermore, there are many emerging fields that would benefit from developments in person verification technology such as advanced human-computer interfaces and tele-services including tele-sh...",
            "group": 1042,
            "name": "10.1.1.45.4933",
            "keyword": "",
            "title": "Learning Salient Features for Real-Time Face Verification"
        },
        {
            "abstract": "This paper presents an active database discrimination network algorithm called Gator. Gator is a generalization of the widely-known Rete and TREAT algorithms. Gator pattern matching is explained, and it is shown how a discrimination network can speed up condition testing for multi-table triggers. The structure of a Gator network optimizer is described, along with a discussion of optimizer performance, output quality, and accuracy. This optimizer can choose an efficient Gator network for testing the conditions of a set of triggers, given information about the structure of the triggers, database size, attribute cardinality, and update frequency distribution. The optimizer uses a randomized strategy to deal with the problem of a large search space. Optimizer validation was performed, showing a strong correlation between predicted cost of a Gator network and its actual cost when used for trigger condition testing. The results show that optimized Gator networks normally have a shape which i...",
            "group": 1043,
            "name": "10.1.1.45.4989",
            "keyword": "",
            "title": "Optimized Trigger Condition Testing in Ariel Using Gator Networks"
        },
        {
            "abstract": "A massive data set is considered as a set of experimentally acquired values of a number of variables each of which is associated with the respective node of an undirected adjacency graph that presets the fixed structure of the data set. The class of data analysis problems under consideration is outlined by the assumption that the ultimate aim of processing can be represented as a transformation of the original data array into a secondary array of the same structure but with node variables of, generally speaking, different nature, i.e. different ranges. Such a generalized problem is set as the formal problem of optimization (minimization or maximization) of a real-valued objective function of all the node variables. The objective function is assumed to consist of additive constituents of one or two arguments, respectively, node and edge functions. The former of them carry the data-dependent information on the sought-for values of the secondary variables, whereas the latter ones are mean...",
            "group": 1044,
            "name": "10.1.1.45.5169",
            "keyword": "",
            "title": "Bellman Functions on Trees for Segmentation, Generalized Smoothing, Matching and Multi-Alignment in Massive Data Sets"
        },
        {
            "abstract": "this paper a fully connected network is optimized using the Scaled Conjugate Gradient method (SCG) developed by Moller [7] and modified by Blue and Grother [8]. The SCG method is used as a starting network for the Boltzmann weight pruning algorithm. The network has an input layer with thirty-two input nodes, a variable size hidden layer with sixteen, thirty-two or sixty-four nodes and an output layer with ten nodes. The initial network is a fully connected network. The pruning was carried out by selecting a normalized temperature,",
            "group": 1045,
            "name": "10.1.1.45.5450",
            "keyword": "",
            "title": "Optimization of Neural Network Topology and. . ."
        },
        {
            "abstract": "In this paper the task of training sub-symbolic systems is considered as a combinatorial optimization problem and solved with the heuristic scheme of the Reactive Tabu Search (RTS) proposed by the authors and based on F. Glover's Tabu Search. An iterative optimization process based on a \"modified greedy search\" component is complemented with a meta-strategy to realize a discrete dynamical system that discourages limit cycles and the confinement of the search trajectory in a limited portion of the search space. The possible cycles are discouraged by prohibiting (i.e., making tabu) the execution of moves that reverse the ones applied in the most recent part of the search, for a prohibition period that is adapted in an automated way. The confinement is avoided and a proper exploration is obtained by activating a diversification strategy when too many configurations are repeated excessively often. The RTS method is applicable to non-di#erentiable functions, it is robust with respect to the...",
            "group": 1046,
            "name": "10.1.1.45.5553",
            "keyword": "",
            "title": "Training Neural Nets with the Reactive Tabu Search"
        },
        {
            "abstract": "There exists a wide variety of applications in which data availability must be continuous, that is, where the system is never taken off-line and any interruption in the accessibility of stored data causes significant disruption in the service provided by the application. Examples include on-line transaction processing systems such as airline reservation systems and automated teller networks in banking systems. In addition, there exist many applications for which a high degree of data availability is important, but continuous operation is not required. An example is a research and development environment, where access to a centrally-stored CAD system is often necessary to make progress on a design project. These applications and many others mandate both high performance and high availability from their storage subsystems. Redundant disk arrays are systems in which a high level of I/O performance is obtained by grouping together a large number of small disks, rather than building one lar...",
            "group": 1047,
            "name": "10.1.1.45.6472",
            "keyword": "data storageredundant disk arraysfailure recoverydata availabilitydata reconstructionparity declusteringclustered RAIDreconstruction algorithmsRAID",
            "title": "On-Line Data Reconstruction In Redundant Disk Arrays"
        },
        {
            "abstract": "This paper theoretically compares the performance of simulated annealing and evolutionary algorithms. Our main result is that under mild conditions a wide variety of evolutionary algorithms can be shown to have greater probability of success than simulated annealing after a sufficiently large number of function evaluations. This class of EAs includes variants of evolution strategies and evolutionary programming, genetic programming, the canonical genetic algorithm, as well as a variety of genetic algorithms that have been applied to combinatorial optimization problems. The proof of this result is based on a performance analysis of a very general class of stochastic optimization algorithms, which has implications for the performance of a variety of other optimization algorithms.  1 Introduction  This paper concerns the performance of algorithms that minimize an objective function of the form f : S ! R,  jSj ! 1. In particular, this paper concerns the relative performance of evolutionary...",
            "group": 1048,
            "name": "10.1.1.45.7303",
            "keyword": "",
            "title": "A Theoretical Comparison of Evolutionary Algorithms and Simulated Annealing"
        },
        {
            "abstract": "and System Demonstration) Arne Frick  ?  , Andreas Ludwig, Heiko Mehldau  Universitat Karlsruhe, Fakultat fur Informatik, D-76128 Karlsruhe, Germany Abstract. We present a randomized adaptive layout algorithm for nicely drawing undirected graphs that is based on the spring-embedder paradigm and contains several new heuristics to improve the convergence, including local temperatures, gravitational forces and the detection of rotations and oscillations. The proposed algorithm achieves drawings of high quality on a wide range of graphs with standard settings. Moreover, the algorithm is fast, being thus applicable on general undirected graphs of substantially larger size and complexity than before [9, 6, 3]. Aesthetically pleasing solutions are found in most cases. We give empirical data for the running time of the algorithm and the quality of the computed layouts. 1 Introduction  The problem of obtainingan aesthetically pleasing drawing of a given graph G = (V; E)  is receiving increasing...",
            "group": 1049,
            "name": "10.1.1.45.7675",
            "keyword": "",
            "title": "A Fast Adaptive Layout Algorithm for Undirected Graphs (Extended Abstract and System Demonstration)"
        },
        {
            "abstract": ". This paper presents a new approach to the pagination problem. This problem has traditionally been solved offline for a variety of applications like the pagination of Yellow Pages or newspapers, but since these services have appeared in Internet, a new approach is needed to solve the problem in real time. This paper is concerned with the problem of paginating a selection of articles from web newspapers that match a query sent to a personalized news site by a user. The result should look like a real newspaper and adapt to the client's computer configuration (font faces and sizes, screen size and resolution, etc.). A combinatorial approach based on Simulated Annealing and written in JavaScript is proposed to solve the problem online in the client's computer. Experiments show that the GA achieves real time layout optimization for up to 50 articles. 1 Introduction  Simulated annealing (SA) [1, 8] is a Monte Carlo approach for optimization tasks inspired by the roughly analogous physical p...",
            "group": 1050,
            "name": "10.1.1.45.9387",
            "keyword": "",
            "title": "Optimizing Web Newspaper Layout using Simulated Annealing"
        },
        {
            "abstract": "This paper presents a novel technique to perform dynamic high-level exploration of a behavioral specification that is being partitioned for a multi-device architecture. The technique, unlike in traditional HLS, performs a global search on the four-dimensional design space formed by multiple partition segments of the behavior. Hence, the proposed technique effectively satisfies the global latency constraint on the entire design, as well as the area constraints on the individual partition segments. Since the technique is based on a rigorous exploration model, it employs an efficient lowcomplexity heuristic instead of an exhaustive search. We have provided a number of results by integrating the exploration technique with two popular partitioning algorithms: (i) simulated annealing and (ii) fiducciamattheyses. The proposed technique is highly effective in guiding any partitioning algorithm to a constraint satisfying solution, and in a fairly short execution time. At tight constraint values...",
            "group": 1051,
            "name": "10.1.1.45.9981",
            "keyword": "",
            "title": "A Technique for Dynamic High-Level Exploration During Behavioral-Partitioning for Multi-Device Architectures"
        },
        {
            "abstract": "this paper we compare the performance of six heuristics with suboptimal solutions for the data distribution of two dimensional meshes that are used for the numerical solution of Partial Differential Equations (PDEs) on multicomputers. The data mapping heuristics are evaluated with respect to seven criteria covering load balancing, interprocessor communication, flexibility and ease of use for a class of single-phase iterative PDE solvers. Our evaluation suggests that the simple and fast block distribution heuristic can be as effective as the other five complex and computational expensive algorithms.",
            "group": 1052,
            "name": "10.1.1.46.433",
            "keyword": "",
            "title": "A Comparison of Optimization Heuristics for the Data Mapping Problem"
        },
        {
            "abstract": "We propose a declarative-based implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [DW97]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. PCCP also includes synchronisation and concurrency aspects. However, for the purpose of this work, the (probabilistic) CLP fragment of PCCP is sufficient. We present a meta-interpreter for this language. This is just a standard prolog meta-interpreter, suitably extended so as to deal with probabilistic choice. For the constraint solving, the meta-interpreter exploits existing constraint handling facilities (and in more concrete terms to the SICStus 3.#6 system). This is possible because the design of PCCP ...",
            "group": 1053,
            "name": "10.1.1.46.554",
            "keyword": "",
            "title": "Implementing Randomised Algorithms in Constraint Logic Programming"
        },
        {
            "abstract": "this paper is the application of a multivariate optimization technique called \"simulated annealing\" that can in principle be applied to any minimizant or maximizant in NP-hard problems. In particular, we'll investigate whether it can be practically applied to the minimizants of profile and fill to produce better orderings of A. Previous work in [1] looked at the minimizants of profile, wavefront, and bandwidth. We will compare our results with this work where they overlap. 2 Simulated Annealing (Most of this section follows the presentation in [11], although [8] presented the original idea.) The statistical behaviour of physical systems with large numbers of degrees of freedom inspired the simulated annealing technique. To \"anneal\" is \"to heat (glass, metals, etc.) and then cool slowly to prevent brittleness\". This is just one instance of a fundamental observation about nature: given sufficient time and a mechanism to do so, a system will always tend to adjust itself to a minimal energy state. For instance ffl The surface of a lake is flat. ffl (Slowly) cooled liquids form highly-regular crystals. ffl Air molecules spread evenly in a room. All of these represent systems with large numbers of degrees of freedom achieving global minima. Most iterative techniques that attempt to solve global optimization problems are, in a sense, \"greedy\": as soon as they find a better solution, they adopt it. For this reason, these techniques don't always behave well  5 in the presence of local minima. How, then, are molecules able to \"solve\" the minimal energy problem globally? Because nature gives them sufficient time and energy to rearrange themselves in a way that ultimately satisfies the global minimum. The Boltzmann distribution permits a system to exist in a state that is energy E...",
            "group": 1054,
            "name": "10.1.1.46.1224",
            "keyword": "",
            "title": "Simulated Annealing for Profile and Fill Reduction of Sparse Matrices"
        },
        {
            "abstract": " Simulated annealing --- moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions --- has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios...",
            "group": 1055,
            "name": "10.1.1.46.1344",
            "keyword": "",
            "title": "Annealed Importance Sampling"
        },
        {
            "abstract": "Computers that \"program themselves\"; science fact or fiction? Genetic Programming uses novel optimisation techniques to \"evolve\" simple programs; mimicking the way humans construct programs by progressively re-writing them. Trial programs are repeatedly modified in the search for \"better/fitter\" solutions. The underlying basis is Genetic Algorithms (GAs). Genetic Algorithms, pioneered by Holland [Hol92], Goldberg [Gol89] and others, are evolutionary search techniques inspired by natural selection (i.e survival of the fittest). GAs work with a \"population\" of trial solutions to a problem, frequently encoded as strings, and repeatedly select the \"fitter\" solutions, attempting to evolve better ones. The power of GAs is being demonstrated for an increasing range of applications; financial, imaging, VLSI circuit layout, gas pipeline control and production scheduling [Dav91]. But one of the most intriguing uses of GAs - driven by Koza [Koz92] - is automatic program generation. Genetic Progra...",
            "group": 1056,
            "name": "10.1.1.46.1986",
            "keyword": "Machine LearningGenetic AlgorithmsGenetic Programming",
            "title": "Genetic Programming - Computers using \"Natural Selection\" to generate programs"
        },
        {
            "abstract": "Despite the increasing availability of parallel platforms, their wide-spread use in the solution of large computing problems remains restricted to a fairly narrow set of applications. This is due in part to the difficulty of parallel application development which is itself largely the result of a lack of sophisticated environments for parallel application development. Further, though the number of parallel platforms is increasing, the convergence of parallel architectures and operating systems does not appear to be similarly increasing. Given that most development environments are targeted towards a particular architecture, it is difficult to amortize development costs over a wide base of installed machines. In this research, we address these problems through the application of two significant technologies, object-oriented design techniques and the actor model of concurrent computation. Our approach is manifested in the ProperCAD II library, a C ++ object library supporting actor concu...",
            "group": 1057,
            "name": "10.1.1.46.2017",
            "keyword": "",
            "title": "ProperCAD II: A Run-Time Library For Portable, Parallel, Object-Oriented Programming With Applications To VLSI CAD"
        },
        {
            "abstract": "Current technology provides a means to obtain sampled data that digitally describes three-dimensional surfaces and objects. Three-dimensional digitizing cameras can be used to obtain sampled data that maps the surface of three dimensional figures and models. Data obtained from such sources enable accurate renderings of the original surface. However, the digitizing process often provides much more data than is needed to accurately recreate the surface or object. In order to use such data in real-time visual simulators, a significant reduction in the data needed to accurately render the sampled surfaces is required. The techniques presented were developed to drastically reduce the number of data points required to depict an object without sacrificing the detail and accuracy inherent in the digitizing process.  * Contact author.  1. INTRODUCTION  As our technologically oriented civilization becomes increasingly more complex and sophisticated, the cost of training operators and technicians...",
            "group": 1058,
            "name": "10.1.1.46.2166",
            "keyword": "",
            "title": "Simplification of Objects Rendered by Polygonal Approximations"
        },
        {
            "abstract": "We propose a declarative-based implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [DW97]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. PCCP also includes synchronisation and concurrency aspects. However, for the purpose of this work, the (probabilistic) CLP fragment of PCCP is sufficient. We present a meta-interpreter for this language. This is just a standard prolog metainterpreter, suitably extended so as to deal with probabilistic choice. For the constraint solving, the meta-interpreter exploits existing constraint handling facilities (and in more concrete terms to the SICStus 3.#6 system). This is possible because the design of PCCP d...",
            "group": 1059,
            "name": "10.1.1.46.2869",
            "keyword": "",
            "title": "Randomised Algorithms and Constraint Logic Programming"
        },
        {
            "abstract": "Grouping is often intended as a general-purpose early vision stage which gathers together image features of perceptual salience, usually having a well-definable structure. This work addresses the problem of generic part-based grouping and recognition from single two-dimensional edge images following a strategy that employs generic part models at all stages: the key underlying idea is to perform a purposive grouping of simple parts and these parts can be conveniently represented by generic deformable part models. This paper describes the proposed computational method, which is more extensively treated in [25]. Keywords: Perceptual Grouping, Segmentation, Minimum Description Length Deformable Models    Corresponding author. Now at: Digital Media Department, Hewlett-Packard Laboratories, Filton Road, Stoke Gifford, Bristol BS12 6QZ, UK. Email: mp@hplb.hpl.hp.com  1 Introduction  Since the early days of computer vision research, part segmentation and recognition has been acknowledged an i...",
            "group": 1060,
            "name": "10.1.1.46.2991",
            "keyword": "",
            "title": "Model-Driven Grouping and Recognition of Generic Object Parts from Single Images"
        },
        {
            "abstract": "We advocate a new methodology for empirically analysing the behaviour of Las Vegas Algorithms, a large class of probabilistic algorithms comprising prominent methods such as local search algorithms for SAT and CSPs, like WalkSAT and the Min-Conflicts Heuristic, as well as more general metaheuristics like Genetic Algorithms, Simulated Annealing, Iterated Local Search, and Ant Colony Optimization. Our method is based on measuring and analysing run-time distributions (RTDs) for individual problem instances. We discuss this empirical methodology and its application to Las Vegas Algorithms for various problem domains. Our experience so far strongly suggests that using this approach for studying the behaviour of Las Vegas Algorithms can provide a basis for improving the understanding of these algorithms and thus facilitate further successes in their development and application. ",
            "group": 1061,
            "name": "10.1.1.46.3307",
            "keyword": "Tabu Search [2Simulated Annealing [11Genetic Algorithms [3Evolution Strategies [1415Ant Colony Optimisation [1or I",
            "title": "On the Empirical Evaluation of Las Vegas Algorithms"
        },
        {
            "abstract": "Before we apply nonlinear techniques, for example those inspired by chaos theory, to dynamical phenomena occurring in nature, it is necessary to first ask if the use of such advanced techniques is justified by the data.  While many processes in nature seem very unlikely a priori to be linear, the possible nonlinear nature might not be evident in specific aspects of their dynamics. The method of surrogate data has become a very popular tool to address such a question. However, while it was meant to provide a statistically rigorous, foolproof framework, some limitations and caveats have shown up in its practical use. In this paper, recent efforts to understand the caveats, avoid the pitfalls, and to overcome some of the limitations, are reviewed and augmented by new material. In particular, we will discuss specific as well as more general approaches to constrained randomisation, providing a full range of examples. New algorithms will be introduced for unevenly sampled and multivariate da...",
            "group": 1062,
            "name": "10.1.1.46.3999",
            "keyword": "",
            "title": "Surrogate Time Series"
        },
        {
            "abstract": "It has recently been shown that local search is surprisingly good at finding satisfying assignments for certain classes of CNF formulas (Selman  et al. 1992). In this paper we demonstrate that the power of local search for satisfiability testing can be further enhanced by employing a new strategy, called \"mixed random walk\", for escaping from local minima. We present a detailed comparison of this strategy with simulated annealing, and show that mixed random walk is the superior strategy on several classes of computationally difficult problem instances. We also present results demonstrating the effectiveness of local search with walk for solving circuit synthesis and diagnosis problems. Finally, we show that mixed random walk improves upon the results of Hansen and Jaumard on MAX-SAT problems.  1 Introduction  Local search algorithms have been successfully applied to many optimization problems. Hansen and Jaumard (1990) describe experiments using local search for MAX-SAT, i.e., the prob...",
            "group": 1063,
            "name": "10.1.1.46.4061",
            "keyword": "",
            "title": "Local Search Strategies for Satisfiability Testing"
        },
        {
            "abstract": "In most commercial Field-Programmable Gate Arrays (FPGAs) the number of wiring tracks in each channel is the same across the entire chip. A long-standing open question for both FPGAs and channelled gate arrays is whether or not some uneven distribution of routing tracks across the chip would lead to an area benefit. For example, many circuit designers intuitively believe that most congestion occurs near the center of a chip, and hence expect that having wider routing channels near the chip center would be beneficial. In this paper we determine the relative area-efficiency of several different routing track distributions. We first investigate FPGAs in which horizontal and vertical channels contain different numbers of tracks in order to determine if such a directional bias provides a density advantage. Secondly, we examine routing track distributions in which the track capacities vary from channel to channel. We compare the area-efficiency of these non-uniform routing architectures to t...",
            "group": 1064,
            "name": "10.1.1.46.4766",
            "keyword": "",
            "title": "Effect of the Prefabricated Routing Track Distribution on FPGA Area-Efficiency"
        },
        {
            "abstract": "The fourth-order cumulant matching method has been  developed recently for estimating amixed-phase wavelet  from a convolutional process. Matching between the  trace cumulant and the wavelet moment is done in a minimum  mean-squared error sense under the assumption  of a non-Gaussian, stationary, and statistically independent  reflectivity series. This leads to a highly nonlinear  optimization problem, usually solved by techniques that  require a certain degree of linearization, and that invariably  converge to the minimum closest to the initial  model. Alternatively, we propose a hybrid strategy that  makes use of a simulated annealing algorithm to provide  reliability of the numerical solutions by reducing the risk  of being trapped in local minima.  Beyond the numerical aspect, the reliability of the derived  wavelets depends strongly on the amount of data  available. However, by using a multidimensional taper to  smooth the trace cumulant, we show that the method can  be used even ...",
            "group": 1065,
            "name": "10.1.1.46.4771",
            "keyword": "",
            "title": "Simulated Annealing Wavelet Estimation Via Fourth-Order Cumulant Matching"
        },
        {
            "abstract": "We describe the capabilities of and algorithms used in a new FPGA CAD tool,  Versatile Place and Route (VPR). In terms of minimizing routing area, VPR outperforms  all published FPGA place and route tools to which we can compare.  Although the algorithms used are based on previously known approaches, we  present several enhancements that improve run-time and quality. We present placement  and routing results on a new set of large circuits to allow future benchmark  comparisons of FPGA place and route tools on circuit sizes more typical of today's  industrial designs.  VPR is capable of targeting a broad range of FPGA architectures, and the source  code is publicly available. It and the associated netlist translation / clustering tool  VPACK have already been used in a number of research projects worldwide, and  should be useful in many areas of FPGA architecture research.  ",
            "group": 1066,
            "name": "10.1.1.46.4815",
            "keyword": "",
            "title": "VPR: A New Packing, Placement and Routing Tool for FPGA Research"
        },
        {
            "abstract": "The aim of this paper is to contribute to a central issue in neural network that is of combining expert knowledge and observations (data) for learning. It is generally known that neural networks, as other adaptive models, have good learning and generalization capabilities because of their statistical consistency. However, such consistency is theoretically valid only for large size training sets. To enhance learning with small size sets, it is naturally desirable to incorporate additional knowledge (herein referred to as expert knowledge) in the architecture of the network to allow a task-oriented rather than just a generic problem-free architecture. In this paper we show how expert knowledge in terms of fuzzy rules can be incorporated into the architecture of a neural network. The parameters of the network are therefore adapted from the available data using a gradient descent learning algorithm which is an adaptation of backpropagation. The neuro-fuzzy system introduced is successfully...",
            "group": 1067,
            "name": "10.1.1.46.4969",
            "keyword": "Key WordsFuzzy modelingNeural networksHybrid systemsNeuro-fuzzy",
            "title": "Combining Fuzzy Knowledge and Data for Neuro-Fuzzy Modeling"
        },
        {
            "abstract": "Task Requirements input output Reconfigurable Modular Manipulator Systems Reconfigurable Modular Manipulator System? Is Task Program executable by - VAL II - C code - .... - D-H parameters - Material specifications - Motor speicifications - Module specifications - positions/orientations - force application - accuracy - dexterity - obstacles - .... Figure 1: Definition of a general purpose manipulator. also the notion of the user writing device (or manipulator) independent code. The RMMS raises several theoretical issues and it is our aim to address one of these in this paper. Specifically, we describe a design methodology that accepts a task specification as its input, determines a kinematic configuration of the desired manipulator and selects the modules to create this manipulator. In order to support the current practice of picking the best configuration amongst available robots, several expert systems have been built to aid the user or the applications development engineer [15]. A s...",
            "group": 1068,
            "name": "10.1.1.46.5427",
            "keyword": "",
            "title": "Design of Modular Fault Tolerant Manipulators"
        },
        {
            "abstract": "The aim of this paper is to validate the claim that neural networks appear to have much in common with the behavioristic view of learning. Neural networks are, according to Rumelhart & McClelland (1986), not behavioristic because of their explicit concern with the problem of internal representation and \"mental\" processing. The claim that neural networks are behavioristic has epistemological implications. Neural network learning theories, hereunder supervised and unsupervised learning, are compared to psychological learning theories in the two epistemological doctrines: empiricism and rationalism. The results indicate that neural networks exhibit interesting features of self-organization, implicit clustering of inner representation, and plasticity. However, the discussion also indicates that neural networks have similar features as the behavioristic learning theories of psychology.  Keywords: Philosophy of Science, Learning theories, Neural Networks, Artificial Intelligence   I want to ...",
            "group": 1069,
            "name": "10.1.1.46.7471",
            "keyword": "",
            "title": "The Epistemology of Learning in Artificial Neural Networks"
        },
        {
            "abstract": "Automatic design optimization is highly sensitive to problem formulation. The choice of objective function, constraints and design parameters can dramatically impact the computational cost of optimization and the quality of the resulting design. The best formulation varies from one application to another. A design engineer will usually not know the best formulation in advance. In order to address this problem, we have developed a system that supports interactive formulation, testing and reformulation of design optimization strategies. Our system includes an executable, data-flow language for representing optimization strategies. The language allows an engineer to define multiple stages of optimization, each using different approximations of the objective and constraints or different abstractions of the design space. We have also developed a set of transformations that reformulate strategies represented in our language. The transformations can approximate objective and constraint functi...",
            "group": 1070,
            "name": "10.1.1.46.8326",
            "keyword": "",
            "title": "A Transformation System for Interactive Reformulation of Design Optimization Strategies"
        },
        {
            "abstract": "We explore neural network and related heuristic methods for the fast approximate solution of the Maximum Clique problem. One of these algorithms, Mean Field Annealing, is implemented on the Connection Machine CM-5 and a fast annealing schedule is experimentally evaluated on random graphs, as well as on several benchmark graphs. The other algorithms, which perform certain randomized local search operations, are evaluated on the same benchmark graphs, and on Sanchis graphs. One of our algorithms adjusts its internal parameters as its computation evolves. On Sanchis graphs, it finds significantly larger cliques than the other algorithms do. Another algorithm, GSD(;), works best overall, but is slower than the others. All our algorithms obtain significantly larger cliques than other simpler heuristics but run slightly slower; they obtain significantly smaller cliques on average than exact algorithms or more sophisticated heuristics but run considerably faster. All our algorithms are simple...",
            "group": 1071,
            "name": "10.1.1.46.8546",
            "keyword": "",
            "title": "Approximately Solving Maximum Clique using Neural Network and Related Heuristics"
        },
        {
            "abstract": "Genetic programming suffers difficulty in discovering useful numeric constants for the terminal nodes of its sexpression trees. In earlier work we postulated a solution to this problem called  numeric mutation. Here, we provide empirical evidence to demonstrate that this method provides a statistically significant improvement in GP system performance on a variety of problems.  1 Introduction  A weakness of genetic programming (GP) is the difficulty it suffers in discovering useful numeric constants for the terminal nodes of the s-expression trees. At first glance, this weakness is somewhat surprising: Genetic Algorithms, from which GP is derived, are highly suited to the task of optimizing numeric parameters. GP's difficulty with numeric constants derives from their representation as tree nodes, while the reproduction operations (including mating and mutation) affect only the structure of the trees, not the composition of the nodes. Therefore the individual numeric constants are not af...",
            "group": 1072,
            "name": "10.1.1.46.8616",
            "keyword": "",
            "title": "Numeric Mutation Improves the Discovery of Numeric Constants in Genetic Programming"
        },
        {
            "abstract": "We investigate a topological design and routing problem for Low-Earth Orbit (LEO) satellite communication networks where each satellite can have a limited number of direct inter-satellite links (ISL's) to a subset of satellites within its line-of-sight. First, we model LEO satellite network as a FSA (Finite State Automaton) using satellite constellation information. Second, we solve a combined topological design and routing problem for each configuration corresponding to a state in the FSA. The topological design (or link assignment) problem deals with the selection of ISL's, and the routing problem handles the traffic distribution over the selected links to maximize the number of carried calls. In this paper, this NP-complete mixed integer optimization problem is solved by a two-step heuristic algorithm that first solves the topological design problem, and then finds the optimal routing. The algorithm is iterated using the simulated annealing technique until the near-optimal solution ...",
            "group": 1073,
            "name": "10.1.1.46.9249",
            "keyword": "LEO satellite networktopological designroutingsimulated",
            "title": "Topological Design And Routing For Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "The past twenty years has seen a rapid growth of interest in stochastic  search algorithms, particularly those inspired by natural processes in physics  and biology. Impressive results have been demonstrated on complex practical optimisation  problems and related search applications taken from a variety of fields,  but the theoretical understanding of these algorithms remains weak. This results  partly from the insufficient attention that has been paid to results showing certain  fundamental limitations on universal search algorithms, including the so-called  \"No Free Lunch\" Theorem. This paper extends these results and draws out some  of their implications for the design of search algorithms, and for the construction  of useful representations. The resulting insights focus attention on tailoring algorithms  and representations to particular problem classes by exploiting domain  knowledge. This highlights the fundamental importance of gaining a better theoretical  grasp of the ways i...",
            "group": 1074,
            "name": "10.1.1.46.9783",
            "keyword": "",
            "title": "Fundamental Limitations on Search Algorithms: Evolutionary Computing in Perspective"
        },
        {
            "abstract": "We investigate a topological design and routing problem for Low-Earth Orbit (LEO)  satellite communication networks where each satellite can have a limited number of direct  inter-satellite links to a subset of satellites within its line-of-sight. First, we model LEO  system as a FSA(Finite State Automata) using satellite constellation information. Second,  we solve a topological design and routing problem on the configuration corresponding  to each state in the FSA. The topological design (or link assignment) problem deals  with the selection of inter-satellite links, and the routing problem handles the traffic  distribution over the selected links to maximize the number of carried calls. In this paper,  this NP-complete mixed integer optimization problem is solved by a two-step heuristic  algorithm that first solves the topological design problem, and then finds the optimal  routing. The algorithm is iterated using the simulated annealing technique until the nearoptimal  solution is ...",
            "group": 1075,
            "name": "10.1.1.47.170",
            "keyword": "LEO satellite networktopological designroutingsimulated annealing y",
            "title": "Topological Design and Routing for Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "In hard real-time systems, a timing fault may lead the environment to catastrophe. Thus, a  real-time program must be executed in accordance with the timing constraints specified in the  control program. Dynamic scheduling approach which has been dominant for real-time systems  are unacceptable due to run-time scheduling overhead and unpredictable risks. In contrast, a  static approach takes advantage of thorough search for scheduling, no scheduling overhead, and  guaranteed execution of compiled program.  However, finding an optimal schedule for real-time tasks with precedence is known as NPhard  problem, demanding a heuristic approach. In this paper, we propose an adaptive genetic  approach to find a valid schedule. Adaptive genetic approach implies that the algorithm relies  on not only stochastic behavior of the search but also deterministic behavior based on program  structure. This paper includes new computation model with arbitrary before and after  constraints, description of m...",
            "group": 1076,
            "name": "10.1.1.47.190",
            "keyword": "",
            "title": "Adaptive Genetic Algorithm: Scheduling Hard Real-time Control Programs with Arbitrary Timing Constraints"
        },
        {
            "abstract": "We propose a new framework for the link assignment (i.e., topological design) problem that arises from the use of inter-satellite links (ISL's) in Low-Earth Orbit (LEO) satellite networks. In the proposed framework, we model a LEO satellite network as a Finite State Automaton (FSA) where each state corresponds to an equal-length interval in the system period of the LEO satellite network. This FSA-based framework allows the link assignment problem in LEO satellite networks to be treated as a set of link assignment problems in fixed topology networks. Within this framework, we study various link assignment and routing schemes. In particular, both regular link assignment and link assignment optimized by simulated annealing are considered. For each link assignment, both static and dynamic routing schemes are considered. Our simulation results show that the optimized link assignment combined with static routing achieves the best performance in terms of both newly initiated call blocking pro...",
            "group": 1077,
            "name": "10.1.1.47.370",
            "keyword": "",
            "title": "FSA-Based Link Assignment and Routing in Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "INTRODUCTION  Computer scientists should have a knowledge of abstract statistical thermodynamics. First, computer systems are dynamical systems, much like physical systems, and therefore an important first step in their characterization is in finding properties and parameters that are constant over time (i.e., constants of motion). Second, statistical thermodynamics successfully reduces macroscopic properties of a system to the statistical behavior of large numbers of microscopic processes. As computer systems become large assemblages of small components, an explanation of their macroscopic behavior may also be obtained as the aggregate statistical behavior of its component parts. If not, the elegance of the statistical thermodynamical approach can at least provide inspiration for new classes of models.  1  Third, the components of computer systems are approaching the same size as the microscopic pr",
            "group": 1078,
            "name": "10.1.1.47.422",
            "keyword": "",
            "title": "Thermodynamics and Garbage Collection"
        },
        {
            "abstract": " ",
            "group": 1079,
            "name": "10.1.1.47.525",
            "keyword": "",
            "title": "Steiner Minimal Trees: An Introduction, Parallel Computation, and Future Work"
        },
        {
            "abstract": "This paper introduces the notion of using co-evolution to adapt the penalty factors of a fitness function incorporated in a genetic algorithm for numerical optimization. The proposed approach produces solutions even better than those previously reported in the literature for other (GA-based and mathematical programming) techniques that have been particularly fine-tuned using a normally lengthy trial and error process to solve a certain problem or set of problems. The present technique is also easy to implement and suitable for parallelization, which is a necessary further step to improve its current performance.  Key words: genetic algorithms, constraint handling, co-evolution, penalty  functions, self-adaptation, evolutionary optimization, numerical optimization.  1 Introduction  The importance of genetic algorithms (GAs) as a powerful tool for engineering optimization has been widely shown in the last few years through a vast amount of applications ([1,2]). However, even when GAs hav...",
            "group": 1080,
            "name": "10.1.1.47.637",
            "keyword": "",
            "title": "Use of a Self-Adaptive Penalty Approach for Engineering Optimization Problems"
        },
        {
            "abstract": "An analysis of the dynamic behavior of Evolution Strategies applied to Traveling Salesman Problems is presented. For a special class of Traveling Salesman Problems a stochastic model of the optimization process is introduced. Based on this model different features determining the optimization process of Evolution Strategies are analyzed. In addition the stochastic model is extended to explain some aspects of Simulated Annealing.   ",
            "group": 1081,
            "name": "10.1.1.47.1223",
            "keyword": "",
            "title": "The Dynamics of Evolution Strategies in the Optimization of Traveling Salesman Problems"
        },
        {
            "abstract": "We compare the performances of two different routing schemes for LEO satellite  networks through simulation. The first one is a dynamic routing scheme based on the  shortest-path algorithm. Its routing table is updated according to cost estimation based  on link status information that is broadcasted periodically. The second one is an optimal  routing scheme proposed by the authors of this paper, where a LEO satellite network is  modeled by a Finite State Automaton (FSA). The states of the FSA are derived from  the relative visibility on the basis of the constellation data. In each state, the LEO satellite  network has a fixed topology. Since LEO satellite networks exhibit periodic orbit  movements, they can have only a finite number of states. In the proposed scheme, the  link assignment and routing problems are solved off-line for each state in the FSA using  simulated annealing and the optimal routing algorithm. The resulting link assignment  table and the routing table are stored a...",
            "group": 1082,
            "name": "10.1.1.47.2839",
            "keyword": "LEO satellite networkFSAlink assignmentroutingongoing call blocking",
            "title": "Performance Comparison of Optimal Routing and Dynamic Routing On Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required an ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches - CI tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying Bayesian network structure using a non CI test based method. Results of the evaluation of the algorithm on a number of databases (e.g. ALARM, LED and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems. KEYWORDS: Bayesian Networks, Probabilistic Networks, Probabilistic Model Construction, Conditional Independence. 1. INTRODUCTION In very general terms, different methods of learning probabilistic network structures from data can be classified into three groups. Some of these A preliminary version of this paper was presented in [1]. y Curre...",
            "group": 1083,
            "name": "10.1.1.47.3463",
            "keyword": "Bayesian NetworksProbabilistic NetworksProbabilistic Model ConstructionConditional Independence",
            "title": "Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm"
        },
        {
            "abstract": "In previous work we presented an algorithm for matching features extracted from an image with those extracted from a model, using a probabilistic relaxation method. We tested the algorithm by using it to locate on a map road segments extracted from an aerial image. The algorithm performed well, and in particular converged very rapidly. Because the algorithm compares each possible match with all other possible matches, the main obstacle to its use on large data sets is that both the computation time and the memory usage are proportional to the square of the number of possible matches. This paper describes some improvements to the algorithm to alleviate these problems. The key sections of the algorithm are the generation, storage and use of the compatibility coefficients. We describe three different schemes that reduce the number of these coefficients, thereby reducing considerably the memory requirements and computation time for the algorithm. The execution time is improved in each case...",
            "group": 1084,
            "name": "10.1.1.47.3721",
            "keyword": "",
            "title": "Matching of Road Segments Using Probabilistic Relaxation: Reducing the Computational Requirements"
        },
        {
            "abstract": ". I present a new Markov chain sampling method appropriate for distributions with isolated modes. Like the recently-developed method of \"simulated tempering\", the \"tempered transition\" method uses a series of distributions that interpolate between the distribution of interest and a distribution for which sampling is easier. The new method has the advantage that it does not require approximate values for the normalizing constants of these distributions, which are needed for simulated tempering, and can be tedious to estimate. Simulated tempering performs a random walk along the series of distributions used. In contrast, the tempered transitions of the new method move systematically from the desired distribution, to the easily-sampled distribution, and back to the desired distribution. This systematic movement avoids the inefficiency of a random walk, an advantage that unfortunately is cancelled by an increase in the number of interpolating distributions required. Because of this, the sa...",
            "group": 1085,
            "name": "10.1.1.47.4403",
            "keyword": "",
            "title": "Sampling from Multimodal Distributions Using Tempered Transitions"
        },
        {
            "abstract": "We present an optimal solution to the problem of allocating communicating periodic tasks to heterogeneous processing nodes (PNs) in a distributed real-time system. The solution is optimal in the sense of minimizing the maximum normalized task response time, called the system hazard , subject to the precedence constraints resulting from intercommunication among the tasks to be allocated. Minimization of the system hazard ensures that the solution algorithm will allocate tasks so as to meet all task deadlines under an optimal schedule, whenever such an allocation exists. The task system is modeled with a task graph (TG), in which computation and communication modules, communication delays, and intertask precedence constraints are clearly described. Tasks described by this TG are assigned to PNs by using a branch-and-bound (B&B) search algorithm. The algorithm traverses a search tree whose leaves correspond to potential solutions to the task allocation problem. We use a bounding method th...",
            "group": 1086,
            "name": "10.1.1.47.4490",
            "keyword": "",
            "title": "Assignment and Scheduling Communicating Periodic Tasks in Distributed Real-Time Systems"
        },
        {
            "abstract": "This paper presents two heuristics for automatic hardware/software partitioning of system level specifications. Partitioning is performed at the granularity of blocks, loops, subprograms, and processes with the objective of performance optimization with a limited hardware and software cost. We define the metric values for partitioning and develop a cost function that guides partitioning towards the desired objective. We consider minimization of communication cost and improvement of the overall parallelism as essential criteria during partitioning. Two heuristics for hardware /software partitioning, formulated as a graph partitioning problem, are presented: one based on simulated annealing and the other on tabu search. Results of extensive experiments, including real-life examples, show the clear superiority of the tabu search based algorithm. Keywords: Hardware/software partitioning, Co-synthesis, Iterative improvement heuristics, Simulated annealing, Tabu search * This work has been p...",
            "group": 1087,
            "name": "10.1.1.47.4721",
            "keyword": "",
            "title": "System Level Hardware/Software Partitioning Based on Simulated Annealing and Tabu Search"
        },
        {
            "abstract": "Recognizing and locating objects is crucial to robotic operations in unstructured environments. To satisfy this need, we have developed an interactive system for creating object models from range data based on simulated annealing and supervisory control  a  . This interactive modeling system maximizes the advantages of both manual and autonomous methods while minimizing their weaknesses. Therefore, it should outperform purely autonomous or manual techniques. We have designed and executed experiments for the purpose of evaluating the performance of our application as compared to an identical but purely manually driven application. These experiments confirmed the following hypotheses:  . Interactive modeling should outperform purely manual modeling in total task time and fitting accuracy.  . Operator effort decreases significantly when utilizing interactive modeling.  . User expertise does not have a significant effect on interactive modeling task time.  . Minimal human interaction will ...",
            "group": 1088,
            "name": "10.1.1.47.6336",
            "keyword": "",
            "title": "Evaluation Of An Interactive Technique For Creating Site Models From Range Data"
        },
        {
            "abstract": "In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures. ",
            "group": 1089,
            "name": "10.1.1.47.6497",
            "keyword": "",
            "title": "MIMIC: Finding Optima by Estimating Probability Densities"
        },
        {
            "abstract": " ",
            "group": 1090,
            "name": "10.1.1.47.6882",
            "keyword": "",
            "title": "Procedure for ranking heuristics . . . "
        },
        {
            "abstract": "This paper explores how communication can be understood as an adaptation by agents to their environment. We model agents as recurrent neural networks. After arguing against systems which use discrete symbols to evolve communication, we supply our agents with a number of continuous communications channels. The agents use these channels to initiate real-valued signals which propagate through the environment, decaying over distance, perhaps being perturbed by environmental noise. Initially, the agents' signals appear random; over time, a structure emerges as the agents learn to communicate task-specific information about their environment. We demonstrate how different communication schemes can evolve for a task, and then discover a commonality between the schemes in terms of information passed between agents. From this we discuss what it means to communicate, and describe how a semantics emerges in the agents' signals relative to their task domain. Key Words: communication; evolutionary a...",
            "group": 1091,
            "name": "10.1.1.47.6902",
            "keyword": "",
            "title": "The Evolution of Communication in Adaptive Agents"
        },
        {
            "abstract": "The space layout planning problem is one of the most difficult in architectural design. It is practically important in architectural design because it is the basis of the development of most designs. It is important in a wider context because it maps onto a large class of location-allocation problems including VLSI floorplanning, process layouts and facilities layout problems. We will use the formalization of the space layout problem as a particular case of a combinatorial optimization problem - a quadratic assignment problem [9]. As such it is NP-complete and presents all the difficulties associated with this class of problems. Over the years a number of approximate algorithms based on combinations of global and local search techniques and heuristics have been developed specifically for this class of problems. Although they are reasonably efficient for small-scale problems, the computational cost is still too high for large-scale problems. Another shortcoming of t...",
            "group": 1092,
            "name": "10.1.1.47.6905",
            "keyword": "",
            "title": "Evolving Design Genes in Space Layout Planning Problems"
        },
        {
            "abstract": ". The analogy between combinatorial optimization and statistical mechanics has proven to be a fruitful object of study. Simulated annealing, a metaheuristic for combinatorial optimization problems, is based on this analogy. In this paper we use the statistical mechanics formalism based on the above mentioned analogy to analyze the asymptotic behavior of a special class of combinatorial optimization problems characterized by a combinatorial conditions which is well known in the literature. Our result is analogous to results of other authors derived by purely probabilistic means: Under natural probabilistic conditions on the coefficients of the problem, the ratio between the optimal value and size of a feasible solution approaches almost surely the expected value of the coefficients, as the size of the problem tends to infinity. Our proof shows clearly why the above mentioned combinatorial condition which characterizes the class of investigated problems is essential.  Keywords: combinato...",
            "group": 1093,
            "name": "10.1.1.47.6974",
            "keyword": "combinatorial problemasymptotic behaviorprobabilistic analysisstatistical mechanics",
            "title": "An Asymptotical Study of Combinatorial Optimization Problems By Means of Statistical Mechanics"
        },
        {
            "abstract": "This report studies the problem of automatically selecting a suitable system architecture for implementing a real-time application. Given a library of hardware components, it is shown how an architecture can be synthesized with the goal of fulfilling the real-time constraints stated in the system's specification. In case the selected architecture contains several processing units, the specification is partitioned by assigning tasks to processing units. We investigate the use of three meta-heuristic search algorithms to solve the problem: genetic algorithms, simulated annealing, and tabu search; and it is described in detail how these can be adapted to the architecture synthesis problem. Their relative merits are discussed at length, as is the importance of scheduling to the solution quality. This work has been supported by the Swedish National Board for Industrial and Technical Development (NUTEK).   Contents  1 Introduction 1 2 Related work 1 3 Problem description 3  3.1 Behavioural...",
            "group": 1094,
            "name": "10.1.1.47.7213",
            "keyword": "",
            "title": "Three Search Strategies for Architecture Synthesis and Partitioning of Real-Time Systems"
        },
        {
            "abstract": "this paper. I express great thanksr for his kindness. Finally, I must thank my parents for their support and encouragement throughout my undergraduate and graduate studies. They allowed me to choose my own way, and always supported and encouraged me throughout my life. I will close my acknowledgment by wishing them both health and happiness. Takashi Kido February, 1996 ii  Contents 1 Introduction 1",
            "group": 1095,
            "name": "10.1.1.47.7238",
            "keyword": "",
            "title": "A Study of Evolutional Mechanism on Cooperative Problem Solving - A Hybrid Genetic Algorithm for Multi-Algorithm Problem"
        },
        {
            "abstract": "this paper, we examine the application of the combinatorial optimisation technique of Guided Local Search to the Radio Link Frequency Assignment Problem (RLFAP). RLFAP stems from real world situations in military telecommunications and it is known to be an NP-hard problem. Guided Local Search is a metaheuristic that sits on top of local search procedures allowing them to escape from local minima. GLS is shown to be superior to other methods proposed in the literature for the problem, making it the best choice for solving RLFAPs. 2. INTRODUCTION",
            "group": 1096,
            "name": "10.1.1.47.7676",
            "keyword": "",
            "title": "Solving the Radio Link Frequency Assignment Problem using Guided Local Search"
        },
        {
            "abstract": "The interconnection of geographically distributed supercomputers via highspeed networks makes available the needed compute power for large-scale scientific applications, such as finite element applications. In this paper we propose a two-level data decomposition method for efficient execution of finite element applications on a network of supercomputers. Our method exploits the following features that may be different for each supercomputer in the system: processor speed, number of processors used from each supercomputer, local network performance, wide area network performance and wide area topology. Preliminary experiments involving a nonlinear, finite element application executed on a network of two supercomputers, one located at Argonne National Laboratory and the other one at the Cornell Theory Center, demonstrate a 20% reduction in execution time when the proposed decomposition is used as compared with naively applying conventional decompositions that are applicable to single sup...",
            "group": 1097,
            "name": "10.1.1.47.8178",
            "keyword": "",
            "title": "A Decomposition Method for Efficient Use of Distributed Supercomputers for Finite Element Applications"
        },
        {
            "abstract": "this paper. For example, output from testing Rosenbrock's function for 12 variables consists of the following:  20 X0 VECTOR: -1.20 1.00-1.20 1.00 -1.20 1.00-1.20 1.00 -1.20 1.00-1.20 1.00 Y VECTOR: -1.09 0.77-0.88 0.64 0.71 0.58 0.94-0.90 -0.62 0.77-0.90-0.98 ENTERING TESTGH ROUTINE: THE FUNCTION VALUE AT X = 1.45200000E+02 THE FIRST-ORDER TAYLOR TERM, (G, Y) = 3.19353760E+02 THE SECOND-ORDER TAYLOR TERM, (Y,HY) = 5.39772665E+03 EPSMIN = 1.42108547E-14 EPS F TAYLOR DIFF. RATIO 5.0000E-01 1.09854129E+03 9.79592712E+02 1.18948574E+02 2.5000E-01 4.07080835E+02 3.93717398E+02 1.33634374E+01 8.90104621E+00 1.2500E-01 2.28865318E+02 2.27288959E+02 1.57635878E+00 8.47740855E+00 6.2500E-02 1.75893210E+02 1.75702045E+02 1.91165417E-01 8.24604580E+00 3.1250E-02 1.57838942E+02 1.57815414E+02 2.35282126E-02 8.12494428E+00 1.5625E-02 1.50851723E+02 1.50848805E+02 2.91806005E-03 8.06296382E+00 7.8125E-03 1.47860040E+02 1.47859677E+02 3.63322099E-04 8.03160629E+00 3.9063E-03 1.46488702E+02 1.46488657E+02 4.53255493E-05 8.01583443E+00 1.9531E-03 1.45834039E+02 1.45834033E+02 5.66008660E-06 8.00792506E+00 9.7656E-04 1.45514443E+02 1.45514443E+02 7.07160371E-07 8.00396463E+00 4.8828E-04 1.45356578E+02 1.45356578E+02 8.83731524E-08 8.00198196E+00 DIFF IS SMALL (LESS THAN 2.97291798E-08 IN ABSOLUTE VALUE) Note that the RATIO is larger than eight when EPS is larger and then decreases steadily. A small error in the code would produce much different values. We encourage the student to try this testing routine on several subroutines that compute objective functions and their derivatives; errors should be introduced into the derivative codes systematically to examine the ability of TESTGH to detect them and provide the right diagnosis, as outlined above.  Methods for Unconstrained Continuous...",
            "group": 1098,
            "name": "10.1.1.47.8187",
            "keyword": "",
            "title": "MO Mathematical Optimization"
        },
        {
            "abstract": "this paper we present a simple optimisation technique based on combining the Langevin Equation (LE)-based optimisation and the Hopfield Model (HM) optimisation features. Computer simulations of the resulting model - reffered as the Stochastic Model (SM) - are presented and discussed. Preliminary experimental results obtained for the Travelling Salesman Problem (TSP) of small sizes (5 \\Gamma 10 cities) show that the model is efficient in solving the problem.",
            "group": 1099,
            "name": "10.1.1.47.8406",
            "keyword": "",
            "title": "Non-delta-correlated gaussian noises and the Hopfield optimisation circuit: an empirical study"
        },
        {
            "abstract": "We consider time--inhomogeneous Markov chains on a finite state--space, whose transition probabilities p ij (t) = c ij ffl(t)  V ij  are proportional to powers of a vanishing small parameter ffl(t). We determine the precise relationship between this chain, and the corresponding time--homogeneous chains p ij = c ij ffl  V ij  , as ffl & 0. Let f  ffl i g be the steady--state distribution of this time--homogeneous chain. We characterize the orders  fj i g in   ffl i = \\Theta(ffl  j i  ). We show that if ffl(t) & 0 slowly enough, then the time--wise occupation measures fi i := supfq ? 0 j  P 1  t=1 ffl(t)  q  Prob(x(t) = i) = +1g, called the recurrence orders, satisfy fi i \\Gamma fi j = j j \\Gamma j i . Moreover, if G := fj i j j i = min j j j g  is the set of \"ground states\" of the time--homogeneous chain, then x(t) ! G, in an appropriate sense, whenever j(t) is \"cooled\" slowly. We also show that there exists a critical ae    such that x(t) ! G if and only if  P 1  t=1  ffl(t)  ae    = +...",
            "group": 1100,
            "name": "10.1.1.47.8450",
            "keyword": "",
            "title": "Quasi-Statically Cooled Markov Chains"
        },
        {
            "abstract": "this paper we address this difficulty with an adaptive multi-start methodology that is based on new insights into global structure of optimization cost surfaces. For instances of the symmetric TSP and graph bisection, we study correlations between the cost of a local minimum and its average distance to all other local minima (as well as its distance to the best-found local minimum). Our analyses show evidence of a \"big valley\" governing local minima in the optimization cost surface, and motivate the adaptive multi-start methodology. In other words, our evidence suggests a",
            "group": 1101,
            "name": "10.1.1.47.8505",
            "keyword": "",
            "title": ".00 15.00 20.00 25.00 30.00 35.00 40.00 45.00 50.00 55.00 60.00 19.00 20.00 21.00 22.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 45.00 50.00 55.00 60.00 0.00 5.00 10.00 15.00 20.00 25.00"
        },
        {
            "abstract": ". Genetic Algorithms have been an active research area for more than three decades, but the industrial applications of this search technique have been scarce. There may be several reasons for this. The EVALIA  1  project (EVolutionary ALgorithms for Industrial Applications) attempts to test the value of Genetic Algorithms on realistic industrial problems. Further a general framework is developed to ease the implementation of optimisation programs for industrial problems. This article reports on the first results of this project, when testing the framework on a real-world planning problem at Odense Steel Shipyard (OSS). Further this article will report on the possibilities of specialised Genetic Algorithm techniques: Adaptive Operators which are used in what we call the shotgun approach, the Pareto technique for multi-objective optimisation and Co-evolutionary Constraint Satisfaction. Keywords: Genetic Algorithms, Simulated Annealing, real world scheduling. 1 Introduction  The rise of c...",
            "group": 1102,
            "name": "10.1.1.47.8643",
            "keyword": "Genetic AlgorithmsSimulated Annealingreal world scheduling",
            "title": "Genetic Algorithms for Industrial Planning"
        },
        {
            "abstract": "Computationalorigami is a parallel-processing concept in which a regular array of processors can be folded along any dimension so that it can be simulated by a smaller number of processors. The problem of assigning functions to each of the processors is very much like the generalized electrical circuit layout problem. This paper presents a simple, polynomial time algorithm for placing and routing functions in an origami architecture. Empirical results are analyzed and optimizations suggested. 1 Introduction  Computational origami is a parallel-processing concept developed by Alan Huang [7, 8, 9]. An origami machine consists of a regular array of processors superimposed on a tessellable mosaic architecture [11]. The array can be arbitrarily \"folded\" along any of its dimensions so that it can be simulated by a smaller number of processors. The latency and throughput of the system can be adjusted by folding the array widthwise or depthwise, respectively. The folding of an origami array is...",
            "group": 1103,
            "name": "10.1.1.47.9310",
            "keyword": "",
            "title": "A Simple Placement and Routing Algorithm for a Two-Dimensional Computational Origami Architecture"
        },
        {
            "abstract": "Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and rele...",
            "group": 1104,
            "name": "10.1.1.47.9423",
            "keyword": "",
            "title": "Learning to Act using Real-Time Dynamic Programming"
        },
        {
            "abstract": "Genetic algorithms have been used for neural networks in two main ways: to optimize the network architecture and to train the weights of a fixed architecture. While most previous work focuses on only one of these two options, this paper investigates an alternative evolutionary approach called Breeder Genetic Programming (BGP) in which the architecture and the weights are optimized simultaneously. The genotype of each network is represented as a tree whose depth and width are dynamically adapted to the particular application by specifically defined genetic operators. The weights are trained by a next-ascent hillclimbing search. A new fitness function is proposed that quantifies the principle of Occam's razor. It makes an optimal trade-off between the error fitting ability and the parsimony of the network. Simulation results on two benchmark problems of differing complexity suggest that the method finds minimal size networks on clean data. The experiments on noisy data show that using Oc...",
            "group": 1105,
            "name": "10.1.1.47.9891",
            "keyword": "",
            "title": "Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor"
        },
        {
            "abstract": "Neural Network models have received increased attention in the recent years. Aimed at achieving human-like performance in tasks of the cognitive sciences domain, these models are composed of a highly interconnected mesh of nonlinear computing elements, whose structure is drawn from our current knowledge of biological neural systems. Several Neural Network Learning Algorithms have been developed in the past years. In these algorithms, a set of rules defines the evolution process undertaken by the synaptic connections of the networks, thus allowing them to learn how to perform specified tasks. In this article, several such algorithms are surveyed. They range from simple associative learning paradigms to more complex reinforcement learning systems. A detailed description of each algorithm is presented, and a discussion of their capabilities and limitations is included.  1 Introduction The history of artificial neural networks starts with the work by McCulloch and Pitts [64] in which neur...",
            "group": 1106,
            "name": "10.1.1.48.942",
            "keyword": "",
            "title": "Learning Algorithms in Neural Networks"
        },
        {
            "abstract": "Given a set of models and some training data, we would like to find the model that best describes the data. Finding the model with the lowest generalization error is a computationally expensive process, especially if the number of testing points is high or if the number of models is large. Optimization techniques such as hill climbing or genetic algorithms are helpful but can end up with a model that is arbitrarily worse than the best one or cannot be used because there is no distance metric on the space of discrete models. In this paper we develop a technique called \"racing\" that tests the set of models in parallel, quickly discards those models that are clearly inferior and concentrates the computational effort on differentiating among the better models. Racing is especially suitable for selecting among lazy learners since training requires negligible expense, and incremental testing using leave-one-out cross validation is efficient. We use racing to select among various lazy learnin...",
            "group": 1107,
            "name": "10.1.1.48.954",
            "keyword": "",
            "title": "The Racing Algorithm: Model Selection for Lazy Learners"
        },
        {
            "abstract": "A fast simulated annealing algorithm is developed for automatic object recognition. The object recognition problem is addressed as the problem of best describing a match between a hypothesized object and an image. The normalized correlation coefficient is used as a measure of the match. Templates are generated on-line during the search by transforming model images. Simulated annealing reduces the search time by orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of how landmarks, for example, traffic signs, can be recognized by an autonomous vehicle or a navigating robot. Images are assumed to be taken while the robot or the vehicle is moving through its environment. It tries to match them with templates created online from models stored in a database. We illustrate the performance of our algorithm with real-world images of complicated scenes with traffic signs. False positive matches occur only for templates with very small information con...",
            "group": 1108,
            "name": "10.1.1.48.1042",
            "keyword": "",
            "title": "Fast Object Recognition in Noisy Images Using Simulated Annealing"
        },
        {
            "abstract": "The paper introduces duty measure for optimization methods. Duty expresses the relationship between the quality of the result and the time required to obtain the result. The usefulness of the duty measure is demonstrated on a case study involving a local optimization of a large traveling salesman problem. Using duty, a deterministic method and a probabilistic method are combined into a hybrid method. The hybrid method exhibits the best quality-time tradeoff of the three methods. The performance of the hybrid method is analyzed and some future research questions are addressed.  Keywords: combinatorial optimization, local search, traveling salesman problem, duty, quality-time tradeoff. 1 Introduction  Local optimization has been widely used for solving numerous practical and theoretical problems. It is being applied to hard problems for which no deterministic, practical algorithms are known. Such hard problems exhibit large search spaces with irregular structure. Local optimization provi...",
            "group": 1109,
            "name": "10.1.1.48.1336",
            "keyword": "combinatorial optimizationlocal searchtraveling salesman problemdutyquality-time tradeoff",
            "title": "Using the Quality-Time Tradeoff in Local Optimization"
        },
        {
            "abstract": "Parity-declustered data layouts were developed to reduce the time for on-line failure recovery in disk arrays. They generally require perfect balancing of reconstruction workload among the disks; this restrictive balance condition makes such data layouts difficult to construct. In this paper, we consider approximately balanced data layouts, where some variation in the reconstruction workload over the disks is permitted. Such layouts are considerably easier to construct than perfectly balanced layouts. We consider three methods for constructing approximately balanced data layouts, and analyze their performance both theoretically and experimentally. We conclude that on uniform workloads, approximately balanced layouts have performance nearly identical to that of perfectly balanced layouts.   A preliminary version of this paper appears in the Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, pp. 41--54, May 1996.  y  schwabe@eecs.nwu.edu. Supported by N...",
            "group": 1110,
            "name": "10.1.1.48.1479",
            "keyword": "RAID systemsdisk arraysfault tolerancedata layoutsparity declustering",
            "title": "Evaluating Approximately Balanced Parity-Declustered Data Layouts for Disk Arrays"
        },
        {
            "abstract": "Traditional connectionist theory-refinement systems map the dependencies of a domainspecific rule base into a neural network, and then refine this network using neural learning techniques. Most of these systems, however, lack the ability to refine their network's topology and are thus unable to add new rules to the (reformulated) rule base. Therefore, on domain theories that are lacking rules, generalization is poor, and training can corrupt the original rules, even those that were initially correct. We present TopGen, an extension to the Kbann algorithm, that heuristically searches for possible expansions to Kbann's network. TopGen does this by dynamically adding hidden nodes to the neural representation of the domain theory, in a manner analogous to adding rules and conjuncts to the symbolic rule base. Experiments indicate that our method is able to heuristically find effective places to add nodes to the knowledge bases of four realworld problems, as well as an artificial chess domai...",
            "group": 1111,
            "name": "10.1.1.48.2048",
            "keyword": "network-growing",
            "title": "Dynamically Adding Symbolically Meaningful Nodes to Knowledge-Based Neural Networks"
        },
        {
            "abstract": "Mappings that preserve neighbourhood relationships are important in many contexts, from neurobiology to multivariate data analysis. It is important to be clear about precisely what is meant by preserving neighbourhoods. At least three issues have to be addressed: how neighbourhoods are defined, how a perfectly neighbourhood preserving mapping is defined, and how an objective function for measuring discrepancies from perfect neighbourhood preservation is defined. We review several standard methods, and using a simple example mapping problem show that the different assumptionsof each lead to non-trivially different answers. We also introduce a particular measure for topographic distortion, which has the form of a quadratic assignmentproblem. Many previous methods are closely related to this measure, which thus serves to unify disparate approaches. 1 Introduction  Problems of mapping occur frequently both in understanding biological processes and in formulating abstract methods of data an...",
            "group": 1112,
            "name": "10.1.1.48.2128",
            "keyword": "",
            "title": "Quantifying Neighbourhood Preservation in Topographic Mappings"
        },
        {
            "abstract": "Automatically synthesizing device drivers, the hardware and software needed to interface a device to a processor, is an important element of hardware/software co-design. Driver software consists of the sequences of instructions needed for the processor to control its interactions with the device. Driver hardware consists of the digital logic necessary to physically connect the devices and generate signal events while meeting timing constraints. We describe an approach that begins with device specifications in the form of timing and state diagrams and determines which signals can be controlled directly from software and which require indirect control through intervening hardware. Minimizing this hardware requires solving a simultaneous scheduling and partitioning problem whose goal is to limit the number of wires whose events are not directly generated by the processor software. We show that even the simplest version of this problem is NP-hard and discuss a heuristic solution that shoul...",
            "group": 1113,
            "name": "10.1.1.48.2305",
            "keyword": "",
            "title": "Automatic Synthesis of Device Drivers for Hardware/Software Co-design"
        },
        {
            "abstract": "This thesis investigates the problem of growing decision trees from data, for the purposes of classification and prediction. After a comprehensive, multi-disciplinary survey of work on decision trees, some algorithmic extensions to existing tree growing methods are considered. The implications of using (1) less greedy search and (2) less restricted splits at tree nodes are systematically studied. Extending the traditional axis-parallel splits to oblique splits is shown to be practical and beneficial for a variety of problems. However, the use of more extensive search heuristics than the traditional greedy heuristic is argued to be unnecessary, and often harmful. Any effort to build good decision trees from real-world data involves \"massaging \" the data into a suitable form. Two forms of data massaging, domain-independent and domain-specific, are distinguished in this work. A new framework is outlined for the former, and the importance of the latter is illustrated in the context of two ...",
            "group": 1114,
            "name": "10.1.1.48.2462",
            "keyword": "",
            "title": "On Growing Better Decision Trees from Data"
        },
        {
            "abstract": "We compare the performances of two routing schemes for LEO satellite networks through simulation. The first one is a dynamic routing scheme based on the shortest-path algorithm, in which the routing table is updated according to cost estimation based on link status information that is broadcasted periodically. The second one is an optimal routing scheme proposed by the authors of this paper, where an LEO satellite network is modeled as a Finite State Automaton (FSA). In the optimal routing scheme, the link assignment and routing problems are solved offline for each state in the FSA using simulated annealing and the optimal routing algorithm. The simulation results show that in the view of newly initiated call blocking and ongoing call blocking, optimal routing gives better performance than dynamic routing. The simulation results also show that if there is constraint that probability of ongoing call blocking must be limited within certain bound then optimal routing is much superior.  Ke...",
            "group": 1115,
            "name": "10.1.1.48.2578",
            "keyword": "LEO satellite networkFSAlink assignmentroutingongoing",
            "title": "Performance Comparison of Optimal Routing and Dynamic Routing in Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "We present a general multi-layer area router for performing detailed routing in integrated circuits. This router is based on a novel grid construction scheme which considers the differing design rules of the routing layers and produces more wiring tracks than a uniform grid scheme. Our router is very general and flexible and is designed to handle all the physical constraints of a CMOS custom cell layout problem for an arbitrary number of routing layers. The router has been incorporated into the Custom Cell Synthesizer project at MCC. It has produced better results than uniform gridded routers and improved the capability of the system by providing routing flexibility and supporting features needed to handle a wide range of design styles in generating CMOS custom cells.  1 Introduction  Automatic custom cell synthesis presents some unique constraints to the routing problem not present in simple channel routing [1], [2] and switchbox routing models [3], [4]. The boundary of the routing pr...",
            "group": 1116,
            "name": "10.1.1.48.2775",
            "keyword": "",
            "title": "Echelon: A Multi-layer Detailed Area Router"
        },
        {
            "abstract": "We introduce a meta-heuristic to combine simulated annealing with local search methods for CO problems. This new class of Markov chains leads to significantly more powerful optimization methods than either simulated annealing or local search. The main idea is to embed deterministic local search techniques into simulated annealing so that the chain explores only local optima. It makes large, global changes, even at low temperatures, thus overcoming large barriers in configuration space. We have tested this meta-heuristic for the traveling salesman and graph partitioning problems. Tests on instances from public libraries and random ensembles quantify the power of the method. Our algorithm is able to solve large instances to optimality, improving upon state of the art local search methods very significantly. For the traveling salesman problem with randomly distributed cities in a square, the procedure improves on 3-opt by 1.6%, and on Lin-Kernighan local search by 1.3%. For the partitioni...",
            "group": 1117,
            "name": "10.1.1.48.3141",
            "keyword": "",
            "title": "Combining Simulated Annealing with Local Search Heuristics"
        },
        {
            "abstract": "The scheduling of loops for architectures which support instruction level parallelism is an important area of research. Many polynomial time, heuristic algorithms for software pipelining have been proposed for this NPcomplete problem. In this research, genetic algorithms and simulated annealing are used to test the feasibility of applying artificial intelligence techniques to the problem of software pipelining. Both algorithms are iterative search algorithms which adjust their response based on feedback from the fitness function. Results indicate these techniques are superior to deterministic polynomial time algorithms. Key Words: software pipelining, instruction-level parallelism, Petri nets, cyclic scheduling, artificial intelligence, genetic algorithm, simulated annealing. 1 Introduction  Frequently code optimization concentrates on loops as loop execution dominates the total execution time of most programs. Software pipelining is a loop optimization technique in which the body of t...",
            "group": 1118,
            "name": "10.1.1.48.3363",
            "keyword": "Key Wordssoftware pipelininginstruction-level parallelismPetri netscyclic schedulingartificial intelligence",
            "title": "Software Pipelining via Stochastic Search Algorithms"
        },
        {
            "abstract": "This paper addresses the problem of designing finite precision one-dimensional (1-D) infinite impulse response (IIR) digital filters for video processing. The design algorithm is based on simultaneous minimization of magnitude, phase and stability errors in a discrete space of solutions using simulated annealing. It is demonstrated that the approach results in filters characterized by a substantially reduced non-linearity of the phase response in filter pass band, which is critical in any video processing application. To reduce image degradations due to ripples of the filter step response, another error term is introduced into the cost function. It is demonstrated that this additional term permits significant reduction of step response overshoots, and thus the visibility of degradations in a filtered image. The designed IIR filters are compared with their finite impulse response (FIR) counterparts in terms of characteristic parameters as well as distortion visibility in processed image...",
            "group": 1119,
            "name": "10.1.1.48.3654",
            "keyword": "",
            "title": "On The Design Of Finite Wordlength IIR Filters For Video Applications"
        },
        {
            "abstract": "We describe a new, linear time heuristic for the improvement of graph bisections. The method is a variant of local search with sophisticated neighborhood relations. It is based on graph-theoretic observations that were used to find upper bounds for the bisection width of regular graphs. Efficiently implemented, the new method can serve as an alternative to the commonly used local heuristics, not only in terms of the quality of attained solutions, but also in terms of space and time requirements. We compare our heuristic with a number of well known bisection algorithms. Extensive measurements show that the new method is a real improvement for graphs of certain types.  Keywords: Graph Partitioning, Graph Bisection, Recursive Bisection, Edge Separators, Mapping, Local Search, Parallel Processing.   This work was partly supported by the German Research Foundation (DFG Forschergruppe \"Effiziente Nutzung massiv paralleler Systeme\") and by the ESPRIT Basic Research Action No. 7141 (ALCOM II)....",
            "group": 1120,
            "name": "10.1.1.48.4299",
            "keyword": "Graph PartitioningGraph BisectionRecursive BisectionEdge SeparatorsMappingLocal SearchParallel Processing",
            "title": "Using Helpful Sets to Improve Graph Bisections"
        },
        {
            "abstract": "We present a new inverse, interactive approach to acoustic design that applies optimization techniques to an acoustic simulation system. The work is motivated by the challenges inherent in achieving desirable acoustic behavior in 3D environments, and it builds on previous work in computer graphics dealing with the analogous problem in lighting design. The user interactively indicates a range of acceptable material and geometric modifications for an auditorium or similar space, and specifies acoustic goals by choosing target values for a set of acoustic measures. Given this set of goals and constraints, the system performs an optimization of surface material and geometric parameters using a combination of simulated annealing and steepest descent techniques. Visualization tools extract and present the simulated sound field data for points sampled in space and time. The user then manipulates the visualizations to create an intuitive expression of acoustic design goals. We demonstrate an i...",
            "group": 1121,
            "name": "10.1.1.48.4801",
            "keyword": "Acoustic modelingspatialized soundoptimizationvisualizationinteractive designsimulated annealingbeam",
            "title": "Audioptimization: Goal Based Acoustic Design"
        },
        {
            "abstract": "Belief networks (or probabilistic networks) and neural networks are two forms of network representations that have been used in the development of intelligent systems in the field of artificial intelligence. Belief networks provide a concise representation of general probability distributions over a set of random variables, and facilitate exact calculation of the impact of evidence on propositions of interest. Neural networks, which represent parameterized algebraic combinations of nonlinear activation functions, have found widespread use as models of real neural systems and as function approximators because of their amenability to simple training algorithms. Furthermore, the simple, local nature of most neural network training algorithms provides a certain biological plausibility and allows for a massively parallel implementation. In this paper, we show that similar local learning algorithms can be derived for belief networks, and that these learning algorithms can operate using only ...",
            "group": 1122,
            "name": "10.1.1.48.5125",
            "keyword": "",
            "title": "Adaptive Probabilistic Networks"
        },
        {
            "abstract": "The combination of local search heuristics and genetic algorithms has been shown to be an effective approach for finding near-optimum solutions to the traveling salesman problem. In this paper, previously proposed genetic local search algorithms for the symmetric and asymmetric traveling salesman problem are revisited and potential improvements are identified. Since local search is the central component in which most of the computation time is spent, improving the efficiency of the local search operators is crucial for improving the overall performance of the algorithms. The modifications of the algorithms are described and the new results obtained are presented. The results indicate that the improved algorithms are able to arrive at better solutions in significantly less time. I. Introduction Consider a salesman who wants to start from his home city, visit each of a set of n cities exactly once, and then return home. Since the salesman is interested in finding the shortest possible r...",
            "group": 1123,
            "name": "10.1.1.48.5310",
            "keyword": "",
            "title": "Genetic Local Search for the TSP: New Results"
        },
        {
            "abstract": ". Recent advances in the theory of deterministic global optimization have resulted in the development of very efficient algorithmic procedures for identifying the global minimum of certain classes of nonconvex optimization problems. The adventof powerful multiprocessormachines combinedwith such developmentsmake it possible to tackle with substantial efficiency otherwise intractable global optimization problems. In this paper, we will discuss implementation issues and computational results associated with the distributed implementation of the decomposition--based global optimization algorithm, GOP, [5], [6]. The NP-complete character of the global optimization problem, translated into extremely high computational requirements, had made it difficult to address problems of large size.The parallel implementation made it possible to successfully tackle the increased computational requirements in in order to identify the global minimum in computationally realistic times. The key computationa...",
            "group": 1124,
            "name": "10.1.1.48.7265",
            "keyword": "and probabilistic. Deterministic methods includeLipschitzian methods9Branch and Bound methods1Cutting Plane methods7Difference of Convex Function methods",
            "title": "Distributed Decomposition--based Approaches in Global Optimization"
        },
        {
            "abstract": "In this paper we try to describe the main characters of Heuristics \"derived\" from Nature, a border area between Operations Research and Artificial Intelligence, with applications to graph optimization problems. These algorithms take inspiration from physics, biology, social sciences, and use a certain amount of repeated trials, given by one or more \"agents\" operating with a mechanism of competition-cooperation. Two introductory sections, devoted respectively to a presentation of some general concepts and to a tentative classification of Heuristics from Nature open the work. The paper is then composed of six review sections: each of them concerns a heuristic and its application to an NP-hard combinatorial optimization problem. We consider the following topics: genetic algorithms with timetable problems, simulated annealing with dial-a-ride problems, sampling & clustering with communication spanning tree problems, tabu search with job-shop-scheduling problems, neural nets with location p...",
            "group": 1125,
            "name": "10.1.1.48.8160",
            "keyword": "",
            "title": "Heuristics From Nature For Hard Combinatorial Optimization Problems"
        },
        {
            "abstract": "this paper, we consider optimality of the natural code in the much larger space of general codes, which are taken to be surjective maps from fA; C; G; Tg",
            "group": 1126,
            "name": "10.1.1.48.8584",
            "keyword": "",
            "title": "How Optimal is the Genetic Code?"
        },
        {
            "abstract": "This paper presents an incremental concept learning approach to identification of concepts with high overall accuracy. The main idea is to address the concept overlap as a central problem when learning multiple descriptions. Many traditional inductive algorithms, as those from the disjunctive version space family considered here, face this problem. The approach focuses on combinations of confident, possibly overlapping, concepts with an original stochastic complexity formula. The focusing is efficient because it is organized as a simulated annealing-based beam search. The experiments show that the approach is especially suitable for developing incremental learning algorithms with the following advantages: first, it generates highly accurate concepts; second, it overcomes to a certain degree the sensitivity to the order of examples; and third, it handles noisy examples.",
            "group": 1127,
            "name": "10.1.1.48.9707",
            "keyword": "",
            "title": "Stochastically Guided Disjunctive Version Space Learning"
        },
        {
            "abstract": "Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global m...",
            "group": 1128,
            "name": "10.1.1.48.9779",
            "keyword": "",
            "title": "When Gravity Fails: Local Search Topology"
        },
        {
            "abstract": ". Grouping is often intended as a general-purpose early vision stage which gathers together image features of perceptual salience, usually having a well-definable structure. This work addresses the problem of generic partbased grouping and recognition from single two-dimensional edge images following a strategy that employs generic part models at all stages: the key underlying idea is to perform a purposive grouping of simple parts and these parts can be conveniently represented by generic part models. This paper outlines the proposed computational method, which is extensively treated in [18]. 1 Introduction  Since the early days of computer vision research, part segmentation and recognition has been acknowledged an important role towards the realization of a generic object recognition system. A reliable segmentation of generic objects into their constituent parts would, for instance, tremendously ease object grasping and manipulation, fast indexing to large object databases and so for...",
            "group": 1129,
            "name": "10.1.1.48.9840",
            "keyword": "",
            "title": "Model-Driven Grouping and Recognition of Generic Object Parts from Single Images"
        },
        {
            "abstract": "In this report we describe the conversion of a simple Master-Worker parallel program from global blocking communications to non-blocking communications. The program is MPI-based and has been run on different computer architectures. By moving the communication to the background the processors can use the former waiting time for computation. However we find that the computing time increases by the time the communication time decreases in the used MPICH implementation on a cluster of workstations. Also using non-global communication instead of the global communication slows the algorithm down on computers with optimized global communication routines like the Cray T3D.  Keywords: MPI, MPICH, blocking communication, non-blocking communication 1 Introduction  The available computing power has vastly increased since the invention of the microprocessor. Moore's law even says that the increase in computing power per processor goes exponentially in time. However, the price for the custom process...",
            "group": 1130,
            "name": "10.1.1.49.704",
            "keyword": "MPIMPICHblocking communicationnon-blocking communication",
            "title": "Blocking vs. Non-blocking Communication under MPI on a Master-Worker Problem"
        },
        {
            "abstract": "In this paper we present a new algorithm for the k-partitioning problem which achieves an improved solution quality compared to known heuristics. We apply the principle of so called \"helpful sets\", which has shown to be very efficient for graph bisection, to the direct  k-partitioning problem. The principle is extended in several ways. We introduce a new abstraction technique which shrinks the graph during runtime in a dynamic way leading to shorter computation times and improved solutions qualities. The use of stochastic methods provides further improvements in terms of solution quality. Additionally we present a parallel implementation of the new heuristic. The parallel algorithm delivers the same solution quality as the sequential one while providing reasonable parallel efficiency on MIMD-systems of moderate size. All results are verified by experiments for various graphs and processor numbers. ",
            "group": 1131,
            "name": "10.1.1.49.1121",
            "keyword": "",
            "title": "Combining Helpful Sets and Parallel Simulated Annealing for the Graph-Partitioning Problem"
        },
        {
            "abstract": "In this paper we consider the problem of finding the efficient frontier associated with the standard mean-variance portfolio optimisation model. We extend the standard model to include cardinality constraints that limit a portfolio to have a specified number of assets, and to impose limits on the proportion of the portfolio held in a given asset (if any of the asset is held). We illustrate the differences that arise in the shape of this efficient frontier when such constraints are present. We present three heuristic algorithms based upon genetic algorithms, tabu search and simulated annealing for finding the cardinality constrained efficient frontier. Computational results are presented for five data sets involving up to 225 assets. Keywords: portfolio optimisation, efficient frontier  1 1. INTRODUCTION Each of the larger fund management companies in the UK/US are responsible for the investment of several billion pounds/dollars. This money is invested on behalf of pension funds, unit ...",
            "group": 1132,
            "name": "10.1.1.49.1228",
            "keyword": "portfolio optimisationefficient frontier 1",
            "title": "Heuristics for Cardinality Constrained Portfolio Optimisation"
        },
        {
            "abstract": "this article, we provide a framework for characterizing planning and scheduling problems that focuses on properties of the underlying dynamical system and the capabilities of the  planning system to observe its surroundings. The presentation of specific techniques distinguishes between refinement-based methods that construct plans and schedules piece by piece, and repair-based methods that modify complete plans and schedules. Both refinement- and repair-based methods are generally applied in the context of heuristic search. Most planning and scheduling problems are computationally complex. As a consequence of this complexity, most practical approaches rely on heuristics that exploit knowledge of the planning domain. Current research focuses on improving the efficiency of algorithms based on existing representations and on developing new representations for the underlying dynamics that account for important features of the domain (e.g., uncertainty) and allow for the encoding of appropriate heuristic knowledge. Given the complexity of most planning and scheduling problems, an important area for future research concerns identifying and quantifying tradeoffs, such as those involving solution quality and algorithmic complexity. Planning and scheduling in artificial intelligence cover a wide range of techniques and issues. We have not attempted to be comprehensive in this relatively short article. Citations in the main text provide attribution for specifically mentioned techniques. These citations are not meant to be exhaustive by any means. General references are provided in the `Further Information' section at the end of this article. 5 Defining Terms",
            "group": 1133,
            "name": "10.1.1.49.2340",
            "keyword": "",
            "title": "Planning and Scheduling"
        },
        {
            "abstract": ". The past twenty years has seen a rapid growth of interest in stochastic search algorithms, particularly those inspired by natural processes in physics and biology. Impressive results have been demonstrated on complex practical optimisation problems and related search applications taken from a variety of fields, but the theoretical understanding of these algorithms remains weak. This results partly from the insufficient attention that has been paid to results showing certain fundamental limitations on universal search algorithms, including the so-called \"No Free Lunch\" Theorem. This paper extends these results and draws out some of their implications for the design of search algorithms, and for the construction of useful representations. The resulting insights focus attention on tailoring algorithms and representations to particular problem classes by exploiting domain knowledge. This highlights the fundamental importance of gaining a better theoretical grasp of the ways in which such...",
            "group": 1134,
            "name": "10.1.1.49.2376",
            "keyword": "",
            "title": "Fundamental Limitations on Search Algorithms: Evolutionary Computing in Perspective"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We pr...",
            "group": 1135,
            "name": "10.1.1.49.2633",
            "keyword": "CR CategoriesF.2.2 [Analysis of Algorithms and Problem ComplexityNonnumerical Algorithms and Problems---geo- metric problems and computationsrouting and layout. H.5.2 [Information Interfaces and PresentationUser Interfaces--- screen design. 2.1 [Artificial IntelligenceApplications and Expert Systems---cartography. I.3.5 [Computer GraphicsComputational Geometry and Object Modeling---geometric algorithmslanguagesand systems. General Termsalgorithmsexperimentation. Additional Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealing",
            "title": "Labeling Point Features on Maps and Diagrams"
        },
        {
            "abstract": "This paper shows how scale-based clustering can be done using the Radial Basis Function (RBF) Network, with the RBF width as the scale parameter and a dummy target as the desired output. The technique suggests the \"right\" scale at which the given data set should be clustered, thereby providing a solution to the problem of determining the number of RBF units and the widths required to get a good network solution. The network compares favorably with other standard techniques on benchmark clustering examples. Properties that are required of non-gaussian basis functions, if they are to serve in alternative clustering networks, are identified. The work on the whole points out an important role played by the width parameter in RBFN, when observed over several scales, and provides a fundamental link to the scale space theory developed in computational vision.   The work described here is supported in part by the National Science Foundation under grant ECS-9307632 and in part by ONR Contract N...",
            "group": 1136,
            "name": "10.1.1.49.2732",
            "keyword": "",
            "title": "Scale-based Clustering using the Radial Basis Function Network"
        },
        {
            "abstract": "this paper, we present a variant of local search, namely the Life Span Method (LSM), for generic combinatorial optimization problems. The LSM can be seen as a variation of tabu search introduced by Glover [18, 19]. We outline applications of the LSM to several combinatorial optimization problems such as the maximum stable set problem, the traveling salesman problem, the quadratic assignment problem, the graph partitioning problem, the graph coloring problem, and the job-shop scheduling problem.",
            "group": 1137,
            "name": "10.1.1.49.2833",
            "keyword": "",
            "title": "The Life Span Method -- A New Variant of Local Search --"
        },
        {
            "abstract": "This paper describes the development of an object-oriented parallel programming environment for genetic algorithms. This work, carried out as part of the ESPRIT III initiative PAPAGENA, intends to promote, develop and demonstrate the effectiveness of genetic algorithm (GA) and parallel genetic algorithm (PGA) techniques in a variety of real-world application domains. Central to this task is the development of a generalpurpose programming environment for both parallel and sequential genetic algorithms. GAME (Genetic Algorithm Manipulation Environment) will offer extensive tools for the design, configuration and monitoring of GA applications. This paper gives an overview of the design philosophy behind GAME, indicating the types of service and facilities the finished product will offer. Intrinsic to the design is the provision of an extensive multilevelled GA-specific library, offering GA and PGA applications, algorithms and operators. This will allow application developers the facilities to rapidly customise, configure and test novel GA and PGA designs. To sketch the types of application to be housed in GAME, a description of the applications currently under development within this project is also included. These range from finance through economic modelling to protein structure prediction. Key design requirements for GAME are versatility, together with flexibility. For this reason GAME has been designed to run within both Sun OS and PC DOS operating system, with or without parallel support. 1 Introduction",
            "group": 1138,
            "name": "10.1.1.49.2886",
            "keyword": "",
            "title": "Development Needs For Diverse Genetic Algorithm Design"
        },
        {
            "abstract": "We analyse the effects of analog noise on the synaptic arithmetic during MultiLayer Perceptron training, by expanding the cost function to include noise-mediated terms. Predictions are made in the light of these calculations which suggest that fault tolerance, training quality and training trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving \"inaccurate\" analog neural VLSI.  1 Introduction and Background  Arithmetic inaccuracy at the synapse and neuron level is widely held to be tolerable during neural computation, but not during training. In arriving at this conclusion, parallels are drawn between analog noise-induced \"uncertainty\", and digital inaccuracy, limited by bit-length. This has lead ...",
            "group": 1139,
            "name": "10.1.1.49.2989",
            "keyword": "",
            "title": "Enhanced MLP Performance and Fault Tolerance Resulting from Synaptic Weight Noise During Training"
        },
        {
            "abstract": "Introduction  The development of efficient methods to synthesise chemicals is a crucial task for the chemical industry. New synthetic routes were once developed largely by analogy with known reactions, but more systematic approaches are now used, in which computers play a central role. Most computer-driven synthesis programs incorporate a database of chemical reactions and expert system rules. These are used to propose a reaction sequence which transforms readily-available starting materials into the desired product. Very many possible routes for such transformations may exist, depending upon the number of intermediates and the complexity of the product, and this presents synthesis design programs with various difficulties. For example, expert system rules must be numerous and precise. Furthermore, synthesis design operates in a large search space which, because of its discrete nature, is not smooth or differentiable. Traditional algorithms are adequate for small-scale problems",
            "group": 1140,
            "name": "10.1.1.49.3070",
            "keyword": "",
            "title": "Evolutionary Design of Synthetic Routes in Chemistry"
        },
        {
            "abstract": "Simulated annealing is an established method for global optimization. Perhaps its most salient feature is the statistical promise to deliver a globally optimal solution. In this work, we propose a technique which attempts to combine the robustness of annealing in rugged terrain with the efficiency of local optimization methods in simple search spaces. On a variety of benchmark functions, the proposed method seems to clearly outperform a parallel genetic algorithm and adaptive simulated annealing, two popular and powerful optimization techniques.  1. INTRODUCTION  The goal of optimization is, given a system, to find the setting of its parameters so as to obtain the optimal performance. The performance of the system is given by an evaluation function. Optimization problems are commonly found in a wide range of fields, and it is also of central concern to many problems in artificial intelligence and machine learning. In situations where the space of parameters cannot be searched exhaustiv...",
            "group": 1141,
            "name": "10.1.1.49.3402",
            "keyword": "",
            "title": "Salo: Combining Simulated Annealing And Local Optimization For Efficient Global Optimization"
        },
        {
            "abstract": "INTRODUCTION  Conservation of some quantity is a common constraint in optimisation problems. For instance, in valid solutions of the Travelling Sales-Person (TSP) problem, the number of city-stop assignments equals the number of cities in the tour. In optimisation neural networks, assignments may be represented by active neurons, so that conservation constraints translate into conservation of activation. In their optimisation network for solving the TSP problem, Hopfield and Tank [2] incorporated the following conservation term E c in the energy function: E c (a) =  ff  2  / X  c  X  s  a cs \\Gamma A  ! 2  ; (1) where a is the activation vector with elements a cs 2 [0; 1] (c; s 2 f1; 2; :::;<F32.06",
            "group": 1142,
            "name": "10.1.1.49.3715",
            "keyword": "",
            "title": "Activity-Conserving Dynamics for Optimisation Networks"
        },
        {
            "abstract": "This report presents a selection of combinatorial problems that arise at different stages in the analysis of realistic (and military relevant) imagery. They may serve as a testbed to compare different optimization techniques. This report is intended for researchers specialized in combinatorial optimization. The combinatorial optimization techniques are therefore only briefly introduced. A brief introduction to some image processing terminology is also provided. Only few comparative studies on the effectiveness of combinatorial optimization techniques to solve combinatorial problems in image analysis are currently available. This report gives a survey of the results of these studies. 4 Combinatorische Optimalizatie en Beeldanalyse: een literatuuroverzicht A. Toet SAMENVATTING Het EUCLID (EUropean Cooperation for the Long term In Defence) CALMA (Combinatorial Algorithms for Military Applications) RTP (Research and Technology Project) 6.4 project heeft als voornaamste doel het onderzoeken van de bruikbaarheid van verschillende bestaande combinatorische optimalizatie technieken voor het oplossen van complexe combinatorische problemen Er zijn de laatste dertig jaar verscheidene combinatorische optimalizatietechnieken ontwikkeld (bijv. simulated annealing, mean field annealing, neurale netwerken, genetische algoritmen, tabu search). De meeste daarvan zijn alleen getest op sterk vereenvoudigde problemen (bijv. het \"handelsreizigers probleem\"). Het is momenteel dan ook niet bekend in hoeverre deze methoden in staat zijn om complexe realistische problemen (zoals die in militaire omgevingen voorkomen) op te lossen, en wat de benodigde rekencapaciteit is. zoals die voorkomen in militaire omgevingen. Verschillende stadia van de vereisen het oplossen van grote combinatorische prob...",
            "group": 1143,
            "name": "10.1.1.49.4421",
            "keyword": "",
            "title": "Combinatorial Optimization and Image Analysis: a literature survey."
        },
        {
            "abstract": "Birth-and-death processes or, equivalently, finite Markov chains with three-diagonal transition matrices proved to be adequate models for processes in physics [12], biology [4,5], sociology [13] and economics [1,3,10]. The analysis in this case quite often relies on the stationary distribution of the chain. Representing it as a Gibbs distribution, we study its limit behavior as the number of states increases. We show that the limit nests on the set of global minima of the limit Gibbs potential. If the set consists of a finite number k of singletons a i where the second derivatives ff i of the potential are positive, the limit distribution assigns probability 1=  p  ff i  P k j=1 1=  p  ff j  to a i . When at some points the second derivative is zero, the limit distribution nests only on them, we describe it explicitly. If the set of minima consists of a finite number of singletons and intervals, the limit distribution concentrates only on intervals. We obtain a formula for it.  Key Wor...",
            "group": 1144,
            "name": "10.1.1.49.4446",
            "keyword": " ii",
            "title": "Limit Theorems for Stationary Distributions of Birth-and-Death Processes"
        },
        {
            "abstract": " This article provides a brief review of recent developments in Markov chain Monte Carlo methodology. The methods discussed include the standard Metropolis-Hastings algorithm, the Gibbs sampler, and various special cases of interest to practitioners. It also devotes a section on strategies for improving mixing rate of MCMC samplers, e.g., simulated tempering, parallel tempering, parameter expansion, dynamic weighting, and multigrid Monte Carlo with its generalizations. Other related topics are the simulated annealing, the reversible jump method, and the multiple-try Metropolis rule. Theoretical issues such as bounding the mixing rate, diagnosing convergence, and conducting",
            "group": 1145,
            "name": "10.1.1.49.4469",
            "keyword": "",
            "title": "Markov chain Monte Carlo and related topics "
        },
        {
            "abstract": "We consider computations associated with data parallel iterative solvers used for the numerical solution of Partial Differential Equations (PDEs). The mapping of such computations into load balanced tasks requiring minimum synchronization and communication is a difficult combinatorial optimization problem. Its optimal solution is essential for the efficient parallel processing of PDE computations. Determining data mappings that optimize a number of criteria, like workload balance, synchronization and local communication, often involves the solution of an NP-Complete problem. Although data mapping algorithms have been known for a few years there is lack of qualitative and quantitative comparisons based on the actual performance of the parallel computation. In this paper we present two new data mapping algorithms and evaluate them together with a large number of existing ones using the actual performance of data parallel iterative PDE solvers on the nCUBE II. Comparisons on the performan...",
            "group": 1146,
            "name": "10.1.1.49.4497",
            "keyword": "",
            "title": "Mapping Algorithms and Software Environment for Data Parallel PDE Iterative Solvers"
        },
        {
            "abstract": "This paper is concerned with real-time decision problems. These constitute a generic class of dynamic and stochastic problems. The objective is to provide responses of a required quality in a continuously evolving environment, within a prescribed time frame, using limited resources and information that is often incomplete or uncertain. Furthermore, the outcome of any particular decision may also be uncertain. This paper provides an overview of this class of problems, reviews the relevant Artificial Intelligence literature, proposes a dynamic programming framework, and assesses the potential usefulness of Operations Research approaches for their solution. Throughout the paper, a vehicle dispatching application illustrates the relevant concepts.",
            "group": 1147,
            "name": "10.1.1.49.4572",
            "keyword": "Key wordsreal-time decision problemsartificial intelligenceheuristicsdynamic programmingvehicle dispatching 1",
            "title": "Real-Time Decision Problems: an Operations Research Perspective"
        },
        {
            "abstract": "In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures.  Advances in Neural Information Processing Systems 1997 MIT Press, Cambridge, MA 1 Introduction  Given some cost function C(x) with local minima, we may...",
            "group": 1148,
            "name": "10.1.1.49.6302",
            "keyword": "",
            "title": "MIMIC: Finding Optima by Estimating Probability Densities"
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks -- specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability, and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars, and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-t...",
            "group": 1149,
            "name": "10.1.1.49.6441",
            "keyword": "recurrent neural networksnatural language processing",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "this paper therefore the second possibility of explicit implementation of a priori information in terms of the function values h(x).  4 Lemm, J.C. / How to Implement A Priori Information Furthermore, we consider especially low data situations where the number of available training data is small compared to the complexity of h. We do this because those are the cases where a priori information becomes the essential input. Contrasting the well known uninformative priors of Bayesian statistics [5] one may call this a situation with informative priors.",
            "group": 1150,
            "name": "10.1.1.49.6608",
            "keyword": "density. Contents",
            "title": "How to Implement A Priori Information: A Statistical Mechanics Approach"
        },
        {
            "abstract": "This thesis examines the automated design of computer graphics. We present a methodology that emphasizes optimization, problem representation, stochastic search, and empirical analysis. Two problems are considered, which together encompass and exemplify both 2D and 3D graphics production: label placement and motion synthesis for animation.  Label placement is the problem of annotating various informational graphics with textual labels, subject to constraints that respect proper label-feature associativity, label and feature obscuration, and aesthetically desirable label positions. Examples of label placement include applying textual labels to a geographic map, or item tags to a scatterplot.  Motion synthesis is the problem of composing a visually plausible motion for an animated character, subject to animator-imposed constraints on the form and characteristics of the desired motion. For each problem we propose new solution methods that utilize efficient problem representations combined...",
            "group": 1151,
            "name": "10.1.1.49.6962",
            "keyword": "",
            "title": "Managing Design Complexity: Using Stochastic Optimization in the Production of Computer Graphics"
        },
        {
            "abstract": "In general, neural networks are regarded as models for massively parallel computation. But very often, this parallelism is rather limited, especially when considering symmetric networks. For instance, Hopfield networks do not really compute in parallel as their updating algorithm always requires sequential execution. We describe a recurrent network corresponding to a symmetric network and introduce a method of parallel updating multiple units. We show how this may be extended to Boltzmann machines with continuous activation functions, and point out possible applications of this architecture, e.g. local hill-climbing algorithms to solve the satisfiability problem. In terms of SAT-algorithms, we present an approach which allows the simultaneous change of truth value assignments for more than one propositional variable at a time, such that the theoretical properties of the considered algorithms are preserved, and give experimental evidence that this algorithm is indeed faster than the res...",
            "group": 1152,
            "name": "10.1.1.49.7523",
            "keyword": "",
            "title": "Parallel Local Search Algorithms"
        },
        {
            "abstract": "Neural networks are composed of basic units somewhat analogous to neurons. These units are linked to each other by connections whose strength is modifiable as a result of a learning process or algorithm. Each of these units integrates independently (in parallel) the information provided by its synapses in order to evaluate its state of activation. The unit response is then a linear or nonlinear function of its activation. Linear algebra concepts are used, in general, to analyze linear units, with eigenvectors and eigenvalues being the core concepts involved. This analysis makes clear the strong similarity between linear neural networks and the general linear model developed by statisticians. The linear models presented here are the perceptron, and the linear associator.  The behavior of nonlinear networks can be described within the framework of optimization and approximation techniques with dynamical systems (e.g., like those used to model spin glasses). One of the main notions used w...",
            "group": 1153,
            "name": "10.1.1.49.8133",
            "keyword": "neural networksgeneral linear modelperceptronradial basis functionHopfield networkBoltzmann machineback-propagation networkeigenvectoreigenvalueprincipal component analysisattractorsoptimization",
            "title": "A Neural Network Primer"
        },
        {
            "abstract": "Assignment problems in radio networks, like the channel assignment, may be solved by graph coloring algorithms. In this paper we compare the performance of several recent techniques on a simple coloring problem: the search for a bipartite subgraph with the maximum number of edges of a given graph. The algorithms considered are a neural network, a genetic algorithm, simulated annealing and two heuristics. The results show that one heuristic, a new proposed multiagent system, and the standard simulated annealing technique are faster outperforming the algorithms in the literature. We also show how the algorithms may be easily adapted to deal with the k-  coloring of a graph and can therefore be used for solving assignment problems in telecommunications.  c  #1995  by Lawrence Erlbaum Assoc. Inc. Pub., Hillsdale, NJ 07642  Applications of Neural Networks to Telecommunications 2  pp. 49--56 (1995). ISBN 0-8058-2084-1 1 Introduction  Several issues related to the design of radio networks, as...",
            "group": 1154,
            "name": "10.1.1.49.9042",
            "keyword": "",
            "title": "Graph Coloring Algorithms for Assignment Problems in Radio Networks"
        },
        {
            "abstract": "Introduction  The design of mathematical models of complex real-world (and typically nonlinear) systems is essential in many fields of science and engineering. The developed models can be used, e.g., to explain the behavior of the underlying system as well as for prediction and control purposes. A common approach for building mathematical models is so-called black box modeling (Ljung, 1987; Soderstrom and Stoica, 1989), as opposed to more traditional physical modeling (or white box modeling), where everything is considered known a priori from physics. Strictly speaking, a black box model is designed entirely from data using no physical or verbal insight whatsoever. The structure of the model is chosen from families that are known to be very flexible and successful in past applications. This also means that the model parameters lack physical or verbal significance; they are tuned just to fit the observed data as well as possible. The term \"black box modeling\" is ",
            "group": 1155,
            "name": "10.1.1.49.9487",
            "keyword": "",
            "title": "Fuzzy Identification from a Grey Box Modeling Point of View"
        },
        {
            "abstract": "We present a novel framework for unsupervised texture segmentation, which relies on statistical tests as a measure of homogeneity. Texture segmentation is formulated as a pairwise data clustering problem with a sparse neighborhood structure. The pairwise dissimilarities of texture blocks are computed using a multiscale image representation based on Gabor filters, which are tuned to spatial frequencies at different scales and orientations. We derive and discuss a family of objective functions to pose the segmentation problem in a precise mathematical formulation. An efficient optimization method, known as deterministic annealing, is applied to solve the associated optimization problem. The general framework of deterministic annealing and meanfield approximation is introduced and the canonical way to derive efficient algorithms within this framework is described in detail. Moreover the combinatorial optimization problem is examined from the viewpoint of scale space theory. The novel algorithm has been extensively tested on Brodatz-like microtexture mixtures and on real--word images. In addition, benchmark studies with alternative segmentation techniques are reported. ",
            "group": 1156,
            "name": "10.1.1.50.56",
            "keyword": "",
            "title": "A deterministic annealing framework for unsupervised texture segmentation"
        },
        {
            "abstract": ". E#cient routines for multidimensional numerical integration are provided by quasi-- Monte Carlo methods. These methods are based on evaluating the integrand at a set of representative points of the integration area. A set may be called representative if it shows a low discrepancy. However, in dimensions higher than two and for a large number of points the evaluation of discrepancy becomes infeasible. The use of the e#cient multiple-purpose heuristic threshold-accepting o#ers the possibility to obtain at least good approximations to the discrepancy of a given set of points. This paper presents an implementation of the threshold-accepting heuristic, an assessment of its performance for some small examples, and results for larger sets of points with unknown discrepancy.  Key words. number-theoretic methods, discrepancy, numerical integration, threshold-accepting  AMS subject classifications. 65K10, 62K99  PII. S0036142995286076 1. Introduction. The e#cient evaluation of multidimensional...",
            "group": 1157,
            "name": "10.1.1.50.1049",
            "keyword": "Key words. number-theoretic methodsdiscrepancynumerical integrationthreshold-accepting",
            "title": "Application Of Threshold-Accepting To The Evaluation Of The Discrepancy Of A Set Of Points"
        },
        {
            "abstract": "Concerning the simulation of neural networks with arbitrary topology on distributed memory multiprocessor systems, we introduce our approach to an automatic determination of a near-optimal mapping of neural networks onto a given multiprocessor. Our approach is based on stochastic local search. We propose a decomposition of the mapping into a network partitioning step followed by a placement of partitions onto processors. 1 Introduction  Typical implementations of neural networks on message passing multicomputers do not map each neuron or synapse to a single hardware component but rather simulate clusters of neurons and synapses as network partitions on a coarse-grain [2] or fine-grain [10] multiprocessor system. In our case, the multiprocessor system consists of a cubic lattice of processing units with six bidirectional serial links to adjacent processing units [10]. The message routing mechanism is an autonomous component of the processing units, which generates direct paths to a dema...",
            "group": 1158,
            "name": "10.1.1.50.1144",
            "keyword": "",
            "title": "How To Find A Near Optimal Mapping Of Neural Networks Onto Message Passing Multicomputers"
        },
        {
            "abstract": "In most commercial Field-Programmable Gate Arrays (FPGAs) the number of wiring tracks in each channel is the same across the entire chip. A long-standing open question for both FPGAs and channelled gate arrays is whether or not some uneven distribution of routing tracks across the chip would lead to an area benefit. For example, many circuit designers intuitively believe that most congestion occurs near the center of a chip, and hence expect that having wider routing channels near the chip center would be beneficial. In this paper we determine the relative area-efficiency of several different routing track distributions. We first investigate FPGAs in which horizontal and vertical channels contain different numbers of tracks in order to determine if such a directional bias provides a density advantage. Secondly, we examine routing track distributions in which the track capacities vary from channel to channel. We compare the area-efficiency of these non-uniform routing architectures to t...",
            "group": 1159,
            "name": "10.1.1.50.1252",
            "keyword": "",
            "title": "Effect of the Prefabricated Routing Track Distribution on FPGA Area-Efficiency"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We pr...",
            "group": 1160,
            "name": "10.1.1.50.1509",
            "keyword": "CR CategoriesF.2.2 [Analysis of Algorithms and Problem ComplexityNonnumerical Algorithms and Problems---geo- metric problems and computationsrouting and layout. H.5.2 [Information Interfaces and PresentationUser Interfaces--- screen design. 2.1 [Artificial IntelligenceApplications and Expert Systems---cartography. I.3.5 [Computer GraphicsComputational Geometry and Object Modeling---geometric algorithmslanguagesand systems. General Termsalgorithmsexperimentation. Additional Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealing",
            "title": "Labeling Point Features on Maps and Diagrams"
        },
        {
            "abstract": "We propose a declarative-based implementation of randomised algorithms, which exploits the Constraint Logic Programming (CLP) paradigm. For the high-level formalisation of probabilistic programs expressing such algorithms we actually refer to a generalisation of CLP, namely the Probabilistic Concurrent Constraint Programming (PCCP) language, previously introduced in [DW97]. This language provides a construct for probabilistic choice which allows us to express randomness in a program. PCCP also includes synchronisation and concurrency aspects. However, for the purpose of this work, the (probabilistic) CLP fragment of PCCP is sufficient. We present a meta-interpreter for this language. This is just a standard prolog metainterpreter, suitably extended so as to deal with probabilistic choice. For the constraint solving, the meta-interpreter exploits existing constraint handling facilities (and in more concrete terms to the SICStus 3.#6 system). This is possible because the design of PCCP d...",
            "group": 1161,
            "name": "10.1.1.50.1546",
            "keyword": "",
            "title": "Randomised Algorithms and Constraint Logic Programming"
        },
        {
            "abstract": "Some apparently powerful algorithms for automatic label placement on maps use heuristics that capture considerable cartographic expertise but are hampered by provably inefficient methods of search and optimization. On the other hand, no approach to label placement that is based on an efficient optimization technique has been applied to the production of general cartographic maps --- those with labeled point, line, and area features --- and shown to generate labelings of acceptable quality. We present an algorithm for label placement that achieves the twin goals of practical efficiency and high labeling quality by combining simple cartographic heuristics with effective stochastic optimization techniques. To appear in Cartographica. 1 Introduction  Many apparently compelling techniques for automatic label placement use sophisticated heuristics for capturing cartographic knowledge, but, as noted by Zoraster (1991), also use inferior optimization strategies for finding good tradeoffs betwe...",
            "group": 1162,
            "name": "10.1.1.50.1712",
            "keyword": "",
            "title": "A General Cartographic Labeling Algorithm"
        },
        {
            "abstract": "The efficient representation and encoding of signals with limited resources, e.g., finite storage capacity and restricted transmission bandwidth, is a fundamental problem in technical as well as biological information processing systems. Typically, under realistic circumstances, the encoding and communication of messages has to deal with different sources of noise and disturbances. In this paper, we propose a unifying approach to data compression by robust vector quantization, which explicitly deals with channel noise, bandwidth limitations, and random elimination of prototypes. The resulting algorithm is able to limit the detrimental effect of noise in a very general communication scenario. In addition, the presented model allows us to derive a novel competitive neural networks algorithm, which covers topology preserving feature maps, the so-called neural-gas algorithm, and the maximum entropy soft-max rule as special cases. Furthermore, continuation methods based on these noise models i...",
            "group": 1163,
            "name": "10.1.1.50.2253",
            "keyword": "",
            "title": "Competitive Learning Algorithms for Robust Vector Quantization"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We pr...",
            "group": 1164,
            "name": "10.1.1.50.2327",
            "keyword": "CR CategoriesH.5.2 [Information Interfaces and PresentationUser Interfaces---screen design. 2.1 [Artificial IntelligenceApplications and Expert Systems---cartography. I.3.5 [Computer GraphicsComputational Geometry and Object Modeling---geometric algorithmslanguagesand systems. General Termsalgorithmsexperimentation. Additional Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealingheuristic search",
            "title": "An Empirical Study of Algorithms for Point-Feature Label Placement"
        },
        {
            "abstract": "In striving to construct higher level control representations for simulated characters or creatures, one must seek flexible control representations to build upon. We present a method for the synthesis of parameterized, physics-based motions. The method can be applied to both periodic and aperiodic motions. The basis of the method is a low-level control representation in which linear combinations of controllers generally produce predictable in-between motions.  1.0 Introduction  Many of the most interesting objects to animate, such as humans, animals, and robots, are capable of controlling their own motion using muscles or actuators. While realistic motion can be obtained for these active systems[15] by applying the Newtonian laws of physics, this also requires solving an associated control problem. Informally stated control problems for animation such as \"jump from A to B, then walk to the left\" can be posed in a variety of ways. In this paper we examine how to produce control solution...",
            "group": 1165,
            "name": "10.1.1.50.2520",
            "keyword": "",
            "title": "Synthesizing Parameterized Motions"
        },
        {
            "abstract": "this paper, we first extend Donath's technique to a three-dimensional placement. We then compute a significantly more accurate estimate by taking into account the inherent features of the optimal placement process.",
            "group": 1166,
            "name": "10.1.1.50.2649",
            "keyword": "",
            "title": "Estimating Interconnection Lengths in Three-Dimensional Computer Systems"
        },
        {
            "abstract": "This paper presents an approach for deriving 3-D structures of polypeptide chains which have minimum energy. The well-known optimisation algorithms are usually applied on the model which contains hard constraints over coordinates of the atoms. The drawback of such strategies is an inefficient search for the optimum solution. Our approach proceeds in two steps: first, the standard model is transformed into an equivalent model without hard constraints, and second, an algorithm is performed by combining simulated annealing and first- and second-derivative techniques. The empirical results demonstrate that the hybrid algorithm on the proposed model outperforms traditional search algorithms especially with respect to running times. 1 Introduction  In the pharmaceutical industry new methods for predicting protein structures and engineering proteins are used in the development of new `designer drugs'. Despite the fact that the 3-D structures of 500 proteins are known, one is still unable to f...",
            "group": 1167,
            "name": "10.1.1.50.2937",
            "keyword": "",
            "title": "Minimising the Energy of Polypeptides by Simulated Annealing"
        },
        {
            "abstract": "This paper presents an approach for deriving 3-D structures of polypeptide chains which have minimum energy. The well-known optimisation algorithms are usually applied on the model which contains hard constraints over coordinates of the atoms. The drawback of such strategies is an inefficient search for the optimal solution. Our approach proceeds in two steps: first, the standard model is transformed into an equivalent model without hard constraints, and second, the simulated annealing local search algorithm is performed. The empirical results demonstrate that simulated annealing on the proposed model outperforms traditional search algorithms especially with respect to running times. 1: Introduction  In the pharmaceutical industry new methods for predicting protein structures and engineering proteins are used in the development of new `designer drugs'. Despite the fact that the 3-D structures of 500 proteins are known, one is still unable to formulate general rules to predict the struc...",
            "group": 1168,
            "name": "10.1.1.50.3162",
            "keyword": "",
            "title": "Minimising the Energy of the Alanine Dipeptide by Simulated Annealing"
        },
        {
            "abstract": ") J. Abello  Network Services Research, AT&T Labs Research, 180 Park Avenue Florham Park, NJ 07932-0971, USA e-mail: abello@research.att.com P. M. Pardalos  Center for Applied Optimization, University of Florida, 303 Weil Hall Gainesville, FL 32611, USA e-mail: pardalos@ufl.edu and M. G. C. Resende  Information Sciences Research, AT&T Labs Research, 180 Park Avenue Florham Park, NJ 07932-0971, USA e-mail: mgcr@research.att.com Abstract  We present an approach to clique computations in very large multi-digraphs. We discuss graph decomposition schemes used to break up the problem into several pieces of manageable dimensions. A two-stage (out-of-memory and in-memory) greedy randomized adaptive search procedure (GRASP) for finding approximate solutions to the maximum clique problem in very large sparse graphs is presented. We experiment with this heuristic on real data sets collected in the telecomunications industry. These graphs contain on the order of millions of vertices and edges.  Ke...",
            "group": 1169,
            "name": "10.1.1.50.3255",
            "keyword": "very large sparse graphsapplications",
            "title": "On Very Large Maximum Clique Problems (Extended Abstract)"
        },
        {
            "abstract": "The satisfiability (SAT) problem is a fundamental problem in mathematical logic, inference, automated reasoning, VLSI engineering, and computing theory. In this paper, following CNF and  DNF local search methods, we introduce the Universal SAT problem model, UniSAT, that transforms the discrete SAT problem on Boolean space f0; 1g  m  into an unconstrained global optimization problem on real space E  m  . A direct correspondence between the solution of the SAT problem and the global minimum point of the UniSAT objective function is established. Many existing global optimization algorithms can be used to solve the UniSAT problems. Combined with backtracking /resolution procedures, a global optimization algorithm is able to verify satisfiability as well as unsatisfiability. This approach achieves significant performance improvements for certain classes of conjunctive normal form (CNF ) formulas. It offers a complementary approach to the existing SAT algorithms. ",
            "group": 1170,
            "name": "10.1.1.50.3350",
            "keyword": "Constraint satisfaction problem (CSPlocal searchglobal optimizationsatisfiability (SAT) problemUniSAT problem model. 1",
            "title": "Global Optimization for Satisfiability (SAT) Problem"
        },
        {
            "abstract": "Vector quantization is a data compression method where a set of data points is encoded by a reduced set of reference vectors, the codebook. We discuss a vector quantization strategy which jointly optimizes distortion errors and the codebook complexity, thereby, determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression, i.e., we quantize the wavelet coefficients of gray level images and measure the reconstruction error. Our approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantizati...",
            "group": 1171,
            "name": "10.1.1.50.3505",
            "keyword": "Vector quantizationcomplexity costsmaximum entropy estimationimage compressionneural networks",
            "title": "Vector Quantization with Complexity Costs"
        },
        {
            "abstract": "In software testing, it is often desirable to find test inputs that exercise specific program features. To find these inputs by hand is extremely time-consuming, especially when the software is complex. Therefore, numerous attempts have been made to automate the process. Random test data generation consists of generating test inputs at random, in the hope that they will exercise the desired software features. Often, the desired inputs must satisfy complex constraints, and this makes a random approach seem unlikely to succeed. In contrast, combinatorial optimization techniques, such as those using genetic algorithms, are meant to solve difficult problems involving the simultaneous satisfaction of many constraints. However, test data generation has only been applied to very simple programs in the past. Since they may not present great difficulties to random test data generation, it is difficult to compare the efficacy of different approaches when such programs are used as benchmarks. In ...",
            "group": 1172,
            "name": "10.1.1.50.3866",
            "keyword": "",
            "title": "Genetic Algorithms for Dynamic Test Data Generation"
        },
        {
            "abstract": "An off-line planning tool that supports the programmer in developing his real-time application is mandatory in the design of time-triggered real-time systems. This paper describes the architecture and the functions of such a tool, the Cluster Compiler, that is in development at our institute. We emphasize on the principle of a strict separation of the local from the global parts of a distributed system and on the consequences for the structure of the design tool arising from this principle. Introduction  At present, real-time systems are often designed unsystematically. Conventional software modules are integrated by \"real-time specialists\" who tune the system parameters (e.g., task priorities, buffer sizes, etc.) during an extensive trial and error period, consuming more than 50% of a project's resources. Why the system performs its functions at the end is sometimes a miracle, even to the \"real-time specialists\". To change this deplorable situation we need a proper real-time system ar...",
            "group": 1173,
            "name": "10.1.1.50.4008",
            "keyword": "",
            "title": "The Cluster Compiler - A Tool for the Design of Time-Triggered Real-Time Systems"
        },
        {
            "abstract": "We present an overview of the state of the art and future trends in high performance parallel and distributed computing, and discuss techniques for using such computers in the simulation of complex problems in computational science. The use of high performance parallel computers can help improve our understanding of complex systems, and the converse is also true --- we can apply techniques used for the study of complex systems to improve our understanding of parallel computing. We consider parallel computing as the mapping of one complex system --- typically a model of the world --- into another complex system --- the parallel computer. We study static, dynamic, spatial and temporal properties of both the complex systems and the map between them. The result is a better understanding of which computer architectures are good for which problems, and of software structure, automatic partitioning of data, and the performance of parallel machines. ",
            "group": 1174,
            "name": "10.1.1.50.4093",
            "keyword": "",
            "title": "Parallel Computers and Complex Systems"
        },
        {
            "abstract": "Field-Programmable Gate Arrays (FPGAs) are currently used in two major classes of systems, namely in logic emulation of digital circuits, and in reconfigurable computer systems. This paper reviews the current status of research into both these areas, especially reconfigurable computers, and comments on their future prospects. Three papers are reviewed in some detail, but several others are also reviewed, and placed in a larger context. Two new classificationsof reconfigurable computer systems are proposed, to provide more insight, and bring order to the field. A view of what is done well by today's machines is presented. Program characteristics needed for good performance today on reconfigurable computers are identified. What more remains to be done is presented, suggesting directions for future research. 1 Introduction  Dynamically reprogrammable systems are a class of machines based on Field Programmable Gate Arrays (FPGAs), a kind of digital chip first introduced in 1986 by Xilinx [...",
            "group": 1175,
            "name": "10.1.1.50.4166",
            "keyword": "",
            "title": "Prospects for FPGA-based Reprogrammable Systems"
        },
        {
            "abstract": "Scheduling problems are often modeled as resource constrained problems in which critical resource assignments to tasks are known and the best assignment of resource time must be made subject to these constraints. Generalization to resource scheduling, where resource assignments are chosen concurrently with times results in a problem which is much more difficult. A simplified model of the general resource scheduling model is possible, however, in which tasks must be assigned a single primary resource, subject to constraints resulting from preassignment of secondary, or auxiliary, resources. This paper describes extensions and enhancements of Tabu Search for the special case of the resource scheduling problem described above. The class of problems is further restricted to those where it is reasonable to enumerate both feasible time and primary resource assignments. Potential applications include shift oriented production and manpower scheduling problems as well as course scheduling where...",
            "group": 1176,
            "name": "10.1.1.50.5056",
            "keyword": "",
            "title": "TABU SEARCH FOR A CLASS OF SCHEDULING PROBLEMS (Preprint)"
        },
        {
            "abstract": "",
            "group": 1177,
            "name": "10.1.1.50.5114",
            "keyword": "",
            "title": "Heuristics for the MinLA Problem: An Empirical and Theoretical Analysis (Extended Abstract)"
        },
        {
            "abstract": "In signal processing area, applications involve a large amount of computation, suggesting the use of multiprocessors to speed up processing. However, obtaining good performance is not easy because the machine should take advantage of the potential parallelisms of the studied application. That is why several parallel implementation methods using mapping and scheduling algorithms has been developped. One of development shells aims is application partitioning so that every part will be processed by a different processor, like SynDEx [1] or Ptolemy[2]. These shells use some graph models to exhibit both potential parallelisms of the application and the available multiprocessor parallelisms [3], but the task granularity problem is not considered when the application is modelized. The purpose of this paper is to emphasize the problem of the task granularity when the application is modelized by means of a graph and to study the impact on speedup. As a solution for this problem, this paper pres...",
            "group": 1178,
            "name": "10.1.1.50.5236",
            "keyword": "",
            "title": "Efficient implementation on multiprocessors: the problem of Signal Processing Applications modelling"
        },
        {
            "abstract": ": Quantization of the parameters of a Perceptron is a central problem in hardware implementation of neural networks using a numerical technology. An interesting property of neural networks used as classifiers is their ability to provide some robustness on input noise. This paper presents efficient learning algorithms for the maximization of the robustness of a Perceptron and especially designed to tackle the combinatorial problem arising from the discrete weights. Keywords: Perceptron, learning, robustness, weight quantization, combinatorial optimization, tabu search. 1 Introduction  Artificial neural networks (ANN) are proposed today as alternative solutions for a wide variety of problems. Whatever is the architecture (feedfoward, feedback), the transfer function (threshold, sigmoidal, Gaussian) or the learning mode (supervised or unsupervised) , the efficiency of these models depends mostly on their implementations (simulation or dedicated hardware) and on the learning process used. ...",
            "group": 1179,
            "name": "10.1.1.50.5570",
            "keyword": "Perceptronlearningrobustnessweight quantizationcombinatorial optimizationtabu search",
            "title": "Maximizing the Robustness of a Linear Threshold Classifier with Discrete Weights"
        },
        {
            "abstract": "Structural topology optimization is addressed through Genetic Algorithms: A set of designs is evolved following the Darwinian survival-of-fittest principle. This approach demonstrates high flexibility, and breaks many limits of standard optimization algorithms, in spite of the heavy requirements in term of computational effort: Alternate optimal solutions to the same problem can be found; Structures can be optimized with respect to multiple loadings; The prescribed loadings can be applied on the unknown boundary of the solution, rather than on the fixed boundary of the design domain; Different materials as well as different mechanical models can be used, as witnessed by the first results of Topological Optimum Design ever obtained in the large displacements model. But these results could not have been obtained without careful specific handling of the specific aspects of topological genetic optimization: First, specific genetic operators were introduced; Second, special attention was pa...",
            "group": 1180,
            "name": "10.1.1.50.5680",
            "keyword": "",
            "title": "Optimisation Topologique De Formes Par Algorithmes G\u00e9n\u00e9tiques"
        },
        {
            "abstract": "In this article, we consider words over f0; 1g. The autodistance of such a word is the lowest among the Hamming distances between the word and its images by circular permutations other than identity; the word's reverse autodistance is the highest among these distances. For each  l  2, we study the words of length l whose autodistance and reverse autodistance are close to  l=2 (we call such words synchronizing sequences).  We establish, for every l  3, an upper bound on the autodistance of words of length l. This upper bound, called up (l), is very close to l=2.  We briefly describe the maximal period linear recurring sequences, a previously known family of words over f0; 1g; such words exist for every length of the form l = 2  n  \\Gamma 1 and their autodistances achieve the upper bound up (l). Examples of words whose autodistance and reverse autodistance are both equal or close to up (l) are discussed; we describe the method (based on simulated annealing) which was used to find the exa...",
            "group": 1181,
            "name": "10.1.1.50.6454",
            "keyword": "",
            "title": "Binary Periodic Synchronizing Sequences"
        },
        {
            "abstract": "In this paper, we present an algorithm for circuit partitioning with complex resource constraints in large FPGAs. Traditional partitioning methods estimate the capacity of an FPGA device by counting the number of logic blocks, however this is not accurate with the increasing capacity and diverse resource types in the new FPGA architectures. We propose a network flow based method to optimally check whether a circuit or a sub-circuit is feasible for a set of available heterogeneous resources. The feasibility checking procedure is integrated in the FM-based algorithm for circuit partitioning. Incremental flow technique is employed for efficient implementation. Experimental results on the MCNC benchmark circuits show that our partitioning algorithm not only yields good results, but also is efficient. Our algorithm for partitioning with complex resource constraints is applicable for both multiple FPGA designs (e.g. logic emulation systems) and partitioning-based placement algorithms for a s...",
            "group": 1182,
            "name": "10.1.1.50.6473",
            "keyword": "",
            "title": "Circuit Partitioning with Complex Resource Constraints in FPGAs"
        },
        {
            "abstract": "In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures. Advances in Neural Information Processing Systems 1997 MIT Press, Cambridge, MA 1 Introduction Given some cost function C(x) with local minima, we may...",
            "group": 1183,
            "name": "10.1.1.50.6732",
            "keyword": "",
            "title": "MIMIC: Finding Optima by Estimating Probability Densities"
        },
        {
            "abstract": "this paper, we present a variant of local search, namely the Life Span Method (LSM), for generic combinatorial optimization problems. The LSM can be seen as a variation of tabu search introduced by Glover [18, 19]. We outline applications of the LSM to several combinatorial optimization problems such as the maximum stable set problem, the traveling salesman problem, the quadratic assignment problem, the graph partitioning problem, the graph coloring problem, and the job-shop scheduling problem.",
            "group": 1184,
            "name": "10.1.1.50.7270",
            "keyword": "",
            "title": "The Life Span Method - A New Variant of Local Search -"
        },
        {
            "abstract": "The control of structural vibration in cars, aeroplanes, ships, etc., is of great importance in achieving low noise targets. Currently, such control is effected using viscoelastic coating materials although much current research is concerned with active, anti-noise based control measures. This paper is concerned with a third approach: that of using advanced structural analysis methods and modern optimization techniques to produce designs with inherently superior vibration performance. It gives a brief review of a ten year programme of work, illustrating some of the difficulties that have been encountered and the progress that has been made. Central to this process has been the availability of a library of optimization techniques that can be readily applied to the problems of interest. In recent years this suite has been expanded to include a number of evolutionary computing methods such as the genetic algorithm, simulated annealing, evolutionary programming, etc. These have shown signi...",
            "group": 1185,
            "name": "10.1.1.50.7516",
            "keyword": "",
            "title": "Experiences With Optimizers In Structural Design"
        },
        {
            "abstract": "In this paper, we propose a domain decomposition scheme that seeks to minimize total parallel execution time by considering the relative importance of two competing concerns --- balancing the load and minimizing communication --- for a particular application and architecture. A simulated annealing approach is used to optimize an objective function with components that measure both load balance and communication requirements. We develop an analytical model of execution time based upon a finite element code executed on the Intel Paragon. This model is used to compare partitions with varying degrees of load imbalance. Most literature in the area of decomposition methods heavily emphasizes load balancing over the minimization of communication. Our results indicate that this restrictive approach to load balancing can be relaxed without performance degradation. Further, our results indicate that the degree of relaxation possible is dependent upon the target machine and the application; neith...",
            "group": 1186,
            "name": "10.1.1.50.7602",
            "keyword": "",
            "title": "Balancing Load versus Decreasing Communication: Exploring the Tradeoffs"
        },
        {
            "abstract": "This paper proposes a new approach to the design of two-dimensional (2-D) infinite impulse response (IIR) filters with finite precision coefficients. An objective function is proposed which combines magnitude, phase, step response and stability errors. This function being multidimensional and, in general, non-convex is minimized using simulated annealing. Development of this method constitutes the first step in a feasibility study of the application of 2D IIR filters to the processing of video signals. Initial results on the design of low-pass filters are very encouraging and compare favourably with similar finite impulse response (FIR) designs.  1. INTRODUCTION  In this paper we propose a new approach to the design of finite wordlength 2-D IIR filters. This approach rests on a multiple-term objective function minimized using simulated annealing and is influenced by a potential application of the method to the design of filters for video processing. As has been demonstrated in [1], 2-D...",
            "group": 1187,
            "name": "10.1.1.50.7699",
            "keyword": "",
            "title": "Design Of Finite Wordlength 2-D IIR Filters Using Simulated Annealing"
        },
        {
            "abstract": "In this paper, first, we show some of the bifurcation properties of Potts mean-fieldtheory annealing applied to traveling salesman problems. Due to these bifurcation properties, this approach, in general, produces non-optimal and non-unique solutions. As an alternative approach, we propose a nonequilibrium version of the Potts spin neural network, called Chaotic Potts Spin (CPS). CPS has several parameters, and bifurcations over each parameter are investigated. Next, experimental results are shown comparing CPS with several related approaches. CPS is good at obtaining optimal solutions for small-scale problems and semi-optimal solutions for relatively large-scale problems. We also describe a couple of CPS modifications: CPS with a heuristic method and CPS with a \"chaotic annealing\" method. These modified algorithms can produce even better CPS solutions.  Chaotic Potts Spin 2 1 Introduction  There have been many studies on artificial neural network models applied to combinatorial optim...",
            "group": 1188,
            "name": "10.1.1.50.7994",
            "keyword": "",
            "title": "Chaotic Potts spin model for combinatorial optimization problems"
        },
        {
            "abstract": "Any nonassociative reinforcement learning algorithm can be viewed as a method for performing function optimization through (possibly noise-corrupted) sampling of function values. We describe the results of simulations in which the optima of several deterministic functions studied by Ackley [1] were sought using variants of REINFORCE algorithms [19], [20]. Results obtained for certain of these algorithms compare favorably to the best results found by Ackley.",
            "group": 1189,
            "name": "10.1.1.50.8294",
            "keyword": "",
            "title": "Reinforcement Learning Algorithms as Function Optimizers"
        },
        {
            "abstract": "Introduction  Many reasoning systems must reach conclusions based on stored information; we can often model this as deriving logical conclusions from a given knowledge base of facts. We of course prefer derivation systems that draw all and only the correct conclusions, and that reach these conclusions as quickly as possible. Unfortunately, a sound and complete derivation process can be intractable, if not undecidable, in the worse case [LB85]. This position paper discusses the general challenge of producing an derivation process that is as effective as possible, and argues for using a (cautious) adaptive derivation process here. Section 2 first provides a trivial example to explain the ideas and motivate our \"adaptive process\" approach; Section 3 then explains adaptive systems in general, and focuses on one implementation of this idea, palo. Section 4 concludes by suggesting some of the extensions and applications relevant to knowledge compilation and presenting",
            "group": 1190,
            "name": "10.1.1.50.8400",
            "keyword": "",
            "title": "Adaptive Derivation Processes"
        },
        {
            "abstract": "In this paper we present a new algorithm for the k-partitioning problem which achieves an improved solution quality compared to known heuristics. We apply the principle of so called \"helpful sets\", which has shown to be very efficient for graph bisection, to the direct  k-partitioning problem. The principle is extended in several ways. We introduce a new abstraction technique which shrinks the graph during runtime in a dynamic way leading to shorter computation times and improved solutions qualities. The use of stochastic methods provides further improvements in terms of solution quality. Additionally we present a parallel implementation of the new heuristic. The parallel algorithm delivers the same solution quality as the sequential one while providing reasonable parallel efficiency on MIMD-systems of moderate size. All results are verified by experiments for various graphs and processor numbers. 1 Introduction  Graph partitioning is one of the fundamental problems in the design and u...",
            "group": 1191,
            "name": "10.1.1.50.8617",
            "keyword": "",
            "title": "Combining Helpful Sets and Parallel Simulated Annealing for the Graph-Partitioning Problem"
        },
        {
            "abstract": "Dynamic load balancing in multicomputers can improve the utilization of processors and the efficiency of parallel computations through migrating workload across processors at runtime. We present a survey and critique of dynamic load balancing strategies that are  iterative: workload migration is carried out through transferring processes across nearest neighbor processors. Iterative strategies have become prominent in recent years because of the increasing popularity of point-to-point interconnection networks for multicomputers.  Key words: dynamic load balancing, multicomputers, optimization, queueing theory, scheduling.  INTRODUCTION  Multicomputers are highly concurrent systems that are composed of many autonomous processors connected by a communication network  1;2  . To improve the utilization of the processors, parallel computations in multicomputers require that processes be distributed to processors in such a way that the computational load is evenly spread among the processors...",
            "group": 1192,
            "name": "10.1.1.50.8722",
            "keyword": "Key wordsdynamic load balancingmulticomputersoptimizationqueueing theoryscheduling",
            "title": "Iterative Dynamic Load Balancing in Multicomputers"
        },
        {
            "abstract": "Can stochastic search algorithms outperform existing deterministic heuristics for the NP-hard problem Number Partitioning if given a sufficient, but practically realizable amount of time? In a thorough empirical investigation using a straightforward implementation of one such algorithm, simulated annealing, Johnson et al. (1991) concluded tentatively that the answer is \"no.\" In this paper we show that the answer can be \"yes\" if attention is devoted to the issue of problem representation (encoding). We present results from empirical tests of several encodings of Number Partitioning with problem instances consisting of multiple-precision integers drawn from a uniform probability distribution. With these instances and with an appropriate choice of representation, stochastic and deterministic searches can---routinely and in a practical amount of time---find solutions several orders of magnitude better than those constructed by the best heuristic known (Karmarkar and Karp, 1982), which does...",
            "group": 1193,
            "name": "10.1.1.50.8943",
            "keyword": "",
            "title": "Easily Searched Encodings for Number Partitioning"
        },
        {
            "abstract": "this paper is built on the idea of quantitative benchmark selection. We briefly explain the idea and present experimental results on the quantitative selection and validation of benchmarks. We lay out theoretically and statistically sound foundations to address the key issues related to the synthesis and analysis of synthetic benchmarks for behavioral synthesis systems. The key component of the new approach is the synthetic design example generator which composes the behavioral level specification of a design in such a way that a sunthesized design possesses the properties specified by a given set of numerical parameters. Experimental generation and application of synthetic design examples demonstrates the effectiveness of the proposed approach and the developed algorithms.",
            "group": 1194,
            "name": "10.1.1.50.9018",
            "keyword": "",
            "title": "A Quantitative Approach to Development and Validation of Synthetic Benchmarks for Behavioral Synthesis"
        },
        {
            "abstract": "We introduce a large family of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N-bit  parity and the detection of hidden symmetries. 1 Introduction  Boltzmann machines (Ackley, Hinton, & Sejnowski, 1985) have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines--- as originally conceived---also have some serious drawbacks...",
            "group": 1195,
            "name": "10.1.1.50.9174",
            "keyword": "",
            "title": "Learning in Boltzmann Trees"
        },
        {
            "abstract": "The goals of this thesis were to investigate the applicability of genetic algorithms to the problem of real-time scheduling of aircraft landings. In particular we investigate whether the constraints and the constantly changing conditions can be represented and whether adequate schedules can be computed in the time available. When a new plane needs to be placed in the schedule, the best schedules from the previous iteration are used as the starting point. We tested the genetic algorithm in generating landing schedules for ten data sets of aircraft landing data of between 30 and 40 planes. The genetic approach is compared with the performance of a heuristic search and currently it does not perform as well. The results indicate that good solutions can be obtained in a reasonable time, however it is apparent that the parameters for the genetic algorithm could be fine tuned, to produce better results.  Contents  1 Introduction 4  1.1 Goals : : : : : : : : : : : : : : : : : : : : : : : : : ...",
            "group": 1196,
            "name": "10.1.1.50.9231",
            "keyword": "",
            "title": "An Approach To Scheduling Aircraft Landing Times Using Genetic Algorithms"
        },
        {
            "abstract": "This article presents and evaluates the Slack Method, a new constructive heuristic for the allocation (mapping) of periodic hard realtime tasks to multiprocessor or distributed systems. The Slack Method is based on task deadlines, in contrast with other constructive heuristics, such as List Processing. The presented evaluation shows that the Slack Method is superior to list-processing-based approaches with regard to both finding more feasible solutions as well as finding solutions with better objective function values. In a comparative survey we evaluate the Slack Method against several alternative allocation techniques. This includes comparisons with optimal algorithms, non-guided search heuristics (e.g. Simulated Annealing), and other constructive heuristics. The main practical result of the comparison is that a combination of non-guided search and constructive approaches is shown to perform better than either of them alone, especially when using the Slack Method. 1 Introduction  Man...",
            "group": 1197,
            "name": "10.1.1.50.9524",
            "keyword": "",
            "title": "The Slack Method: A New Method for Static Allocation of Hard Real-Time Tasks"
        },
        {
            "abstract": "Parallel Architecture Applications ProperEXT Extraction ProperDRC Layout Verification ProperTEST ATPG ProperGATEST ATPG ProperSYN Synthesis ProperMIS Synthesis ProperPLACE Placement ProperROUTE Routing ProperHITEC ATPG ProperPROOFS Fault Simulation ProperSIM Circuit Simulation ProperVHDL VHDL Simulation MIS/SIS HITEC/PROOFS TimberWolfSC ... 6 Existing Serial Algorithms Parallel Application Figure 1.1: An overview of the ProperCAD project. for unstructured problems. ProperCAD II is an object-oriented library supporting the design of actor-based parallel programs [3, 6]. The library easily allows the design of data structures with parallel semantics for use in irregular applications. Because the foundation is based on C++ , inheritance mechanisms allow creation of the distributed data structures from standard C++ objects. The major goal of the ProperCAD project [7] is to develop portable parallel algorithms for VLSI CAD applications that will run on a range of parallel machines inc...",
            "group": 1198,
            "name": "10.1.1.51.949",
            "keyword": "",
            "title": "Parallel Algorithms For Standard Cell Placement Using Simulated Annealing"
        },
        {
            "abstract": "This paper describes a new local search algorithm that provides very high quality solutions to vehicle routing problems. The method uses greedy local search, but avoids local minima by using a large neighbourhood based upon rescheduling selected customer visits using constraint programming techniques. The move operator adopted is completely generic, in that virtually any side constraint can be efficiently incorporated into the search process. Computational results show that a naive implementation of the method produces results bettering the best produced by competing techniques using minima-escaping methods. 1 Introduction In recent years, the method of choice for solving vehicle routing problems has been to use a local search technique. These local search methods have been favoured since they quickly provide solutions to problems of practical size that have not been solved by exact methods. However, because local search techniques only make small changes to the solution, they can onl...",
            "group": 1199,
            "name": "10.1.1.51.1273",
            "keyword": "",
            "title": "A New Local Search Algorithm Providing High Quality Solutions to Vehicle Routing Problems"
        },
        {
            "abstract": "Manufacturing enterprises are now moving towards open architectures for integrating their activities with those of their suppliers, customers and partners within wide supply chain networks. Agent-based technology provides a natural way to design and implement such environments. This paper presents an agent-based manufacturing enterprise infrastructure. After a brief review of recent advancements in this domain, we describe the main features of the proposed infrastructure and the functions of its components. A machine-centered dynamic scheduling/rescheduling mechanism is then detailed and a prototype implementation is presented. Keywords: Enterprise integration, distributed manufacturing systems, manufacturing scheduling, agent, multi-agent systems, mediator. 1 Introduction Manufacturing enterprises are now moving towards open architectures for integrating their activities with those of their suppliers and customers within wide supply chain networks. To compete effectively in today's...",
            "group": 1200,
            "name": "10.1.1.51.1641",
            "keyword": "Enterprise integrationdistributed manufacturing systemsmanufacturing schedulingagentmulti-agent systemsmediator",
            "title": "An Agent-Based Manufacturing Enterprise Infrastructure for Distributed Integrated Intelligent Manufacturing Systems"
        },
        {
            "abstract": ". We propose a new approach to 3D object segmentation and description. Beginning with multiview range images of a 3D object, we segment the object into parts at deep surface concavities. Motivated by physics, we detect these concavities by locating surface points where the simulated electrical charge density achieves a local minimum. The individual parts are then described by parametric geons. The latter are defined as seven distinctive volumetric shapes characterized by constrained superellipsoids, with deformation parameters controlling the tapering and bending. We obtain a unique part model by fitting all parametric geons to each part and classifying the fitting residuals. The advantage of our classification approach is its ability to approximate the shape of an object, even one not composed of perfect geon-like parts. The resulting 3D shape description is a prerequisite for generic object recognition. 1 Introduction  Biederman has proposed a theory of Recognition-by-Components(RBC)...",
            "group": 1201,
            "name": "10.1.1.51.2296",
            "keyword": "",
            "title": "Segmenting 3D Objects into Geons"
        },
        {
            "abstract": "Successive, well organized application of transformations has been widely recognized as an exceptionally effective, but complex and difficult CAD task. We introduce a new potential-driven statistical approach for ordering transformations. Two new synthesis ideas are the backbone of the approach. The first idea is to quantify the characteristics of all transformations and the relationship between them based on their potential to reorganize a computation such that the complexity of the corresponding implementation is reduced. The second one is based on the observation that transformations may disable each other not only because they prevent the application of the other transformation, but also because both transformations target the same potential of the computation. These two observations drastically reduce the search space to find efficient and effective scripts for ordering transformations. A key algorithmic novelty is that both conceptual and optimization insights as well as all opti...",
            "group": 1202,
            "name": "10.1.1.51.2303",
            "keyword": "",
            "title": "Potential-Driven Statistical Ordering of Transformations"
        },
        {
            "abstract": "... this article, we present a generalized version of the Gibbs sampler that allows flexible conditional moves defined by groups of transformations. We explore its connection with the multigrid Monte Carlo (MGMC) of Goodman & Sokal and propose its use in designing more efficient samplers. The generalized Gibbs sampler provides a framework encompassing a class of recently proposed tricks, such as parameter expansion, reparameterization, blocking and grouping. To illustrate, this new method is applied to Bayesian inference problems for the multivariate Gaussian, nonlinear state-space models, ordinal data, and stochastic differential equations with discrete observations.",
            "group": 1203,
            "name": "10.1.1.51.2321",
            "keyword": "Some key wordsAuxiliary variableHaar measureMarkov chain Monte CarloMonte Carlo bridgingParameter expansionPartial resamplingState-space modelsStochastic differential equationTransformation",
            "title": "Generalized Gibbs sampler and multigrid Monte Carlo for Bayesian computation"
        },
        {
            "abstract": "Many large-scale engineering and scientific calculations involve repeated updating of variables on an unstructured mesh. To do these types of computations on distributed memory parallel computers, it is necessary to partition the mesh among the processors so that the load balance is maximized and inter-processor communication time is minimized. This can be approximated by the problem of partitioning a graph so as to obtain a minimum cut, a well-studied combinatorial optimization problem. Graph partitioning is NP complete, so for real world applications, one resorts to heuristics, i.e., algorithms that give good but not necessarily optimum solutions. These algorithms include recursive spectral bisection, local search methods such as Kernighan-Lin, and more general purpose methods such as simulated annealing. We show that a general procedure enables us to combine simulating annealing with Kernighan-Lin. The resulting algorithm is both very fast and extremely effective. 1 Introduction  Co...",
            "group": 1204,
            "name": "10.1.1.51.2493",
            "keyword": "",
            "title": "Partitioning of Unstructured Meshes for Load Balancing"
        },
        {
            "abstract": "ion Spaces  Thomas Ellman Saibal Patra Department of Computer Science Hill Center for Mathematical Sciences Rutgers University New Brunswick, New Jersey 08903 (908) 932 - 4184  fellman,patrag@cs.rutgers.edu  LCSR-TR-198 Abstract  Hillclimbing search has been shown to be useful for solving constraint satisfaction problems that are too large to be attacked using backtracking search. Nevertheless, hillclimbing search can be computationally expensive when the length of each climb is long, or when many climbs are required due to the presence of local, but non-global optima. \"Hierarchic Hillclimbing\" (HHC) is an extension of ordinary \"Flat Hillclimbing \" that is designed to attack such difficulties. HHC carries out hillclimbing search in a hierarchy of abstraction spaces, starting with the most abstract and proceeding to the most concrete. HHC takes as input a description of the abstraction hierarchy, as well as an evaluation function for each abstraction level. The HHC algorithm has been im...",
            "group": 1205,
            "name": "10.1.1.51.2609",
            "keyword": "Content AreasKnowledge CompilationAnalytic LearningSpeedup LearningConstraintbased reasoning",
            "title": "Hillclimbing in a Hierarchy of Abstraction Spaces"
        },
        {
            "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. Paper to appear in the special issue on Recurrent Networks of the IEEE Transactions on Neural Networks 1 Introduction We are interested in training recurrent neural networks to map input sequences to output sequences, for applications in sequence recognition, production or time-...",
            "group": 1206,
            "name": "10.1.1.51.2881",
            "keyword": "",
            "title": "Learning Long-Term Dependencies with Gradient Descent is Difficult"
        },
        {
            "abstract": "In this paper we describe synergy effects of combining state-of-the-art Geographic Information Technology (GIT) with novel methods for planning and scheduling from the field of Constraint Reasoning (CR). An integrated problem solving strategy has been developed for a problem arising in forest management. In particular, we have developed a method for solving what we have called the Clear-Cut Scheduling Problem (CCSP), where the task is to assign clear-cutting times to regions in a given forest area over a long term horizon. The schedule must satisfy certain ecological, recreational and economical constraints, and, in addition, optimise on a number of partially conflicting criteria. Our approach is based on the combination of advanced spatial analysis with an Iterative Improvement Technique in tandem with the Tabu Search meta heuristic. We have developed a software prototype called ECOPLAN with functionality for generation, presentation and modi\u00f8cation of harvest schedules in an integrate...",
            "group": 1207,
            "name": "10.1.1.51.2901",
            "keyword": "Forest harvestingSchedulingGISOptimisationTabu Search",
            "title": "Integration of Geographical Information Technology and Constraint Reasoning - A Promising Approach to Forest Management"
        },
        {
            "abstract": "Neighbourhood structure and size are important parameters in local search algorithms. This is also true for generalised local search algorithms like simulated annealing. It has been shown that the performance of simulated annealing can be improved by adopting a suitable neighbourhood size. However, previous studies usually assumed that the neighbourhood size was fixed during search. This paper presents a simulated annealing algorithm with a dynamic neighbourhood size which depends on the current \"temperature\" value during search. A method of dynamically deciding the neighbourhood size by approximating a continuous probability distribution is given. Four continuous probability distributions are used in our experiments to generate neighbourhood sizes dynamically, and the results are compared. 1 Introduction  Simulated Annealing (SA) algorithms can find very good near optimal solutions to a wide range of hard problems, but at the high computational cost. Various methods have been proposed...",
            "group": 1208,
            "name": "10.1.1.51.2904",
            "keyword": "",
            "title": "Comparison of Different Neighbourhood Sizes in Simulated Annealing"
        },
        {
            "abstract": "This paper presents an active database discrimination network algorithm called Gator. Gator is a generalization of the widely-known Rete and TREAT algorithms. Gator pattern matching is explained, and it is shown how a discrimination network can speed up condition testing for multi-table triggers. The structure of a Gator network optimizer is described, along with a discussion of optimizer performance, output quality, and accuracy. This optimizer can choose an efficient Gator network for testing the conditions of a set of triggers, given information about the structure of the triggers, database size, attribute cardinality, and update frequency distribution. The optimizer uses a randomized strategy to deal with the problem of a large search space. Optimizer validation was performed, showing a strong correlation between predicted cost of a Gator network and its actual cost when used for trigger condition testing. The results show that optimized Gator networks normally have a shape which i...",
            "group": 1209,
            "name": "10.1.1.51.2930",
            "keyword": "",
            "title": "Optimized Trigger Condition Testing in Ariel Using Gator Networks"
        },
        {
            "abstract": "In seeking to understand how and why genetic algorithms (GAs) work, attention has been focussed on the landscapes on which they search. While it is relatively simple to analyse the landscapes induced by traditional neighbourhood search operators, the position is considerably complicated for the operators normally used by a GA. Problems which have a single global optimum on a standard `Hamming' landscape such as the familiar Onemax  function actually possess an exponentially increasing number of local optima on the landscape associated with crossover. Nevertheless, GAs can solve such problems: the obvious question is how they succeed. In this paper, an attempt is made to answer this question for the case of the well-studied  Onemax problem. 1 Introduction  It is probably fair to say that a complete understanding of how genetic algorithms (GAs) work still eludes us. One popular approach in the GA community has been to restrict the analysis to a particular class of problems in an endeavou...",
            "group": 1210,
            "name": "10.1.1.51.3653",
            "keyword": "",
            "title": "The Crossover Landscape for the Onemax Problem"
        },
        {
            "abstract": "In this chapter we describe a stock market simulation in which stock market participants use genetic algorithms to gradually improve their trading strategies over time. A variety of experiments show that, under certain conditions, some market participants can make consistent profits over an extended period of time, a finding that might explain the success of some real-world money managers. These experiments suggest a four parameter model of market participants. Each participant can be described along four dimensions: information set, constraint set, algorithm set, and model set. The information set captures what data the participant has access to (e.g., the participant has access to all historical price data). The constraint set describes under what restrictions the participant operates (e.g., the participant can borrow money at 1% above the prime rate). The algorithm set indicates what programs the participant can use (e.g., the participant is restricted to hill-climbing optimization ...",
            "group": 1211,
            "name": "10.1.1.51.3752",
            "keyword": "Simulationgenetic algorithmfutures markets",
            "title": "A Model of Stock Market Participants"
        },
        {
            "abstract": "A fiber optic ring network, such as Fiber Distributed Data Interface (FDDI), can be operated over multiple wavelengths on its existing fiber plant consisting of point-to-point fiber links. Using wavelength division multiplexing (WDM) technology, FDDI nodes can be partitioned to operate over multiple subnetworks, with each subnetwork operating independently on a different wavelength, and inter-subnetwork traffic forwarding performed by a bridge. For this multi-wavelength version of FDDI, which we refer to as Wavelength Distributed Data Interface (WDDI), we examine the necessary upgrades to the architecture of a FDDI node, including its possibility to serve as a bridge. The main motivation behind this study is that, as network traffic scales beyond (the single-wavelength) FDDI's information-carrying capacity, its multi-wavelength version, WDDI, can gracefully accommodate such traffic growth. A number of design choices exist in constructing a good WDDI network. Specifically, we investigat...",
            "group": 1212,
            "name": "10.1.1.51.4103",
            "keyword": "Index TermsFDDIlightwave technologywavelength-division multiplexingpartitioningbridge",
            "title": "Optimized Partitioning of a Wavelength Distributed Data Interface Ring Network"
        },
        {
            "abstract": "A combination of distributed computation, positive feedback and constructive greedy heuristic is proposed as a new approach to stochastic optimization and problem solving. Positive feedback accounts for rapid discovery of very good solutions, distributed computation avoids premature convergence, and greedy heuristic helps the procedure to find acceptable solutions in the early stages of the search process. An application of the proposed methodology to the classical travelling salesman problem shows that the system can rapidly provide very good, if not optimal, solutions. We report on many simulation results and discuss the working of the algorithm. Some hints about how this approach can be applied to a variety of optimization problems are also given. 1. Introduction  In this paper we explore the emergence of global properties from the interaction of many simple agents. In particular, we are interested in the distribution of search activities over so-called \"ants\", i.e., agents that use...",
            "group": 1213,
            "name": "10.1.1.51.4214",
            "keyword": "",
            "title": "Ant System: An Autocatalytic Optimizing Process"
        },
        {
            "abstract": "To implement high-density and high-speed FPGA circuits, designers need tight control over the circuit implementation process. However, current design tools are unsuited for this purpose as they lack fast turnaround times, interactiveness, and integration. We present a system for the Xilinx XC6200 FPGA, which addresses these issues. It consists of a suite of tightly integrated tools for the XC6200 architecture centered around an architecture-independent tool framework. The system lets the designer easily intervene at various stages of the design process and features design cycle times (from an HDL specification to a complete layout) in the order of seconds. 1 Introduction  Fully automatic circuit synthesis from an HDL description is a difficult and computationally intensive task, especially for FieldProgrammable Gate Arrays (FPGAs). Ideally, circuits are mapped, placed and routed without human intervention. However, to implement  high-density or high-speed circuits with FPGAs, today's d...",
            "group": 1214,
            "name": "10.1.1.51.4271",
            "keyword": "",
            "title": "Fast Integrated Tools for Circuit Design with FPGAs"
        },
        {
            "abstract": "We consider a new class of optimization heuristics which combine local searches with stochastic sampling methods, allowing one to iterate local optimization heuristics. We have tested this on the Euclidean Traveling Salesman Problem, improving 3-opt by over 1.6% and Lin-Kernighan by 1.3%.    This work was supported in part by the grants DOE-FG03-85ER25009 and NSF-ECS8909127, and by a grant from the PSC-CUNY Research Award Program. Correspondence regarding this work should be addressed to S. Otto.  y  This manuscript was published in Operation Research Letters, v. 11, pp. 219--24, 1992.  1 Introduction  Given N cities labeled by i = 1; N , separated by distances d ij , the Traveling Salesman Problem (TSP) consists in finding the shortest tour, i.e., the shortest closed path visiting every city exactly once. To be specific, we will consider the symmetric TSP where d ij = d ji , but our method generalizes to the asymmetric case also. The problem of finding the optimal tour is a difficult...",
            "group": 1215,
            "name": "10.1.1.51.4400",
            "keyword": "TSPsimulated annealingMarkovOptimization",
            "title": "Large-Step Markov Chains for the TSP Incorporating Local Search Heuristics"
        },
        {
            "abstract": "Many randomized optimization algorithms operate by searching a graph whose nodes are all the potential solutions to a given optimization problem. Each graph node has a cost associated with it and the goal is to identify the node with the optimum cost. In principle, the set of edges in the graph can be arbitrary. Nevertheless, the behavior and effectiveness of many of these algorithms depend on the shape of the cost function over these graphs, which is directly affected by how nodes are connected. In particular, some of these algorithms are very effective when that shape forms a `well' with smooth sides and uneven bottom. It is thus beneficial for designers of optimization modules to connect nodes so that a `well' is formed. In this paper, we study the notion of `well' in the context of random graphs, and identify several factors that affect the formation or not of a `well' in them. These include the average node degree in the graph, the average cost difference between adjacent nodes in...",
            "group": 1216,
            "name": "10.1.1.51.4620",
            "keyword": "",
            "title": "Cost Wells in Random Graphs"
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks -- specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability, and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars, and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-t...",
            "group": 1217,
            "name": "10.1.1.51.4855",
            "keyword": "recurrent neural networksnatural language processing",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "Simulated annealing based standard cell placement for VLSI designs has long been acknowledged as a compute-intensive process, and as a result several research efforts have been undertaken to parallelize this algorithm. Most previous parallel approaches to cell placement annealing have used a parallel moves approach. In this paper we investigate two new approaches that have been proposed for generalized parallel simulated annealing but have not been applied to the cell placement problem. Results are presented on the effectiveness of implementations of these algorithms when applied to the cell placement problem. We find that the first, multiple Markov chains, appears to be promising since it uses parallelism to obtain near linear speedups with no loss in quality. The second, speculative computation, while maintaining quality is not suitable since no speedups are achieved due to the specific nature of the cell placement problem. The two algorithms are compared with the parallel moves appr...",
            "group": 1218,
            "name": "10.1.1.51.5577",
            "keyword": "",
            "title": "Parallel Simulated Annealing Strategies for VLSI Cell Placement"
        },
        {
            "abstract": "Nondestructively determining the essential parameters that describe the structure of a semiconductor wafer is a challenging inverse problem. We describe use of an optical inspection technology and show that it can be used effectively in conjunction with genetic algorithms (GAs) and local optimization methods. We also use this concrete application to investigate GA/local search hybrids, compare them to simulated annealing, and investigate the value of the recombination operator relative to the \"random crossover\" variant suggested by T. Jones. 1 INTRODUCTION  Nondestructive measurements of the structure of semiconductor wafers are critical for the efficient operation of modern fabrication facilities. Basic parameters that might need to be determined include the thickness, refractive index, and extinction coefficient for any of the layers on the wafer (see Figure 1). In some situations, the dispersion or scattering characteristics of a layer might also need to be determined. The method ex...",
            "group": 1219,
            "name": "10.1.1.51.5589",
            "keyword": "",
            "title": "Using Genetic Algorithms with Local Search for Thin Film Metrology"
        },
        {
            "abstract": "Join is one of the fundamental and most expensive operations in Database Management Systems (DBMSs). Although several techniques exist for computing multi-way joins in traditional databases, spatial joins involving more than two relations are not efficiently supported by current Spatial DBMSs and Geographic Information Systems (GISs). This paper proposes techniques for processing multi-way spatial joins by exploring their close correspondence with constraint satisfaction problems. In addition, it provides cost models and optimization methods that compute the execution plan using dynamic programming and hill climbing algorithms. Finally, it evaluates the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations. Contact Author: Dimitris Papadias Tel: ++852-23586971 http://www.cs.ust.hk/~dimitris/ Fax: ++852-23581477 E-mail: dimitris@cs.ust.hk The Hong Kong University of Science & Technology Techni...",
            "group": 1220,
            "name": "10.1.1.51.6560",
            "keyword": "",
            "title": "Constraint-based Processing of Multiway Spatial Joins"
        },
        {
            "abstract": "In this paper, a new heuristic for approximating the maximum clique problem is proposed, based on a detailed analysis of a class of continuous optimization models which yield a complete solution to this NP-hard combinatorial problem. The idea is to alter a regularization parameter iteratively in such a way that an iterative procedure with the updated parameter value would avoid unwanted, inefficient local solutions, i.e., maximal cliques which contain less than the maximum possible number of vertices. The local search procedure is performed with the help of the replicator dynamics, and the regularization parameter is chosen deliberately as to render dynamical instability of the (formerly) stable solutions which we want to discard in order to get an improvement. In this respect, the proposed procedure differs from usual simulated annealing approaches which mostly use a \"black-box\" cooling schedule. To demonstrate the validity of this approach, we report on the performance applied to sel...",
            "group": 1221,
            "name": "10.1.1.51.7169",
            "keyword": "",
            "title": "Annealed Replication: A New Heuristic for the Maximum Clique Problem"
        },
        {
            "abstract": "The efficient representation and encoding of signals with limited resources, e.g., finite storage capacity and restricted transmission bandwidth, is a fundamental problem in technical as well as biological information processing systems. Typically, under realistic circumstances, the encoding and communication of messages has to deal with different sources of noise and disturbances. In this paper, we propose a unifying approach to data compression by robust vector quantization, which explicitly deals with channel noise, bandwidth limitations, and random elimination of prototypes. The resulting algorithm is able to limit the detrimental effect of noise in a very general communication scenario. In addition, the presented model allows us to derive a novel competitive neural networks algorithm, which covers topology preserving feature maps, the so-called neural-gas algorithm, and the maximum entropy soft-max rule as special cases. Furthermore, continuation methods based on these noise model...",
            "group": 1222,
            "name": "10.1.1.51.7216",
            "keyword": "",
            "title": "Competitive Learning Algorithms for Robust Vector Quantization"
        },
        {
            "abstract": ". Complete enumeration of all the sequences to establish global optimality is not feasible as the search space, for a general job-shop scheduling problem, P G has an upper bound of (n!)  m  . Since the early fifties a great deal of research attention has been focused on solving P G , resulting in a wide variety of approaches such as Branch and Bound, Simulated Annealing, Tabu Search, etc. However limited success has been achieved by these methods due to the shear intractability of this generic scheduling problem. Recently, much effort has been concentrated on using neural networks to solve P G as they are capable of adapting to new environments with little human intervention and can mimic thought processes. Major contributions in solving P G using a Hopfield neural network, as well as applications of back-error propagation to general scheduling problems are presented. To overcome the deficiencies in these applications a modified back-error propagation model, a simple yet powerful paral...",
            "group": 1223,
            "name": "10.1.1.51.7342",
            "keyword": "",
            "title": "Job-Shop Scheduling Using Neural Networks"
        },
        {
            "abstract": "This thesis addresses the problem of estimating the surface reflection model of objects observed in a terrestrial scene, illuminated by natural illumination; that is, a scene which is illuminated by sun and sky light alone. This is a departure from the traditional analysis of laboratory scenes, which are illuminated by idealised light sources with positions and radiance distributions that are precisely controlled. Natural illumination presents a complex hemispherical light source which changes in both spatial and spectral distribution with time, terrestrial location, and atmospheric conditions. An image-based approach to the measurement of surface reflection is presented. The use of a sequence of images, taken over a period of time, allows the varying reflection from the scene due to the changing natural illumination to be measured. It is shown that the temporal change in image pixel values is suitable for the parameters of a reflection model to be estimated. These parameters are estim...",
            "group": 1224,
            "name": "10.1.1.51.7535",
            "keyword": "",
            "title": "Surface Reflection Model Estimation from Naturally Illuminated Image Sequences"
        },
        {
            "abstract": "This thesis addresses the problem of estimating the surface reflection model of objects observed in a terrestrial scene, illuminated by natural illumination; that is, a scene which is illuminated by sun and sky light alone. This is a departure from the traditional analysis of laboratory scenes, which are illuminated by idealised light sources with positions and radiance distributions that are precisely controlled. Natural illumination presents a complex hemispherical light source which changes in both spatial and spectral distribution with time, terrestrial location, and atmospheric conditions. An image-based approach to the measurement of surface reflection is presented. The use of a sequence of images, taken over a period of time, allows the varying reflection from the scene due to the changing natural illumination to be measured. It is shown that the temporal change in image pixel values is suitable for the parameters of a reflection model to be estimated. These parameters are estim...",
            "group": 1225,
            "name": "10.1.1.51.7535",
            "keyword": "",
            "title": "Surface Reflection Model Estimation from Naturally Illuminated Image Sequences"
        },
        {
            "abstract": "We describe implementations of two closely related variants of dynamic rooted trees. These variants are similar to Sleator and Tarjan's dynamic trees, but instead of the operation of finding the minimum key on the path from a given node to the root, we consider the operation of finding the minimum key in the subtree of a given node (the first variant) and the operation of finding a fixed key value in the subtree of a given node (the second variant). The first variant may provide support for edge deletions in the dynamic minimum spanning tree problem. The second variant arises in local search methods for the degree-constrained minimum spanning tree problem. Our implementations are based on Sleator and Tarjan's implementation of dynamic trees which uses splay trees. The preliminary experimental results suggests that in certain applications our implementations may be faster than more straightforward approaches. 1. Introduction  The dynamic tree problem is to maintain a collection of node-...",
            "group": 1226,
            "name": "10.1.1.52.247",
            "keyword": "",
            "title": "Implementations of Dynamic Tree Collections Based on Splay Trees"
        },
        {
            "abstract": "Many connectionist learning algorithms consists of minimizing a cost of the form  C(w) = E(J(z; w)) =  Z  J(z; w)dP (z) where dP is an unknown probability distribution that characterizes the problem to learn, and  J , the loss function, defines the learning system itself. This popular statistical formulation has led to many theoretical results. The minimization of such a cost may be achieved with a stochastic gradient descent algorithm, e.g.:  w t+1 = w t \\Gamma ffl t rwJ(z; w t ) With some restrictions on J and C, this algorithm converges, even if J is non differentiable on a set of measure 0. Links with simulated annealing are depicted. R'esum'e  De nombreux algorithmes connexionnistes consistent `a minimiser un cout de la forme C(w) = E(J(z; w)) =  Z  J(z; w)dP (z) o`u dP est une distribution de probabilit'e inconnue qui caract'erise le probl`eme, et J , le crit`ere local, d'ecrit le syst`eme d'apprentissage lui meme. Cette formulation statistique bien connue a donn'e lieu `a de nom...",
            "group": 1227,
            "name": "10.1.1.52.361",
            "keyword": "LearningLoss functionStochastic gradientConvergence TopicTheory",
            "title": "Stochastic Gradient Learning in Neural Networks"
        },
        {
            "abstract": "Deep dyslexia is an acquired reading disorder marked by the occurrence of semantic errors (e.g., reading RIVER as \"ocean\"). In addition, patients exhibit a number of other symptoms, including visual and morphological effects in their errors, a part-of-speech effect, and an advantage for concrete over abstract words. Deep dyslexia poses a distinct challenge for cognitive neuropsychology because there is little understanding of why such a variety of symptoms should co-occur in virtually all known patients. Hinton and Shallice (1991) replicated the co-occurrence of visual and semantic errors by lesioning a recurrent connectionist network trained to map from orthography to semantics. While the success of their simulations is encouraging, there is little understanding of what underlying principles are responsible for them. In this paper we evaluate and, where possible, improve on the most important design decisions made by Hinton and Shallice, relating to the task, the network architecture,...",
            "group": 1228,
            "name": "10.1.1.52.389",
            "keyword": "",
            "title": "Deep Dyslexia: A Case Study of Connectionist Neuropsychology"
        },
        {
            "abstract": "Iterative improvement partitioning algorithms such as the FM algorithm of Fiduccia and Mattheyses [8], the algorithm of Krishnamurthy [13], and Sanchis's extensions of these algorithms to multi-way partitioning [16], all rely on efficient data structures to select the modules to be moved from one partition to the other. The implementation choices for one of these data structures, the gain bucket, is investigated. Surprisingly, selection from gain buckets maintained as LIFO (Last-In-First-Out) stacks leads to significantly better results than gain buckets maintained randomly (as in previous studies of the FM algorithm [13] [16]) or as FIFO (First-In-First-Out) queues. In particular, LIFO buckets result in a 36% improvement over random buckets and a 43% improvement over FIFO buckets for minimum-cut bisection. Eliminating randomization from the bucket selection not only improves the solution quality, but has a greater impact on FM performance than adding the Krishnamurthy gain vector. The...",
            "group": 1229,
            "name": "10.1.1.52.407",
            "keyword": "",
            "title": "On Implementation Choices for Iterative Improvement Partitioning Algorithms"
        },
        {
            "abstract": ". We present a new approach to shape-based segmentation and tracking of multiple, deformable anatomical structures in cardiac MR images. We propose to use an energy-minimizing geometrically deformable template (GDT) which can deform into similar shapes under the influence of image forces. The degree of deformation of the template from its equilibrium shape is measured by a penalty function associated with mapping between the two shapes. In 2D, this term corresponds to the bending energy of an idealized thin-plate of metal. By minimizing this term along with the image energy terms of the classic deformable model, the deformable template is attracted towards objects in the image whose shape is similar to its equilibrium shape. This framework allows the simultaneous segmentation of multiple deformable objects using intra- as well as inter-shape information. The energy minimization problem of the deformable template is formulated in a Bayesian framework and solved using relaxation techniqu...",
            "group": 1230,
            "name": "10.1.1.52.444",
            "keyword": "",
            "title": "Shape-based Segmentation and Tracking in 4D Cardiac MR Images"
        },
        {
            "abstract": "The goal of this paper is to study the application of two-dimensional (2-D) finiteprecision infinite impulse response (IIR) filters to enhanced NTSC coding. It is wellknown that suitable two- or three-dimensional digital filtering greatly improves the quality of NTSC pictures by suppressing the interference between the luminance Y  and the chrominances I, Q. Thus far, 2-D and 3-D finite impulse response (FIR) filters have been used to reduce or eliminate these cross effects. To achieve good performance, however, they require many coefficients. Since, in general, IIR filters need fewer coefficients to approximate a given magnitude response, we investigate here the possibility of applying 2-D IIR filters to the NTSC encoding/decoding. We also study the feasibility of using digital filters for NTSC channel filtering; this would permit a digital-only encoder. To design suitable filters, we use a recently proposed method based on multiple constraint optimization and simulated annealing. We ...",
            "group": 1231,
            "name": "10.1.1.52.939",
            "keyword": "",
            "title": "The application of 2-D finite-precision IIR filters to enhanced NTSC coding"
        },
        {
            "abstract": "A MAP-MRF based scheme is proposed for simultaneous recovery of the depth and the focused image of a scene from two defocused images. The space-variant blur parameter and the focused image of the scene are both modeled as MRFs and their MAP estimates are obtained using simulated annealing. The performance of the proposed scheme is tested on synthetic as well as real data and the estimates of the depth are found to be better than that of existing window-based techniques.  I. Introduction  Depth from defocus (DFD) is a passive ranging technique that uses a single camera. Two images of an object (which may or may not be focused) acquired with different but known camera parameter settings are processed to determine the depth. In comparison to stereo vision and structure from motion methods, the problem of feature correspondence does not arise in DFD. Notwithstanding the simplicity, DFDbased techniques usually yield somewhat inferior results compared to stereo. Related works on DFD can be f...",
            "group": 1232,
            "name": "10.1.1.52.1071",
            "keyword": "",
            "title": "Optimal Recovery of Depth from Defocused Images Using an MRF Model"
        },
        {
            "abstract": "Iterative improvement partitioning algorithms such as the FM algorithm of Fiduccia and Mattheyses [8], the algorithm of Krishnamurthy [13], and Sanchis' extensions of these algorithms to multi-way partitioning [16], all rely on efficient data structures to select the modules to be moved from one partition to the other. The implementation choices for one of these data structures, the gain bucket, is investigated. Surprisingly, selection from gain buckets maintained as LIFO (Last-In-First-Out) stacks leads to significantly better results than gain buckets maintained randomly (as in previous studies of the FM algorithm [13] [16]) or as FIFO (First-In-First-Out) queues. In particular, LIFO buckets result in a 35% improvement over random buckets and a 42% improvement over FIFO buckets for minimum-cut bisection. Eliminating randomization from the bucket selection not only improves the solution quality, but has a greater impact on FM performance than adding the Krishnamurthy gain vector. The ...",
            "group": 1233,
            "name": "10.1.1.52.1265",
            "keyword": "",
            "title": "On Implementation Choices for Iterative Improvement Partitioning Algorithms"
        },
        {
            "abstract": "Introduction  The planned Next Generation Internet (NGI) infrastructure will allow high performance computers to interconnect via high performance networks. Distributed systems connected by NGI networks may provide an economical means for obtaining, through aggregation of distributed resources, computer power necessary to solve large scale, complex problems in various scientific fields, such as structure discovery problems in biology research, molecular dynamics simulations in chemistry research, structure determination in materials science research, and prediction models in global climate research. In order to efficiently use the NGI fabric for the execution of demanding parallel and distributed computations, pressing issues must be addressed. Among them, NGI-aware load balancing and CPU scheduling mechanisms are critical to these applications. Current load balancing mechanisms are designed for systems with fixed characteristics. For example, in the [1] framework, fixed system",
            "group": 1234,
            "name": "10.1.1.52.1637",
            "keyword": "",
            "title": "The Basis of Dynamically Adaptive Mechanisms for Efficient Computations on the Next Generation Internet"
        },
        {
            "abstract": "This study is aimed at developing 2D parallel adaptive mesh refinement algorithms for engineering applications such as penetration mechanics, manufacturing and combustion which use the finite element method for simulation. A new algorithmic approach, called piecewise adaptive mesh refinement for parallelization, is developed and implemented to run on a sequential machine. Performance of the new method is compared against a conventional advancing front technique proposed earlier. A parallel implementation model to run on a massively parallel computer is proposed and its expected efficiency is analyzed theoretically. Furthermore, some issues concerning parallelization and task partitioning are also discussed. In addition, a visualization tool is developed for pre- and post-processing of the finite element meshes that were generated. This tool runs under X-window environment and is used to display the triangular elements that form the mesh along with the physical values at the nodal point...",
            "group": 1235,
            "name": "10.1.1.52.2436",
            "keyword": "",
            "title": "Parallel Adaptive Mesh Refinement Algorithms"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We pr...",
            "group": 1236,
            "name": "10.1.1.52.2501",
            "keyword": "CR CategoriesH.5.2 [Information Interfaces and PresentationUser Interfaces---screen design. 2.1 [Artificial IntelligenceApplications and Expert Systems---cartography. I.3.5 [Computer GraphicsComputational Geometry and Object Modeling---geometric algorithmslanguagesand systems. General Termsalgorithmsexperimentation. Additional Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealingheuristic search",
            "title": "An Empirical Study of Algorithms for Point-Feature Label Placement"
        },
        {
            "abstract": "this paper, we consider optimality of the natural code in the much larger space of general codes, which are taken to be surjective maps from fA; C; G; Tg",
            "group": 1237,
            "name": "10.1.1.52.2619",
            "keyword": "",
            "title": "How Optimal is the Genetic Code?"
        },
        {
            "abstract": "... these new methods, we develop a prototype, called Novel (Nonlinear Optimization Via External Lead), that solves nonlinear constrained and unconstrained problems in a unified framework. We show experimental results in applying Novel to solve nonlinear optimization problems, including (a) the learning of feedforward neural networks, (b) the design of quadrature-mirror-filter digital filter banks, (c) the satisfiability problem, (d) the maximum satisfiability problem, and (e) the design of multiplierless quadrature-mirror-filter digital filter banks. Our method achieves better solutions than existing methods, or achieves solutions of the same quality but at a lower cost. ",
            "group": 1238,
            "name": "10.1.1.52.2962",
            "keyword": "",
            "title": "Global Search Methods For Solving Nonlinear Optimization Problems"
        },
        {
            "abstract": "The problem considered in this paper is motivated by the independence between logical and physical topology in Wavelength Division Multiplexing (WDM) based local and metropolitan lightwave networks. This paper suggests logical embeddings of digraphs into multihop lightwave networks to maximize the throughput under nonuniform traffic conditions. Defining congestion as the maximum flow carried on any link, two perturbation heuristics are presented to find a good logical embedding on which the routing problem is solved with minimum congestion. A constructive proof for a lower bound of the problem is given, and obtaining an optimal solution for integral routing is shown to be NP-Complete. The performance of the heuristics is empirically analyzed on various traffic models. Simulation results show that our heuristics perform, on the average, 20% from a computed lower bound. Since this lower bound is not quite tight, we suspect that the actual performance is better. In addition, we show that ...",
            "group": 1239,
            "name": "10.1.1.52.3472",
            "keyword": "",
            "title": "Logical Embeddings for Minimum Congestion Routing in Lightwave Networks"
        },
        {
            "abstract": "This article was published in the May 1996 issue of IEEE Signal Processing Magazine. Due to the editorial process, the published article is somewhat different than this manuscript. The reader is advised to obtain a copy of the published article.  deconvolution. Experience shows that in practice some information is needed to successfully restore the image. There are several motivating factors behind the use of blind deconvolution for image processing applications. In practice, it is often costly, dangerous or physically impossible to obtain a priori information about the imaged scene. For example, in applications like remote sensing and astronomy it is difficult to statistically model the original image or even know specific information about scenes never imaged before [3], [4]. In addition, the degradation from blurring cannot be accurately specified. In aerial imaging and astronomy, the blurring cannot be accurately modelled as a random process, since fluctuations in the PSF are difficult to characterize [13]. In realtime image processing, such as medical video-conferencing, the parameters of the PSF cannot be pre-determined to instantaneously deblur images [14]. Moreover, on-line identification techniques used to estimate the degradation may result in significant error, which can create artefacts in the restored image [15]. In other applications, the physical requirements for improved image quality are unrealizable. For instance, in space exploration, the physical weight of a high resolution camera exceeds practical constraints. Similarly, in x-ray imaging, improved image quality occurs with increased incident xray beam intensity, which is hazardous to a patient's health [5]. Thus, blurring is unavoidable. In such situations, the hardware available to measure the PSF...",
            "group": 1240,
            "name": "10.1.1.52.4099",
            "keyword": "",
            "title": "Blind Image Deconvolution: An Algorithmic Approach to Practical Image Restoration"
        },
        {
            "abstract": "Given a heuristic estimate of the relative safety of a hybrid dynamical system trajectory, we transform the initial safety problem for dynamical systems into a global optimization problem. We compare untuned performance of several Simulated Annealing and Multi Level Single Linkage method variants, and discuss the dynamic use of knowledge gained during optimization. *This work was supported by the Defense Advanced Research Projects Agency and the National Institute of Standards and Technology under Cooperative Agreement 70NANB6H0075, \"Model-Based Support of Distributed Collaborative Design\". Author's address: Knowledge Systems Laboratory, Gates Building 2A, Stanford University, Stanford CA 94305-9020, USA",
            "group": 1241,
            "name": "10.1.1.52.4807",
            "keyword": "",
            "title": "Heuristic Optimization and Dynamical System Safety Verification"
        },
        {
            "abstract": "A non-deterministic minimization algorithm recently proposed is analyzed. Some characteristics are analytically derived from the analysis of positive definite quadratic forms. An improvement is proposed and compared with the basic algorithm. Different variants of the basic algorithm are finally compared to a standard Conjugate Gradient minimization algorithm in the computation of the Rayleigh coefficient of a positive definite symmetric matrix. 1. Introduction  Function minimization is a widespread need in the scientific community. The major shortcoming of the most used minimization algorithms is the sensitivity to local minima. Deterministic methods, which are guaranteed to get the global minima (like those based on interval analysis, see [7]), have a complexity which is exponential in the dimension of the domain, making them often unusable. Methods like simulated annealing promise a reduced sensitivity (see [2], [3]) to local minima but their success depends on the choice of an appro...",
            "group": 1242,
            "name": "10.1.1.52.4884",
            "keyword": "",
            "title": "On Random Minimization of Functions"
        },
        {
            "abstract": "This paper describes the SCAN (Signal Channelling Attentional Network) model, a scalable neural-network model for attentional scanning. The building block of SCAN is a gating lattice, a sparsely-connected neural network defined as a special case of the Ising lattice from statistical mechanics. The process of spatial selection through covert attention is interpreted as a biological solution to the problem of translation-invariant pattern processing. In SCAN, a sequence of pattern translations combines active selection with translation-invariant processing. Selected patterns are channelled through a gating network, formed by a hierarchical fractal structure of gating lattices, and mapped onto an output window. We show how the incorporation of an expectation-generating classifier network (e.g., Carpenter and Grossberg's ART network) into SCAN allows attentional selection to be driven by expectation. Simulation studies show the SCAN model to be capable of attending and identifying object p...",
            "group": 1243,
            "name": "10.1.1.52.4893",
            "keyword": "",
            "title": "SCAN: A Scalable Model of Attentional Selection"
        },
        {
            "abstract": "Vehicle routing problems have been extensively studied using a variety of search methods and meta-heuristics. However, most search methods are carried out using only a single improvement heuristic. Here, we report on a study using four improvement heuristics embedded in a simple steepest descent search engine. We conclude that reasonable results can be produced quickly by this simple method. A small study of the effectiveness of the different improvement heuristics was also undertaken---it is shown that the RELOCATE operator is invaluable and that solutions with longer vehicle routes benefited more from using a rich set of improvement heuristics.  1 Introduction  Heuristic approaches to the vehicle routing problem have been well studied over the last decade by the Operations Research and Artificial Intelligence fields. As processing power has increased, so researchers have moved from simple construction heuristics (e.g. [1]), to partitioning techniques (e.g. [3, 8]), general improveme...",
            "group": 1244,
            "name": "10.1.1.52.5096",
            "keyword": "",
            "title": "Study of Greedy Search with Multiple Improvement Heuristics for Vehicle Routing Problems"
        },
        {
            "abstract": "Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called ANFIS (Adaptive-Network-based Fuzzy Inference System), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed. Keywords--- Fuzzy logic, neural networks, fuzzy modeling, neuro-fuzzy modeling, neuro-fuzzy control, ANFIS. I. Introduction In 1965, Zadeh published the first paper on a novel way of characterizing non-probabilistic uncertainties, which he called fuzzy sets [118]. This year marks the 30th anniversary of fuzzy logic and fuzzy set theory, which has now evolved into a fruitful area containin...",
            "group": 1245,
            "name": "10.1.1.52.5165",
            "keyword": "",
            "title": "Neuro-Fuzzy Modeling and Control"
        },
        {
            "abstract": "Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called ANFIS (Adaptive-Network-based Fuzzy Inference System), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed. Keywords--- Fuzzy logic, neural networks, fuzzy modeling, neuro-fuzzy modeling, neuro-fuzzy control, ANFIS. I. Introduction In 1965, Zadeh published the first paper on a novel way of characterizing non-probabilistic uncertainties, which he called fuzzy sets [118]. This year marks the 30th anniversary of fuzzy logic and fuzzy set theory, which has now evolved into a fruitful area containin...",
            "group": 1246,
            "name": "10.1.1.52.5165",
            "keyword": "",
            "title": "Neuro-Fuzzy Modeling and Control"
        },
        {
            "abstract": "Many problems in computer vision can be formulated as an optimization problem. Developping the efficient global optimizational technique adaptive to the vision proplem becomes more and more important. In this paper, we present a geometric primitive extraction method, which plays a crucial role in content-based image retrieval and other vision problems. We formulate the problem as a cost function minimization problem and we present a new optimization technique called Evolutionary Tabu Search (ETS). Genetic algorithm and Tabu Search Algorithm are combined in our method. Specificly, we incorporates \"the survival of strongest\" idea of evolution algorithm into tabu search. In experiments, we use our method for shape extraction in images and compare our method with other three global optimization methods including genetic algorithm, simulated Annealing and tabu search. The results show that the new algorithm is a practical and effective global optimization method, which can yield good near-o...",
            "group": 1247,
            "name": "10.1.1.52.5376",
            "keyword": "Geometric primitiveGlobal OptimizationEvolutionary Tabu SearchTabu SearchGenetic AlgorithmSimulated Annealing",
            "title": "Evolutionary Tabu Search for Geometric Primitive Extraction"
        },
        {
            "abstract": "We investigate a topological design and routing problem for Low-Earth Orbit (LEO) satellite communication networks where each satellite can have a limited number of direct inter-satellite links (ISL's) to a subset of satellites within its line-of-sight. First, we model LEO satellite network as a FSA (Finite State Automaton) using satellite constellation information. Second, we solve a combined topological design and routing problem for each configuration corresponding to a state in the FSA. The topological design (or link assignment) problem deals with the selection of ISL's, and the routing problem handles the traffic distribution over the selected links to maximize the number of carried calls. In this paper, this NP-complete mixed integer optimization problem is solved by a two-step heuristic algorithm that first solves the topological design problem, and then finds the optimal routing. The algorithm is iterated using the simulated annealing technique until the near-optimal solution ...",
            "group": 1248,
            "name": "10.1.1.52.5939",
            "keyword": "LEO satellite networktopological designroutingsimulated",
            "title": "Topological Design And Routing For Low-Earth Orbit Satellite Networks"
        },
        {
            "abstract": "In this paper we describe a possible application of computing techniques inspired by natural life mechanisms (genetic algorithms and artificial neural networks) to an artificial life creature, namely a small mobile robot, called KitBorg. We proposed in a previous work (Bessi\u00e8re 1990) Probabilistic Inference as a possible underlying theory or mathematical metaphor for numerous works in the field of formal neural networks. Probabilistic Inference suggests that any cognitive problem may be split in two optimization problems. The first one called the \"dynamic inference problem\" is an abstraction of \"learning\", the second one, namely, the \"static inference problem\", being a mathematical metaphor of \"pattern association\". In this previous paper, for instance, Boltzmann machines have been shown to be a special case of probabilistic inference, where the two optimization problems are dealt with using simulated annealing (Kirckpatrick 1983) for the pattern association part and using simple gradi...",
            "group": 1249,
            "name": "10.1.1.52.6089",
            "keyword": "",
            "title": "GENETIC ALGORITHMS APPLIED TO FORMAL NEURAL NETWORKS: parrallel genetic implementation of a Boltzmann machine and associated robotic experimentations"
        },
        {
            "abstract": "A distributed hard real time system can be composed from a number of communicating tasks. One of the difficulties with building such systems is the problem of where to place the tasks. In general there are P  T  ways of allocating T  tasks to P processors, and the problem of finding an optimal feasible allocation (where all tasks meet physical and timing constraints) is known to be NP-Hard. This paper describes an approach to solving the task allocation problem using a technique known as simulated annealing. It also defines a distributed hard realtime architecture and presents new analysis which enables timing requirements to be guaranteed. 1. INTRODUCTION  Building real-time systems on distributed architectures presents engineers with a number of challenging problems. One issue is that of scheduling the communication media, another concerns the allocation of software components to the available processing resources. Distributed systems typically consist of a mixture of periodic and sp...",
            "group": 1250,
            "name": "10.1.1.52.6114",
            "keyword": "",
            "title": "Allocating Hard Real Time Tasks + (An NP-Hard Problem Made Easy)"
        },
        {
            "abstract": "Many learning systems search through a space of possible performance elements, seeking an element whose expected utility, over the distribution of problems, is high. As the task of finding the globally optimal element is often intractable, many practical learning systems instead hill-climb to a local optimum. Unfortunately, even this is problematic as the learner typically does not know the underlying distribution of problems, which it needs to determine an element's expected utility. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present a general algorithm, palo, that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by presenting three distinct applications, that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the util...",
            "group": 1251,
            "name": "10.1.1.52.6128",
            "keyword": "",
            "title": "PALO: A Probabilistic Hill-Climbing Algorithm"
        },
        {
            "abstract": "An operation scheme for the Boltzmann Machine Optimizer is proposed that is suitable for parallelization and is based on the notion of group updates. It has the advantage of suggesting transitions between states that differ in more than one unit and exhibits greater flexibility in accepting such transitions compared to the pure sequential case. The performance of the method is evaluated on the Weighted Maximum Independent Set problem and comparisons with the pure Boltzmann Machine are presented concerning both solution quality and convergence speed. A parallel algorithm is formulated that ensures accurate cost calculations. Implementation on both shared memory and distributed memory architectures has yielded very good speedup. 1 Introduction  The Boltzmann Machine [1, 3] constitutes an approach to combinatorial optimization that can be considered as a neural network implementation of the Simulated Annealing methodology [14] and has been successfully employed for providing near-optimal ...",
            "group": 1252,
            "name": "10.1.1.52.6323",
            "keyword": "",
            "title": "A Parallelizable Operation Scheme of the Boltzmann Machine Optimizer Based on Group Updates"
        },
        {
            "abstract": ": A combination of distributed computation, positive feedback and constructive greedy heuristic is proposed as a new approach to stochastic optimization and problem solving. Positive feedback accounts for rapid discovery of very good solutions, distributed computation avoids premature convergence, and greedy heuristic helps the procedure to find acceptable solutions in the early stages of the search process. An application of the proposed methodology to the classical travelling salesman problem shows that the system can rapidly provide very good, if not optimal, solutions. We report on many simulation results and discuss the working of the algorithm. Some hints about how this approach can be applied to a variety of optimization problems are also given. Keywords: Ant Systems, Ant Colonies, Adaptive Systems, Artificial Life, Combinatorial Optimization, Parallel Algorithms. ---ooOoo--- To obtain a copy of this report please fill in your name and address and return this page to: Laboratori...",
            "group": 1253,
            "name": "10.1.1.52.6342",
            "keyword": "Ant SystemsAnt ColoniesAdaptive SystemsArtificial LifeCombinatorial OptimizationParallel Algorithms.---ooOoo",
            "title": "Positive Feedback as a Search Strategy"
        },
        {
            "abstract": "This paper presents an active database discrimination network algorithm called Gator, and its implementation in a modified version of the Ariel active DBMS. Gator is a generalization of the widely known Rete and TREAT algorithms. Gator pattern matching is explained, and it is shown how a discrimination network can speed up condition testing for multi-table triggers. The structure of a Gator network optimizer is described. This optimizer can choose an efficient Gator network for testing the conditions of a set of triggers, given information about the structure of the triggers, database size, attribute cardinality, and update frequency distribution. The optimizer uses a randomized strategy to deal with the problem of a large search space. The results show that optimal Gator networks normally have a shape which neither pure Rete nor pure TREAT, but an intermediate form where one or a few inner joins (fi nodes) are materialized. In addition, this study shows that it is indeed feasible to o...",
            "group": 1254,
            "name": "10.1.1.52.6878",
            "keyword": "",
            "title": "Optimized Trigger Condition Testing in Ariel using Gator Networks"
        },
        {
            "abstract": "In this paper, structural topology optimization is addressed through Genetic Algorithms. A set of designs is evolved following the Darwinian survival-of-fittest  principle. The standard crossover and mutation operators are tailored for the needs of 2D topology optimization. The genetic algorithm based on these operators is experimented on plane stress problems of cantilever plates: the goal is to optimize the weight of the structure under displacement constraints. The main advantage of this approach is that it can both find out alternative optimal solutions, as experimentally demonstrated on a problem with multiple solutions, and handle different kinds of mechanical model: some results in elasticity with large displacements are presented. In that case, the nonlinear geometrical effects of the model lead to non viable solutions, unless some constraints are imposed on the stress field.  1 INTRODUCTION  Since the seminal work of Holland (1975) and the comprehensive study of Goldberg (1989...",
            "group": 1255,
            "name": "10.1.1.52.7020",
            "keyword": "",
            "title": "Structural Topology Optimization in Linear and Nonlinear Elasticity Using Genetic Algorithms"
        },
        {
            "abstract": "Many large-scale engineering and scientific calculations involve repeated updating of variables on an unstructured mesh. To do these types of computations on distributed memory parallel computers, it is necessary to partition the mesh among the processors so that the load balance is maximized and inter-processor communication time is minimized. This can be approximated by the problem of partitioning a graph so as to obtain a minimum cut, a well-studied combinatorial optimization problem. Graph partitioning is NP complete, so for real world applications, one resorts to heuristics, i.e., algorithms that give good but not necessarily optimum solutions. These algorithms include recursive spectral bisection, local search methods such as Kernighan-Lin, and more general purpose methods such as simulated annealing. We show that a general procedure enables us to combine simulating annealing with Kernighan-Lin. The resulting algorithm is both very fast and extremely effective. 1 Introduction  Co...",
            "group": 1256,
            "name": "10.1.1.52.7107",
            "keyword": "",
            "title": "Partitioning of Unstructured Meshes for Load Balancing"
        },
        {
            "abstract": "  Incorporating functional partitioning into a synthesis methodology leads to several important advantages. In functional partitioning, we first partition a functional specification into smaller sub-specifications and then synthesize structure for each, in contrast to the current approach of first synthesizing structure for the entire specifi...",
            "group": 1257,
            "name": "10.1.1.52.7334",
            "keyword": "",
            "title": "Functional Partitioning Improvements Over  Structural Partitioning for Packaging Constraints and Synthesis-Tool Performance  "
        },
        {
            "abstract": "The combinatorial optimization problem of MAP estimation is converted to one of constrained real optimization and then solved by using the proposed augmented Lagrange-Hopfield (ALH) method. The ALH effectively overcomes instabilities that are inherent in the penalty method or the Lagrange multiplier method in constrained optimization. It produces good solutions with reasonable costs. I. Introduction  The aim of image restoration is to recover a degraded image and that of image segmentation is to partition an image into regions of similar image properties. Efficient restoration and segmentation are very important for numerous image analysis applications. Both problems can be posed generally as one of image estimation where the underlying image or segmentation map is to be estimated from the degraded image. Due to various uncertainties, an optimal solution is sought. A popular optimality criterion is the maximum a posteriori  (MAP) probability principle in which both the prior distributi...",
            "group": 1258,
            "name": "10.1.1.52.7657",
            "keyword": "",
            "title": "MAP Image Restoration and Segmentation By Constrained Optimization"
        },
        {
            "abstract": "The usual approaches of using local search can hardly be generalized. The advance in efficiency is the primary goal, whereas generality is often disregarded. This manifests in very domain-specific problem encodings and specialized satisfaction methods, e.g. the solving of a TSP by edge-exchanging techniques based on a graph representation. Other work takes the general constraint programming framework as starting point and tries to introduce local search methods for the satisfaction, e.g. Minton's min-conflicts heuristic. These methods often fail, as they have just a very limited view of the unknown search space structure. This work tries to overcome the drawbacks of these two approaches by using global constraints. The use of global constraints for local search allows to revise a current state on a more global level with domain-specific knowledge, while preserving features like declarativeness, reusability, and maintenance. The proposed strategy is demonstrated on a dynamic job-shop sc...",
            "group": 1259,
            "name": "10.1.1.52.7685",
            "keyword": "",
            "title": "Using Global Constraints for Local Search"
        },
        {
            "abstract": ". The question of satisfiability for a given propositional formula arises in many areas of AI. Especially finding a model for a satisfiable formula is very important though known to be NP-complete. There exist complete algorithms for satisfiability testing like the Davis-Putnam-Algorithm, but they often do not construct a satisfying assignment for the formula, are not practically applicable for more than 400 or 500 variable problems, or in practice take too much time to find a solution. Recently, a (in practice) very fast, though incomplete, procedure, the model generating algorithm GSAT, has been introduced and several refined variants were created. Another method is Simulated Annealing (SA). Both approaches have already been compared with different results. We clarify these differences and do a more elaborate comparison showing that the performance of an already optimized variant of GSAT and an ordinary SA algorithm are more or less the same and that the attempts to further improve G...",
            "group": 1260,
            "name": "10.1.1.52.7836",
            "keyword": "",
            "title": "GSAT versus Simulated Annealing"
        },
        {
            "abstract": "This paper discusses the issues involved in the design of a complete information retrieval system based on useroriented clustering schemes. Clusters are constructed taking into account the users' perception of similarity between documents. The system accumulates feedback from the users and employs it to construct useroriented clusters. An optimization function to improve the effectiveness of the clustering process is developed. A retrieval process based on the clustering scheme is described. The system developed is experimentally validated and compared with existing systems. 1 Introduction  An information retrieval (ir) system is characterized by a collection of documents and a set of users who perform queries on the collection to fulfill their information needs. To improve the efficiency of retrieval, it has been proposed that the documents which are generally retrieved together in response to some query, should be kept close together within the system in the form of clusters [28, 30]...",
            "group": 1261,
            "name": "10.1.1.52.8015",
            "keyword": "",
            "title": "Cluster-Based Adaptive Information Retrieval"
        },
        {
            "abstract": "Neural networks are one of the most widely used artificial intelligence methods for financial time series analysis. In this paper we describe the standard application of neural networks and suggest that it has two shortcomings. First, backpropagation search takes place in sum of squared errors space instead of risk-adjusted return space. Second, the standard neural network has difficulty ignoring noise and focusing in on discoverable regularities. Both problems are illustrated with simple examples. We suggest ways of overcoming these problems. 1 Introduction  Ever since McCulloch and Pitts [8] published their landmark paper describing a neural calculus, researchers have explored how biologically inspired networks might be employed to solve a wide class of problems. In recent years, Wall Street, in its never ending search for new ways to beat the market, has turned to neural networks as a possible answer. The attention focused on neural networks for financial time series analysis stems ...",
            "group": 1262,
            "name": "10.1.1.52.8360",
            "keyword": "",
            "title": "A Critique of the Standard Neural Network Application to Financial Time Series Analysis"
        },
        {
            "abstract": "We address the problem of maximizing the speedup of an individual parallel job through the selection of an appropriate number of processors on which to run it. If a parallel job exhibits speedup that increases monotonically in the number of processors, the solution is clearly to make use of all available processors. However, many applications do not have this characteristic: they reach a point beyond which the use of additional processors degrades performance. For these applications, it is important to choose a processor allocation carefully. Our approach to this problem is to provide a runtime system that adjusts the number of processors used by the application based on dynamic measurements of performance gathered during its execution. Our runtime system has a number of advantages over user specified fixed allocations, the currently most common approach to this problem: (1) we are resilient to changes in an application's speedup behavior due to the input data; and (2) we are able to c...",
            "group": 1263,
            "name": "10.1.1.52.8671",
            "keyword": "",
            "title": "Maximizing Speedup Through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "This paper presents a search technique for scheduling problems, called Heuristic-Biased Stochastic Sampling (HBSS). The underlying assumption behind the HBSS approach is that strictly adhering to a search heuristic often does not yield the best solution and, therefore, exploration off the heuristic path can prove fruitful. Within the HBSS approach, the balance between heuristic adherence and exploration can be controlled according to the confidence one has in the heuristic. By varying this balance, encoded as a bias function,  the HBSS approach encompasses a family of search algorithms of which greedy search and completely random search are extreme members. We present empirical results from an application of HBSS to the realworld problem of observation scheduling. These results show that with the proper bias function, it can be easy to outperform greedy search. Introducing HBSS  This paper presents a search technique, called  Heuristic-Biased Stochastic Sampling (HBSS), that was design...",
            "group": 1264,
            "name": "10.1.1.52.8819",
            "keyword": "",
            "title": "Heuristic-Biased Stochastic Sampling"
        },
        {
            "abstract": "This paper is the first to present a parallelization of a highly efficient best-first branch-and-bound algorithm to solve large symmetric traveling salesman problems on a massively parallel computer containing 1024 processors. The underlying sequential branch & bound algorithm is based on 1-tree relaxation introduced by Held and Karp (Lagrangean approach) and improved by Volgenant and Jonker. The parallelization of the branch & bound algorithm is fully distributed. Every processor performs the same sequential algorithm but on a different part of the solution tree. To distribute subproblems among the processors we use a new directneighbor dynamic load-balancing strategy. The general principle can be applied to all other branch-and-bound algorithms leading to an \"automatic\" parallelization. At present we can efficiently solve traveling salesman problems up to a size of 318 cities on networks of up to 1024 transputers. On hard problems we achieve an almost linear speedup.",
            "group": 1265,
            "name": "10.1.1.52.8992",
            "keyword": "Combinatorial OptimizationDistributed AlgorithmsDynamic Load-BalancingParallel Branch-and-BoundSymmetric Traveling Salesman Problem (STSP",
            "title": "  Solving the Traveling Salesman Problem with a Distributed Branch-and-Bound Algorithm on a 1024 Processor Network"
        },
        {
            "abstract": "Is effective collaboration possible among autonomous software agents that are distributed over a network of computers? Both empirical evidence and theory suggest that there are simple rules for designing problem-solving organizations in which collaboration among such agents is automatic and scale-effective (adding agents tends to improve solution-quality; adding computers tends to improve solution-speed). This paper develops some of these rules for off-line problems, and argues that the rules can be extended for the on-line (real-time) control of distributed systems, such as electric power networks.  Keywords: autonomous agents, collaboration, multi-agent systems, organizations.  1. INTRODUCTION  This paper deals with the skills that unsupervised (autonomous) software agents must have if they are to collaborate effectively. This section explains the terminology that will be used, formulates the collaboration problem and outlines an approach to its resolution. 1.1. Terminology  Let a \"s...",
            "group": 1266,
            "name": "10.1.1.52.9082",
            "keyword": "autonomous agentscollaborationmulti-agent systemsorganizations",
            "title": "Collaboration Rules for Autonomous Software Agents"
        },
        {
            "abstract": "Something must have wrong for it to lead to the end for Marcus Brutus? For a perfect crime to be committed there would be no errors, which is not the case in this story. Brutus the catalyst of an assassination on Julius Cesar wrote the death sentence for him and the other conspirators. Havoc broke loose because of Brutus\u0092 compassion, envy, greed and revenge.",
            "group": 1267,
            "name": "10.1.1.52.9125",
            "keyword": "",
            "title": "Methods of Category Classification Applied to Word-Sense Disambiguation and Discourse Analysis"
        },
        {
            "abstract": "Given a set of models and some training data, we would like to find the model which best describes the data. Finding the model with the lowest generalization error is a computationally expensive process, especially if the number of testing points is high or if the number of models is large. Optimization techniques such as hill climbing or genetic algorithms are helpful but can end up with a model which is arbitrarily worse than the best one (local minima) or even worse, cannot be used because of the lack of a distance metric on the space of discrete models. In this paper we develop a technique called \"racing\" which tests the set of models in parallel, quickly discarding those models which are clearly inferior and concentrating the computational effort on differentiating among the better models. We use racing to select among various memory based models and to find relevant features in applications ranging from robot juggling to lesion detection in MRI scans.",
            "group": 1268,
            "name": "10.1.1.52.9198",
            "keyword": "",
            "title": "A racing algorithm: Model selection for memory based learners"
        },
        {
            "abstract": "Research on potential interactions between connectionist learning systems, i.e., artificial neural networks (ANNs), and evolutionary search procedures, like genetic algorithms (GAs), has attracted a lot of attention recently. Evolutionary ANNs (EANNs) can be considered as the combination of ANNs and evolutionary search procedures. This paper first distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures and of learning rules. Then it reviews each kind of evolution in detail and analyses critical issues related to different evolutions. The review shows that although a lot of work has been done on the evolution of connection weights and of architectures, few attempts have been made to understand the evolution of learning rules. Interactions among different evolutions are seldom mentioned in current research. However, the evolution of learning rules and its interactions with other kinds of evolution play a vital role in EANNs. As t...",
            "group": 1269,
            "name": "10.1.1.52.9598",
            "keyword": "",
            "title": "A Review of Evolutionary Artificial Neural Networks"
        },
        {
            "abstract": "Speech coding systems for mobile communication have to cope with noisy channels. In particular, vector quantization as central data reduction scheme is highly sensitive to transmission errors due to the low redundancy in the encoded data. Here we present three methods for the design of a vector quantizer with enhanced robustness against transmission errors. First the optimization of the index assignment of a LBG vector quantizer via simulated annealing is investigated. Second a neighborhood conserving vector quantizer is designed by using a topology conserving feature map. Third we discuss a method in which the error characteristic of the channel is used in the optimization of a vector quantizer as well as in the quantization of a data vector. Simulation experiments and results for a binary symmetric channel with bit error probabilities up to 10% are presented for vector quantization of speech signals and predictor parameters. 1 INTRODUCTION  Speech coding systems have to operate in fu...",
            "group": 1270,
            "name": "10.1.1.52.9607",
            "keyword": "",
            "title": "Robust Vector Quantization For Low Bit Rate Speech Coding"
        },
        {
            "abstract": "Structural topology optimization is addressed through Genetic Algorithms: A set of designs is evolved following the Darwinian survival-of-fittest principle. The goal is to optimize the weight of the structure under displacement constraints. This approach demonstrates high flexibility, and breaks many limits of standard optimization algorithms, in spite of the heavy requirements in term of computational effort: Alternate optimal solutions to the same problem can be found; Structures can be optimized with respect to multiple loadings; The prescribed loadings can be applied on the unknown boundary of the solution, rather than on the fixed boundary of the design domain; Different materials as well as different mechanical models can be used, as witnessed by the first results of Topological Optimum Design ever obtained in the large displacements model. But these results could not have been obtained without careful specific handling of the specific aspects of topological genetic optimization: First, specific genetic operators (crossover, mutation) were introduced; Second, special attention was paid to the design of the objective function; The nonlinear geometrical effects of the large displacement model lead to non viable solutions, unless some constraints are imposed on the stress field. 1",
            "group": 1271,
            "name": "10.1.1.52.9785",
            "keyword": "",
            "title": "Topological Optimum Design using Genetic Algorithms"
        },
        {
            "abstract": "In this paper we investigate the problem of codebook generation for Vector Quantizers which optimize nonEuclidean as well as Euclidean distortions. A new algorithm for minimizing non-Euclidean distortions is introduced together with some speed-up heuristics. The structure of local minima for different distortion measures is analyzed statistically and using techniques for multidimensional scaling. The latter are also used to improve compression rates by codebook ordering. 1. Introduction  Clustering of points in multidimensional spaces is useful in many fields, from image compression to pattern recognition. Many algorithms have been proposed in the past to solve the problem of clustering [1]. In particular clustering is used for vector quantization. Given a set of clusters, represented by their centers, the vector space is quantized by assigning each point to the nearest center. In this paper we will focus on clustering using nonEuclidean distances. The reason is twofold. On the one han...",
            "group": 1272,
            "name": "10.1.1.52.9992",
            "keyword": "",
            "title": "On the Generation of Codebooks for Vector Quantization"
        },
        {
            "abstract": "The uniform design (UD) has been widely used in various fields and is generated by the socalled good lattice point (glp) set. In this paper, we shall propose some new UDs which are based on cyclic latin squares (CLS) and can significantly reduce the discrepancy of CLS of Wang and Fang (1981). Some properties of CLS are given for reducing computational load. The threshold accepting algorithm is employed for finding the \"best\" CLS with low discrepancy. Key Words and Phrases : Discrepancy, experimental design, latin squares, L 2 -discrepancy, threshold accepting algorithm, uniform design, uniformity. 1991 Mathematics Subject Classification : 62K99, 62K05, 65K10. 1. Introduction 1. Introduction 1. Introduction The uniform design (UD) is a kind of experimental designs such that experimental points are uniformly scattered in the experimental domain. The UD was proposed by Fang and Wang (Fang, 1980; Wang and Fang, 1981) and has been successfully applied in pharmaceutics, industry, space techn...",
            "group": 1273,
            "name": "10.1.1.53.75",
            "keyword": "Key Words and PhrasesDiscrepancyexperimental designlatin squaresL 2-discrepancythreshold",
            "title": "Uniform Design Based on Latin Squares"
        },
        {
            "abstract": "Test functions are commonly used to evaluate the effectiveness of different search algorithms. However, the results of evaluation are as dependent on the test problems as they are on the algorithms that are the subject of comparison. Unfortunately, developing a test suite for evaluating competing search algorithms is difficult without clearly defined evaluation goals. In this paper we discuss some basic principles that can be used to develop test suites and we examine the role of test suites as they have been used to evaluate evolutionary search algorithms. Current test suites include functions that are easily solved by simple search methods such as greedy hill-climbers. Some test functions also have undesirable characteristics that are exaggerated as the dimensionality of the search space is increased. New methods are examined for constructing functions with different degrees of nonlinearity, where the interactions and the cost of evaluation scale with respect to the dimensionality of...",
            "group": 1274,
            "name": "10.1.1.53.134",
            "keyword": "",
            "title": "Evaluating Evolutionary Algorithms"
        },
        {
            "abstract": "The high throughput computation requirements of real-time digital signal processing (dsp) systems usually dictate hardware intensive solutions. Often attendant to hardware approaches are problems of high development costs, slow turnaround, susceptibility to errors, and difficulty in testing and debugging, all of which tend to inhibit the rapid implementation of such systems. Research is underway into the synthesis of application specific hardware to aid the system designer by automatically generating hardware that is \"correct by construction\". The creation of configurable, pre-fabricated hardware that has been designed for high speed computations forms part of this research and is the main topic of this thesis. This work contains a survey of some typical real-time dsp algorithms drawn from video and speech processing and summarizes the particular computation challenges posed by this class of algorithms. Currently available hardware choices and their trade-offs and limitations are discu...",
            "group": 1275,
            "name": "10.1.1.53.265",
            "keyword": "",
            "title": "Programmable Arithmetic Devices for High Speed Digital Signal Processing"
        },
        {
            "abstract": "Partitioning a data set and extracting hidden structure arises in different application areas of pattern recognition, data analysis and image processing. We formulate data clustering for data characterized by pairwise dissimilarity values as an assignment problem with an objective function to be minimized. An extension to tree--structured clustering is proposed which allows a hierarchical grouping of data. Deterministic annealing algorithms are derived for unconstrained and tree--structured pairwise clustering.  1 Introduction  Pairwise data clustering is a non--trivial generalization of Euclidian clustering or vector quantization. While in vector quantization the data is given in an explicit vector representation, an instance of a pairwise clustering problem is characterized by pairwise dissimilarity values D ik , summarized in a proximity matrix D. Pairwise data clustering problems arise as a data analysis problem in experimental sciences, e.g., psychology, linguistics, genetics and ...",
            "group": 1276,
            "name": "10.1.1.53.298",
            "keyword": "",
            "title": "Hierarchical Pairwise Data Clustering by Mean-Field Annealing"
        },
        {
            "abstract": "Several factors may interact to determine the periodicity of ocular dominance stripes in cat and monkey visual cortex. Previous theoretical work has suggested roles for the width of cortical interactions and the strength of between-eye correlations. Here, a model based on an explicit optimization is presented that allows a thorough characterization of how these and other parameters of the afferent input could affect ocular dominance stripe periodicity. The principle conclusions are that increasing the width of within-eye correlations leads to wider columns, and, surprisingly, that increasing the width of cortical interactions can sometimes lead to narrower columns.  1 Introduction  In cats, monkeys and humans, layer 4 of the primary visual cortex (V1) is divided up into alternating regions dominated by input from the left and right eyes (e.g. Hubel & Wiesel (1977)). These regions segregate from a spatially uniform pattern during development (Rakic, 1976; LeVay et al, 1978). A characte...",
            "group": 1277,
            "name": "10.1.1.53.333",
            "keyword": "",
            "title": "The Influence of Neural Activity and Intracortical Connections on the Periodicity of Ocular Dominance Stripes"
        },
        {
            "abstract": "This paper describes a new approach to the design of multidimensional (M-D) finitewordlength digital filters with specifications in the frequency and spatial domains. The approach is based on stochastic optimization and extends previous work on finite impulse response (FIR) filters in two ways: by inclusion of spatial constraints and by application to the case of infinite impulse response (IIR) filters. The formulation proposed is based on a multiple-term objective function that, in addition to magnitude constraints, also includes step response, group delay and stability constraints. Our attention to these characteristics stems from the application of such filters to video processing that we are actively pursuing. Since filter coefficients are of finite precision and since the objective function is multivariable, non-differentiable and likely to have multiple minima, we use simulated annealing for optimization. We show numerous examples of the design of practical filters such as channe...",
            "group": 1278,
            "name": "10.1.1.53.1137",
            "keyword": "",
            "title": "Design of multidimensional finite-wordlength FIR and IIR filters by simulated annealing"
        },
        {
            "abstract": "Image segmentation by energy--minimizing active contour models (snakes) suffers from the fact that classic numerical optimization algorithms find only local energy minima, which makes the approach very sensible to noise and initialization. This paper presents a robust adaptive snake model using a stochastic relaxation technique, Simulated Annealing (SA), to find a global energy minimum in noisy cine MR images. Once a reliable segmentation has been done, a much faster probabilistic relaxation technique, Iterated Conditional Modes (ICM), is used for energy minimization in the following time frames. Introduction  Conventional invasive cardiovascular imaging methods using cardiac catheterization are associated with a small morbidity and mortality. New imaging techniques like magnetic resonance (MR) provide a noninvasive method for evaluating cardiovascular functional aspects [8] [9]. However, the imaging of the cardiovascular system by MR is very difficult due to movement artifacts. The re...",
            "group": 1279,
            "name": "10.1.1.53.1274",
            "keyword": "",
            "title": "Contour fitting using stochastic and probabilistic relaxation for Cine MR Images"
        },
        {
            "abstract": "This paper briefly reviews the behaviour of four different evolutionary optimization methods when applied to a pair of difficult, high dimension test functions. The methods considered are the Genetic Algorithm (GA)",
            "group": 1280,
            "name": "10.1.1.53.1344",
            "keyword": "",
            "title": "A Brief Comparison Of Some Evolutionary Optimization Methods"
        },
        {
            "abstract": "The authors present a summary of classic literature addressing the topic of Artificial Life (A--Life). From roots in mathematics, computer science, and biology, this new discipline has proven itself as a valuable tool in a wide variety of engineering applications. The most commonly used A--Life technique in engineering applications is genetic algorithms (GAs). However, there are many other variations and methods based upon the evolutionary or biological analogy. This paper presents a survey of the entire family of A--Life techniques available to the engineering research community. The paper concludes with a detailed investigation of one particularly promising application, a new hybrid field encompassing engineering, robotics, and A-Life: evolutionary robotics (ER).   Mechanical Engineering Department and Iowa Center for Emerging Manufacturing Technology, Iowa State University, Ames, IA, 50011, email: (volkerjoliver)@icemt.iastate.edu  1 Introduction: Why Artificial Life?  Darwin[17] w...",
            "group": 1281,
            "name": "10.1.1.53.1447",
            "keyword": "",
            "title": "A Survey of Artificial Life and Evolutionary Robotics"
        },
        {
            "abstract": ". Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required an ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches - CI tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying Bayesian network structure using a non CI based method. Results of the evaluation of the algorithm on a number of networks (ex. ALARM, LED and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems. Key words. Bayesian Networks, Conditional Independence, Probabilistic Model Construction. y  A preliminary version of this paper was presented in [Singh and Valtorta, 93].  z  Current address is the Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104 - msingh@gradient.cis.upenn.edu  1 INTRODUCTION  In v...",
            "group": 1282,
            "name": "10.1.1.53.1474",
            "keyword": "Key words. Bayesian NetworksConditional IndependenceProbabilistic Model Construction",
            "title": "Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm"
        },
        {
            "abstract": ". The channel assignment problem has become increasingly important in mobile telephone communication. Since the usable range of the frequency spectrum is limited, the optimal assignment problem of channels has become increasingly important. Recently Genetic Algorithms (GAs) have been proposed as new computational tools for solving optimization problems. GAs are more attractive than other optimization techniques, such as neural networks  or simulated annealing, since GAs are generally good at finding an acceptably good global optimal solution to a problem  very quickly. In this paper, a new channel assignment algorithm using GAs is proposed. The channel assignment problem is formulated as an energy minimization problem that is implemented by GAs. Appropriate GAs operators such as reproduction, crossover and mutation are developed and tested. In this algorithm, the cell frequency is not fixed before the assignment procedures as in the previously reported channel assignment algorithm usin...",
            "group": 1283,
            "name": "10.1.1.53.1628",
            "keyword": "Key wordsCellular mobile networkChannel assignmentGenetic algorithmsWireless communication 2 J-S. KIM ET AL",
            "title": "Channel Assignment in Cellular Radio using Genetic Algorithms"
        },
        {
            "abstract": "We develop a binary relaxation scheme for graph matching in chemical databases. The technique works by iteratively pruning the list of matching possibilities for individual atoms based upon contextual information. Its key features include delayed decisionmaking, robustness to noise, and fast and efficient neural implementation. We illustrate the utility of the technique by comparing it with probabilistic relaxation for a small database of 2D structures, and suggest that it may be applicable to matching in large databases of both 2D and 3D chemical graphs. INTRODUCTION  Fast and accurate structure matching is important in the pharmaceutical industry where chemists routinely need to assess the similarity of structures within very large chemical databases. The assumption is that structural similarity is an indicator of shared molecular behaviour. By searching the database the chemist hopes to retrieve existing molecules which are similar to a prespecified query structure of interest, ther...",
            "group": 1284,
            "name": "10.1.1.53.1828",
            "keyword": "",
            "title": "A Neural Relaxation Technique For Chemical Graph Matching"
        },
        {
            "abstract": "Stochastic combinatorial optimization techniques, such as simulated annealing and genetic algorithms, have become increasingly important in design automation as the size of design problems have grown and the design objectives have become increasingly complex. However, stochastic algorithms are often slow since a large number of random design perturbations are required to achieve an acceptable result -- they have no built-in \"intelligence\". In this paper, we show that incremental, statistical learning techniques can improve the quality of results and reduce the number of expensive cost-function evaluations for stochastic optimization for a particular solution quality. In particular, simulated annealing was selected as representative stochastic optimization approach and the cell-based layout placement problem was used to evaluate the utility of such a learning-based approach. In this work, we used regression to learn the properties of the solution space and have tested the trained algori...",
            "group": 1285,
            "name": "10.1.1.53.1936",
            "keyword": "",
            "title": "Learning as Applied to Simulated Annealing"
        },
        {
            "abstract": "ion Paul R. Cooper  Institute for the Learning Sciences Northwestern University Evanston, IL cooper@ils.nwu.edu  Peter N. Prokopowicz  Institute for the Learning Sciences Northwestern University Evanston, IL prokopowicz@ils.nwu.edu Abstract  Network vision systems must make inferences from evidential information across levels of representational abstraction, from low level invariants, through intermediate scene segments, to high level behaviorally relevant object descriptions. This paper shows that such networks can be realized as Markov Random Fields (MRFs). We show first how to construct an MRF functionally equivalent to a Hough transform parameter network, thus establishing a principled probabilistic basis for visual networks. Second, we show that these MRF parameter networks are more capable and flexible than traditional methods. In particular, they have a well-defined probabilistic interpretation, intrinsically incorporate feedback, and offer richer representations and decision ca...",
            "group": 1286,
            "name": "10.1.1.53.2072",
            "keyword": "",
            "title": "Markov Random Fields Can Bridge Levels of Abstraction"
        },
        {
            "abstract": "An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call Ant System. We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical Traveling Salesman Problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the Ant System (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadrat...",
            "group": 1287,
            "name": "10.1.1.53.2844",
            "keyword": "",
            "title": "The Ant System: Optimization by a colony of cooperating agents"
        },
        {
            "abstract": "This paper presents a new heuristic for graph partitioning called Path Optimization  (PO), and the results of an extensive set of empirical comparisons of the new algorithm with two very well-known algorithms for partitioning: the Kernighan-Lin algorithm and simulated annealing. Our experiments are described in detail, and the results are presented in such a way as to reveal performance trends based on several variables. Sufficient trials are run to obtain 99% confidence intervals small enough to lead to a statistical ranking of the implementations for various circumstances. The results for geometric graphs, which have become a frequently-used benchmark in the evaluation of partitioning algorithms, show that PO holds an advantage over the others. In addition to the main test suite described above, comparisons of PO to more recent partitioning approaches are also given. We present the results of comparisons of PO with a parallelized implementation of Goemans' and Williamson's 0.878 appr...",
            "group": 1288,
            "name": "10.1.1.53.2851",
            "keyword": "",
            "title": "Path Optimization for Graph Partitioning Problems"
        },
        {
            "abstract": "Due to the lack of exact quantitative information or the difficulty associated with obtaining or processing such information, qualitative spatial knowledge representation and reasoning often become an essential means for solving spatial constraint problems as found in science and engineering. This paper presents a computational approach to representing and reasoning about spatial constraints in two-dimensional  Euclidean space, where the a priori spatial information is not precisely expressed in quantitative terms. The spatial quantities considered in this work are qualitative distances and qualitative orientation angles. Here, we explicitly define the semantics of these quantities and thereafter formulate a representation of qualitative trigonometry  (QTRIG). The resulting QTRIG formalism provides the necessary inference rules for qualitative spatial reasoning. In the paper, we illustrate how the QTRIG relationships can be employed in generating qualitative spatial descriptions in two...",
            "group": 1289,
            "name": "10.1.1.53.3096",
            "keyword": "Spatial reasoningqualitative trigonometryknowledge representationkinematic",
            "title": "A Method of Spatial Reasoning Based on Qualitative Trigonometry"
        },
        {
            "abstract": "We report the results of testing the performance of a new, efficient, and highly generalpurpose parallel optimization method based upon simulated annealing. This optimization algorithm was applied to analyze the network of interacting genes that control embryonic development and other fundamental biological processes. We found several sets of algorithmic parameters that lead to optimal parallel efficiency for up to 100 processors on distributedmemory MIMD architectures. Our strategy contains two major elements. First, we monitor and pool performance statistics obtained simultaneously on all processors. Second, we mix states at intervals to ensure a Boltzmann distribution of energies. The central scientific issue is the inverse problem, the determination of the parameters of a set of nonlinear ordinary differential equations by minimizing the total error between the model behavior and experimental observations.  1 Introduction Simulated annealing (SA) is an effective method for the opt...",
            "group": 1290,
            "name": "10.1.1.53.3488",
            "keyword": "Simulated annealingParallel processingInverse problems",
            "title": "Parallel Simulated Annealing by Mixing of States"
        },
        {
            "abstract": "In many recent low earth orbit (LEO) satellite networks, line-of-sight (LOS) wireless links, called intersatellite links (ISL's), are used to enhance the capability of the networks. These ISL's are continuously deleted and established since the visibility between satellites changes over time. When such topological change occurs, calls that have been serviced by deleted links need to be rerouted. In this paper, we propose a call admission control scheme that aims to reduce the number blocked ongoing calls that occur at the time of the topological change. To estimate the number of blocked ongoing calls, which is needed to control the admission of new calls, the proposed scheme uses two techniques: traffic recasting and traffic projection. The traffic recasting maps the current traffic onto the network after the topological change whereas the traffic projection projects the recast traffic from the current time to the time of the topological change. The projected traffic is used to estimat...",
            "group": 1291,
            "name": "10.1.1.53.3637",
            "keyword": "",
            "title": "A Predictive Call Admission Control Scheme for Low Earth Orbit Satellite Networks"
        },
        {
            "abstract": "Spacecraft design optimization is a difficult problem, due to the complexity of optimization cost surfaces and the human expertise in optimization that is necessary in order to achieve good results. In this paper, we propose the use of a set of generic, metaheuristic optimization algorithms (e.g., genetic algorithms, simulated annealing), which is configured for a particular optimization problem by an adaptive problem solver based on artificial intelligence and machine learning techniques. We describe work in progress on OASIS, a selfconfiguring optimization system based on these principles. 1 Introduction Many engineering design optimization problems are instances of constrained optimization problems. Given a set of decision variables X and a set of constraints C on X , the constrained optimization is the problem of assigning values to X to minimize or maximize an objective function F (X), subject to the constraints C. Although constrained optimization is a mature field that has bee...",
            "group": 1292,
            "name": "10.1.1.53.3726",
            "keyword": "",
            "title": "Towards a Self-Configuring Optimization System for Spacecraft Design"
        },
        {
            "abstract": "We present a fully automatic method for the alignment SAR images, which is capable of precise and robust alignment. A multiresolution SAR image matching metric is first used to automatically determine tie-points, which are then used to perform coarse-to-fine resolution image alignment. A formalism is developed for the automatic determination of tie-point regions that contain sufficiently distinctive structure to provide strong constraints on alignment. The coarse-to-fine procedure for the refinement of the alignment estimate both improves computational efficiency and yields robust and consistent image alignment.  Keywords: Registration, Alignment, Coarse-to-fine, Multiresolution, Tie-Point, Texture 1. INTRODUCTION  The ability to bring multiple synthetic aperture radar (SAR) images into alignment is crucial in many applications. For example, a need for accurate alignment commonly occurs when multiple partially overlapping image are \"stitched\" together to form a single larger image. Eve...",
            "group": 1293,
            "name": "10.1.1.53.3745",
            "keyword": "RegistrationAlignmentCoarse-to-fineMultiresolutionTie-PointTexture",
            "title": "Structure-driven SAR image registration"
        },
        {
            "abstract": " This paper reports about research projects of the University of Paderborn in the field of distributed combinatorial optimization. We give an introduction into combinatorial optimization and a brief definition of some important applications. As a first exact solution method we describe branch & bound and present the results of our work on its distributed implementation. Results of our distributed implementation of iterative deepening conclude the first part about exact methods. In the second part we give an introduction into simulated annealing as a heuristic method and present results of its parallel implementation. This part is concluded with a brief description of genetic algorithms and some other heuristic methods together with some results of their distributed implementation.",
            "group": 1294,
            "name": "10.1.1.53.4719",
            "keyword": "Key wordsCombinatorial OptimizationBranch & BoundIterative DeepeningSimulated AnnealingLoad BalancingParallel ProcessingTransputer Systems",
            "title": " Distributed Combinatorial Optimization"
        },
        {
            "abstract": "An important area of research in computational biochemistry is the design of molecules for specific applications. The design of these molecules depends on the accurate determination of their three-dimensional structure or conformation. Under the assumption that molecules will settle into a configuration for which their energy is at a minimum, this design problem can be formulated as a global optimization problem. The solution of the molecular conformation problem can then be obtained, at least in principle, through any number of optimization algorithms. Unfortunately, it can easily be shown that there exist a large number of local minima for most molecules which makes this an extremely difficult problem for any standard optimization method. In this study, we present results for a direct search method applied to a molecular conformation problem. We compare the new method against genetic algorithms and simulated annealing. The major result of this study is that the direct search method w...",
            "group": 1295,
            "name": "10.1.1.53.5342",
            "keyword": "",
            "title": " A Direct Search Method for the Molecular Conformation Problem"
        },
        {
            "abstract": " The vehicle routing problem (VRP) is a variant of the familiar travelling salesman problem (TSP). In the VRP we are to perform a number of visits, using a limited number of vehicles, while minimizing the distance travelled. The VRP can be further complicated by associating time windows on visits, capacity constraints on vehicles, sequencing constraints between visits, and so on. In this paper we introduce a constraint-based model of the capacitated VRP with time windows and side constraints. The model is implemented using a constraint programming toolkit. We investigate the performance of a number of construction and improvement techniques, and show that as problems become richer and more constrained conventional techniques fail while constraint directed techniques continue to perform acceptably. This suggests that constraint programming is an appropriate technology for real world VRP's. ",
            "group": 1296,
            "name": "10.1.1.53.5392",
            "keyword": "",
            "title": "A Comparison of Traditional and Constraint-based Heuristic Methods on Vehicle Routing Problems with Side Constraints"
        },
        {
            "abstract": ". Local hill-climbing algorithms to solve the satisfiability problem have shown to be more efficient than complete systematic methods in many aspects. Many variants and refinements have been developed in the last few years. We now present a neural network approach to evaluate such local GSAT-like algorithms in a parallel manner, i.e. enlarging the neighbourhood of each possible move in the search space. We present an approach which allows the simultaneous change of truth value assignment for more than one variable at a time, such that the theoretical properties of the considered algorithms are preserved, and give experimental evidence that this algorithm is indeed faster than the respective sequential variants.  1 INTRODUCTION  Existing complete procedures for the satisfiability (SAT-) problem are not practically applicable for more than 400 or 500 variable random problems and take too much time to find a solution for smaller problems. So the search for (in practice) faster procedures ...",
            "group": 1297,
            "name": "10.1.1.53.5533",
            "keyword": "",
            "title": "Multi-Flip Networks: A Parallelization of local SAT-Algorithms"
        },
        {
            "abstract": ". It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \"Hybrid Monte Carlo\" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \"free energy\" differences, it should also be possible to compare the merits of different network architectures. The work described here should also be applicable to a wide variety of st...",
            "group": 1298,
            "name": "10.1.1.53.5868",
            "keyword": "",
            "title": "Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method"
        },
        {
            "abstract": "An approach is presented for treating discrete optimization problems mapped on the architecture of the Hopfield neural network. The method constitutes a modification to the local minima escape (LME) algorithm which has been recently proposed as a method that uses perturbations in the network's parameter space in order to escape from local minimum states of the Hofield network. Our approach (LMESA) adopts this perturbation mechanism but, in addition, introduces randomness in the selection of the next local minimum state to be visited in a manner analogous with the case of Simulated Annealing. Experimental results using instances of the Weighted Maximum Independent Set problem indicate that the proposed method leads to significant improvement over the conventional LME approach in terms of quality of the obtained solutions, while requiring a comparable amount of computational effort.  ",
            "group": 1299,
            "name": "10.1.1.53.6082",
            "keyword": "",
            "title": "Improved Exploration in Hopfield Network State-Space through Parameter Perturbation Driven by Simulated Annealing"
        },
        {
            "abstract": ". In the optimal plant location and sizing problem it is desired to optimize a cost function involving plant sizes, locations, and production schedules in the face of supply-demand and plant capacity constraints. We will use simulated annealing (SA) and a genetic algorithm (GA) to solve this problem. We will compare these techniques with respect to computational expenses, constraint handling capabilities, and the quality of the solution obtained in general. Simulated Annealing is a combinatorial stochastic optimization technique which has been shown to be effective in obtaining fast suboptimal solutions for computationally hard problems. The technique is especially attractive since solutions are obtained in polynomial time for problems where an exhaustive search for the global optimum would require exponential time. We propose a synergy between the cluster analysis technique, popular in classical stochastic global optimization, and the GA to accomplish global optimization. This synergy...",
            "group": 1300,
            "name": "10.1.1.53.6151",
            "keyword": "",
            "title": "Solution Of The Optimal Plant Location And Sizing Problem Using Simulated Annealing And Genetic Algorithms"
        },
        {
            "abstract": "We survey learning algorithms for recurrent neural networks with hidden units and attempt to put the various techniques into a common framework. We discuss fixpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non-fixpoint algorithms, namely backpropagation through time, Elman's history cutoff nets, and Jordan's output feedback architecture. Forward propagation, an online technique that uses adjoint equations, is also discussed. In many cases, the unified presentation leads to generalizations of various sorts. Some simulations are presented, and at the end, issues of computational complexity are addressed. This research was sponsored in part by The Defense Advanced Research Projects Agency, Information Science and Technology Office, under the title \"Research on Parallel Computing\", ARPA Order No. 7330, issued by DARPA/CMO under Contract MDA972-90-C-0035 and in part by the National Science Foundation under grant number EET-8716324 and i...",
            "group": 1301,
            "name": "10.1.1.53.6525",
            "keyword": "learningsequencestemporal structurerecurrent neural networksfixpoints Contents",
            "title": "Dynamic Recurrent Neural Networks"
        },
        {
            "abstract": "this paper these mutation-based versions of SA and SIHC are referred to as MUSA and MU SIHC. Because the same problem suite as the one used in this chapter was used to evaluate MUSA and MU SIHC, and for completeness, the results of these algorithms are included in the tables of this chapter.",
            "group": 1302,
            "name": "10.1.1.53.6793",
            "keyword": "",
            "title": "?"
        },
        {
            "abstract": "Stochastic methods such as monte-carlo techniques and simulated annealing have been studied extensively for non-convex optimization problems. Recently, Fast Simulated Annealing (FSA) and Simulated Annealing with convolution Smoothing (SAS) have been proposed as variant algorithms that give better results with fewer number of function evaluations. FSA uses biased distributions with a rapid cooling schedule. SAS on the other hand recognizes that a multi-extremal function can be represented as superpositions of many uni-extremal functions super-imposed with some multi-dimensional \"noise.\" The minimization is then performed on these \"uniextremal functions\" to locate the global minimum. In spite of these improvements, the number of function evaluations needed and the error in the global minimum estimate still remains large. In this paper we present an algorithm called Semi-stochastic High-dimensional Optimization based on Occupied Territories (SHOOT). For any point x in the domain of the fu...",
            "group": 1303,
            "name": "10.1.1.53.7017",
            "keyword": "",
            "title": "A Semi-Stochastic Algorithm for Optimizing High Dimensional Functions"
        },
        {
            "abstract": "We describe a theoretical formulation for stereo in terms of the Bayesian approach to vision. This formulation enables us to integrate the depth information from different types of matching primitives, or from different vision modules. We solve the correspondence problem using compatibility constraints between features and prior assumptions on the interpolated surfaces that result from the matching. We use techniques from statistical physics to show how our theory relates to previous work and to develop novel algorithms. Finally we show that, by a suitable choice of prior assumptions about surfaces, the theory is consistent with some psychophysical experiments which investigate the relative importance of different matching primitives.  1 Introduction  Computational vision (Marr, 1982) attempts to construct theoretical models of visual processes and relate them to psychophysical and physiological experiments. With the vast number of research papers on these topics it is desirable to de...",
            "group": 1304,
            "name": "10.1.1.53.7354",
            "keyword": "",
            "title": "Stereo Integration, Mean Field Theory and Psychophysics"
        },
        {
            "abstract": "Power is becoming a critical constraint for designing embedded applications. Current power analysis techniques based on circuit-level or architectural-level simulation are either impractical or inaccurate to estimate the power cost for a given piece of application software. In this paper, an instruction-level power analysis model is developed for an embedded DSP processor based on physical current measurements. Significant points of difference have been observed between the software power model for this custom DSP processor and the power models that have been developed earlier for some general-purpose commercial microprocessors [1, 2]. In particular, the effect of circuit state on the power cost of an instruction stream is more marked in the case of this DSP processor. In addition, the processor has special architectural features that allow dual-memory accesses and packing of instructions into pairs. The energy reduction possible through the use of these features is studied. The on-chi...",
            "group": 1305,
            "name": "10.1.1.53.7436",
            "keyword": "",
            "title": "Power Analysis and Minimization Techniques for Embedded DSP Software"
        },
        {
            "abstract": "General purpose parallel processing machines are increasingly being used to speed up a variety of VLSI CAD applications. This paper addresses logic simulation on parallel machines by exploiting the concurrency in the circuit being simulated (called  data parallelism) as opposed to exploiting parallelism inherent in the simulation algorithm itself (called functional parallelism). The most crucial step in obtaining the maximum parallelism using data parallelism is the partitioning of circuit elements. We introduce a cost function which tries to model the simulation of a logic circuit in a parallel environment. The cost function tries to estimate the parallel run time for logic simulation given the processor assignment and the underlying multiprocessor architecture. We then present different heuristic algorithms to partition the circuit and evaluate the efficiency of these algorithms using the proposed cost function. Partitioning algorithms for both event-driven and compiled code simulati...",
            "group": 1306,
            "name": "10.1.1.53.7593",
            "keyword": "",
            "title": "Efficient Circuit Partitioning Algorithms For Parallel Logic Simulation"
        },
        {
            "abstract": "This paper presents two heuristics for automatic hardware/software partitioning of system level specifications. Partitioning is performed at the granularity of loops, subprograms, and processes with the objective of performance optimization with a limited hardware and software cost. We define the metric values for partitioning and develop a cost function that guides partitioning towards the desired objective. We consider minimization of communication cost and improvement of the overall parallelism as essential criteria during partitioning. The two heuristics for hardware/software partitioning, formulated as a graph partitioning problem, are presented: one based on simulated annealing and the other on tabu search. Results of extensive experiments, including real life examples, show the clear superiority of the tabu search based algorithm. This work has benn partially supported by the Swedish National Board for Industrial and Technical Development (NUTEK).  - 1 -  1. Introduction  New t...",
            "group": 1307,
            "name": "10.1.1.53.7629",
            "keyword": "",
            "title": "Performance Guided System Level Hardware/Software Partitioning with Iterative Improvement Heuristics"
        },
        {
            "abstract": "We report on GADGET, a new software test generation system that uses combinatorial optimization to obtain condition/decision coverage of C/C++ programs. The GADGET system is fully automatic and supports all C/C++ language constructs. This allows us to generate tests for programs more complex than those previously reported in the literature. We address a number of issues that are encountered when automatically generating tests for complex software systems. These issues have not been discussed in earlier work on test-data generation, which concentrates on small programs (most often single functions) written in restricted programming languages. 1 Dynamic test data generation  In this paper, we introduce the GADGET system, which uses a test data generation paradigm commonly known as dynamic test data generation. Dynamic test data generation was originally proposed by [Miller and Spooner, 1976] and then investigated further with the TESTGEN system of [Korel, 1990, Korel, 1996], the QUEST/Ad...",
            "group": 1308,
            "name": "10.1.1.53.8418",
            "keyword": "",
            "title": "Automated Software Test Data Generation for Complex Programs"
        },
        {
            "abstract": ". Many systems in chemical engineering are difficult to optimize using gradient-based algorithms. These include process models with multimodalobjective functions and discontinuities. Herein, a stochastic algorithm is applied for the optimal design of a fermentation process, to determine multiphase equilibria, for the optimal control of a penicillin reactor, for the optimal control of a non-differentiable system, and for the optimization of a catalyst blend in a tubular reactor. The advantages of the algorithm for the efficient and reliable location of global optima are examined. The properties of these algorithms, as applied to chemical processes, are considered, with emphasis on the ease of handling constraints and the ease of implementation and interpretation of results. For the five processes, the efficiency of computation is improved compared with selected stochastic and deterministic algorithms. Results closer to the global optimum are reported for the optimal control of the penic...",
            "group": 1309,
            "name": "10.1.1.53.8706",
            "keyword": "",
            "title": "Global Optimization of Chemical Processes using Stochastic Algorithms"
        },
        {
            "abstract": "This paper presents a new approach to the estimation of two-dimensional motion vector fields from time-varying images. The approach is stochastic, both in its formulation and in the solution method. The formulation involves the specification of a deterministic structural model, along with stochastic observation and motion field models. Two motion models are proposed: a globally smooth model based on vector Markov random fields and a piecewise smooth model derived from coupled vector-binary Markov random fields. Two estimation criteria are studied. In the Maximum A Posteriori Probability (MAP) estimation the a posteriori probability of motion given data is maximized, while in the Minimum Expected Cost (MEC) estimation the expectation of a certain cost function is minimized. The MAP estimation is performed via simulated annealing, while the MEC algorithm performs iteration-wise averaging. Both algorithms generate sample fields by means of stochastic relaxation implemented via the Gibbs s...",
            "group": 1310,
            "name": "10.1.1.53.8756",
            "keyword": "",
            "title": "Bayesian Estimation Of Motion Vector Fields"
        },
        {
            "abstract": "This paper gives an overview on learning and representation of discrete-time, discrete-space dynamical systems in discretetime, continuous-space recurrent neural networks. We limit our discussion to dynamical systems (recurrent neural networks) which can be represented as finite-state machines (e.g. discrete event systems [53]). In particular, we discuss how a symbolic representation of the learned states and dynamics can be extracted from trained neural networks, and how (partially) known deterministic finitestate automata (DFAs) can be encoded in recurrent networks. While the DFAs that can be learned exactly with recurrent neural networks are generally small (on the order of 20 states), there exist subclasses of DFAs with on the order of 1000 states that can be learned by small recurrent networks. However, recent work in natural language processing implies that recurrent networks can possibly learn larger state systems [35].  I. Introduction  Answering the questions \"What are the rul...",
            "group": 1311,
            "name": "10.1.1.53.9380",
            "keyword": "",
            "title": "Learning, Representation, and Synthesis of Discrete Dynamical Systems in Continuous Recurrent Neural Networks"
        },
        {
            "abstract": "In this paper we explore the reuse of components of known good schedules in new scheduling problems. This involves accumulating a case-base of good quality schedules, retrieving a case (or cases) similar to a new scheduling problem and building a new schedule from components of the retrieved cases. We start by introducing the components of Case-Based Reasoning (CBR) and we describe a CBR solution to a Travelling Salesman Problem in order to illustrate the use of CBR in optimisation problems. Two CBR solutions to a single machine scheduling problem with sequence dependent setup times are described. These are evaluated by comparing them with two more conventional alternative techniques -- simulated annealing and myopic search. Both CBR techniques are shown to provide good quality solutions quickly.  2  Case-Based Reasoning in Scheduling: Reusing Solution Components  1 Introduction  There is a lot of optimism at the moment about the usefulness of case-based reasoning (CBR) in the develop...",
            "group": 1312,
            "name": "10.1.1.53.9709",
            "keyword": "",
            "title": "Case-Based Reasoning in Scheduling: Reusing Solution Components"
        },
        {
            "abstract": "An important area of research in computational biochemistry is the design of molecules for specific applications. The design of these molecules, which depends on the accurate determination of their three-dimensional structure, can be formulated as a global optimization problem. In this study, we present results from the application of a new conformation searching method based on direct search methods. We compare these results to some earlier results using genetic algorithms and simulated annealing.  ",
            "group": 1313,
            "name": "10.1.1.54.1152",
            "keyword": "global optimizationmolecular conformationnonlinear programming. Published in Journal of Computational ChemistryVol. 15No. 6627-632",
            "title": "On the Use of Direct Search Methods for the Molecular Conformation Problem"
        },
        {
            "abstract": ": Simulated annealing is applied to the synthesis of arrays in order to reduce the peaks of side lobes by acting on the elements' positions and weight coefficients. In the case considered, the number of array elements and the spatial aperture of an unequally spaced array are a priori fixed. Thanks to the high flexibility of simulated annealing, the results obtained for a 25-element array over an aperture of 50l improve those reported in the literature. EDICS NUMBER: SP 1.3 CORRESPONDING AUTHOR:  Dr. Vittorio Murino DIBE, Dept. of Biophysical and Electronic Engineering University of Genoa Via Opera Pia 11A, I-16145 Genova, Italy tel: + 39 10 3532 187, fax: + 39 10 3532 134 e-mail: dafne@dibe.unige.it  1  Synthesis of Unequally Spaced Arrays by Simulated Annealing  Vittorio Murino, Member, IEEE, Andrea Trucco, Student Member, IEEE,  and Carlo S. Regazzoni, Member, IEEE  Abstract - Simulated annealing is applied to the synthesis of arrays in order to reduce the peaks of side lobes by act...",
            "group": 1314,
            "name": "10.1.1.54.2219",
            "keyword": "Vittorio MurinoMemberIEEEAndrea TruccoStudent MemberIEEE",
            "title": "Synthesis of Unequally Spaced Arrays by Simulated Annealing"
        },
        {
            "abstract": ". This paper describes work on two different aspects of the application of genetic algorithms to component design. Namely structural design optimisation and the evolution of free-form 3D shapes. On the first aspect, a thorough comparison of ten different search techniques applied to a wing-box design optimisation problem is described. The techniques used vary from deterministic gradient descent to stochastic Simulated Annealing (SA) and Genetic Algorithms (GAs). The stochastic techniques produced as good solutions as the best found by the deterministic techniques. However, only the stochastic techniques consistently produced very good solutions every run. Significantly, only a distributed genetic algorithm (DGA) and hybrid methods (SA with gradient descent, DGA with gradient descent) had a reliable fast decent to good regions of solution space. On the free-form 3D shape aspect, an interactive systems for exploring the evolution of 3D shapes is described. An important element of the sys...",
            "group": 1315,
            "name": "10.1.1.54.2310",
            "keyword": "",
            "title": "Two Applications of Genetic Algorithms to Component Design"
        },
        {
            "abstract": "The evolution of RNA molecules in replication assays, viroids and RNA viruses can be viewed as an adaptation process on a 'fitness' landscape. The dynamics of evolution is hence tightly linked to the structure of the underlying landscape. Global features of landscapes can be described by statistical measures like number of optima, lengths of walks, and correlation functions. The evolution of a quasispecies on such landscapes exhibits three dynamical regimes depending on the replication fidelity: Above the \"localization threshold\" the population is centered around a (local) optimum. Between localization and \"dispersion threshold\" the population is still centered around a consensus sequence, which, however, changes in time. For very large mutation rates the population spreads in sequence space like a gas. The critical mutation rates separating the three domains depend strongly on characteristics properties of the fitness landscapes. Statistical characteristics of RNA landscapes are acces...",
            "group": 1316,
            "name": "10.1.1.54.2499",
            "keyword": "abc",
            "title": "Landscapes - Complex Optimization Problems and Biopolymer Structures"
        },
        {
            "abstract": "A method for histogram partitioning based on the maximization of a criterium function is proposed. The choice of the criterium can be made using a priori information to mimic human performance as far as possible. ",
            "group": 1317,
            "name": "10.1.1.54.2538",
            "keyword": "analysis",
            "title": "Optimal Histogram Partitioning Using a Simulated Annealing Technique."
        },
        {
            "abstract": "This paper discusses the modeling and visualization of the synaptic interaction between two mammalian neurons. The olivary neuron makes a climbing fiber synapse on the cerebellar Purkinje cell. This is a very strong synapse, which evokes a typical complex response involving the opening of calcium channels in the Purkinje cell dendrite. Both neurons were modeled by realistic compartmental models (about 500 compartments for the olivary neuron and 4500 for the Purkinje cell), with voltage dependent ionic channels in both the soma and dendrites. This included the modeling of submembrane calcium concentrations. The complexity of the models, required that we simulate them on separate processors; an Intel Touchstone Delta (a parallel supercomputer) performed all computations for the Purkinje cell simulation, and a unix workstation for the olivary neuron. The synaptic interaction was carried as a boolean signal over a T3 network communication line, between the Delta at Caltech and the unix wor...",
            "group": 1318,
            "name": "10.1.1.54.2668",
            "keyword": "",
            "title": "Realistic Modeling of Brain Structures with Remote Interaction Between Simulations of an Inferior Olivary Neuron and a Cerebellar Purkinje Cell"
        },
        {
            "abstract": "Genetic algorithms are general purpose optimization and search techniques based on biological principles, that maneuver through complex spaces in a near optimal way. This paper addresses an application of genetic algorithms to the mapping problem: the placement of communicating processes on a parallel distributed memory architecture. A population of individuals representing possible solutions is maintained. Search proceeds by applying genetic operators on individuals of the population. Standard genetic algorithms with large populations suffer from lack of efficiency (quite long execution time). A massively parallel genetic algorithm is proposed, an implementation on a reconfigurable transputer network and results of various benchmarks are given. A comparative analysis of our approach with hill-climbing and simulated annealing algorithms is also presented. The experimental measures show encouraging results. 1. Introduction To execute an application on a parallel machine, the mere transl...",
            "group": 1319,
            "name": "10.1.1.54.2913",
            "keyword": "",
            "title": "A New Approach for the Mapping Problem: A Parallel Genetic Algorithm"
        },
        {
            "abstract": "All the previous Kernighan-Lin (KL) based circuit partitioning algorithms employ the locking mechanism, which enforces each cell to be moved exactly once per pass. In this paper, we propose two novel approaches for multiway circuit partitioning to overcome this limitation. The proposed approaches are based on our claim that the performance of a KL--based partitioning algorithm gets better by allowing each cell to be moved more than once per pass. The first approach uses the locking mechanism in a relaxed manner. It allows multiple moves for each cell in a pass by introducing the phase concept so that each pass can contain more than one phase, and each cell has a chance to be moved only once in each phase. The second approach does not use the locking mechanism at all. It introduces the mobility concept, which allows a cell to be moved as many times per pass as possible based on its mobility values. Each approach leads to a KL based generic algorithm whose parameters can be set in such a...",
            "group": 1320,
            "name": "10.1.1.54.3367",
            "keyword": "",
            "title": "Two Novel Multiway Circuit Partitioning Algorithms Using Relaxed Locking"
        },
        {
            "abstract": "Any nonassociative reinforcement learning algorithm can be viewed as a method for performing function optimization through (possibly noise-corrupted) sampling of function values. We describe the results of simulations in which the optima of several deterministic functions studied by Ackley (1987) were sought using variants of REINFORCE algorithms (Williams, 1987; 1988). Some of the algorithms used here incorporated additional heuristic features resembling certain aspects of some of the algorithms used in Ackley's studies. Differing levels of performance were achieved by the various algorithms investigated, but a number of them performed at a level comparable to the best found in Ackley's studies on a number of the tasks, in spite of their simplicity. One of these variants, called REINFORCE/MENT, represents a novel but principled approach to reinforcement learning in nontrivial networks which incorporates an entropy maximization strategy. This was found to perform especially well on more hierarchically organized tasks.",
            "group": 1321,
            "name": "10.1.1.54.3433",
            "keyword": "",
            "title": "Function Optimization Using Connectionist Reinforcement Learning Algorithms"
        },
        {
            "abstract": "FPGA-based ASIC development systems have become important tools in contemporary ASIC design. Existing systems exhibit low per-FPGA gate utilization (10 to 20 percent) due to limited inter-chip communication. Attempts at overcoming this limitation through the use of high dimensional interconnection topologies have met with limited success. This paper focuses on the prototype hardware and software interfaces that have been developed for an FPGA-based ASIC emulation system based on a new technique for overcoming inter-chip communication limitations. This technique, referred to as virtual wires, intelligently multiplexes each physical FPGA wire among a number of logical wires. The Virtual Wires Emulation System exhibits high FPGA gate utilization while achieving system speeds comparable to existing logic emulators. A two-dimensional mesh interconnection topology of FPGAs is used to eliminate the cost of signal switching elements and to facilitate scalability. A system capable of emulating ...",
            "group": 1322,
            "name": "10.1.1.54.3673",
            "keyword": "FPGAlogic emulationprototypingmesh topologystatic routingvirtual wires",
            "title": "The Virtual Wires Emulation System: A Gate-Efficient ASIC Prototyping Environment"
        },
        {
            "abstract": "Simulated annealing has proven to be a good technique for solving hard combinatorial optimization problems. Some attempts at speeding up annealing algorithms have been based on shared memory multiprocessor systems. Also parallelizations for certain problems on distributed memory multiprocessor systems are known. In this paper, we present a problem independent general purpose parallel implementation of simulated annealing on large distributed memory message-passing multiprocessor systems. The sequential algorithm is studied and we give a classification of combinatorial optimization problems together with their neighborhood structures. Several parallelization approaches are examined considering their suitability for problems of the various classes. For typical representatives of the different classes good parallel simulated annealing implementations are presented. We describe in detail several 'tricks' increasing efficiency and attained solution quality of the different parallel implemen...",
            "group": 1323,
            "name": "10.1.1.54.3712",
            "keyword": "network",
            "title": "Problem Independent Distributed Simulated Annealing and Its Applications"
        },
        {
            "abstract": "New placement techniques are presented which substantially improve the process of automatic layout generation of analog IC's. Extremely tight specifications can be enforced on high-performance analog circuits by using simultaneous placement and module optimization. An algorithmic approach to module generation provides alternative sets of modules optimized with respect to area and performance but equivalent in terms of parasitics and topology. The final module selection is performed during the placement phase, based on Simulated Annealing. The flexibility of the annealing algorithm has been significantly improved, thus making it possible to more efficiently exploit the tradeoffs between area, parasitics and matching. 1 Introduction  Layout design automation of analog IC's has seen considerable improvements in recent years despite a continuous increase of complexity and sophistication of analog and mixed-signal systems. On the one hand, denser and more advanced technologies have led to f...",
            "group": 1324,
            "name": "10.1.1.54.4169",
            "keyword": "",
            "title": "Simultaneous Placement and Module Optimization of Analog IC's"
        },
        {
            "abstract": "Terms or Journal Scope to attributes present a number of problems. This is due to the size of the domain of the attributes (for text, the domain may contain in the order of 20, 000 - 100, 000 elements (Lang, 1995)), and the number of values in each field (many supervised learning algorithms assume that an instance contains a single symbolic or numeric value for each attribute). Title Terms a agent an in interface investigation issues learn learning mail of that  Authors \"Peter Edwards\", \"Terry R. Payne\"  Abstract Terms a able adapt agent allow an and are assist autonomous change component construct context data develop different employ filter from has have here identify in increase interest interface internet ...  Publication Type journal  Publication Name \"Applied Artificial Intelligence\"  Editor \"Robert Trappl\"  Technical Scope advanced scientific  Journal Scope a about act address administration advances ai also and application applied article artificial as comparative concerns cult...",
            "group": 1325,
            "name": "10.1.1.54.4246",
            "keyword": "",
            "title": "Dimensionality Reduction for Agent-Based Learning"
        },
        {
            "abstract": "Contents 0.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 0.2 Calibration automatique d'une cam'era . . . . . . . . . . . . . . . . . 6 0.2.1 Introduction th'eorique . . . . . . . . . . . . . . . . . . . . . . 6 0.2.2 R'ealisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 0.2.3 Bilan du calibrage de la cam'era . . . . . . . . . . . . . . . . . 38 0.3 Calibration de deux robots . . . . . . . . . . . . . . . . . . . . . . . . 40 0.3.1 Position du probl`eme . . . . . . . . . . . . . . . . . . . . . . . 40 0.3.2 Principe de l'approche. . . . . . . . . . . . . . . . . . . . . . . 40 0.3.3 Localisation d'un objet. . . . . . . . . . . . . . . . . . . . . . 41 0.3.4 Prise en compte de la r'ealit'e physique . . . . . . . . . . . . . 41 0.4 Syst`eme de commande des deux robots . . . . . . . . . . . . . . . . . 45 0.4.1 Pr'esentation de KALI: . . . . . . . . . . . . . . . . . . . . . . 45 0.4.2 Pr'esentation de VxWorks: . . . . . . . . . .",
            "group": 1326,
            "name": "10.1.1.54.4264",
            "keyword": "",
            "title": "Planification Et Contr\u00f4le D'ex\u00e9cution D'op\u00e9rations De Manipulation De Pi`eces M'ecaniques Par Un Robot Mobile/manipulateur Dans Un Contexte De Maintenance."
        },
        {
            "abstract": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function, that is solving the problem of hypersurface reconstruction. From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. This paper considers the problems of an exact representation and, in more detail, of the approximation of linear and nonlinear mappings in terms of simpler functions of fewer variables. Kolmogorov's theorem concerning the representation of functions of several variables in terms of functions of one variable turns out to be almost irrelevant in the context of networks for learning. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GR...",
            "group": 1327,
            "name": "10.1.1.54.4527",
            "keyword": "",
            "title": "A Theory of Networks for Approximation and Learning"
        },
        {
            "abstract": ". Can stochastic search algorithms outperform existing deterministic heuristics for the NP-hard problem Number Partitioning if given a sufficient, but practically realizable amount of time? In a thorough empirical investigation using a straightforward implementation of one such algorithm, simulated annealing, Johnson et al. (Ref. 1) concluded tentatively that the answer is \"no.\" In this paper we show that the answer can be \"yes\" if attention is devoted to the issue of problem representation (encoding). We present results from empirical tests of several encodings of Number Partitioning with problem instances consisting of multiple-precision integers drawn from a uniform probability distribution. With these instances and with an appropriate choice of representation, stochastic and deterministic searches can---routinely and in a practical amount of time---find solutions several orders of magnitude better than those constructed by the best heuristic known (Ref. 2), which does not employ se...",
            "group": 1328,
            "name": "10.1.1.54.5375",
            "keyword": "",
            "title": "Easily Searched Encodings for Number Partitioning"
        },
        {
            "abstract": "In this paper we present a new algorithm for the k-  partitioning problem which achieves an improved solution quality compared to known heuristics. We apply the principle of so called \"helpful sets\", which has shown to be very efficient for graph bisection, to the direct  k-partitioning problem. The principle is extended in several ways. We introduce a new abstraction technique which shrinks the graph during runtime in a dynamic way leading to shorter computation times and improved solutions qualities. The use of stochastic methods provides further improvements in terms of solution quality. Additionally we present a parallel implementation of the new heuristic. The parallel algorithm delivers the same solution quality as the sequential one while providing reasonable parallel efficiency on moderately sized MIMD-systems. All results are verified by experiments for various graphs and processor numbers. 1 Introduction  In this paper we study graph partitioning as one of the fundamental pro...",
            "group": 1329,
            "name": "10.1.1.54.5741",
            "keyword": "",
            "title": "A Parallel Local-Search Algorithm for the k-Partitioning Problem"
        },
        {
            "abstract": "In this work, the following k-way  graph partitioning (GP) problem is considered: given an undirected weighted graph G(V; E),  partition the nodes of G into k parts of almost equal size such that the partition-cost (sum of the weights on edges with nodes in different parts) is minimized. Two simple and fast algorithms are proposed, namely, direct algorithm  Auction and iterative algorithm GreedyCycle.  In algorithm Auction, the idea of using auction and biddings is introduced using the master-workers paradigm. Algorithm GreedyCycle  is a greedy algorithm where the idea of cyclic node passing among parts during the iterative improvement stage is introduced. Cyclic node passing is a k-way generalization of the 2way node exchange found in the Kernighan-Lin approach. Experimental results show that, as compared to the existing algorithms, these algorithms are extremely fast, and they produce solutions of reasonable quality. 1 Introduction  Dividing a graph into sets of nodes with the aim of...",
            "group": 1330,
            "name": "10.1.1.54.5981",
            "keyword": "",
            "title": "Fast Graph Partitioning Algorithms"
        },
        {
            "abstract": "This paper presents a hardware-software partitioning algorithm that exploits a loop pipelining technique. The partitioning algorithm is based on iterative improvement. The algorithm tries to minimize hardware cost through hardware sharing and hardware implementation selection without violating given performance constraint. The proposed loop pipelining technique, which is an adaptation of a compiler optimization technique for instruction level parallelism, increases parallelism within a loop by transforming the structure of an input system description. By combining this technique with our partitioning algorithm, we can further reduce the hardware cost and/or improve the performance of the partitioned system. Experiments show about 19% performance improvement and 44% reduced hardware for a JPEG encoder design, compared to the results without loop pipelining.  1. Introduction  Mixed hardware and software implementation is common in the design of digital systems such as communication syste...",
            "group": 1331,
            "name": "10.1.1.54.6395",
            "keyword": "",
            "title": "Loop Pipelining in Hardware-Software Partitioning"
        },
        {
            "abstract": "In this paper we examine the problem of dynamic self-reconfiguration of a class of modular robotic systems referred to as metamorphic systems. A metamorphic robotic system is a collection of mechatronic modules, each of which has the ability to connect, disconnect, and climb over adjacent modules. We define a concept of distance between metamorphic robot configurations which satisfies the formal properties of a metric. This metric, called the configuration metric is then applied to the automatic self-reconfiguration of metamorphic systems from any initial to any final specified configuration. The technique of Simulated Annealing is used to drive the re-configuration process with the configuration metric as the cost function. The relative performance of simulated annealing with this cost function is shown to be superior then simulated annealing using other cost functions. 1 Introduction  A metamorphic robotic system [Ch94] is a collection of independently controlled mechatronic modules,...",
            "group": 1332,
            "name": "10.1.1.54.6499",
            "keyword": "",
            "title": "A Useful Metric For Modular Robot Motion Planning"
        },
        {
            "abstract": "Mean field annealing (MFA) algorithm, proposed for solving combinatorial optimization problems, combines the characteristics of neural networks and simulated annealing. Previous works on MFA resulted with successful mapping of the algorithm to some classic optimization problems such as traveling salesperson problem, scheduling problem, knapsack problem and graph partitioning problem. In this paper, MFA is formulated for the circuit partitioning problem using the so called net-cut model. Hence, the deficiencies of using the graph representation for electrical circuits are avoided. An efficient implementation scheme, which decreases the complexity of the proposed algorithm by asymptotical factors is also developed. Comparative performance analysis of the proposed algorithm with two well-known heuristics, simulated annealing and Kernighan-Lin, indicates that MFA is a successful alternative heuristic for the circuit partitioning problem. Keywords : Mean field annealing, circuit partitionin...",
            "group": 1333,
            "name": "10.1.1.54.7438",
            "keyword": "Mean field annealingcircuit partitioningnet-cut model",
            "title": "Circuit Partitioning Using Mean Field Annealing"
        },
        {
            "abstract": "OF THE THESIS Large Scale Circuit Partitioning With Loose/Stable Net Removal And Signal Flow Based Hierarchical Clustering by Sung Kyu Lim Master of Science in Computer Science University of California, Los Angeles, 1997 Professor Jason Cong, Chair In this thesis, we present an efficient large-scale circuit partitioning algorithm called FMLSRb that combines a signal flow based Maximum Fanout Free Subgraph (MFFS) clustering and iterative improvement based Loose and Stable net Removal (LSR) partitioning. The MFFS based clustering approach [CLL  +  97] generalizes the existing MFFC decomposition method [CD93] from combinational to sequential circuits, which enables us to handle cycles in circuits naturally and make hierarchical clustering and declustering possible. We also study the properties of the nets that straddle the cutline carefully and introduce the concept of loose and stable nets. The LSR based partitioning approach does not require lookahead capability such as LA3 [Kri84] or l...",
            "group": 1334,
            "name": "10.1.1.54.7616",
            "keyword": "Contents",
            "title": "Large Scale Circuit Partitioning with Loose/Stable Net Removal and Signal Flow Based Hierarchical Clustering"
        },
        {
            "abstract": "In this paper we describe a method for designing adaptive procedures for segmentation of anatomical structures in 3D medical data sets. With our method, the user first manually traces one or more 2D contours of a structure of interest on parallel planes arbitrarily cutting the data set. Such contours are then used as training examples to design a contour detector by means of a genetic algorithm. Finally, the contour detector is used to segment the structure of interest across the whole dataset, using a contour-tracking strategy also designed by the genetic algorithm. We report results obtained on a software-generated phantom and on actual tomographic images. 1 Introduction  Segmentation is a central issue in most computer vision and imaging systems and a fundamental pre-processing step required by most visualization systems [1, 2, 3]. In medical practice, many clinical procedures, from diagnosis to radiation treatment-planning, can benefit from tools that help speed up and ease the seg...",
            "group": 1335,
            "name": "10.1.1.54.8198",
            "keyword": "",
            "title": "Optimized contour-based segmentation of 3D medical data using genetic algorithms"
        },
        {
            "abstract": "To investigate the relations between structure and function in both artificial and natural neural networks, we present a series of simulations and analyses with modular neural networks. We suggest a number of design principles in the form of explicit ways in which neural modules can cooperate in recognition tasks. These results may supplement recent accounts of the relation between structure and function in the brain. The networks used consist out of several modules, standard subnetworks that serve as higher-order units with a distinct structure and function. The simulations rely on a particular network module called CALM (Murre, Phaf, and Wolters, 1989, 1992). This module, developed mainly for unsupervised categorization and learning, is able to adjust its local learning dynamics. The way in which modules are interconnected is an important determinant of the learning and categorization behaviour of the network as a whole. Based on arguments derived from neuroscience, psychology, compu...",
            "group": 1336,
            "name": "10.1.1.54.8248",
            "keyword": "",
            "title": "The Design and Evolution of Modular Neural Network Architectures"
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks -- specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability, and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars, and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-t...",
            "group": 1337,
            "name": "10.1.1.54.8435",
            "keyword": "recurrent neural networksnatural language processing",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "We present a new inverse, interactive approach to acoustic design that applies optimization techniques to an acoustic simulation system. The work is motivated by the challenges inherent in achieving desirable acoustic behavior in 3D environments, and it builds on previous work in computer graphics dealing with the analogous problem in lighting design. The user interactively indicates a range of acceptable material and geometric modifications for an auditorium or similar space, and specifies acoustic goals by choosing target values for a set of acoustic measures. Given this set of goals and constraints, the system performs an optimization of surface material and geometric parameters using a combination of simulated annealing and steepest descent techniques. Visualization tools extract and present the simulated sound field data for points sampled in space and time. The user then manipulates the visualizations to create an intuitive expression of acoustic design goals. We demonstrate an i...",
            "group": 1338,
            "name": "10.1.1.54.8498",
            "keyword": "Acoustic modelingspatialized soundoptimizationvisualizationinteractive designsimulated annealingbeam",
            "title": "Audioptimization: Goal-Based Acoustic Design"
        },
        {
            "abstract": "We propose parametric geons as a coarse description of object components for qualitative object recognition. Parametric geons are seven qualitative shape types defined by parameterized equations which control the size and degree of tapering and bending. Model recovery is performed by a procedure of model fitting and selection by minimizing an objective function measuring the similarities in both size and shape between models and objects. Multiple view data, parametric model constraints and global optimization are employed to obtain unique models and to compensate for noise and minor changes in object shape. This approach has been studied in experiments with both synthetic 3D data and actual rangefinder data of perfect and imperfect geon-like objects. ",
            "group": 1339,
            "name": "10.1.1.54.8572",
            "keyword": "computer vision3D object representationobject descriptionmulti-view integrationglobal optimizationvolumetric primitivesparametric modelsgeons",
            "title": "3-D Object Representation Using Parametric Geons"
        },
        {
            "abstract": ". Simulated annealing --- moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions --- has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios...",
            "group": 1340,
            "name": "10.1.1.54.9006",
            "keyword": "",
            "title": "Annealed Importance Sampling"
        },
        {
            "abstract": ". Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidian space. The quality of a data embedding is measured by a cost function called stress which compares proximity values with Euclidian distances of the respective points. We present a novel deterministic annealing  algorithm to efficiently determine embedding coordinates for this continuous optimization problem. Experimental results demonstrate the superiority of the optimization technique compared to conventional gradient descent methods. Furthermore, we propose a transformation of dissimilarities to reduce the mismatch between a high-dimensional data space and a low-dimensional embedding space.  1 1 Introduction  Visualizing experimental data arises as a fundamental pattern recognition problem for exploratory data analysis in empirical sciences. Data are quite frequently represented as vectors in an often high-dimensional Euclidian vector space. Visual...",
            "group": 1341,
            "name": "10.1.1.54.9822",
            "keyword": "",
            "title": "Multidimensional Scaling by Deterministic Annealing"
        },
        {
            "abstract": "this paper is to minimize the total processing time (flow time) F (\\Pi) =",
            "group": 1342,
            "name": "10.1.1.54.9989",
            "keyword": "",
            "title": "Applications of Modern Heuristic Search Methods to Continuous Flow-Shop Scheduling Problems"
        },
        {
            "abstract": "Hill-climbing, simulated annealing and genetic algorithms are general search techniques that can be applied to most combinatorial optimization problems. In this paper, the three algorithms are used to solve the mapping problem: optimal static allocation of communicating processes on distributed memory parallel architectures. Each algorithm is independently evaluated and optimized according to its parameters. The parallelization of the algorithms is also considered. As an example, a massively parallel genetic algorithm is proposed for the problem, and results of its implementation on a 128-processor Supernode (reconfigurable network of transputers) are given. A comparative study of the algorithms is then carried out. The criteria of performances considered are the quality of the solutions obtained and the amount of search time used for several benchmarks. A hybrid approach consisting in a combination of genetic algorithms and hill-climbing is also proposed and evaluated. 1. Introduction...",
            "group": 1343,
            "name": "10.1.1.55.144",
            "keyword": "",
            "title": "General Heuristics for the Mapping Problem"
        },
        {
            "abstract": "In this paper we consider the problem of finding the efficient frontier associated with the standard mean-variance portfolio optimisation model. We extend the standard model to include cardinality constraints that limit a portfolio to have a specified number of assets, and to impose limits on the proportion of the portfolio held in a given asset (if any of the asset is held). We illustrate the differences that arise in the shape of this efficient frontier when such constraints are present. We present three heuristic algorithms based upon genetic algorithms, tabu search and simulated annealing for finding the cardinality constrained efficient frontier. Computational results are presented for five data sets involving up to 225 assets. Keywords: portfolio optimisation, efficient frontier  1 1. INTRODUCTION Each of the larger fund management companies in the UK/US are responsible for the investment of several billion pounds/dollars. This money is invested on behalf of pension funds, unit ...",
            "group": 1344,
            "name": "10.1.1.55.505",
            "keyword": "portfolio optimisationefficient frontier 1",
            "title": "Heuristics for Cardinality Constrained Portfolio Optimisation"
        },
        {
            "abstract": ". It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \"Hybrid Monte Carlo\" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \"free energy\" differences, it should also be possible to compare the merits of different network architectures. The work described here should also be applicable to a wide variety of st...",
            "group": 1345,
            "name": "10.1.1.55.524",
            "keyword": "",
            "title": "Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method"
        },
        {
            "abstract": "We report on GADGET, a new software test generation system that uses combinatorial optimization to obtain condition/decision coverage of C/C++ programs. The GADGET system is fully automatic and supports all C/C++ language constructs. This allows us to generate tests for larger programs than those previously reported in the literature. We address a number of issues that are encountered when automatically generating tests for larger software systems. These issues have not been discussed in earlier work on testdata generation, which concentrated on small programs written in restricted programming languages. Keywords: Automatic test data generation, software testing, software code coverage, combinatorial optimization, genetic algorithms  1 Introduction  An important aspect of software testing involves judging how well a series of test inputs tests a piece of code. Usually the goal is to uncover as many faults as possible with a potent set of tests, since a test series that has the potenti...",
            "group": 1346,
            "name": "10.1.1.55.922",
            "keyword": "Automatic test data generationsoftware testingsoftware code coveragecombinatorial optimizationgenetic",
            "title": "Opportunism and Diversity in Automated Software Test Data Generation"
        },
        {
            "abstract": ". We present a new approach to shape-based segmentation and tracking of multiple, deformable anatomical structures in cardiac MR images. We propose to use an energy-minimizing geometrically deformable template (GDT) which can deform into similar shapes under the influence of image forces. The degree of deformation of the template from its equilibrium shape is measured by a penalty function associated with mapping between the two shapes. In 2D, this term corresponds to the bending energy of an idealized thin-plate of metal. By minimizing this term along with the image energy terms of the classic deformable model, the deformable template is attracted towards objects in the image whose shape is similar to its equilibrium shape. This framework allows for the simultaneous segmentation of multiple deformable objects using intra- as well as inter-shape information. The energy minimization problem of the deformable template is formulated in a Bayesian framework and solved using relaxation tech...",
            "group": 1347,
            "name": "10.1.1.55.932",
            "keyword": "",
            "title": "Geometrically Deformable Templates for Shape-based Segmentation and Tracking in Cardiac MR Images"
        },
        {
            "abstract": "Domain decomposition is an important step for parallel scientific applications, in particular finite element analyses. A good decomposition will minimize both the time spent on local computation and on interprocessor communication. It is often the case that these two goals cannot be satisfied simultaneously. In this paper, we use analytical and experimental results to illustrate the importance of considering the target architecture as well as the application when determining which factor to emphasize in a decomposition method. In particular, we derive a parameter j  0 that provides some guidelines as to which goal should be given primary focus. Our results yield two interesting facts: (1) allowing some load imbalance can provide some reduction in communication and total execution time and (2) as larger numbers of processors are applied to a problem, larger amounts of load imbalance are beneficial. Corresponding author:  Eric J. Schwabe Department of ECE Northwestern University 2145 She...",
            "group": 1348,
            "name": "10.1.1.55.1679",
            "keyword": "",
            "title": "Balancing Load versus Decreasing Communication: Parameterizing the Tradeoff"
        },
        {
            "abstract": "A cyber agent is any program, machine or person engaged in computer-enabled work. Thus, cyber agents can vary considerably in complexity and intelligence. Can they, despite their variety, be organized to collaborate effectively ? Both empirical evidence and theory suggest that they can. Moreover, there seem to be simple rules for designing problem-solving organizations in which collaboration among cyber agents is automatic and scale-effective (adding agents tends to improve solution-quality; adding computers tends to improve solution-speed). This paper develops some of these rules.  1. INTRODUCTION  Computer networks make it possible to interconnect and therefore, organize, large numbers of distributed cyber agents, varying in type from simple programs to skilled humans. Our goal is to develop a class of organizations in which such agents can collaborate easily and effectively. More specifically, our goal is to develop methods for routinely solving arbitrary instances of the following ...",
            "group": 1349,
            "name": "10.1.1.55.1795",
            "keyword": "",
            "title": "Autonomous Cyber Agents: Rules For Collaboration"
        },
        {
            "abstract": "This paper briefly presents a new discrimination network algorithm called Gator that is a generalization of the widely known Rete and TREAT algorithms. Gator is designed as a target for a discrimination network optimizer. Several strategies for optimizing Gator networks were implemented, and then tested and compared using randomly generated simulated rule structures as input. The optimization methods evaluated include dynamic programming (akin to traditional relation database query optimization), simulated annealing, iterative improvement (hill climbing from multiple random starting points), and a blend of simulated annealing and iterative improvement called two-phase optimization. Regardless of the optimization algorithm used, simulation indicates that optimized Gator networks can be expected to substantially outperform Rete and TREAT networks in most cases. The results indicate that two-phase optimization is the preferred optimization method, followed closely by iterative improvement...",
            "group": 1350,
            "name": "10.1.1.55.2325",
            "keyword": "",
            "title": "Optimizing Gator Networks for Rule Condition Testing"
        },
        {
            "abstract": "Introduction  The spatial arrangement of leaves in a plant canopy is governed by a number of factors such as radiative and diffusive couplings and architectural support. Concentrating on how the plant's canopy structure affects its carbon balance through its efficacy in trapping radiation, the aim of this research is to investigate nearoptimal arrangements of leaves using an optimizing simulation technique. A Monte Carlo simulation method called simulated annealing is being developed to perform the optimization. Leaves are modelled as line segments which join points on a regular two-dimensional lattice. A net carbon balance is computed for a given configuration of leaves as the sun transcends a number of sky sectors. To improve a given leaf configuration, a    Poster presented at ESA95 -- Ecological Society of Australia Open Forum & Symposium, Hobart, 1995.  small change is made to it which is then considered to be an improvement based on a ",
            "group": 1351,
            "name": "10.1.1.55.2413",
            "keyword": "",
            "title": "Simulated Annealing of Two-Dimensional Plant Canopies"
        },
        {
            "abstract": "This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees. 1. Introduction  Current data collection technology provides a unique challenge and opportunity for automated machine learning techniques. The advent of major scientific projects such as the Human Genome Project, the Hubble Space Telescope, and the human brain mappi...",
            "group": 1352,
            "name": "10.1.1.55.2611",
            "keyword": "",
            "title": "A System for Induction of Oblique Decision Trees"
        },
        {
            "abstract": ":  The large number of system components and their complex interactions in a parallel I/O system, together with dynamically changing I/O patterns in scientific applications, impose a great challenge in selecting optimal I/O plans for an anticipated I/O workload in a target execution environment. Previous research has shown that a model-based approach that uses a performance model of the parallel I/O system to predict the performance of the parallel I/O system for a given I/O plan, coupled with an effective search algorithm, i.e., simulated annealing, can identify a high quality I/O plan automatically. However, to be truly successful, such automatic strategies must not only be capable of selecting  optimal I/O plans for a parallel I/O system, but also be able to select them  quickly. In this paper, we study the cost of optimization when using the modelbased approach. We identify the major performance factors that affect the optimization time, and present techniques used to speed up the ...",
            "group": 1353,
            "name": "10.1.1.55.2651",
            "keyword": "",
            "title": "Speeding Up Automatic Parallel I/O Performance Optimization In Panda"
        },
        {
            "abstract": "This paper describes a new approach to the scheduling problem in high-level synthesis that meets timing constraints while attempting to minimize hardware resource costs. The approach is based on a modified control/data flow graph (CDFG) representation called SALSA. SALSA provides a simple move set that allows alternative schedules to be quickly explored while maintaining timing constraints. It is shown that this move set is complete in that any legal schedule can be reached using some sequence of move applications. In addition, SALSA provides support for scheduling with conditionals, loops, and subroutines. Scheduling with SALSA is performed in two steps. First, an initial schedule that meets timing constraints is generated using a constraint solution algorithm adapted from layout compaction. Second, the schedule is improved using the SALSA move set under control of a simulated annealing algorithm. Results show the scheduler's ability to find good schedules which meet timing constraint...",
            "group": 1354,
            "name": "10.1.1.55.2693",
            "keyword": "",
            "title": "SALSA: A New Approach to Scheduling with Timing Constraints"
        },
        {
            "abstract": "This dissertation examines a number of geometric interconnection, partitioning, and placement problems arising in the field of VLSI physical design automation. In particular, many of the results concern the geometric Steiner tree problem: given a set of terminals in the plane, find a minimum-length interconnection of those terminals according to some geometric distance metric. Two new algorithms are introduced that compute optimal rectilinear Steiner trees. Both are provably faster than any previous algorithm for instances small enough to solve in practice, and both are also fast in practice. The first algorithm is a dynamic programming algorithm based on decomposing a rectilinear Steiner tree into full trees. A full tree is a Steiner tree in which every terminal is a leaf. Its time complexity is O(n3^n), where n is the number of terminals. The second algorithm modifies the first by the use of full-set screening, which is a process by which some candidate full trees are eliminated f...",
            "group": 1355,
            "name": "10.1.1.55.3775",
            "keyword": "",
            "title": "Geometric Interconnection and Placement Algorithms"
        },
        {
            "abstract": "All the previous Kernighan-Lin based (KLbased) circuit partitioning algorithms employ the locking mechanism, which enforces each cell to move exactly once per pass. In this paper, we propose two novel approaches for multiway circuit partitioning to overcome this limitation. Our approaches allow each cell to move more than once. Our first approach still uses the locking mechanism but in a relaxed way. It introduces the phase concept such that each pass can include more than one phase and a phase can include at most one move of each cell. Our second approach does not use the locking mechanism at all. It introduces the mobility concept such that each cell can move as freely as allowed by its mobility. Each approach leads to KL-based generic algorithms whose parameters can be set to obtain algorithms with different performance characteristics. We generated three versions of each generic algorithm and evaluated them on a subset of common benchmark circuits in comparison with Sanchis' algori...",
            "group": 1356,
            "name": "10.1.1.55.4024",
            "keyword": "",
            "title": "Two Novel Multiway Circuit Partitioning Algorithms Using Relaxed Locking"
        },
        {
            "abstract": "A scheduling process that maintains, repairs and configures a space shuttle orbiter must satisfy temporal, resource and configuration constraints. The GPSS, which stands for ground processing scheduling system, has been successfully used to schedule tasks that prepare an orbiter for its next launch. The objective of this project is to enhance and to improve the GPSS. The GPSS uses an iterative repair method that starts with a rough schedule, which satisfies only the temporal constraints. Then it repairs the resource and configuration constraint violations one by one until no more violations exist or no further improvements can be made. The conflict resolution, which repairs all the resource and the configuration constraint violations, forms the bulk of the cognitive and computational complexity of the system. We started with a study of the solution space of the GPSS which will help us to develop algorithms and strategies to improve the efficiency of the deconfliction process. To our kn...",
            "group": 1357,
            "name": "10.1.1.55.4152",
            "keyword": "",
            "title": "Enhancement Of The Ground Processing Scheduling System (gpss)"
        },
        {
            "abstract": "Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods. To Appear in:  IEEE Transactions on Neural Networks January  The Ohio State University January 17, 1996 1  ...",
            "group": 1358,
            "name": "10.1.1.55.4191",
            "keyword": "",
            "title": "An Evolutionary Algorithm that Constructs Recurrent Neural Networks"
        },
        {
            "abstract": "In this paper, a systematic optimization approach for clustering proximity or similarity data is developed. Starting from fundamental invariance and robustness properties, a set of axioms is proposed and discussed to distinguish different cluster compactness and separation criteria. The approach covers the case of sparse proximity matrices, and is extended to nested partitionings for hierarchical data clustering. To solve the associated optimization problems, a rigorous mathematical framework for deterministic annealing and mean--field approximation is presented. Efficient optimization heuristics are derived in a canonical way, which also clarifies the relation to stochastic optimization by Gibbs sampling. Similarity-based clustering techniques have a broad range of possible applications in computer vision, pattern recognition, and data analysis. As a major practical application we present a novel approach to the problem of unsupervised texture segmentation, which relies on statistical...",
            "group": 1359,
            "name": "10.1.1.55.4384",
            "keyword": "clusteringproximity datasimilaritydeterministic annealingtexture segmentationdocument retrieval",
            "title": "A Theory of Proximity Based Clustering: Structure Detection by Optimization"
        },
        {
            "abstract": ". We propose in this paper a novel way of looking at local search algorithms for combinatorial optimization problems which better suits constraint programming by performing branch-and-bound search at their core. We concentrate on neighborhood exploration and show how the framework described yields a more efficient local search and opens the door to more elaborate neighborhoods. Numerical results are given in the context of the traveling salesman problem with time windows. This work on neighborhood exploration is part of ongoing research to develop constraint programming tabu search algorithms applied to routing problems.  Introduction  Local search methods in operations research (or) date back to over thirty years ago ([Lin65]). Applied to difficult combinatorial optimization problems, this heuristic approach yields high-quality solutions by iteratively considering small modifications (called local moves) of a good solution in the hope of finding a better one. Used within a strategy de...",
            "group": 1360,
            "name": "10.1.1.55.4464",
            "keyword": "",
            "title": "A View of Local Search in Constraint Programming"
        },
        {
            "abstract": " This dissertation presents the thesis that good and usable instruction sets can be automatically derived for a specified data path and benchmark set. This is achieved by a multistep process: generating execution traces for the benchmark programs, sampling these traces to form a large set of small code segments, optimally recompiling these segments using exhaustive search, and finding the cover of the new instructions generated that optimizes the performance metric. The complete process is illustrated by generating an instruction set for a processor optimized for executing compiled Prolog programs. The generated instruction set is compared with the hand-designed VLSI-BAM instruction set. The automatically designed instruction set is smaller and has only a few percent less performance on th...",
            "group": 1361,
            "name": "10.1.1.55.4770",
            "keyword": "",
            "title": "Automatic Design of Computer Instruction Sets"
        },
        {
            "abstract": "Most problems in the design of real-time applications like task allocation or scheduling belong to the class of NP-complete problems and can be solved efficiently only by heuristics. Genetic Algorithms are a relatively new method to attack these problems. Conventional Genetic Algorithms, however, have a number of drawbacks that reduce their applicability to design problems of real-time systems. The Genetic Algorithm presented in this paper implements enhancements to standard Genetic Algorithms to eliminate these problems. It allows arbitrary gene values and supports multiple chromosomes per individuum. The paper focuses on the advantages of the use of Genetic Algorithms in real-time system design in comparison to other heuristic problem solving techniques. The applicability of the enhanced algorithm is shown by solving a typical problem of real-time system design, the determination of a bus access schedule for a real-time LAN.",
            "group": 1362,
            "name": "10.1.1.55.4877",
            "keyword": "",
            "title": "Solving NP-Complete Problems in Real-Time System Design by Multichromosome Genetic Algorithms"
        },
        {
            "abstract": "This paper presents a new segmentation algorithm by fitting active contour models (or snakes) to objects using adaptive splines. The adaptive spline model describes the contour of an object by a set of piecewisely interpolating C  2  polynomial spline patches which are locally controlled. Thus the resulting description of the object contour is continuous and smooth. Polynomial splines provide a fast and efficient way for interpolating the object contour and allow us to compute its internal energy due to bending and elasticity deformations analytically. The adaptive spline model can be represented by its spline control points. The accuracy of the model is gradually increased during the segmentation process by inserting new control points. For estimating the optimal position of the control points, two different relaxation techniques based on Markov--Random--Fields (MRFs) have been combined and evaluated: Simulated Annealing (SA), which is a stochastic relaxation technique, and Iterated C...",
            "group": 1363,
            "name": "10.1.1.55.5774",
            "keyword": "",
            "title": "Contour Fitting Using an Adaptive Spline Model"
        },
        {
            "abstract": "We describe the capabilities of and algorithms used in a new FPGA CAD tool, Versatile Place and Route (VPR). In terms of minimizing routing area, VPR outperforms all published FPGA place and route tools to which we can compare. Although the algorithms used are based on previously known approaches, we present several enhancements that improve run-time and quality. We present placement and routing results on a new set of large circuits to allow future benchmark comparisons of FPGA place and route tools on circuit sizes more typical of today's industrial designs. VPR is capable of targeting a broad range of FPGA architectures, and the source code is publicly available. It and the associated netlist translation / clustering tool VPACK have already been used in a number of research projects worldwide, and should be useful in many areas of FPGA architecture research.  1 Introduction  In FPGA research, one must typically evaluate the utility of new architectural features experimentally. That ...",
            "group": 1364,
            "name": "10.1.1.55.5800",
            "keyword": "",
            "title": "VPR: A New Packing, Placement and Routing Tool for FPGA Research"
        },
        {
            "abstract": "Exact and inexact methods can be used for solving Constraint Satisfaction Problems (CSP), i.e. for finding a variable assignment which violates none of the constraints or minimizes the number of violated constraints. Based on a Backtrack tree search, exact methods are able to produce an optimal assignment, when no time limit is imposed. Based on local improvement mechanisms, inexact methods cannot guarantee that, but may produce better quality assignments in a limited time. In this paper, we show how an inexact method, coming from statistical physics, and more precisely from the Mean Field Theory, can boost an exact method by providing it with a good quality assignment, whose valuation can be used as an initial upper bound, and with two heuristics for ordering variables and values. Experiments on randomly generated classical and partial CSPs show significative gains in terms of time, even when adding the times used by both the Mean Field and the Backtrack methods. ",
            "group": 1365,
            "name": "10.1.1.55.6400",
            "keyword": "",
            "title": "Using Mean Field Methods for Boosting Backtrack Search in Constraint Satisfaction Problems"
        },
        {
            "abstract": "This paper discusses the use of genetic algorithms (GAs) for automatic software test data generation. This research extends previous work on dynamic test data generation where the problem of test data generation is reduced to one of minimizing a function [Miller and Spooner, 1976, Korel, 1990]. In our work, the function is minimized by using one of two genetic algorithms in place of the local minimization techniques used in earlier research. We describe the implementation of our GA-based system, and examine the effectiveness of this approach on a number of programs, one of which is significantly larger than those for which results have previously been reported in the literature. We also examine the effect of program complexity on the test data generation problem by executing our system on a number of synthetic programs that have varying complexities. 1 Introduction An important aspect of software testing involves judging how well a series of test inputs tests a piece of code. Usuall...",
            "group": 1366,
            "name": "10.1.1.55.6736",
            "keyword": "",
            "title": "Generating Software Test Data by Evolution"
        },
        {
            "abstract": "We introduce a visual approach for finding a good modularization of object-oriented systems. We regard modularization as system partitioning where each class belongs to exactly one module. A good modularization requires that class dependencies are as local to a module as possible which mirrors the well-known quality principles of strong module cohesion and weak module coupling. In our approach, there are three kinds of (inter-)class dependencies which are explicitly described in a system specification: (i) relations between classes, (ii) global state invariants constraining the state of several objects belonging to different classes and (iii) global system operations which change several objects of different classes within one atomic step. As a good modularization is not uniquely determined in general and as there are no common rules for system partitioning, a support for this difficult development task is needed. Therefore, we use an algorithm which takes the class dependencies for cl...",
            "group": 1367,
            "name": "10.1.1.55.6833",
            "keyword": "",
            "title": "Visual Support For The Modularization Of Object-Oriented Systems"
        },
        {
            "abstract": "Many learning systems search through a space of possible performance elements, seeking an element whose expected utility, over the distribution of problems, is high. As the task of finding the globally optimal element is often intractable, many practical learning systems instead hill-climb to a local optimum. Unfortunately, even this is problematic as the learner typically does not know the underlying distribution of problems, which it needs to determine an element's expected utility. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present a general algorithm, palo, that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by presenting three distinct applications, that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the util...",
            "group": 1368,
            "name": "10.1.1.55.7085",
            "keyword": "",
            "title": "PALO: A Probabilistic Hill-Climbing Algorithm"
        },
        {
            "abstract": "Thinning of large antenna arrays in order to obtain low sidelobes is a difficult and nontrivial problem. Evolutionary programming (EP), a multi-agent stochastic search method, is proposed for optimizing thinned phased arrays with a large number of elements. Experiments are conducted to determine the efficiency of EP procedure in terms of the reliability in producing acceptable solutions and the quality of the final solution. Antenna arrays with up to 200 elements are thinned to obtain maximum sidelobe levels of less than-20 dB. Simulation results indicate that when the number of elements in the array is greater than 120, EP is always successful in finding solutions that have maximum sidelobe levels of-20 dB or lower.  1 Introduction  Antennas play a vital role in the design and development of communication systems. Long distance communication systems require antenna systems with high gains. In comparison with single-element antennas, multi-element antenna arrays have very high gains. T...",
            "group": 1369,
            "name": "10.1.1.55.7090",
            "keyword": "",
            "title": "Optimization of Thinned Phased Arrays using Evolutionary Programming"
        },
        {
            "abstract": "This paper presents a search technique for scheduling problems, called Heuristic-Biased Stochastic Sampling (HBSS). The underlying assumption behind the HBSS approach is that strictly adhering to a search heuristic often does not yield the best solution and, therefore, exploration off the heuristic path can prove fruitful. Within the HBSS approach, the balance between heuristic adherence and exploration can be controlled according to the confidence one has in the heuristic. By varying this balance, encoded as a bias function,  the HBSS approach encompasses a family of search algorithms of which greedy search and completely random search are extreme members. We present empirical results from an application of HBSS to the realworld problem of observation scheduling. These results show that with the proper bias function, it can be easy to outperform greedy search. Introducing HBSS  This paper presents a search technique, called  Heuristic-Biased Stochastic Sampling (HBSS), that was design...",
            "group": 1370,
            "name": "10.1.1.55.8225",
            "keyword": "",
            "title": "Heuristic-Biased Stochastic Sampling"
        },
        {
            "abstract": ": : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : xii I Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 A. Global Optimization : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 1. Evolutionary Algorithms : : : : : : : : : : : : : : : : : : : : : : : 2 2. Simulated Annealing : : : : : : : : : : : : : : : : : : : : : : : : : : 5 3. Go-With-the-Winners : : : : : : : : : : : : : : : : : : : : : : : : : 5 B. Local Search : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8 C. Global-Local Hybrid Algorithms : : : : : : : : : : : : : : : : : : : : : 9 1. Evolutionary Algorithm with Local Search : : : : : : : : : : : : : : 9 2. Simulated Annealing : : : : : : : : : : : : : : : : : : : : : : : : : : 10 3. Go-With-the-Winners : : : : : : : : : : : : : : : : : : : : : : : : : 10 D. Dissertation Overview : : : : : : : : : : : : : : : : : : : : : : : : : : : 11 II Background : : : : : : : : : : : : : : : : : : : : : : : :...",
            "group": 1371,
            "name": "10.1.1.55.8986",
            "keyword": "TABLE OF CONTENTS Signature Pageiii Dedicationiv Table of Contentsv List of Figuresvii List of Tablesix",
            "title": "Evolutionary Algorithms with Local Search for Combinatorial Optimization"
        },
        {
            "abstract": "A framework for the expression and analysis of statically defined communication patterns present in computations targeted for execution in the NuMeshmulticomputer environment is developed. A system which solves optimization problems to statically allocate network resources for network traffic with real time constraints has been implemented. Results from the system, which uses simulated annealing and linear programming techniques to automate the process of placing and routing this traffic are presented. Thesis Supervisor: Stephen A. Ward Title: Professor of Electrical Engineering and Computer Science  Acknowledgements To Professor Steve Ward: During one's stay at university (In this case one called M.I.T.) There is with no great trouble found Professors professing knowledge sound. Seen with a little less likelihood Are those who teach more than equations would. For in Education's ivied walls, Its fabled classrooms, hallowed halls, One's not quite as sure to find An Educator's kind of m...",
            "group": 1372,
            "name": "10.1.1.55.9354",
            "keyword": "",
            "title": "Scheduled Routing for the Numesh"
        },
        {
            "abstract": "Parallel algorithms developed for CAD problems today suffer from three important drawbacks. First, they are machine specific and tend to perform poorly on architectures other than the one for which they were designed. Second, they cannot use the latest advances in improved versions of the sequential algorithms for solving the problem. Third, the quality of results degrade significantly during parallel execution. In this paper we address these three problems for an important CAD application: standard cell placement. We have developed a new parallel placement algorithm that is portable across a range of MIMD parallel architectures. The algorithm is part of the ProperCAD project which allows the development and implementation of a parallel algorithm such that it can be executed on a wide variety of parallel machines without any change to the source. The parallel placement algorithm is based on an existing implementation of the sequential simulated annealing algorithm,  TimberWolfSC 6.0 [1...",
            "group": 1373,
            "name": "10.1.1.55.9525",
            "keyword": "",
            "title": "ProperPLACE: A Portable Parallel Algorithm for Standard Cell Placement"
        },
        {
            "abstract": "We address the problem of maximizing the speedup of an individual parallel job through the selection of an appropriate number of processors on which to run it. If a parallel job exhibits speedup that increases monotonically in the number of processors, the solution is clearly to make use of all available processors. However, many applications do not have this characteristic: they reach a point beyond which the use of additional processors degrades performance. For these applications, it is important to choose a processor allocation carefully. Our approach to this problem is to provide a runtime system that adjusts the number of processors used by the application based on dynamic measurements of performance gathered during its execution. Our runtime system has a number of advantages over user specified fixed allocations, the currently most common approach to this problem: we are resilient to changes in an application's speedup behavior due to the input data; we are able to change the al...",
            "group": 1374,
            "name": "10.1.1.55.9865",
            "keyword": "",
            "title": "Maximizing Speedup Through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "A learning rule for stochastic neural networks is described, which corresponds to biological neural systems in all major aspects. Instead of backpropagating a vector through the synapses, only a few scalars are broadcast across the whole network, corresponding to the role played by the neurotransmitter dopamine. In addition, the annealing process avoids local optima in the learning process and corresponds to the difference in learning between adults and children. Some more detailed predictions are made for future comparison with neurophysiological data. 1 Introduction  Neural networks are studied both as computational tools and biological models. It is of great interest if one type of neural networks simultaneously exhibits computational advantage and biological plausibility. Such a connection would be beneficial to both research subjects. On the one hand, biological neural networks have had millions of years of evolution, which could offer insights to the design and improvement of neu...",
            "group": 1375,
            "name": "10.1.1.55.9925",
            "keyword": "",
            "title": "A Possible Link between Artificial and Biological Neural Network Learning Rules"
        },
        {
            "abstract": "We survey learning algorithms for recurrent neural networks with hidden units, and put the various techniques into a common framework. We discuss fixedpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non-fixedpoint algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an online technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. We discuss advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones, continue with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. We present some simulations, and at the end, address issues of computational complexity and learning speed.  Keywords--- Recurrent neural networks, backpropagation through time, real ti...",
            "group": 1376,
            "name": "10.1.1.55.9942",
            "keyword": "",
            "title": "Gradient Calculations for Dynamic Recurrent Neural Networks: A Survey"
        },
        {
            "abstract": "We introduce a meta-heuristic to combine simulated annealing with local search methods for CO problems. This new class of Markov chains leads to significantly more powerful optimization methods than either simulated annealing or local search. The main idea is to embed deterministic local search techniques into simulated annealing so that the chain explores only local optima. It makes large, global changes, even at low temperatures, thus overcoming large barriers in configuration space. We have tested this meta-heuristic for the traveling salesman and graph partitioning problems. Tests on instances from public libraries and random ensembles quantify the power of the method. Our algorithm is able to solve large instances to optimality, improving upon local search methods very significantly. For the traveling salesman problem with randomly distributed cities in a square, the procedure improves on 3-opt by 1.6%, and on Lin-Kernighan local search by 1.3%. For the partitioning of sparse rand...",
            "group": 1377,
            "name": "10.1.1.56.120",
            "keyword": "",
            "title": "Combining Simulated Annealing with Local Search Heuristics"
        },
        {
            "abstract": "The ever increasing complexity of Digital Signal Processing and other data independent scientific computations means that single processor systems in many cases do not provide the required computational capacity in order to respect given time constraints. Architectures with multiple DSP processors are therefore becoming popular alternatives. This raises, however, a natural requirement for formal methods which can help the system designer to create realistic and efficient multiprocessor solutions. Of particular importance in this design process is static multiprocessor scheduling which 1) assigns nodes (i.e., subalgorithms) to the individual processors, 2) specifies the node execution order, and 3) on the basis of estimated execution and communication times calculates invocation times for the individual nodes. We have found no or very little reported work on 1) the overall formal design trajectory, 2) realistic strategies for static multiprocessor scheduling of DSP algorithms, and 3) gu...",
            "group": 1378,
            "name": "10.1.1.56.628",
            "keyword": "",
            "title": "Strategies for Realistic and Efficient Static Scheduling of Data Independent Algorithms onto Multiple Digital Signal Processors."
        },
        {
            "abstract": "Parallel genetic algorithms (PGA) use two major modifications compared to the genetic algorithm. Firstly, selection for mating is distributed. Individuals live in a 2-D world. Selection of a mate is done by each individual independently in its neighborhood. Secondly, each individual may improve its fitness during its lifetime by e.g. local hill-climbing. The PGA is totally asynchronous, running with maximal efficiency on MIMD parallel computers. The search strategy of the PGA is based on a small number of intelligent and active individuals, whereas a GA uses a large population of passive individuals. We will show the power of the PGA with two combinatorial problems - the traveling salesman problem and the m graph partitioning problem. In these examples, the PGA has found solutions of very large problems, which are comparable or even better than any other solution found by other heuristics. A comparison between the PGA search strategy and iterated local hill-climbing is made. KEYWORDS  ...",
            "group": 1379,
            "name": "10.1.1.56.761",
            "keyword": "genetic algorithmiterated Lin-Kernighan searchtraveling salesman",
            "title": "Parallel Genetic Algorithm in Combinatorial Optimization"
        },
        {
            "abstract": "dentified many constraints on the form and processing of human languages. By incorporating these constraints into a language learning system, it is possible to build a system that learns to translate (infers functions and grammars for machine translation) from an aligned bilingual corpus of sentences using understandable, symbolic linguistic principles and representations. This work focuses on one particular constraint, the Marker Hypothesis, which is shown to be powerful, understandable, and computationally accessible. This hypothesis has been incorporated into a family of systems that infer such transfer functions using standard multivariate optimization techniques. These systems have been tested on a variety of language pairs and corpora, demonstrating the language and corpus independence of this approach. Furthermore, the design  iv principles are in theory independent of any particular inference technique or grammatical representation and reflect only the constraints of the Marke",
            "group": 1380,
            "name": "10.1.1.56.846",
            "keyword": "and Transfer Functions",
            "title": "Learning to Translate: A Psycholinguistic Approach to the Induction of Grammars and Transfer Functions"
        },
        {
            "abstract": "This paper addresses the development of more efficient global optimization techniques. Combinatorial optimization consists of a set of problems that are central to computer science and engineering. Simulated annealing is a very general optimization technique with a wide range of applications. It has already been successfully used to solve computer design problems such as efficient wiring of electric systems and component placement for microprocessors. Simulated 'tempering' is an extension of this idea. The tempering technique was implemented on the Traveling Salesman Problem. Automatic calculation of an initial and final value for the temperature parameter was added. 1 Introduction  The amount of calculation required for exact solution of an NP-complete  1  problem increases as e  N  with problem size. Even super-computers cannot find exact solutions to problems of this class once they exceed a certain size. Many optimization problems are NPcomplete.  Stochastic methods provide probabl...",
            "group": 1381,
            "name": "10.1.1.56.1151",
            "keyword": "",
            "title": "Improved Algorithms for Global Optimization"
        },
        {
            "abstract": ": Quantization of the parameters of a Perceptron is a central problem in hardware implementation of neural networks using a numerical technology. An interesting property of neural networks used as classifiers is their ability to provide some robustness on input noise. This paper presents efficient learning algorithms for the maximization of the robustness of a Perceptron and especially designed to tackle the combinatorial problem arising from the discrete weights. Keywords: Perceptron, learning, robustness, weight quantization, combinatorial optimization, tabu search. 1 Introduction  Artificial neural networks (ANN) are proposed today as alternative solutions for a wide variety of problems. Whatever is the architecture (feedfoward, feedback), the transfer function (threshold, sigmoidal, Gaussian) or the learning mode (supervised or unsupervised) , the efficiency of these models depends mostly on their implementations (simulation or dedicated hardware) and on the learning process used. ...",
            "group": 1382,
            "name": "10.1.1.56.1282",
            "keyword": "Perceptronlearningrobustnessweight quantizationcombinatorial optimizationtabu search",
            "title": "Maximizing the Robustness of a Linear Threshold Classifier with Discrete Weights"
        },
        {
            "abstract": ": Currents flowing in the power and ground (P&G) lines of CMOS digital circuits affect both circuit reliability and performance by causing excessive voltage drops. Maximum current estimates are therefore needed in the P&G lines to determine the severity of the voltage drop problems and to properly design the supply lines to eliminate these problems. These currents, however, depend on the specific input patterns that are applied to the circuit. Since it is prohibitively expensive to enumerate all possible inputs, this problem has, for a long time, remained largely unsolved. In [1], we proposed a pattern-independent, linear time algorithm (iMax) that estimates an upper bound envelope of all possible current waveforms that result from the application of different input patterns to the circuit. While the bound produced by iMax is fairly tight on many circuits, there can be a significant loss in accuracy due to correlations between signals internal to the circuit. In this paper, we present ...",
            "group": 1383,
            "name": "10.1.1.56.1312",
            "keyword": "",
            "title": "Resolving Signal Correlations for Estimating Maximum Currents in CMOS Combinational Circuits"
        },
        {
            "abstract": ". ProperCAD II is a C++ object oriented library supporting actor based parallel program design. The library easily allows the design of data structures with parallel semantics for use in irregular applications. Inheritance mechanisms allow creation of the distributed data structures from standard C++ objects. This paper discusses the use of such distributed data structures in the context of a particular VLSI CAD application, standard cell placement. The library and associated runtime system currently run on a wide range of platforms. 1 Introduction  The use of parallel platforms, despite increasing availability, remains largely restricted to well-structured numeric codes. Irregular applications in terms of data access patterns as well as control flow are difficult to effectively and efficiently parallelize. The use of object-oriented design techniques and the actor model of computation can address the use of parallel platforms for unstructured problems. ProperCAD II is an object orient...",
            "group": 1384,
            "name": "10.1.1.56.1343",
            "keyword": "",
            "title": "Distributed Object Oriented Data Structures and Algorithms for VLSI CAD"
        },
        {
            "abstract": "This report addresses the issues arising from the use of parallel machines and considers the various techniques used by members of the consortium in this context. Before considering the algorithms in detail, we describe, in section 2, the main types of parallel architecture and survey various attempts at providing a taxonomy. Then, in section 3, we address the difficult issue of the measurement of processor performance in order to quantify any enhancement obtained by implementing an algorithm in parallel. Section 4 presents the main features of in general, and PVM ( ) in particular. The latter is an application that is used to generate distributed versions of sequential algorithms for use on networks of workstations. The parallel implementations of the GA toolkit, , and the associated simulated annealing toolkit, , both developed at UEA, have been produced using PVM. Exact algorithms will always find the optimal solution to a problem given enough time and space. Subject to these constraints, they must always be the preferred method of solution. In practice, the time and space constraints can prevent the use of an exact algorithm and thus the potential of parallelism to reduce these factors becomes an important factor. Total enumeration is embarassingly parallel. With processors it is reasonable to expect an-fold reduction in time to undertake such a thorough search. Such a saving is seldom sufficient to make the method viable so we will concentrate on other exact methods here. In section 5, we review parallel branchand -bound, reprinting a survey paper written by the UEA partners in the consortium and previously published in [1]. Because of the interest in interior point methods for the CALMA project and its widely cited potential for parallelisation, this provides the ...",
            "group": 1385,
            "name": "10.1.1.56.1515",
            "keyword": "",
            "title": "Parallelism in Combinatorial Optimisation"
        },
        {
            "abstract": "We present a new algorithm for the robust and accurate tracking of the aorta in cardiovascular MR images. First, a rough estimate of the location and diameter of the aorta is obtained by applying a multiscale medial response function using the available a-priori knowledge. Then, this estimate is refined using an energy-minimizing deformable model which we define in a Markov-Random-Field (MRF) framework. In this context we propose a global minimization technique based on stochastic relaxation, Simulated Annealing (SA), which is shown to be superior to other minimization techniques, for minimizing the energy of the deformable model. We have evaluated the performance and robustness of the algorithm on clinical compliance studies in cardiovascular MR images. The segmentation and tracking has been successfully tested in spin-echo MR images of the aorta. The results show the ability of the algorithm to produce not only accurate but also very reliable results in clinical routine applications....",
            "group": 1386,
            "name": "10.1.1.56.2249",
            "keyword": "Geometrically Deformable Models (GDMsSimulated Annealing (SAIterated Conditional Modes (ICM",
            "title": "Automatic tracking of the aorta in cardiovascular MR images using deformable models"
        },
        {
            "abstract": "Many large-scale engineering and scientific calculations involve repeated updating of variables on an unstructured mesh. To do these types of computations on distributed memory parallel computers, it is necessary to partition the mesh among the processors so that the load balance is maximized and inter-processor communication time is minimized. This can be approximated by the problem of partitioning a graph so as to obtain a minimum cut, a wellstudied combinatorial optimization problem. Graph partitioning is NP complete, so for real world applications, one resorts to heuristics, i.e., algorithms that give good but not necessarily optimum solutions. These algorithms include local search methods such as Kernighan-Lin, recursive spectral bisection, and more general purpose methods such as simulated annealing. We show that a general procedure enables us to combine simulating annealing with Kernighan-Lin. The resulting algorithm is both very fast and extremely effective. 1 Introduction  Con...",
            "group": 1387,
            "name": "10.1.1.56.2269",
            "keyword": "",
            "title": "Partitioning of Unstructured Meshes for Load Balancing"
        },
        {
            "abstract": "We describe a new, linear time heuristic for the improvement of graph bisections. The method is a variant of local search with sophisticated neighborhood relations. It is based on graph-theoretic observations that were used to find upper bounds for the bisection width of regular graphs. Efficiently implemented, the new method can serve as an alternative to the commonly used local heuristics, not only in terms of the quality of attained solutions, but also in terms of space and time requirements. We compare our heuristic with a number of well known bisection algorithms. Extensive measurements show that the new method is a real improvement for graphs of certain types.  Keywords: Graph Partitioning, Graph Bisection, Recursive Bisection, Edge Separators, Mapping, Local Search, Parallel Processing.   This work was partly supported by the German Research Foundation (DFG Forschergruppe \"Effiziente Nutzung massiv paralleler Systeme\") and by the ESPRIT Basic Research Action No. 7141 (ALCOM II)....",
            "group": 1388,
            "name": "10.1.1.56.2318",
            "keyword": "Graph PartitioningGraph BisectionRecursive BisectionEdge SeparatorsMappingLocal SearchParallel Processing",
            "title": "Using Helpful Sets to Improve Graph Bisections"
        },
        {
            "abstract": "Adaptive learning dynamics of the Radial Basis Function Network (RBFN) are compared with a scale-based clustering technique [Won93] and a relationship between the two is pointed out. Using this link, it is shown how scale-based clustering can be done using the RBFN, with the Radial Basis Function (RBF) width as the scale parameter. The technique suggests the \"right\" scale at which the given data set must be clustered and obviates the need for knowing the number of clusters beforehand. We show how this method solves the problem of determining the number of RBF units and the widths required to get a good network solution.  I. Introduction  Clustering aims at partitioning data into more or less homogeneous subsets when the apriori distribution of the data is not known. The clustering problem arises in various disciplines and the existing literature is abundant. Traditional approaches to this problem define a cost function which, when minimized, yields desirable clusters. Hence the final c...",
            "group": 1389,
            "name": "10.1.1.56.2511",
            "keyword": "",
            "title": "Scale-based Clustering using the Radial Basis Function Network"
        },
        {
            "abstract": "Simulated annealing is a computational technique reported to give good results in coping with complex combinatorial problems. These problems usually consist of finding a global minimum of a cost function on a (large) set of states (also known as feasible solutions). In this paper we explore modifications to the standard simulated annealing method that we apply to the design of dense interconnection networks, that is networks that have as many nodes as possible for a given maximum number of links from each node to other nodes, and a given maximum distance between all pair of nodes. We are particularly interested in the directed-symmetric case in wich all links have a direction, and the network, roughly speaking, looks the same from any node. In that case each node may execute, without modifications, the same communication software. The set of states are the different networks and a cost function is defined and is used to accept or reject a new network obtained from a modification of a p...",
            "group": 1390,
            "name": "10.1.1.56.3268",
            "keyword": "",
            "title": "Using Simulated Annealing to Design Interconnection Networks"
        },
        {
            "abstract": "Motion estimation is one of the key techniques helping solve problems encountered in image sequence compression and processing, and in computer vision. Redundancy elimination in digital video and tracking of moving objects in surveillance applications are but two interesting tasks where the knowledge of image motion is essential. Due to a strong correlation of image properties in the direction of motion, operations such as prediction, interpolation or filtering are most efficient when applied along motion trajectories. To compute these trajectories, underlying models need to be specified, estimation criteria must be selected and a search strategy for model parameters must be implemented. In this paper, we expose each of those issues in a tutorial-like fashion. First, we discuss various motion representations and their relationship with the images. Then, we describe various estimation criteria: from a simple square of the displaced frame difference to complex Bayesian criteria involving...",
            "group": 1391,
            "name": "10.1.1.56.3299",
            "keyword": "",
            "title": "On Models, Criteria and Search Strategies for Motion Estimation in Image Sequences"
        },
        {
            "abstract": "Simulation is often used in the analysis of complex computing systems. The design and validation of complex real-time systems demands capabilities not found in existing simulation environments. We categorize these capabilities into three major functional areas: search control, the execution engine, and output analysis. Search control is an imporant topic often treated lightly in simulation environments for real-time systems. The search controller chooses values for variable input parameters and launches simulation sto determine the performance of the system under different sets of parameter values. The search control framework presented in this thesis allows both conventional design and search and also validation search",
            "group": 1392,
            "name": "10.1.1.56.3300",
            "keyword": "",
            "title": "A Framework For The Simulation Of Complex Real-Time Systems"
        },
        {
            "abstract": "One of the main obstacles in applying genetic algorithms (GAs) to complex problems has been the high computational cost due to their slow convergence rate. We encountered such a difficulty in an attempt to use the classical GA for estimating parameters of a metabolic model. To alleviate this difficulty, we developed a hybrid approach that combines GA with a stochastic variant of the simplex method in function optimization. Our motivation for developing the stochastic simplex method is to introduce a costeffective exploration component into the conventional simplex method. In an attempt to make effective use of the simplex operation in a hybrid GA framework, we used an elite-based hybrid architecture that applies one simplex step to a top portion of the ranked population. We compared our approach with five alternative optimization techniques including a simplex-GA hybrid independently developed by Renders and Bersini and Adaptive Simulated Annealing (ASA). Our empirical evaluations show...",
            "group": 1393,
            "name": "10.1.1.56.3562",
            "keyword": "",
            "title": "A Hybrid Approach to Modeling Metabolic Systems Using Genetic Algorithm and Simplex Method"
        },
        {
            "abstract": "We present a new multiscale contour fitting process which combines information about the image and the contour of the object at different levels of scale. The algorithm is based on energy minimizing deformable models but avoids some of the problems associated with these models. The segmentation algorithm starts by constructing a linear scale-space of an image through convolution of the original image with a Gaussian kernel at different levels of scale, where the scale corresponds to the standard deviation of the Gaussian kernel. At high levels of scale large scale features of the objects are preserved while small scale features, like object details as well as noise, are suppressed. In order to maximize the accuracy of the segmentation, the contour of the object of interest is then tracked in scale-space from coarse to fine scales. We propose a hybrid Multi-Temperature Simulated Annealing optimization to minimize the energy of the deformable model. At high levels of scale the SA optimiz...",
            "group": 1394,
            "name": "10.1.1.56.3686",
            "keyword": "Image SegmentationContour FittingMultiscale Image AnalysisDeformable ModelsMultiresolution Deformable ModelsSimulated Annealing (SAIterated Conditional Modes (ICM",
            "title": "A Multiscale Approach to Contour Fitting for MR Images"
        },
        {
            "abstract": "We investigate the statistical properties of cut sizes generated by heuristic algorithms which solve approximately the graph bisection problem. On an ensemble of sparse random graphs, we find empirically that the distribution of the cut sizes found by \"local\" algorithms becomes peaked as the number of vertices in the graphs becomes large. Evidence is given that this distribution tends towards a Gaussian whose mean and variance scales linearly with the number of vertices of the graphs. Given the distribution of cut sizes associated with each heuristic, we provide a ranking procedure which takes into account both the quality of the solutions and the speed of the algorithms. This procedure is demonstrated for a selection of local graph bisection heuristics. ",
            "group": 1395,
            "name": "10.1.1.56.3976",
            "keyword": "Key words. graph partitioningheuristicsself-averagingranking",
            "title": "Cut Size Statistics Of Graph Bisection Heuristics"
        },
        {
            "abstract": "We address the problem of maximizing application speedup through runtime, self-selection of an appropriate number of processors on which to run. Automatic, runtime selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data or time dependent. We propose the use of a runtime system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job's processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our runtime system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation. 1. Introduction  We consider the problem of maximizing the speedup of an individual paral...",
            "group": 1396,
            "name": "10.1.1.56.4524",
            "keyword": "",
            "title": "Maximizing Speedup through Self-Tuning of Processor Allocation"
        },
        {
            "abstract": "Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straight-forward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. Then, we experimentally analyze the effectiveness of model regularization, dynamic model modification, and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences usign a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite. 1 Introduction Since their introduction to the computational biology community (Haussler et al., 1993; Krogh et al., 1994a), hidden Markov models (HMMs)...",
            "group": 1397,
            "name": "10.1.1.56.4974",
            "keyword": "Running titleHidden Markov models for sequence analysis KeywordsHidden Markov modelparallel computationmultiple sequence alignmentprotein modelingmotif",
            "title": "Hidden Markov models for sequence analysis: extension and analysis of the basic method"
        },
        {
            "abstract": "Heterogeneous computing (HC) environments are well suited to meet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (defined as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a difficult problem, because comparisons are often clouded by different underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined are Opportunistic Load Balancing, User-Directed Assignment, Fast Greedy, Min-min, Max-min, Greedy, Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu, and A*. This study provides one even basis for comparison and insights into c...",
            "group": 1398,
            "name": "10.1.1.56.5010",
            "keyword": "are Opportunistic Load BalancingUser-Directed AssignmentFast GreedyMin-minMax-minGreedyGenetic Algorithm",
            "title": "A Comparison Study of Static Mapping Heuristics for a Class of Meta-tasks on Heterogeneous Computing Systems"
        },
        {
            "abstract": "Embedded rule-based expert systems must satisfy stringent timing constraints when applied to real-time environments. The paper describes a novel approach to reduce the response time of rule-based expert systems. Our optimization method is based on a construction of the reduced cycle-free finite state transition system corresponding to the input rule-based system. The method makes use of rule-base system decomposition, concurrency and state-equivalency. The new and optimized system is synthesized from the derived transition system. Compared with the original system, the synthesized system has (1) fewer number of rule firings to reach the fixed point, (2) is inherently stable and (3) has no redundant rules. The synthesis method also determines the tight response time bound of the new system. The optimized system is guaranteed to compute correct results independent of the scheduling strategy and execution environment. 1 Introduction  Validation and verification is becoming a very importan...",
            "group": 1399,
            "name": "10.1.1.56.5753",
            "keyword": "",
            "title": "Optimization of Rule-Based Expert Systems Via State Transition System Construction"
        },
        {
            "abstract": "System specification and design consists of describing a system's desired functionality, and of mapping that functionality for implementation on a set of system components, such as processors, ASIC's, memories, and buses. In this article, we describe the key problems of system specification and design, including specification capture, design exploration, hierarchical modeling, software and hardware synthesis, and cosimulation. We highlight existing tools and methods for solving those problems, and we discuss issues that remain to be solved.  .  Contents  1 Introduction 1 2 Specification capture 5  2.1 Model creation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 2.2 Description generation  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  7  3 Exploration 8  3.1 Allocation  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  8 3.2 Partitioning : : : : : : : : : : : : : : : : : : : : : : : : :...",
            "group": 1400,
            "name": "10.1.1.56.6050",
            "keyword": "Contents",
            "title": "Specification and Design of Embedded Software/Hardware Systems"
        },
        {
            "abstract": "This thesis describes a system that synthesizes regularity exposing attributes from large protein databases. After processing primary and secondary structure data, this system discovers an amino acid representation that captures what are thought to be the three most important amino acid characteristics (size, charge, and hydrophobicity) for tertiary structure prediction. A neural network trained using this 16 bit representation achieves a performance accuracy on the secondary structure prediction problem that is comparable to the one achieved by a neural network trained using the standard 24 bit amino acid representation. In addition, the thesis describes bounds on secondary structure prediction accuracy, derived using an optimal learning algorithm and the probably approximately correct (PAC) model.  ",
            "group": 1401,
            "name": "10.1.1.56.6334",
            "keyword": "",
            "title": "Synthesizing Regularity Exposing Attributes in Large Protein Databases"
        },
        {
            "abstract": "Neural networks can be regarded as statistical models, and can be analysed in a Bayesian framework. Generalisation is measured by the performance on independent test data drawn from the same distribution as the training data. Such performance can be quantified by the posterior average of the information divergence between the true and the model distributions. Averaging over the Bayesian posterior guarantees internal coherence; Using information divergence guarantees invariance with respect to representation. The theory generalises the least mean squares theory for linear Gaussian models to general problems of statistical estimation. The main results are: (1) the ideal optimal estimate is always given by average over the posterior; (2) the optimal estimate within a computational model is given by the projection of the ideal estimate to the model. This incidentally shows some currently popular methods dealing with hyperpriors are in general unnecessary and misleading. The extension of in...",
            "group": 1402,
            "name": "10.1.1.56.7079",
            "keyword": "",
            "title": "Information Geometric Measurements of Generalisation"
        },
        {
            "abstract": "A study of global optimization schemes is presented. Simulated annealing, a general and proven technique for minimizing functions with many coexisting states, is used as a foundation for the development of a new, more automatic approach, called simulated tempering. This novel method upholds the eminent attribute of simulated annealing--- the probabilistic guarantee of convergence upon a global minimum. It is unique, however, in that system equilibrium is never disturbed. Although simulated tempering is in its infancy, it is promising indeed, as the preliminary results suggest. The twodimensional Ising Spin Glass Model, an NP-complete optimization problem, is used as the test case. Its theoretical formulation is briefly addressed. 1 Introduction  Global optimization, also known as multivariate and combinatorial optimization, is a technique for minimizing complex functions dependent on many variables. Such problems are prevalent in science and, with the advent of high-performance computi...",
            "group": 1403,
            "name": "10.1.1.56.7163",
            "keyword": "",
            "title": "A New Automatic Simulated Annealing-Type Global Optimization Scheme"
        },
        {
            "abstract": "The paper introduces duty measure for optimization methods. Duty expresses the relationship between the quality of the result and the time required to obtain the result. The usefulness of the duty measure is demonstrated on a case study involving a local optimization of a large traveling salesman problem. Using duty, a deterministic method and a probabilistic method are combined into a hybrid method. The hybrid method exhibits the best quality-time tradeoff of the three methods. The performance of the hybrid method is analyzed and some future research questions are addressed.  Keywords: combinatorial optimization, local search, traveling salesman problem, duty, quality-time tradeoff. 1 Introduction  Local optimization has been widely used for solving numerous practical and theoretical problems. It is being applied to hard problems for which no deterministic, practical algorithms are known. Such hard problems exhibit large search spaces with irregular structure. Local optimization provi...",
            "group": 1404,
            "name": "10.1.1.56.7169",
            "keyword": "combinatorial optimizationlocal searchtraveling salesman problemdutyquality-time tradeoff",
            "title": "Using the Quality-Time Tradeoff in Local Optimization"
        },
        {
            "abstract": "The revival of multilayer neural networks in the mid 80's originated from the discovery of the backpropagation technique as a feasible training procedure. In spite of its shortcomings, it is probably the most widespread technique for training feedforward nets. In recent years, several deterministic methods more efficient than back-propagation have been proposed. In this paper a stochastic minimization algorithm, Iterated Adaptive Memory Stochastic Search, is described which does not use gradient information and is found to perform better than back-propagation on the encoder and parity problems 1 . Keywords: stochastic optimization, learning algorithms, back-propagation 1. Introduction Learning from examples, the problem which neural networks were created to solve, is one of the most important research topics in the AI community. A possible way to formalize learning from examples is to hypothesize the existence of a function that captures the underlying mapping, thereby enabling gen...",
            "group": 1405,
            "name": "10.1.1.56.7191",
            "keyword": "stochastic optimizationlearning algorithmsback-propagation",
            "title": "On Training Neural Nets through Stochastic Minimization"
        },
        {
            "abstract": "In this paper computational aspects of the mathematical modelling of dynamic system evolution have been considered as a problem in information theory. The construction of such models is treated as a decision making process with limited available information. The solution of the problem is associated with a computational model based on heuristics of a Markov Chain in a discrete space-time of events. A stable approximation of the chain has been derived and the limiting cases are discussed. An intrinsic interconnection of constructive, sequential, and evolutionary approaches in related optimization problems provides new challenges for future work.  Key words: decision making with limited information, optimal control theory, hyperbolicity of dynamic rules, generalized dynamic systems, Markov Chain approximation.  1 Introduction  Many mathematical problems in information theory and optimal control related to dynamic system studies can be formulated in the following generic form. A decision...",
            "group": 1406,
            "name": "10.1.1.56.7314",
            "keyword": "",
            "title": "Dynamic System Evolution and Markov Chain Approximation"
        },
        {
            "abstract": "Simulated Annealing (SA) is a general stochastic search algorithm. It is usually employed as an optimization method to find a near optimal solution for hard combinatorial optimization problems, but it is very difficult to give the accuracy of the solution found. In order to find a better solution, an often used strategy is to run the algorithm many times and select the best solution as the final one. This paper gives an algorithm called Genetic Annealing (GA), which connects each run of SA and gradually improve the solution. It introduces the concept of evolution into the annealing process. The basic idea is to use genetic operations adopted in genetic algorithms to inherit the possible benefits of the solutions found in former runs. Experiments have shown that GA is better than classical SA. The parallelization of GA is also discussed in the paper.  keywords --- Simulated Annealing, Genetic Algorithms, Combinatorial Optimization. 1 Introduction  SA is a general stochastic search metho...",
            "group": 1407,
            "name": "10.1.1.56.7606",
            "keyword": "Genetic AlgorithmsCombinatorial Optimization",
            "title": "Optimization by Genetic Annealing"
        },
        {
            "abstract": "A novel feedforward network is proposed which lends itself to cost-effective implementations in digital hardware and has a fast forward-pass capability. It differs from the conventional model in restricting its synapses to the set f\\Gamma1; 0; 1g while allowing unrestricted offsets. The benefit of this configuration is in having 1-bit synapses and a multiplication operation which consists of a single sign-change instruction. The procedure proposed for training these networks is mainly based on steepest descent. It also has a perturbation process to avoid getting trapped in local minima, as well as a unique mechanism for rounding off `nearly-discrete' weights. It incorporates weight elimination implicitly, which simplifies the choice of the start-up network configuration for training. A new theoretical result is also presented which shows that the proposed multiplier-free network is a universal approximator over the space of continuous functions of one variable. Experimental results ind...",
            "group": 1408,
            "name": "10.1.1.56.7972",
            "keyword": "",
            "title": "Multiplier-free Feedforward Networks"
        },
        {
            "abstract": "This paper shows a theoretical property on the Markov chain of genetic algorithms: the stationary distribution focuses on the uniform population with the optimal solution as mutation and crossover probabilities go to zero and some selective pressure defined in this paper goes to infinity. Moreover, as a result, a sufficient condition for ergodicity is derived when a simulated annealing-like strategy is considered. Additionally, the uniform crossover counterpart of the Vose-Liepins formula is derived using the Markov chain model.  Keywords--- genetic algorithms, simulated annealing, Markov chain. I. INTRODUCTION  Genetic algorithms (GAs) are stochastic search techniques widely applied to combinatorial optimization problems [11], [14], [29], [30], [31], [32], [33]. GAs move from population to population. Each population consists of chromosomes (individuals) which represent candidate solutions to the optimization problem. A new population is formed by transforming individuals of the curre...",
            "group": 1409,
            "name": "10.1.1.56.8245",
            "keyword": "",
            "title": "A Further Result on the Markov Chain Model of Genetic Algorithms and Its Application to a Simulated Annealing-like Strategy"
        },
        {
            "abstract": "Classroom scheduling is an important part of course scheduling at Purdue University. The objective is to choose meeting rooms and times for each class that maximize student and instructor preferences without creating student, room or instructor schedule conflicts. An approach for solving classroom scheduling problems of practical size has been developed and implemented in CHRONOS, a scheduling support system developed at Purdue and described in this paper. Requirements for CHRONOS derive from the course scheduling process at Purdue and are specified in a mathematical model of the classroom scheduling problem. Database, preprocessing, and search components provide computerized support to decision makers. Results obtained from preliminary tests and ongoing use scheduling 500 course sections in a set of 31 large lecture rooms are positive. Work is currently under way to implement the system in a client-server environment and improve the qualitative aspects of generated schedules.  1 Intr...",
            "group": 1410,
            "name": "10.1.1.56.9041",
            "keyword": "",
            "title": "Large Scale Classroom Scheduling"
        },
        {
            "abstract": "This paper presents a brief overview of the field of evolutionary computation. Three major research areas of evolutionary computation will be discussed; evolutionary computation theory, evolutionary optimisation and evolutionary learning. The state-of-the-art and open issues in each area will be addressed. It is indicated that while evolutionary computation techniques have enjoyed great success in many engineering applications, the progress in theory has been rather slow. This paper also gives a brief introduction to parallel evolutionary algorithms. Two models of parallel evolutionary algorithms, the island model and the cellular model, are described. 1 Introduction  The field of evolutionary computation has grown rapidly in recent years [1, 2, 3]. Engineers and scientists with quite different backgrounds have come together to tackle some of the most difficult problems using a very promising set of stochastic search algorithms --- evolutionary algorithms (EAs). There are several diffe...",
            "group": 1411,
            "name": "10.1.1.56.9127",
            "keyword": "",
            "title": "An Overview of Evolutionary Computation"
        },
        {
            "abstract": "Given a heuristic estimate of the relative safety of a hybrid dynamical system trajectory, we transform the initial safety problem for dynamical systems into a global optimization problem. We compare untuned performance of several Simulated Annealing and Multi Level Single Linkage method variants, and discuss the dynamic use of knowledge gained during optimization. 1 Introduction  Given a simulated hybrid dynamical system S, a set of possible initial states I , and a set of \"unsafe\" states U , we wish to verify nonexistence of an S-trajectory from I to U within t max  time units. We call this the initial safety problem. Suppose we are given an approximate measure of the relative safety of a trajectory. More specifically, let f be a function taking an initial state i  as input, and evaluating the S trajectory from i such that f(i) = 0 if and only if the S-trajectory  from i enters U within t max time units, and f(i) ? 0 otherwise. Then verification of the initial safety problem can be t...",
            "group": 1412,
            "name": "10.1.1.56.9269",
            "keyword": "",
            "title": "Heuristic Optimization and Dynamical System Safety Verification"
        },
        {
            "abstract": "We introduce a new class of Markov chain Monte Carlo search procedures, leading to more powerful optimization methods than simulated annealing. The main idea is to embed deterministic local search techniques into stochastic algorithms. The Monte Carlo explores only local optima, and it is able to make large, global changes, even at low temperatures, thus overcoming large barriers in configuration space. We test these procedures in the case of the Traveling Salesman Problem. The embedded local searches we use are 3-opt and Lin-Kernighan. The large change or step consists of a special kind of 4-change followed by local-opt minimization. We test this algorithm on a number of instances. The power of the method is illustrated by solving to optimality some large problems such as the LIN318, the AT&T532, and the RAT783 problems. For even larger instances with randomly distributed cities, the Markov chain procedure improves 3-opt by over 1.6%, and Lin-Kernighan by 1.3%, leading to a new best h...",
            "group": 1413,
            "name": "10.1.1.56.9897",
            "keyword": "",
            "title": "Large-Step Markov Chains for the Traveling Salesman Problem"
        },
        {
            "abstract": ". We propose in this paper a novel integration of local search algorithms within a constraint programming framework for combinatorial optimization problems, in an attempt to gain both the efficiency of local search methods and the flexibility of constraint programming while maintaining a clear separation between the constraints of the problem and the actual search procedure. Each neighborhood exploration is performed by branch-and-bound search, whose potential pruning capabilities open the door to more elaborate local moves, which could lead to even better approximate results. Two illustrations of this framework are provided, including computational results for the traveling salesman problem with time windows. These results indicate that it is one order of magnitude faster than the customary constraint programming approach to local search and that it is competitive with a specialized local search algorithm.  Keywords: local search, constraint programming, neighborhood model, interface ...",
            "group": 1414,
            "name": "10.1.1.57.39",
            "keyword": "local searchconstraint programmingneighborhood modelinterface constraintstabu searchtraveling salesman problem with time windowspersonnel scheduling problem",
            "title": "A Constraint Programming Framework for Local Search Methods"
        },
        {
            "abstract": "We discuss the use of genetic algorithms (GAs) for the generation of music. We explain the structure of a typical GA, and outline existing work on the use of GAs in computer music. We propose that the addition of domain-specific knowledge can enhance the quality and speed of production of GA results, and describe two systems which exemplify this. However, we conclude that GAs are not ideal for the simulation of human musical thought (notwithstanding their ability to produce good results) because their operation in no way simulates human behaviour.  Keywords: Genetic Algorithms; Music Generation; Search Space 1 Introduction  In recent years, the idea of Genetic Algorithms (GAs) has generated signficant interest in the artificial intelligence and computer science communities. This has been reflected in a number of publications in the computer music world, some of which we discuss below. However, as GA research proceeds, it is becoming clear that the operation of a GA need not be enormous...",
            "group": 1415,
            "name": "10.1.1.57.1191",
            "keyword": "Genetic AlgorithmsMusic GenerationSearch Space",
            "title": "Evolutionary Methods for Musical Composition"
        },
        {
            "abstract": "In this paper we consider the unconstrained binary quadratic programming problem. This is the problem of maximising a quadratic objective by suitable choice of binary (zero-one) variables. We present two heuristic algorithms based upon tabu search and simulated annealing for this problem. Computational results are presented for a number of publically available data sets involving up to 2500 variables. An interesting feature of our results is that whilst for most problems tabu search dominates simulated annealing for the very largest problems we consider the converse is true. This paper typifies a \"multiple solution technique, single paper\" approach, i.e. an approach that within the same paper presents results for a number of different heuristics applied to the same problem. Issues relating to algorithmic design for such papers are discussed. Keywords: unconstrained binary (zero-one) quadratic programming",
            "group": 1416,
            "name": "10.1.1.57.1403",
            "keyword": "unconstrained binary (zero-one) quadratic programmingtabu searchsimulated annealing 1",
            "title": "Heuristic Algorithms for the Unconstrained Binary Quadratic Programming Problem"
        },
        {
            "abstract": "We propose a variant of the Simulated Annealing method for optimization in the multivariate analysis of differentiable functions. The method uses global actualizations via the Hybrid Monte Carlo algorithm in their generalized version for the proposal of new configurations. We show how this choice can improve upon the performance of simulated annealing methods (mainly when the number of variables is large) by allowing a more effective searching scheme and a faster annealing schedule.  ",
            "group": 1417,
            "name": "10.1.1.57.1592",
            "keyword": "as Physics",
            "title": "Simulated Annealing using Hybrid Monte Carlo"
        },
        {
            "abstract": "Contents  1 Combination of oscillating local search and heuristically ordered implicit enumeration L. Mynard, J.-M. Labat 1 Sophia-Antipolis, France, July 21-24, 1997 INRIA & PRiSM-Versailles  2ND INTERNATIONAL CONFERENCE ON METAHEURISTICS - MIC97 1 Combination of oscillating local search and heuristically ordered implicit enumeration  Laurent Mynard, Jean-Marc Labat  LIP6 Universit'e Pierre et Marie Curie, case courrier 169, 4 place Jussieu, 75252 Paris Cedex 05, France Email(s): mynard@poleia.lip6.fr, labat@poleia.lip6.fr 1 Introduction  Combinatorial optimization problems are nowadays used in many economically important areas [19]. For instance assignment problems are used to assign tasks to workers, to locate plants or radars; knapsack problems are used in finance or for cargo loading, and so on. But most interesting problems are NP-hard [15, chapter 15 & 16]. Therefore approximate methods must be used ",
            "group": 1418,
            "name": "10.1.1.57.1926",
            "keyword": "",
            "title": "Combination of Oscillating Local Search and Heuristically Ordered . . ."
        },
        {
            "abstract": "We present a new heuristic algorithm for graph bisection, based on an implicit notion of clustering. We describe how the heuristic can be combined with stochastic search procedures and a postprocess application of the Kernighan-Lin algorithm. In a series of time-equated comparisons with large-sample runs of pure Kernighan-Lin, the new algorithm demonstrates significant superiority in terms of the best bisections found. 1 Introduction  Given a graph G = (V; E) with an even number of vertices, the graph-bisection problem is to divide  V into two equal-size subsets X and Y such that the number of edges connecting vertices in X to vertices in Y (the size of the cut set , notated cut(X; Y )) is minimized. This problem is NP-complete [7]. Graph bisection and its generalizations  1  have considerable practical significance, especially in the areas of VLSI design and operations research. The benchmark algorithm for graph bisection is due to Kernighan and Lin [13]. (The efficient implementation...",
            "group": 1419,
            "name": "10.1.1.57.2247",
            "keyword": "",
            "title": "A Seed-Growth Heuristic for Graph Bisection"
        },
        {
            "abstract": "There has been substantial recent interest in two new families of search techniques. One family consists of nonsystematic methods such as GSAT; the other contains systematic approaches that use a polynomial amount of justification information to prune the search space. This paper introduces a new technique that combines these two approaches. The algorithm allows substantial freedom of movement in the search space but enough information is retained to ensure the systematicity of the resulting analysis. Bounds are given for the size of the justification database and conditions are presented that guarantee that this database will be polynomial in the size of the problem in question.",
            "group": 1420,
            "name": "10.1.1.57.2580",
            "keyword": "",
            "title": "GSAT and Dynamic Backtracking"
        },
        {
            "abstract": "Partitioning is one of the critical phases of hierarchical design processes like VLSI design. Good partitioning techniques can positively influence the performance and cost of a VLSI product. This paper proposes a partitioning algorithm with a new cost metric. Viewed from a VLSI layout point of view our cost metric minimizes the average delay per net. It can also be interpreted as achieving the minimum number of vias per net. This paper highlights how the seemingly slight difference between our metric and others could cause partitions to be evaluated considerably differently. Experimental results show that in addition to the expected improvements we get on our metric, the proposed algorithm does well on the traditional nets cut metric as well. 1 Introduction  The multi-way partitioning problem is one of partitioning the modules in a network into K subsets (partitions) of \"approximately\" the same size while minimizing the amount of interaction between the K partitions. This problem has ...",
            "group": 1421,
            "name": "10.1.1.57.2925",
            "keyword": "",
            "title": "Multi-Way Partitioning of VLSI Circuits"
        },
        {
            "abstract": "Some apparently powerful algorithms for automatic label placement on maps use heuristics that capture considerable cartographic expertise but are hampered by provably inefficient methods of search and optimization. On the other hand, no approach to label placement that is based on an efficient optimization technique has been applied to the production of general cartographic maps --- those with labeled point, line, and area features --- and shown to generate labelings of acceptable quality. We present an algorithm for label placement that achieves the twin goals of practical efficiency and high labeling quality by combining simple cartographic heuristics with effective stochastic optimization techniques.  Keywords: label placement, text placement, map lettering, diagram lettering, optimization, heuristics.  Contact: Stuart M. Shieber Division of Applied Sciences Harvard University 33 Oxford Street --- 14 Cambridge, MA 02138 USA  A General Cartographic Labeling Algorithm  Abstract  Some...",
            "group": 1422,
            "name": "10.1.1.57.3091",
            "keyword": "label placementtext placementmap letteringdiagram letteringoptimizationheuristics",
            "title": "A General Cartographic Labeling Algorithm"
        },
        {
            "abstract": "We introduce a meta-heuristic to combine simulated annealing with local search methods for CO problems. This new class of Markov chains leads to significantly more powerful optimization methods than either simulated annealing or local search. The main idea is to embed deterministic local search techniques into simulated annealing so that the chain explores only local optima. It makes large, global changes, even at low temperatures, thus overcoming large barriers in configuration space. We have tested this meta-heuristic for the traveling salesman and graph partitioning problems. Tests on instances from public libraries and random ensembles quantify the power of the method. Our algorithm is able to solve large instances to optimality, improving upon local search methods very significantly. For the traveling salesman problem with randomly distributed cities in a square, the procedure improves on 3-opt by 1.6%, and on Lin-Kernighan local search by 1.3%. For the partitioning of sparse rand...",
            "group": 1423,
            "name": "10.1.1.57.3491",
            "keyword": "",
            "title": "Combining Simulated Annealing with Local Search Heuristics"
        },
        {
            "abstract": "This paper addresses this question for a very simple problem: the mapping of 10 \\Theta 10 points in a square array to 1 \\Theta 100 points in a linear array (see figure 1). Our approach is to explicitly optimize several different objective functions from the topographic mapping literature for this case, and thus gain insight into the type of representation that each measure forms.",
            "group": 1424,
            "name": "10.1.1.57.3971",
            "keyword": "",
            "title": "Objective Functions for Topography: A Comparison of Optimal Maps"
        },
        {
            "abstract": "INTRODUCTION 1.1. Parallel Processing for CAD In view of the increasing complexity of very large scale integrated circuits (VLSI), there is a growing need for sophisticated computer-aided design (CAD) tools to automate the synthesis, analysis, and verification steps in the design of VLSI systems. Although the increased performance of today's processors has helped, there are still many tasks in VLSI CAD which continue to take a long time to finish. A recent approach to handling the problem's complexity and decreasing the running time of such tasks has been to apply parallel processing [1]. The advantages of parallel processing include: the ability to solve larger problems sizes, the ability to achieve high-quality results, and the availability of low-cost multiprocessors. Some of the tasks in the automatic design of integrated circuits which have been solved with parallel processing include the following: floor planning [2]",
            "group": 1425,
            "name": "10.1.1.57.3994",
            "keyword": "",
            "title": "Parallel Algorithms For Placement And Routing In VLSI Design"
        },
        {
            "abstract": "We consider the problem of designing ring networks for metropolitan area networks. Given a number of nodes representing locations that may be connected, the task is to construct a ring network by selecting a node subset and corresponding direct links. Any two nodes on the ring are enabled to communicate with each other so that the network provider gains a certain revenue. On the other hand, construction costs are incurred for the design of each direct link. The basic objective is to maximize the sum of all revenues minus the construction costs while building a ring network. We discuss certain relationships to other problems. Mathematical models are presented and used to obtain optimal solutions for small problem instances and upper bounds. We focus on the application of modern heuristic search concepts by means of a framework with generic components for heuristic search which enables the efficient adaptation to real world problems.",
            "group": 1426,
            "name": "10.1.1.57.4091",
            "keyword": "",
            "title": "Ring Network Design for Metropolitan Area Networks"
        },
        {
            "abstract": "In this study we have tackled the NP-hard problem of academic class scheduling (or timetabling) at the university level. We have investigated a variety of approaches based on simulated annealing, including mean-field annealing, simulated annealing with three different cooling schedules, and the use of a rule-based preprocessor to provide a good initial solution for annealing. The best results were obtained using simulated annealing with adaptive cooling and reheating as a function of cost, and a rule-based preprocessor. This approach enabled us to obtain valid schedules for the timetabling problem for a large university, using a complex cost function that includes student preferences. None of the other methods were able to provide a complete valid schedule.   Current address.  1 1 Introduction  The primary objective of this study is to derive an approximate solution to the problem of university class scheduling, or timetabling, which can be summarized as follows: given data sets of cl...",
            "group": 1427,
            "name": "10.1.1.57.4092",
            "keyword": "",
            "title": "A Comparison of Annealing Techniques for Academic Course Scheduling"
        },
        {
            "abstract": "Traditional connectionist theory-refinement systems map the dependencies of a domainspecific rule base into a neural network, and then refine this network using neural learning techniques. Most of these systems, however, lack the ability to refine their network's topology and are thus unable to add new rules to the (reformulated) rule base. Therefore, on domain theories that are lacking rules, generalization is poor, and training can corrupt the original rules, even those that were initially correct. We present TopGen, an extension to the Kbann algorithm, that heuristically searches for possible expansions to Kbann's network. TopGen does this by dynamically adding hidden nodes to the neural representation of the domain theory, in a manner analogous to adding rules and conjuncts to the symbolic rule base. Experiments indicate that our method is able to heuristically find effective places to add nodes to the knowledge bases of four realworld problems, as well as an artificial chess domai...",
            "group": 1428,
            "name": "10.1.1.57.4094",
            "keyword": "Nodes to Knowledge-Based Neural Networks",
            "title": "Dynamically Adding Symbolically Meaningful Nodes to Knowledge-Based Neural Networks"
        },
        {
            "abstract": "We show finite-time regret bounds for the multiarmed bandit problem under the assumption that all rewards come from a bounded and fixed range. Our regret bounds after any number T of pulls are of the form a+b log T +c log  2  T , where  a, b, and c are positive constants not depending on T . These bounds are shown to hold for variants of the popular \"-greedy and Boltzmann allocation rules, and for a new simple deterministic allocation rule. Moreover, our results also apply to an extension of the basic bandit problem in which reward distributions can depend, to some extent, from previous pulls and observed rewards. Finally, we discuss the empirical performance of our algorithms with respect to specific choices of the reward distributions. 1 INTRODUCTION  One of the fundamental issues in reinforcement learning is the exploration versus exploitation dilemma, whose simplest instance is, perhaps, the bandit problem. In its most basic formulation, a bandit problem is a set of N (with  N ? 1)...",
            "group": 1429,
            "name": "10.1.1.57.4710",
            "keyword": "",
            "title": "Finite-time Regret Bounds for the Multiarmed Bandit Problem"
        },
        {
            "abstract": "A new Reactive Local Search (RLS ) algorithm is proposed for the solution of the Maximum-Clique problem. RLS is based on local search complemented by a feedback (memory-based) scheme to determine the amount of diversification. The reaction acts on the single parameter that decides the temporary prohibition of selected moves in the neighborhood, in a manner inspired by Tabu Search. The performance obtained in computational tests appears to be significantly better with respect to all algorithms tested at the the second DIMACS implementation challenge. The worst-case complexity per iteration of the algorithm is O(maxfn;mg) where n and m are the number of nodes and edges of the graph. In practice, when a vertex is moved, the number of operations tends to be proportional to its number of missing edges and therefore the iterations are particularly fast in dense graphs.  Key words: maximum clique problem, heuristic algorithms, tabu search, reactive search.   Dipartimento di Matematica, Univer...",
            "group": 1430,
            "name": "10.1.1.57.4711",
            "keyword": "search",
            "title": "Reactive Local Search for the Maximum Clique Problem"
        },
        {
            "abstract": "this paper, we first present an overview of the various architecture synthesis tasks and analyze their influence on power consumption. A survey of previously proposed techniques is given, and areas of opportunity are identified. We next propose a new architecture synthesis technique for low-power implementation of real-time applications. The technique uses algorithm partitioning to preserve locality in the assignment of operations to hardware units. Preserving locality results in more compact layouts, reduced usage of long high-capacitance buses, and reduced power consumption in multiplexors and buffers. Experimental results show reductions in bus and multiplexor power of up to 80% and 60%, respectively, resulting in 10-25% reduction in total power. 1. Introduction",
            "group": 1431,
            "name": "10.1.1.57.5320",
            "keyword": "",
            "title": "Low-Power Architectural Synthesis and the Impact of Exploiting Locality"
        },
        {
            "abstract": "We present an algorithm for inducing Bayesian networks using feature selection. The algorithm selects a subset of attributes that maximizes predictive accuracy prior to the network learning phase, thereby incorporating a bias for small networks that retain high predictive accuracy. We compare the behavior of this selective Bayesian network classifier with that of (a) Bayesian network classifiers that incorporate all attributes, (b) selective and non-selective naive  Bayesian classifiers, and (c) the decision-tree algorithm C4.5. With respect to (a), we show that our approach generates networks that are computationally simpler to evaluate but display comparable predictive accuracy. With respect to (b), we show that the selective Bayesian network classifier performs significantly better than both versions of the naive Bayesian classifier on almost all databases studied, and hence is an enhancement of the naive method. With respect to (c), we show that the selective Bayesian network class...",
            "group": 1432,
            "name": "10.1.1.57.5366",
            "keyword": "Bayesian networksnaive Bayesian classifierfeature selection",
            "title": "Induction of Selective Bayesian Network Classifiers"
        },
        {
            "abstract": "This paper presents a hardware-software (HW-SW) partitioning algorithm to be used in HW-SW codesign of an embedded real-time system. The algorithm interacts with the period calibration method proposed in [4, 5, 6] such that the period assignment and HWSW partitioning of real-time tasks are considered in a single framework. The partitioning algorithm makes use of three heuristics and one random transformation rule in order to quickly find a feasible HW-SW partition which most likely leads to the minimum HW cost design. As an experimental study, two partitioning algorithms, one that is based on the proposed heuristics and the other based on simulated annealing have been implemented. We have performed preliminary experiments and present the result.  1 Introduction  Hardware-software partitioning is an important problem in designing an embedded real-time system where the design specification is implemented with both HW (ASIC, ASIP, etc) and SW (microprocessor, controller, etc) components. ...",
            "group": 1433,
            "name": "10.1.1.57.5601",
            "keyword": "",
            "title": "Hardware-Software Codesign of Resource-Constrained Real-Time Systems"
        },
        {
            "abstract": " This paper describes a thorough comparison of ten different search techniques applied to a wing-box design optimisation problem. The techniques used vary from deterministic gradient descent to stochastic Simulated Annealing (SA) and Genetic Algorithms (GAs). The stochastic techniques produced as good solutions as the best found by the deterministic techniques. However, only the stochastic techniques consistently produced very good solutions every run. Significantly, only a distributed genetic algorithm (DGA) and hybrid methods (SA with gradient descent, DGA with gradient descent) had a reliable fast decent to good regions of solution space. Of these the hybrid DGA was significantly better than anything else. The issue of generating solutions stable to perturbations of the problem variables, without greatly increasing the runtime of the objective function, is also discussed. We describe a method for producing highly stable solutions with the DGA while increasing the run time of the ob...",
            "group": 1434,
            "name": "10.1.1.57.5896",
            "keyword": "",
            "title": "A Comparison of Search Techniques on a Wing-Box Optimisation Problem"
        },
        {
            "abstract": "An efficient recursive task allocation scheme, based on the Kernighan-Lin mincut bisection heuristic, is proposed for the effective mapping of tasks of a parallel program onto a hypercube parallel computer. It is evaluated by comparison with an adaptive, scaled simulated annealing method. The recursive allocation scheme is shown to be effective on a number of large test task graphs -- its solution quality is nearly as good as that produced by simulated annealing, and its computation time is several orders of magnitude less.  Task Allocation by Recursive Bipartitioning 3 List of Symbols used: V Italic letter (capital) \"vee\"  E Italic letter (capital) \"ee\"  k  th  Italic \"kay\" superscript \"th\"  N Italic letter (capital) \"en\"  i Italic letter (lower case) \"eye\"  j Italic letter (lower case) \"jay\"  w i Italic \"double u\" subscript \"eye\"  c ij Italic \"see\" subscript \"eye\" and \"jay\"  P Italic letter (capital) \"pee\"  E p Italic letter (capital) \"ee\" subscript \"pee\"  K Italic letter (capital) ...",
            "group": 1435,
            "name": "10.1.1.57.5926",
            "keyword": "",
            "title": "Task Allocation onto a Hypercube by Recursive Mincut Bipartitioning"
        },
        {
            "abstract": "Partially Observable Markov Decision Processes (POMDPs) cope with sequential decision processes where an agent tries to maximize some reward without complete knowledge of the process. These models are of interest for quality control, machine maintenance, reinforcement learning, etc. More generally, Monahan [9] has shown that many tasks in partially observable environments can be viewed as POMDPs. A solution for the POMDP gives the best behavior of the agent face to the environment. This gives a solution over all the state space, which is continuous and inside of an integral polytope. The approaches proposed until now use linear programming (LP) to solve the optimization problem in this type of processes. By other side, Neural Networks (NNs) have shown a promising potentiality for finding solutions to optimization problems; particularly, they have been used to solve quadratic 0-1 programming problems [4, 6]. In this paper, we use optimization neural networks as a different way to solve the optimization problem in the POMDP, which allows a parallel hardware implementation.",
            "group": 1436,
            "name": "10.1.1.57.6681",
            "keyword": "",
            "title": "Solving Partially Observable Markov Decision Processes by Optimization Neural Networks"
        },
        {
            "abstract": "A general introduction to the use of feed-back artificial neural networks (ANN) for obtaining good approximate solutions to combinatorial optimization problems is given, assuming no previous knowledge in the field. In particular we emphasize a novel neural mapping technique which efficiently reduces the solution space. This approach maps the problems onto Potts glass rather than spin glass models. The real strength in mapping optimization problems onto neural systems lies in the fact that local minima in the cost functions can be avoided with the use of mean field equations. The system \"feels\" its way towards good solutions. In the settling process it may encounter phase transitions. A systematic prescription can be given for estimating the phase transition temperatures in advance, which facilitates an automatized choice of optimal parameters. This analysis, which can be performed for both serial and synchronous updating of the mean field theory equations, also makes it possible to consistently avoid chaotic behaviour. These techniques are illustrated for the graph partition and the traveling salesman problems (TSP). Also, a realistic high school scheduling problem is dealt with in some detail. These problems are all characterized by equality constraints. The formalism can also be modified to deal with inequality constraints. This is illustrated with the knapsack problem. We also discuss...",
            "group": 1437,
            "name": "10.1.1.57.7267",
            "keyword": "",
            "title": "Combinatorial Optimization with Neural Networks"
        },
        {
            "abstract": "There is a well-developed theory about the algorithmic complexity  of optimization problems. Complexity theory provides negative  results which typically are based on assumptions like NP#=P or NP#=RP.",
            "group": 1438,
            "name": "10.1.1.57.7479",
            "keyword": "",
            "title": "Towards a Theory of Randomized Search Heuristics"
        },
        {
            "abstract": "Automating the scheduling of sport leagues has received considerable attention in recent  years, as these applications involve significant revenues and generate challenging combinatorial  optimization problems. This paper considers the traveling tournament problem (TTP)  proposed in [10, 4] to abstract the salient features of major league baseball (MLB) in the  United States. It proposes a simulated annealing algorithm (TTSA) for the TTP that explores  both feasible and infeasible schedules, uses a large neighborhood with complex moves, and  includes advanced techniques such as strategic oscillation and reheats to balance the exploration  of the feasible and infeasible regions and to escape local minima at very low temperatures.",
            "group": 1439,
            "name": "10.1.1.57.7545",
            "keyword": "",
            "title": "A Simulated Annealing Approach to the Traveling Tournament Problem"
        },
        {
            "abstract": "this paper was submitted for publication a paper by Simpson was submitted entitled Scheduling a bridge club using a genetic algorithm (this issue). The author solve the same scheduling problem, but using a genetic algorithm",
            "group": 1440,
            "name": "10.1.1.57.7598",
            "keyword": "",
            "title": "Scheduling a Bridge Club by Tabu Search"
        },
        {
            "abstract": "We perform a stochastic analysis of evolutionary algorithms. The analysis centers on the question how to efficiently compute probabilities of promising alleles derived from evolving populations under selection and how to use these probabilities to generate new points. We shortly discuss the Univariate Marginal Distribution Algorithm (UMDA). It uses univariate marginals to generate new search points. We extend UMDA to the Factorized Distribution Algorithm (FDA) which uses a factorization of the Boltzmann distribution. We describe a well known algorithm to compute a factorization based on junction trees. We explain the sampling method of FDA and discuss the difference to Simulated Annealing. We introduce mutation into the algorithm with the help of a Bayesian hyper parameter. We show that FDA using Boltzmann selection fulfills an equation which Holland claimed to be necessary for an almost \"optimal\" algorithm. We formulate FDA as a population dynamics algorithm. We conclude with a short discussion about the interdisciplinary research to approximate distributions, especially the Boltzmann distribution.",
            "group": 1441,
            "name": "10.1.1.57.8573",
            "keyword": "",
            "title": "Evolutionary Algorithms and the Boltzmann Distribution"
        },
        {
            "abstract": "Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard. We describe deterministic annealing (Rose et al., 1990) as an appealing alternative to the ExpectationMaximization algorithm (Dempster et al., 1977). Seeking to avoid search error, DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward",
            "group": 1442,
            "name": "10.1.1.57.8859",
            "keyword": "",
            "title": "Annealing Techniques for Unsupervised Statistical Language Learning"
        },
        {
            "abstract": "It is well known that the performance of a stochastic local  search procedure depends upon the setting of its noise parameter,  and that the optimal setting varies with the problem  distribution. It is therefore desirable to develop general  priniciples for tuning the procedures. We present two statistical  measures of the local search process that allow one  to quickly find the optimal noise settings. These properties  are independent of the fine details of the local search strategies,  and appear to be relatively independent of the structure  of the problem domains. We applied these principles to the  problem of evaluating new search heuristics, and discovered  two promising new strategies.",
            "group": 1443,
            "name": "10.1.1.57.8889",
            "keyword": "",
            "title": "Evidence for Invariants in Local Search"
        },
        {
            "abstract": "In the capacitated vehicle routing problem with time windows  (CVRPTW) we have a set of customer visits, each with a demand  and time window in which the visit must be serviced, and a eet of vehicles,  each vehicle of limited capacity. The problem is to visit all customers  whilst minimising total travel distance and the use of vehicles. In the jobshop  scheduling problem (JSSP) we have a set of jobs, composed of a  sequence of activities, and a set of resources. Each activity requires exclusive  use of a resource for a given amount of time. The problem is then to  sequence activities on resources such that all precedence constraints are  respected and the makespan is minimised. We can reformulate a VRP  to an open shop scheduling problem by representing visits as activities,  vehicles as resources on the factory oor, and travel as set up costs between  activities. We also have the inverse reformulation. In this paper  we present two reformulations: from VRP to open shop, and the inverse,  from JSSP to VRP. Not surprisingly, we show that VRP technology performs  poorly on reformulated JSSP, as does scheduling technology on  reformulated VRPs. We then present a pre-processing transformation  that \\compresses\" the VRP, transforming an element of travel into the  duration of the visits. The compressed VRPs are then reformulated as  scheduling problem, to determine if it is primarily distance in the VRP  that causes scheduling technology to degrade on the reformulated problem.",
            "group": 1444,
            "name": "10.1.1.57.9192",
            "keyword": "",
            "title": "On the Reformulation of Vehicle Routing Problems and Scheduling Problems"
        },
        {
            "abstract": "A new method is described for optimising graph partitions which arise in mapping unstructured mesh  calculations to parallel computers. The method employs a combination of iterative techniques to both  evenly balance the workload and minimise the number and volume of interprocessor communications. It  is designed to work efficiently in parallel as well as sequentially and can be applied directly to dynamically  refined meshes. In addition, when combined with a fast direct partitioning technique (such as the Greedy  algorithm) to give an initial partition, the resulting two-stage process proves itself to be both a powerful and  flexible solution to the static graph-partitioning problem. A clustering technique can also be employed to  speed up the whole process. Experiments, on graphs with up to a million nodes, indicate that the resulting  code is up to an order of magnitude faster than existing state-of-the-art techniques such as Multilevel  Recursive Spectral Bisection, whilst providing partitions of equivalent quality.",
            "group": 1445,
            "name": "10.1.1.57.9284",
            "keyword": "",
            "title": "A Parallelisable Algorithm for Optimising Unstructured Mesh Partitions"
        },
        {
            "abstract": "The effectiveness of the memory hierarchy is critical for the performance of current processors. The performance of the memory hierarchy can be improved by means of program transformations such as loop tiling, which is a code transformation targeted to reduce capacity misses. This paper presents a novel systematic approach to perform nearoptimal loop tiling based on an accurate data locality analysis (Cache Miss Equations) and a powerful technique to search the solution space that is based on a genetic algorithm. The results show that this approach can remove practically all capacity misses for all considered benchmarks. The reduction of replacement misses results in a decrease of the miss ratio that can be as significant as a factor of 7 for the matrix multiply kernel.",
            "group": 1446,
            "name": "10.1.1.57.9725",
            "keyword": "",
            "title": "Near-Optimal Loop Tiling by Means of Cache Miss Equations and Genetic Algorithms"
        },
        {
            "abstract": "Can stochastic search algorithms outperform existing deterministic heuristics for the NP-hard problem   if given a su#cient, but practically realizable amount of time? In a thorough empirical investigation using a straightforward implementation of one such algorithm, simulated annealing, Johnson et al. #1991# concluded tentatively that the answer is #no.\" In this paper we show that the answer can be #yes\" if attention is devoted to the issue of problem representation #encoding#. We present results from empirical tests of several encodings of Number Partitioning with problem instances consisting of multiple-precision integers drawn from a uniform probability distribution. With these instances and with an appropriate choice of representation, stochastic and deterministic searches can---routinely and in a practical amount of time---#nd solutions several orders of magnitude better than those constructed by the best heuristic known #Karmarkar and Karp, 1982#, which does not employ searching. The choice of encoding is found to be more important than the choice of search technique in determining search e#cacy. Three alternative explanations for the relative performance of the encodings are tested experimentally. The best encodings tested are found to contain a high proportion of good solutions; moreover, in those encodings, the solutions are organized into a single #bumpy funnel\" centered at a known position in the search space. This is likely to be the only relevant structure in the search space because a blind search performs as well as any other search technique tested when the search space is restricted to the funnel tip. To appear in the Journal of Optimization Theory and Applications, 1995. This work may not be copied or reproduced in whole or in part for any commercial ...",
            "group": 1447,
            "name": "10.1.1.58.344",
            "keyword": "",
            "title": "Easily Searched Encodings for Number Partitioning"
        },
        {
            "abstract": " ",
            "group": 1448,
            "name": "10.1.1.58.500",
            "keyword": "",
            "title": "Planning and Scheduling"
        },
        {
            "abstract": "We apply the optimization algorithm Adaptive Simulated Annealing (ASA) to the  problem of analyzing data on a large population and selecting the best model to predict  the probability that an individual with various traits will have a particular disease. We  compare ASA with traditional forward and backward regression on computer simulated  data. We found that the traditional methods of modeling are best for smaller  datasets whereas a numerically stable ASA seems to perform better on larger and more  complicated datasets.",
            "group": 1449,
            "name": "10.1.1.58.809",
            "keyword": "articial intelligencemodelingsimulated annealing",
            "title": "Using Artificial Intelligence for Model Selection"
        },
        {
            "abstract": "The main objective of this research is the development of a temporal reasoning system able to manage qualitative and quantitative information also affected by vagueness and uncertainty; this is, in fact, the usual way in which temporal information about the external reality reaches us.",
            "group": 1450,
            "name": "10.1.1.58.1100",
            "keyword": "",
            "title": "Towards a Scheduling Application Using Fuzzy Temporal Constraints"
        },
        {
            "abstract": "",
            "group": 1451,
            "name": "10.1.1.58.1586",
            "keyword": "",
            "title": "Memetic Algorithms for Combinatorial Optimization Problems: Fitness Landscapes and Effective Search Strategies"
        },
        {
            "abstract": "",
            "group": 1452,
            "name": "10.1.1.58.1716",
            "keyword": "",
            "title": " Automated Internet Trading Based on Optimized Physics Models of Markets"
        },
        {
            "abstract": "Introduction  Many countries have committed to conserving significant amounts of their native biodiversity (McNeely et al. 1990). Biodiversity includes the diversity of ecosystems and the diversity between and within species. Establishing reserve systems dedicated to nature conservation is the cornerstone of most national, regional and state conservation strategies (Soul 1991). In this chapter, we explore the question of which parcels of land, henceforth termed sites, should be selected for reservation. Implicit in our analysis is a limit on the total size of the reserve system. From the perspective of nature conservation alone, one would attempt to have the largest reserve system possible; however, in reality, the extent of any reserve system will be limited by social and economic constraints. Thus, building a reserve network that will conserve biodiversity effectively is not a process of accumulating as much land as possible, but, rather, necessitates doing so as efficiently as possi",
            "group": 1453,
            "name": "10.1.1.58.1735",
            "keyword": "",
            "title": "Mathematical methods for identifying representative reserve networks"
        },
        {
            "abstract": "This paper presents a novel approach to the detection and  recognition of qualitative parts like geons from real 2D intensity images. Previous",
            "group": 1454,
            "name": "10.1.1.58.1769",
            "keyword": "",
            "title": "Recognition of Geons by Parametric Deformable Contour Models"
        },
        {
            "abstract": "this article can be found at doi: 10.1016/j.jmb.2004.03.044  E-mail address of the corresponding author:  daniel.dimaio@yale.edu  Abbreviations used: PDGF, platelet-derived growth  factor; CAT, chloramphenicol acetyltransferase",
            "group": 1455,
            "name": "10.1.1.58.2344",
            "keyword": "",
            "title": "Selection and Characterization of Small Random"
        },
        {
            "abstract": "In the Internal Border Gateway Protocol (IBGP), route reflection is widely used as an alternative to full mesh IBGP sessions inside an AS for scalability reason. However, some important issues, such as the impact of route reflection on the reliability of IBGP and the construction of reliable reflection topology with unreliable routers or links, have not been well investigated.",
            "group": 1456,
            "name": "10.1.1.58.2371",
            "keyword": "",
            "title": "Reliability-aware IBGP Route Reflection Topology Design"
        },
        {
            "abstract": " ",
            "group": 1457,
            "name": "10.1.1.58.3651",
            "keyword": "",
            "title": "Simulated Annealing: Practice versus Theory"
        },
        {
            "abstract": "This dissertation presents an exploration on solving traffic load balancing problems concentrating  on the IP layer in hop-by-hop networks. The first part of the traffic load balancing research is focused on bandwidth-sensitive routing for premium class traffic in Differentiated Service (DiffServ)  networks, where bandwidth usage of the premium class traffic is critical not only for the traffic itself, but also for other classes of traffic with lower priorities in the same network, such as the assured or best effort traffic. If the traffic in a network is splittable, then the second part of the load balancing research is intra-domain traffic engineering in the network, where the bandwidthsensitive routing solutions in the first part can be used to achieve better traffic load balancing results in this part.",
            "group": 1458,
            "name": "10.1.1.58.4543",
            "keyword": "",
            "title": "Load Balancing In Hop-By-Hop Routing With And Without Traffic Splitting"
        },
        {
            "abstract": "Sensor networks have recently gained a lot of attention from the research community. Sensors are significantly resource-constrained devices and last till the depletion of their batteries. Sensor networks typically have a large number of nodes. To ensure scalability sensor networks are often partitioned into clusters, each managed by a cluster head (gateway). Efficient management of a sensor network for extending the lifetime of the network is among the prominent areas of research in this domain. While most of the previous research focused on the optimal use of sensor's energy, very little attention has been paid to the efficiency of energy usage at the gateway. Tasks need to be allocated to gateways in such a way that maximizes the life of these cluster-heads and eventually the whole network. In this paper, we present an optimization scheme for task allocation to gateways. The task allocation problem is modeled as a zero-one nonlinear program. Simulation results show that substantial energy savings can be obtained with the proposed method.",
            "group": 1459,
            "name": "10.1.1.58.4751",
            "keyword": "",
            "title": "Optimization of Task Allocation in a Cluster-Based Sensor Network"
        },
        {
            "abstract": "Adaptive simulated annealing (ASA) is a global optimization algorithm based on an  associated proof that the parameter space can be sampled much more efficiently than by  using other previous simulated annealing algorithms. The author's ASA code has been  publicly available for over two years. During this time the author has volunteered to help  people via e-mail, and the feedback obtained has been used to further develop the code. Some lessons",
            "group": 1460,
            "name": "10.1.1.58.5091",
            "keyword": "",
            "title": "Adaptive Simulated Annealing (ASA): Lessons Learned"
        },
        {
            "abstract": "In this tutorial introduction to simulation optimization, we present motivating and illustrative examples, summarize most of the major approaches, and briefly describe some software implementations. The focus is on issues and concepts, rather than mathematical rigor, so the format is Q & A rather than theorem-proof.",
            "group": 1461,
            "name": "10.1.1.58.5799",
            "keyword": "",
            "title": "Proceedings of the 2001 Winter Simulation Conference"
        },
        {
            "abstract": "this paper, we compare the performance of three typical and widely used optimization techniques for a specific MEG source localization problem. We first introduce a hybrid algorithm by combining genetic and local search strategies to overcome disadvantages of conventional genetic algorithms. Second, we apply the tabu search, a widely used optimization method in combinational optimization and discrete mathematics, to source localization. To the best of our knowledge, this is the first attempt in the literature to apply tabu search to MEG=EEG source localization. Third, in order to further compare the performance of the above algorithms, simulated annealing is also applied to MEG source localization problem. The computer simulation results show that our local genetic algorithm is the most effective approach to dipole localization, and the tabu search method is also a very good strategy for this problem. Keywords: Magnetoencephalogram (MEG); Dipoles; Global optimization; Genetic algorithms; Simulated annealing; Tabu search  C. R. Categories: G.1.6, I.2.8  1 ",
            "group": 1462,
            "name": "10.1.1.58.6678",
            "keyword": "Magnetoencephalogram (MEGDipolesGlobal optimizationGenetic algorithmsSimulated annealingTabu search C. R. CategoriesG.1.6I.2.8",
            "title": "A Comparative Study Of Global Optimization Approaches To Meg Source Localization"
        },
        {
            "abstract": "Maximum a Posteriori assignment (MAP) is  the problem of finding the most probable instantiation  of a set of variables given the  partial evidence on the other variables in a  Bayesian network. MAP has been shown to  be a NP-hard problem [22], even for constrained  networks, such as polytrees [18].",
            "group": 1463,
            "name": "10.1.1.58.6766",
            "keyword": "",
            "title": "Annealed MAP"
        },
        {
            "abstract": "Constraints on downside risk, measured by shortfall probability, expected shortfall etc., lead to optimal asset allocations which differ from the mean-variance optimum. The resulting optimization problem can become quite complex as it exhibits multiple local extrema and discontinuities, in particular if we also introduce constraints restricting the trading variables to integers, constraints on the holding size of assets or on the maximum number of different assets in the portfolio. In such situations classical optimization methods fail to work efficiently and heuristic optimization techniques can be the only way out. The paper shows how a particular optimization heuristic, called threshold accepting, can be successfully used to solve complex portfolio choice problems.",
            "group": 1464,
            "name": "10.1.1.58.6995",
            "keyword": "JEL codesG11C61C63. KeywordsPortfolio OptimizationDownside Risk MeasuresHeuristic OptimizationThreshold Accepting. HEURISTIC PORTFOLIO OPTIMIZATION 3",
            "title": "A Global Optimization Heuristic for Portfolio Choice with VaR and Expected Shortfall"
        },
        {
            "abstract": "The object of this paper is to present aform ulation for thesegmF tation and restorationproblem using flexiblem dels with a robust regularized network (RRN). A two-steps iterativealgorithm is presented. In the first step an approximoh-- of the classification iscomJq-( by using a localmh)q)F)hfl-5 algorithm and in the second step the param-5hfl of the RRN are updated. The use of robust potentials ismh)N ated by (a) classification errors that can resultfrom the use of localmlh))FfiE algorithm in theimh-fiF) tation, and (b) the need to adapt the RN using localimhgradientinformfl -fi) to imfiE ve fidelity of them odel to the data.",
            "group": 1465,
            "name": "10.1.1.58.7814",
            "keyword": "",
            "title": "Image Segmentation by Flexible Models Based on Robust Regularized Networks"
        },
        {
            "abstract": "We present a peer-level protocol for forming adaptive, self-organizing topologies for data-sharing P2P networks. This protocol is based on the idea that a peer should directly connect to those peers from which it is most likely to download satisfactory content. We show that the resulting topologies are more efficient than standard Gnutella topologies. Furthermore, we show that these adaptive topologies have the added benefits of increased resistance to certain types of attacks, intrinsic rewards for active peers and punishments for malicious peers and freeriders.",
            "group": 1466,
            "name": "10.1.1.58.9492",
            "keyword": "",
            "title": "Adaptive Peer-To-Peer Topologies"
        },
        {
            "abstract": "Four experiments explored participants' understanding of the abstract principles goincipl coinci simulatios o coulat adaptive systems. Experiments 1, 2, and 3shoBU better transfero abstract principlesacroc simulatioA that were relatively dissimilar, and that this e#ect was dueto participantswho perfocip relativelypolat o the initialsimulatioB In Experiment 4, participantsshoic better abstract understandingo asimulatio when it was depicted withcohA@CU rather than idealized graphical elements.Homents fo pom perfos.Aq/ the idealizedversio o the simulatio transferred betterto a newsimulatio gomulat by the same abstractioU The results are interpreted in termso cosAq6BP--A between abstract and codAP)U coAP)U@/A o thesimulatio)/ Individualsproi toiv coivid coividual tendto oodAPU abstractioH whenconA)C@ pro)C@qUA o superficial similarities are salient.",
            "group": 1467,
            "name": "10.1.1.58.9837",
            "keyword": "",
            "title": "The Transfer of Abstract  Principles Governing  Complex Adaptive Systems"
        },
        {
            "abstract": "Fractal terrains provide an easy way to generate realistic landscapes. There are  several methods to generate fractal terrains, but none of those algorithms allow the  user much flexibility in controlling the shape of the final outcome. A few methods to  modify fractal terrains have been presented, both algorithm-based as well as by hand  editing, but none of these provide a general solution.",
            "group": 1468,
            "name": "10.1.1.59.548",
            "keyword": "",
            "title": "An Algorithm for Automated Fractal Terrain Deformation"
        },
        {
            "abstract": "Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novel Markov chain Monte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.",
            "group": 1469,
            "name": "10.1.1.59.1643",
            "keyword": "",
            "title": "An MCMC Approach to Solving Hybrid Factored MDPs"
        },
        {
            "abstract": "In the Broadcast Scheduling Problem (BSP), a finite set of stations are to be scheduled in a time division multiple access (TDMA) frame. In a TDMA frame, time is divided into equal length transmission slots. Unconstrained message transmission can result in a collision of messages, rendering them useless. Therefore, the objective of the BSP is to provide a collision free broadcast schedule which minimizes the total frame length and maximizes the slot utilization within the frame. In this chapter, we introduce the BSP, show that it is NP -complete, and discuss several heuristics which have been applied to the problem. The heuristics are tested on over 60 networks of varying sizes and densities and the results are compared.",
            "group": 1470,
            "name": "10.1.1.59.1719",
            "keyword": "Broadcast Scheduling ProblemAd-hoc NetworksCombinatorial OptimizationNP-completeHeuristics",
            "title": "On The Performance Of Heuristics For Broadcast Scheduling"
        },
        {
            "abstract": "New application scenarios, such as Internet-scale computations, nomadic networks and mobile systems, require decentralized, scalable and open infrastructures. The peerto -peer (P2P) paradigm has been recently proposed to address the construction of completely decentralized systems for the above mentioned environments, but P2P systems frequently lack of dependability. In this paper, we propose an algorithm for increasing fault-tolerance by dynamically adding redundant links to P2P systems with unstructured topology. The algorithm requires only local interactions, is executed asynchronously by each peer and guarantees that the disappearance of any single peer does not affect the overall performance and routing capabilities of the system. 1 ",
            "group": 1471,
            "name": "10.1.1.59.1909",
            "keyword": "",
            "title": "Fault-Tolerant Routing for P2P Systems with Unstructured Topology"
        },
        {
            "abstract": "Multimedy applications arecharacterized bytheir strong timing requirementsand constraintsand thusmultimedy dul storage is a critical issue in the overall system's performanceand functionality. This paperdpery)%% multimedfi dul representationmodre that e#ectivelyguid dec placementtoward the improvement of the Qualityof Presentation for the considxyw multimedw applications. The performance of both constructive   iterative improvement placement algorithms isevaluated and dduated Emphasis is given on placement schemes which arebased on the simulatedannealing optimization algorithm. A placement policy,based on a self-improving version of thesimulated annealing (SISA) algorithm isapplied and evaluated Performance of the placement policies is experimentallyevaluated on asimulated tertiarystorage subsystem. As proven bythe experimentation, the proposed approach shows considI5:yw gain in terms of seekand service times. The improvements of theproposed SISA approach are in the range of 40% whencompared torand5 placementand at the range of 15--35% whencompared to the typicalsimulated annealing algorithm,dgorithm a lot on the initial configurationand theneighborhood search.",
            "group": 1472,
            "name": "10.1.1.59.2450",
            "keyword": "",
            "title": "A Simulated Annealing Approach for Multimedia Data Placement"
        },
        {
            "abstract": " Many computer vision problems can be formulated as graph partition problems that minimize energy functions. Generally applicable algorithms like the Gibbs sampler can perform the minimization task, but they are very slow to converge, especially since the graphs in vision tasks are large (10 -10 nodes). On the other hand, computationally e#ective algorithms like Graph Cuts and Belief Propagation are specialized to particular forms of energy functions, and they cannot be applied for complex statistical models using generative models and high-order priors. In this thesis, a new stochastic algorithm capable of sampling arbitrary energy functions defined on graph partitions is presented. To increase e#ciency, the algorithm uses the image information to make informed jumps in the search space. The image information is given in the form of edge weights and represents an empirical probability that the nodes connected by the edge belong to the same object. At each step, the algorithm creates clusters of nodes by turning on/o# the edges randomly according to their weights, and changes the label of all nodes in one cluster (connected component) in a single move. Each move is accepted or rejected according to an acceptance probability given by a simple and explicit xvii equation. The algorithm is applied to 4 important problems in computer vision: image segmentation, perceptual organization, stereo matching and motion segmentation. To address di#erent computational or representational issues, multigrid, multi-level and multi-cue variants of the algorithm are presented. In image segmentation, the algor...",
            "group": 1473,
            "name": "10.1.1.59.2777",
            "keyword": "2 The Swendsen-Wang Cuts Algorithm............... 7 2.1 BackgroundThe Swendsen-Wang Algorithm and its interpreta- tions.................................. 7 2.1.1 Swen",
            "title": "Cluster Sampling and its Applications to Segmentation, Stereo and Motion"
        },
        {
            "abstract": "Vision tasks, such as segmentation, grouping, recognition, can be formulated as graph partition problems. The recent literature witnessed two popular graph cut algorithms: the Ncut using spectral graph analysis and the minimum-cut using the maximum flow algorithm. This paper presents a third major approach by generalizing the Swendsen-Wang method-- a well celebrated algorithm in statistical mechanics. Our algorithm simulates ergodic, reversible Markov chain jumps in the space of graph partitions to sample a posterior probability. At each step, the algorithm splits, merges, or re-groups a sizable subgraph, and achieves fast mixing at low temperature enabling a fast annealing procedure. Experiments show it converges in 230 seconds in a PC for image segmentation. This is 400 times faster than the single-site update Gibbs sampler, and 20-40 times faster than the DDMCMC algorithm. The algorithm can optimize over the number of models and works for general forms of posterior probabilities, so it is more general than the existing graph cut approaches.",
            "group": 1474,
            "name": "10.1.1.59.3042",
            "keyword": "",
            "title": "Graph Partition by Swendsen-Wang Cuts"
        },
        {
            "abstract": "The use of metaheuristic search techniques for the automatic generation of test data has been a burgeoning interest for many researchers in recent years. Previous attempts to automate the test generation process have been limited, having been constrained by the size and complexity of software, and the basic fact that in general, test data generation is an undecidable problem. Metaheuristic search techniques offer much promise in regard to these problems. Metaheuristic search techniques are high-level frameworks, which utilise heuristics to seek solutions for combinatorial problems at a reasonable computational cost. To date, metaheuristic search techniques have been applied to automate test data generation for structural and functional testing; the testing of grey-box properties, for example safety constraints; and also non-functional properties, such as worst-case execution time. This paper surveys some of the work undertaken in this field, discussing possible new future directions of research for each of its different individual areas.",
            "group": 1475,
            "name": "10.1.1.59.3137",
            "keyword": "data generation",
            "title": "Search-based Software Test Data Generation: A Survey"
        },
        {
            "abstract": "In contrast with the current Web search methods that essentially do document-level ranking and retrieval, we are exploring a new paradigm to enable Web search at the object level. We collect Web information for objects relevant for a specific application domain and rank these objects in terms of their relevance and popularity to answer user queries. Traditional PageRank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationships between objects. This paper introduces PopRank, a domain-independent object-level link analysis model to rank the objects within a specific domain. Specifically we assign a popularity propagation factor to each type of object relationship, study how di#erent popularity propagation factors for these heterogeneous relationships could affect the popularity ranking, and propose e#cient approaches to automatically decide these factors. Our experiments are done using 1 million CS papers, and the experimental results show that PopRank can achieve significantly better ranking results than naively applying PageRank on the object graph.",
            "group": 1476,
            "name": "10.1.1.59.4829",
            "keyword": "Web objectsPageRankPopRankLink analysis",
            "title": "Object-Level Ranking: Bringing Order to Web Objects"
        },
        {
            "abstract": "Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs...",
            "group": 1477,
            "name": "10.1.1.59.4882",
            "keyword": "",
            "title": "Generalizing Swendsen-Wang to Sampling Arbitrary Posterior Probabilities"
        },
        {
            "abstract": "Streaming applications such as video-based surveillance, habitat monitoring, and emergency response are good candidates for executing on high-performance computing (HPC) resources, due to their high computation and communication needs. Such an application can be represented as a coarse-grain dataflow graph, each node corresponding to a stage of the pipeline of transformations that may be applied to the data as it continuously streams through. Mapping such applications to HPC resources has to be sensitive to the computation and communication needs of each stage of the pipeline to ensure QoS criteria such as latency and throughput. Due to the dynamic nature of such applications, they are ideal candidates for using ambient HPC resources made available via the grid. Since grid has evolved out of traditional high-performance computing, the tools available, especially for scheduling, tend to be more appropriate for batch-oriented applications. We have developed a scheduler, called Streamline, that takes into account dynamic nature of the grid and runs periodically to adapt scheduling decisions using application requirements (per-stage computation and communication needs), application constraints (such as co-location of stages on the same node), and current resource availability. The scheduler is designed to be integrated with the existing grid framework using Globus Toolkit. The performance of Streamline is compared with an Optimal placement for small number of resources and approximation algorithms using Simulated Annealing for large resources and dataflow graphs. We have also compared Streamline with a baseline grid scheduler, E-Condor, built on top of Condor for streaming applications. For kernels of such streaming applications, we show that our heuristic performs close to...",
            "group": 1478,
            "name": "10.1.1.59.5159",
            "keyword": "",
            "title": "Streamline: A Scheduling Heuristic for Streaming Applications on the Grid"
        },
        {
            "abstract": "this paper, we investigate face recognition from  range data by facial profiles and surface. An e#cient symmetry plane detection method  for facial range data is presented to help extract facial profile. A global profile matching  method is then exploited to align and compare the two profiles without detecting fiducial  points that is often unreliable. The central profile and two kinds of horizontal profiles ---  nose-crossing profile and forehead-crossing profile --- are employed in recognition. For  each individual, a statistical model is built to represent the distinct discriminative capability  of the di#erent regions on the facial surface. It is then incorporated into a weighted  distance function to measure for the similarity of surfaces. The comparable experimental  results are achieved on a facial range data database with 120 individuals",
            "group": 1479,
            "name": "10.1.1.59.5294",
            "keyword": "3D face recognitionsurface matchingfacial profilesymmetry plane",
            "title": "3D Face Recognition From Range Data"
        },
        {
            "abstract": "An assemble to order policy considers a trade-off between the size of product portfolio and assembly lead time. The concept of modular design is often used to implement the assemble to order policy. Modular design impacts assembly of products and the supply chain. In particular storage, transportation and production are affected by the selected modular structure.",
            "group": 1480,
            "name": "10.1.1.59.5397",
            "keyword": "",
            "title": "Composition of modules' stock using Simulated Annealing"
        },
        {
            "abstract": "The focus of this paper is the design of a mechanism to help economic agents - either autonomously or cooperatively planning - to achieve Pareto-optimal allocation of resources via a completely decentralized coordination of a logistics network. Besides giving a classification and a short review of existing scheduling approaches capable for supply chain management, this article specifies and evaluates protocols employing time-depended price-functions. By performing simulations with the implemented protocol using well-known Job Shop Scheduling Problems as a benchmark we show the efficiency and feasibility of the designed mechanism. The approach enables each agent to exploit the external effects caused by resource constraints of its supply chain contractors by adapting its production planning. Additionally the systems capability to reconfigure itself in case of production resources failure is increased. The evaluation of the protocols concludes with a welfare analysis investigating the payoff distribution along the supply chain. Finally we conclude that future research on this topic should turn to learning agent systems to reduce communication costs.",
            "group": 1481,
            "name": "10.1.1.59.5468",
            "keyword": "",
            "title": "Coordination Of Supply Webs Based On Dispositive Protocols"
        },
        {
            "abstract": "In this paper, we propose an energy-efficient scatternet formation algorithm for Bluetooth based sensor networks. The algorithm is based on first computing a shortest path tree from the base station to all sensor nodes and then solving the degree constraint problem so that the degree of each node in the network is not greater than seven, which is a Bluetooth constaint. In this way, less amount of energy is spent in each round of communication in the sensor network. The algorithm also tries to balance the load evenly on the highenergy consuming nodes which are the nodes that are close to the base station. In this way, the lifetime of the first dying node is also prolonged. We obtained promising results in the simulations.",
            "group": 1482,
            "name": "10.1.1.59.5868",
            "keyword": "",
            "title": "An Energy-Efficient Scatternet Formation Algorithm for Bluetooth-Based Sensor Networks"
        },
        {
            "abstract": "This paper presents a time-series whole clustering system  that incrementally constructs a hierarchy of clusters. The Online DivisiveAgglomerative  Clustering (ODAC) system is an incremental implementation  of divisive analysis clustering, using the correlation between timeseries  as similarity measure. The system tests existing clusters by descending  order of diameters, looking for a possible binary split. If no  cluster deserves division, then the system searches for possible aggregation  of clusters. At each time step, only one splitting or one aggregation  might occur. Main features include a splitting criteria supported by the  Hoe#ding bound, a stopping criteria based on the divisive coe#cient  and an agglomerative phase which decreases the number of unneeded  clusters, also based on the divisive coe#cient which measures the amount  of divisive structure found. Preliminary results show competitive performance  on clustering time-series when compared to a simple batch divisive  analysis clustering algorithm.",
            "group": 1483,
            "name": "10.1.1.59.6192",
            "keyword": "",
            "title": "Hierarchical Time-Series Clustering for Data Streams"
        },
        {
            "abstract": "this paper we present an evolutionary algorithm which is capable of programming the so called \"Wang Tiles\" for the self-assembly of two-dimensional squares",
            "group": 1484,
            "name": "10.1.1.59.6662",
            "keyword": "",
            "title": "Automated Tile Design for Self-Assembly Conformations"
        },
        {
            "abstract": "Simulated annealing (SA) is a provably convergent optimiser for single-objective (SO) problems. Previously proposed MO extensions have mostly taken the form of an SO SA optimising a composite function of the objectives. We propose an MO SA utilising the relative dominance of a solution as the system energy for optimisation, eliminating problems associated with composite objective functions. We also propose a method for choosing perturbation scalings promoting search both towards and across the Pareto front.",
            "group": 1485,
            "name": "10.1.1.59.6709",
            "keyword": "",
            "title": "Dominance Measures for Multi-Objective Simulated Annealing"
        },
        {
            "abstract": "Rapid advances of microarray technologies are making it possible to analyze and manipulate large amounts of gene expression data. Clustering algorithms, such as hierarchical clustering, self-organizing maps, k-means clustering and fuzzy k-means clustering, have become important tools for expression analysis of microarray data. However, the need of prior knowledge of the number of clusters, k, and the fuzziness parameter, b, limits the usage of fuzzy clustering. Few approaches have been proposed for assigning best possible values for such parameters. In this paper, we use simulated annealing and fuzzy k-means clustering to determine the optimal parameters, namely the number of clusters, k, and the fuzziness parameter, b. Our results show that a nearly-optimal pair of k and b can be obtained without exploring the entire search space.",
            "group": 1486,
            "name": "10.1.1.59.7621",
            "keyword": "",
            "title": "A Simulated Annealing Approach to Find the Optimal Parameters for Fuzzy Clustering Microarray Data"
        },
        {
            "abstract": "This paper provides empirical results for the hypothesis that functionally partitioning a specification results in better satisfaction of hardware-part size and I/O constraints, and often yields better design performance than structural partitioning",
            "group": 1487,
            "name": "10.1.1.59.7844",
            "keyword": "General TermsDesign Additional Key Words and PhrasesBehavioral systhesisfunctional partitioningsystem-level design",
            "title": "Functional Partitioning Improvements Over  Structural Partitioning for Packaging Constraints and Synthesis-Tool Performance  "
        },
        {
            "abstract": "Timetabling problems are constraint optimization problems proven to be NP complete. Furthermore, evaluation of violations is costly, and there is no common data format for representing timetabling  problem instances. In this paper, a framework for designing memetic algorithms (MAs) to solve timetabling problems is described and a tool, named Final Exam Scheduler (FES) is introduced. FES is the first tool that accepts Timetabling Markup Language (TTML) documents as input. It utilizes an MA with an adaptive violation directed hierarchical hill climbing method for solving examination timetabling problem instances. Experimental results on a set of benchmark data indicate the success of MA.",
            "group": 1488,
            "name": "10.1.1.59.8155",
            "keyword": "",
            "title": "Final Exam Scheduler - FES"
        },
        {
            "abstract": "In this paper, we present a methodology for interpreting faults from three dimensional  seismic data. Faults are individual fractures across which there are visible offsets  of horizons (or rock layers). 3D seismic data - images of subsurface structure generated  by reflecting seismic waves off rock layers - have been used for hypothesizing subsurface  structures. Since interpretation of seismic data is a highly time-consuming task,  automated tools to assist the interpretation are crucial. Our work focuses on automating  the correlation of horizons across a fault so that helping in defining the fault's geometry.",
            "group": 1489,
            "name": "10.1.1.59.8470",
            "keyword": "",
            "title": "An Approach towards Automated Fault Interpretations in Seismic Data"
        },
        {
            "abstract": "We address the problem of efficiently ranking the best peers w.r.t. a  query with multiple, equally weighted predicates -- conjunctive queries -- in shortcut  overlay networks. This problem occurs when routing queries in unstructured  peer-to-peer networks, such as in peer-to-peer information retrieval applications.",
            "group": 1490,
            "name": "10.1.1.59.8595",
            "keyword": "",
            "title": "Community Based Ranking in Peer-to-Peer Networks"
        },
        {
            "abstract": "We consider a panel of experts asked to assign probabilities to events, both logically simple  and complex. The events evaluated by different experts are based on overlapping sets of  variables but may otherwise be distinct. The union of all the judgments will likely be  probabilistic incoherent. We address the problem of revising the probability estimates of  the panel so as to produce a coherent set that best represents the group's expertise.",
            "group": 1491,
            "name": "10.1.1.59.8748",
            "keyword": "",
            "title": "Aggregating Disparate Estimates of Chance"
        },
        {
            "abstract": "Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments.",
            "group": 1492,
            "name": "10.1.1.59.8907",
            "keyword": "drum classificationMel Frequency Cepstral CoefficientsSupport Vector MachineSimulated",
            "title": "A Simulated Annealing Optimization of Audio Features for Drum Classification"
        },
        {
            "abstract": "Heterogeneous or federated clusters are a significant special case of grid computing. An important factor in efficiently executing applications on such clusters is the mapping (or scheduling) of application tasks onto cluster nodes. The cost/benefit estimating service aims at facilitating the search for efficient mappings both initially and during the evolution of a computation. This service dynamically evaluates alternative mappings using system and application information. Building the necessary database for use with this service involves profiling applications and running benchmark suites on cluster nodes. Recent data obtained at Sandia National Laboratories, NM, demonstrate the existence of differences in computation and communication speeds between the nodes of even simple homogeneous clusters, as well as the feasibility of using such data for predicting the efficiency of application mappings.",
            "group": 1493,
            "name": "10.1.1.59.9677",
            "keyword": "",
            "title": "Developing a Cost/Benefit Estimating Service for Dynamic Resource Sharing in"
        },
        {
            "abstract": "Heterogeneous or federated clusters are a significant special case of grid computing. An important factor in efficiently executing applications on such clusters is the mapping (or scheduling) of application tasks onto cluster nodes. The cost/benefit estimating service aims at facilitating the search for efficient mappings both initially and during the evolution of a computation. This service dynamically evaluates alternative mappings using system and application information. Building the necessary database for use with this service involves profiling applications and running benchmark suites on cluster nodes. Recent data obtained at Sandia National Laboratories, NM, demonstrate the existence of differences in computation and communication speeds between the nodes of even simple homogeneous clusters, as well as the feasibility of using such data for predicting the efficiency of application mappings.",
            "group": 1494,
            "name": "10.1.1.60.399",
            "keyword": "",
            "title": "Developing a Cost/Benefit Estimating Service for Dynamic Resource Sharing in"
        },
        {
            "abstract": "Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments.",
            "group": 1495,
            "name": "10.1.1.60.427",
            "keyword": "drum classificationMel Frequency Cepstral CoefficientsSupport Vector MachineSimulated",
            "title": "A Simulated Annealing Optimization of Audio Features for Drum Classification"
        },
        {
            "abstract": "Non-Sequential Tool Interaction Strategies for Sea-of-Gates Layout Synthesis   Doctor of Philosophy in Computer Science    This research focuses on strategies for managing the interaction among tools for the automated VLSI layout synthesis of regular macro-modules, such as bit-slice datapaths. Compact layouts for such macro-modules may be constructed by including the module's external wiring in the leaf cell layout. However, in most automated layout systems, module and leaf cell layouts are performed by separate non-interacting tools. Leaf cell layouts are synthesized in advance, stored in a library, and then customized to fit the placement and wiring of a particular module via stretching and/or wiring personalization. This approach limits the possible customizations for a cell and the possible layouts for a module, particularly when the use of pre-fabricated transistor arrays (as in Sea-of-Gates) precludes the stretching of cells.",
            "group": 1496,
            "name": "10.1.1.60.822",
            "keyword": "",
            "title": "Non-Sequential Tool Interaction Strategies for Sea-of-Gates Layout Synthesis"
        },
        {
            "abstract": "This research offers a fundamental new approach to topology optimization. To date, topological synthesis approaches are simply augmentations of existing stochastic optimization techniques. While these algorithms are useful in complex parametric design spaces, they are not designed for all topology problems. This paper describes a generalized scheme for solving topological design problems. The approach combines aspects of existing optimization techniques, graph theory, mathematical programming, artificial intelligence, and shape and graph grammars. Specifically, the approach has advantages over modified parametric optimization algorithms in that 1) a two stage technique to developing solutions and searching the parametric design space more closely models the design process performed by humans and more thoroughly maps how various topologies are represented, 2) a graph grammar for generating candidate solutions minimizes the wasted search of infeasible topologies, and 3) a new stochastic guidance strategy intelligently distinguishes and independently controls the variables that alter topologies and the variables that represent dimensions and parameters. This automated design synthesis method is currently being developed to synthesize the best graph for a various applications, where the choice of nodes in the graph and how they are to be connected is determined through computational search. Such topological problems are prevalent in engineering design. Examples include truss structures, designing sheet metal components, designing the location and connection of roadways, and designing the choice and connection of processors in a chemical plant.",
            "group": 1497,
            "name": "10.1.1.60.1238",
            "keyword": "topology optimizationalgorithm developmentmulti-disciplinary",
            "title": "A Generic Scheme for Graph Topology Optimization"
        },
        {
            "abstract": "In an earlier paper [14], we developed the first algorithm (to our knowledge) for computing  the stochastically stable distribution of a perturbed Markov process. The primary tool was a  novel quotient construction on Markov matrices. In this paper, we show that the ideas and  techniques in that paper arise from a more fundamental construction on Markov chains, and  have much wider applicability than simply to game theory (the application discussed in [14]).",
            "group": 1498,
            "name": "10.1.1.60.1325",
            "keyword": "",
            "title": "A Quotient Construction on Markov Chains with Applications to the Theory of Generalized Simulated Annealing"
        },
        {
            "abstract": "We propose a novel method of robustly and automatically creating surface meshes from the very sparsely populated 3D point clouds typically produced by Structure from Motion algorithms. Prior to meshing the point cloud is augmented with new points created by planar intersection thus improving the subsequent reconstruction of edges and corners in the scene. Image-consistent triangulation is then used within a simulated annealing algorithm to create an optimal surface mesh by selecting the subset and triangulation of a 3D point cloud that best represents the actual topology of the scene. This method provides a number of advantages: the reconstruction of crucial areas such as corners and edges is improved, it copes well with noisy data, it produces a simplified mesh, particularly for scenes that contain many planes and, unlike greedy search techniques, it is much more likely to converge to a global minimum. Results are provided for real and synthetic data and it is shown that even using datasets containing large numbers of outliers this method is capable of producing a surface mesh that reliably represents the real surface of the object.",
            "group": 1499,
            "name": "10.1.1.60.2824",
            "keyword": "",
            "title": "Automatic Augmentation and Meshing of Sparse 3D Scene Structure"
        },
        {
            "abstract": "In inverse problems, often there is no available analytical expression relating the physical quantities of interest and the available data. In these cases, one resorts to using a numerical model with a finite number of parameters, resulting in a discrete problem. Also, many discrete inverse problems involve a highly nonlinear mapping between the model parameters and the simulation of the data by the model. Algorithms exist for estimating the model parameters in nonlinear discrete inverse problems. However, one needs to investigate how these estimated models relate to the true structure of the studied system (i.e. the truth model). This is known as model appraisal and it is greatly a#ected by three sources of uncertainty: misleading search, non-uniqueness and errors.",
            "group": 1500,
            "name": "10.1.1.60.2949",
            "keyword": "",
            "title": "Characterising the Parameter Space of a Highly Nonlinear Inverse Problem"
        },
        {
            "abstract": "Systems for multi-level logic optimization are usually based on a set of specialized, loosely-related transformations which work on a network representation. The sequence of transformations in a synthesis scenario (script) is crucial for the performance of the whole system. This paper presents the application of a genetic algorithm for automatic tuning of scenarios, and therefore an approach for optimizing the synthesis process itself. We introduce a general context-free grammar to describe the set of relevant scenarios. For such grammars we develop meaningful genetic operators for the implementation of an evolutionary search. Experiments with an industrial logic synthesis system show, that our method can improve the e#ciency of manually designed standard scenarios by an average of 8%. We also show, that this approach can e#ectively tune scenarios to particular designs styles and/or specific synthesis goals.",
            "group": 1501,
            "name": "10.1.1.60.3328",
            "keyword": "",
            "title": "Grammar-Based Optimization of Synthesis Scenarios"
        },
        {
            "abstract": "This paper introduces a novel solver, namely cross entropy (CE), into the MRF theory for medical image segmentation. The solver, which is based on the theory of rare event simulation, is general and stochastic. Unlike some popular optimization methods such as belief propagation and graph cuts, CE makes no assumption on the form of objective functions and thus can be applied to any type of MRF models. Furthermore, it achieves higher performance of finding more global optima because of its stochastic property. In addition, it is more efficient than other stochastic methods like simulated annealing. We tested the new solver in 4 series of segmentation experiments on synthetic and clinical, vascular and cerebral images. The experiments show that CE can give more accurate segmentation results. ",
            "group": 1502,
            "name": "10.1.1.60.4629",
            "keyword": "",
            "title": "Cross entropy: A new solver for markov random field modeling and applications to medical image segmentation"
        },
        {
            "abstract": "Ant colony optimization metaheuristic (ACO) represents a new class of algorithms particularly suited to solve real- world combinatorial optimization problems. ACO algorithms, published for the first time in 1991 by M. Dorigo and his co-workers, have been applied, particularly starting from 1999 to several kind of optimization problems as the traveling salesman problem, quadratic assignement problem, vehicle routing, sequential ordering, scheduling, graph coloring, management of communications networks and so on.",
            "group": 1503,
            "name": "10.1.1.60.4709",
            "keyword": "",
            "title": "On some applications of Ant Colony Optimization metaheuristic to structural"
        },
        {
            "abstract": "This paper addresses the problem of real-time 3D modelbased tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the properties of these two sensor systems and show that this leads to some non-trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly combining the pose estimates they produce. Further we show how on-line learning can be used to improve the performance of feature tracking. Finally, to aid real-time performance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combination of these techniques results in a system which is capable of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50\u00b0 camera shake at 6Hz.",
            "group": 1504,
            "name": "10.1.1.60.4715",
            "keyword": "",
            "title": "Fusing Points and Lines for High Performance Tracking"
        },
        {
            "abstract": "We propose a method of improving the reconstruction of edges and corners in surface meshes of sparsely populated point clouds by exploiting the presence of planes in the scene, the calculated geometry of the cameras and the images themselves. By robustly identifying co-planar points in the point cloud, isolating suitable plane-pairs for intersection, identifying the regions-of-interest along the lines of intersection and then creating points along these line segments, new points are added to the point cloud. The success of this approach is shown by using a robust Image-Consistent Triangulation method to mesh the point clouds before and after augmentation.",
            "group": 1505,
            "name": "10.1.1.60.5304",
            "keyword": "KEY WORDS Surface ReconstructionPlanar IntersectionStructure from MotionMeshingSimulated Annealing",
            "title": "Augmentation Of Sparsely Populated Point Clouds Using Planar Intersection"
        },
        {
            "abstract": "The relevance of computer science to economic planning is defended. An algorithm for constructing a balanced economic plan is presented and found to be of time order Nlog(N) in the complexity of the economy. The time taken to perform a complete plan optimization in natural units for a whole economy is estimated.",
            "group": 1506,
            "name": "10.1.1.60.6098",
            "keyword": "",
            "title": "Application of Artificial Intelligence Techniques to Economic Planning"
        },
        {
            "abstract": "The technological development is enabling production of increasingly complex electronic systems. All those systems must be verified and tested to guarantee correct behavior. As the complexity grows, testing is becoming one of the most significant factors that contribute to the final product cost. The established low-level methods for hardware testing are not any more sufficient and more work has to be done at abstraction levels higher than the classical gate and register-transfer levels. This thesis reports on one such work that deals in particular with high-level test generation and design for testability techniques. The contribution of this thesis is twofold. First, we investigate the possibilities of generating test vectors at the early stages of the design cycle, starting directly from the behavioral description and with limited knowledge about the final implementation architecture. We have developed for this purpose a novel hierarchical test generation algorithm and demonstrated the usefulness of the generated tests not only for manufacturing test but also for testability analysis. The second part of the thesis concentrates on design for testability. As testing of modern",
            "group": 1507,
            "name": "10.1.1.60.6910",
            "keyword": "",
            "title": "High-Level Test Generation and Built-In Self-Test Techniques for Digital Systems "
        },
        {
            "abstract": "In many fields of engineering science we have to deal with multivariate numerical data. In order to choose the technique that is best suited to a given task, it is necessary to get an insight into the data and to \"understand\" them. Much information allowing the understanding of multivariate data, that is the description of its global structure, the presence and shape of clusters or outliers, can be gained through data visualization. Multivariate data visualization can be realized through a reduction of the data dimensionality, which is often performed by mathematical and statistical tools that are well known. Such tools are Principal Components Analysis or Multidimensional Scaling. Artificial neural networks have developed and found applications mainly in the last two decades, and they are now considered as a mature field of research. This thesis investigates the use of existing algorithms as applied to multivariate data visualization. First an overview of existing neural and statistical techniques applied to data visualization is presented. Then a comparison is made between two chosen algorithms from the point of view of multivariate data visualization. The chosen neural network algorithm is Kohonen's Self-Organizing Maps, and the statistical technique is Multidimensional Scaling. The advantages and drawbacks from the theoretical and practical viewpoints of both approaches are put into light. The preservation of data topology involved by those two mapping techniques is discussed. The multidimensional scaling method was analyzed in details, the importance of each parameter was determined, and the technique was implemented in metric and non-metric versions. Improvements to the algorithm were proposed in order to increase the performance of the mapping process. A graphic...",
            "group": 1508,
            "name": "10.1.1.60.7050",
            "keyword": "",
            "title": "Neural and Statistical Methods for the Visualization of Multidimensional Data"
        },
        {
            "abstract": "The hydrophobic-hydrophilic (H-P) model for protein folding was introduced  by Dill et al. [6]. A problem instance consists of a sequence of amino acids,  each labeled as either hydrophobic (H) or hydrophilic (P). The sequence must be  placed on a 2D or 3D grid without overlapping, so that adjacent amino acids in the  sequence remain adjacent in the grid. The goal is to minimize the energy, which in  the simplest variation corresponds to maximizing the number of adjacent hydrophobic  pairs. Although the model is extremely simple, it captures the main features of  the problem. The protein folding problem in the H-P model is NP-hard in both 2D  and 3D. Recently, Fu and Wang [9] proved an exp(O(n    ln n) algorithm for  d-dimensional protein folding simulation in the HP-model. Our preliminary results  on stochastic search applied to protein folding utilize complete move sets proposed by  Lesh et al. [15] and Blazewicz et al. [3]. We obtain that after (n/# )    Markov chain  transitions, the probability to be in a minimum energy conformation is at least 1    where n is the length of the instance, # is the maximum value of the minimum escape  height from local minima of the underlying energy landscape, and c is a (small) constant.",
            "group": 1509,
            "name": "10.1.1.60.7064",
            "keyword": "",
            "title": "Run-time Estimates for Protein Folding Simulation in the H-P Model"
        },
        {
            "abstract": "This document describes the process we followed for assessing the feasibility and effectiveness of high-level test vector generation. An experimental analysis of the available high-level fault models is first reported, whose purpose is to identify a reference fault model that could be fruitfully used for evaluating testability of circuits by reasoning on their behavior, only. A prototypical high-level fault simulation tool is also described, whose purpose is to support the fault models analysis. Finally, a test generation algorithm is presented that generates high quality test vectors by exploiting the selected fault model and the described high-level fault simulator.",
            "group": 1510,
            "name": "10.1.1.60.7294",
            "keyword": "",
            "title": "Testability in Co-design Environments"
        },
        {
            "abstract": "The problem of minimizing power consumption during the state encoding of a finite state machine is addressed. A new power cost model for state encoding is proposed and encoding techniques that minimize this power cost for two- and multi-level logic implementations are described. These techniques are compared with those which minimize area or the switching activity at the present state bits. Experimental results show significant improvements.  ",
            "group": 1511,
            "name": "10.1.1.60.8028",
            "keyword": "",
            "title": "Low Power State Assignment Targeting Two- and Multi-level Logic Implementation"
        },
        {
            "abstract": "Low power has emerged as a principal theme in today's electronics industry. The need for low power has caused a major paradigm shift in which power dissipation is as important as performance and area. This article presents an in-depth survey of CAD methodologies and techniques for designing low power digital CMOS circuits and systems and describes the many issues facing designers at architectural, logic and physical levels of design abstraction. It reviews some of the techniques and tools that have been proposed to overcome these difficulties and outlines the future challenges that must be met to design low power, high performance systems.",
            "group": 1512,
            "name": "10.1.1.60.8086",
            "keyword": "",
            "title": "Power Minimization in IC Design: Principles and Applications"
        },
        {
            "abstract": "Simulated annealing is a provably convergent optimiser for single-objective problems. Previously proposed multiobjective extensions have mostly taken the form of a singleobjective simulated annealer optimising a composite function of the objectives. We propose a multi-objective simulated annealer utilising the relative dominance of a solution as the system energy for optimisation, eliminating problems associated with composite objective functions. We also demonstrate a method for choosing perturbation scalings promoting search both towards and across the Pareto front. We illustrate",
            "group": 1513,
            "name": "10.1.1.60.8090",
            "keyword": "",
            "title": "Dominance-Based Multi-Objective Simulated Annealing"
        },
        {
            "abstract": "We illustrate our experience in developing and implementing algorithms for map merging, i.e., the problem of fusing two or more partial maps without common reference frames into one large global map. The partial maps may for example be acquired by multiple robots, or during several runs of a single robot from varying starting positions. Our work deals with low quality maps based on probabilistic grids, motivated by the goal to develop multiple mobile platforms to be used in rescue environments. Several contributions to map merging are presented. First of all, we address map merging using a motion planning algorithm. The merging process can be done by rotating and translating the partial maps until similar regions overlap. Second, a motion planning algorithm is presented which is particular suited for this task. Third, a special metric is presented which guides the motion planning algorithm towards the goal of optimally overlapping partial maps. Results with our approach are presented based on data gathered from real robots developed for the RoboCupRescue real robot league.",
            "group": 1514,
            "name": "10.1.1.60.8201",
            "keyword": "",
            "title": "On Map Merging"
        },
        {
            "abstract": "This paper presents a survey of layout techniques for designing low power digital CMOS circuits. It describes the many issues facing designers at the physical level of design abstraction and reviews some of the techniques and tools that have been proposed to overcome these difficulties.",
            "group": 1515,
            "name": "10.1.1.60.8606",
            "keyword": "",
            "title": "Power Optimization in VLSI Layout: A Survey"
        },
        {
            "abstract": "Model checking is the process of verifying whether a model o a coKOO0wG t system satisfies a specified temp pro erty. Symbo70 algow7205  basedo Binary Decision Diagrams (BDDs) have significantly increased the sizeo themo dels that can be verified. The main prow lem in symbo05 mo del checking is the image computVN3z problem, i.e., e#cientlycow09529 the successoG o predecessoP o a seto states. This paper is an in-depth studyo the imagecoew4775Pw proew47 We analyze and evaluate several newheuristics, metrics, and algo029wG fo thisprow5OP The algo7POwG use co binato4wG onato4wG42 techniques such as hill climbing, simulated annealing, and ordering by recursive part -V70BB7 to ow72 better results than was the case.Theow75 ical analysis and systematic experimentatio are usedto evaluate the algo05PwG4   ",
            "group": 1516,
            "name": "10.1.1.60.8849",
            "keyword": "",
            "title": " Using Combinatorial Optimization Methods for Quantification Scheduling  "
        },
        {
            "abstract": "The need for weight-optimised aerospace structures results in flexible structures with highly nonlinear behaviour. Their control presents challenges that require control theory be developed in order to enable the trend toward lighter structures to continue. Two classes of problem of particular interest in control are systems with a high degree of nonlinearity inherent in the structure and those with nonlinear and noncontinuous external forces. These are represented respectively by space structures and those subject to aeroelastic effects.",
            "group": 1517,
            "name": "10.1.1.60.8980",
            "keyword": "StochasticMulti-objectiveOptimal ControlOptimizationSimulation",
            "title": "Constrained Multi-Objective Optimisation of a Controller for a Highly Nonlinear Aeroelastic Structure - Methodology"
        },
        {
            "abstract": "this paper the uplink slot assignment problem in a multi-spot geostationary satellite. Radio interference impose constraints on the slots that can simultaneously be assigned in di#erent cells that have the same frequency. The problem is shown to be an NP-complete one, which motivates us to search for a heuristic solution approach. We describe here a heuristic solution based on simulated annealing. We further investigate how to improve the performance of the simulated annealing and the rate of convergence of the annealing. Numerical experimentations are provided to test our proposed improvements",
            "group": 1518,
            "name": "10.1.1.60.9059",
            "keyword": "",
            "title": "Slot Allocation In A Tdma"
        },
        {
            "abstract": "Model checking is the process of verifying whether a model of a  concurrent system satisfies a specified temporal property. Symbolic algorithms  based on Binary Decision Diagrams (BDDs) have significantly increased the  size of the models that can be verified. The main problem in symbolic model  checking is the image computation problem, i.e., e#ciently computing the successors  or predecessors of a set of states. This paper is an in-depth study of the  image computation problem. We analyze and evaluate several new heuristics,  metrics, and algorithms for this problem. The algorithms use combinatorial  optimization techniques such as hill climbing, simulated annealing, and ordering  by recursive partitioning to obtain better results than was previously the  case. Theoretical analysis and systematic experimentation are used to evaluate  the algorithms.",
            "group": 1519,
            "name": "10.1.1.60.9082",
            "keyword": "",
            "title": "Using Combinatorial Optimization Methods for Quantification Scheduling "
        },
        {
            "abstract": "The need for weight-optimised aerospace structures results in flexible structures with highly nonlinear behaviour. Their control presents challenges that require control theory be developed in order to enable the trend toward lighter structures to continue. Two classes of problem of particular interest in control are systems with a high degree of nonlinearity inherent in the structure and those with nonlinear and noncontinuous external forces. These are represented respectively by space structures and those subject to aeroelastic effects.",
            "group": 1520,
            "name": "10.1.1.60.9105",
            "keyword": "StochasticMulti-objectiveOptimal ControlOptimizationSimulation",
            "title": "Constrained Multi-Objective Optimisation of a Controller for a Highly Nonlinear Aeroelastic Structure - Testbed Design"
        },
        {
            "abstract": "Stemmatology studies relations among different variants of a text that has been gradually altered as a result of imperfectly copying the text over and over again. Applications are mainly in humanities, especially textual criticism, but the methods can be used to study the evolution of any symbolic objects, including chain letters and computer viruses.We propose an algorithm for stemmatic analysis based on a minimum-information criterion and stochastic tree optimization. Our approach is related to phylogenetic reconstruction criteria such as maximum parsimony and maximum likelihood, and builds upon algorithmic techniques developed for bioinformatics. Unlike many earlier methods, the proposed method does not require significant preprocessing of the data but rather, operates directly on aligned text files. We demonstrate our method on a realworld experiment involving all 52 known variants of the legend of St. Henry of Finland, and provide the first computer-generated family tree of the legend. The obtained tree of the variants is supported to a large extent by results obtained with more traditional methods, and identifies a number of previously unrecognized relations.",
            "group": 1521,
            "name": "10.1.1.60.9302",
            "keyword": "",
            "title": "A Compression-Based Method for Stemmatic Analysis"
        },
        {
            "abstract": "Tabu search is one of the most effective heuristics for locating high-quality solutions to  a diverse array of NP-hard combinatorial optimization problems. Despite the widespread  success of tabu search, researchers have a poor understanding of many key theoretical  aspects of this algorithm, including models of the high-level run-time dynamics and identification  of those search space features that influence problem difficulty. We consider these  questions in the context of the job-shop scheduling problem (JSP), a domain where tabu  search algorithms have been shown to be remarkably effective. Previously, we demonstrated  that the mean distance between random local optima and the nearest optimal solution is  highly correlated with problem difficulty for a well-known tabu search algorithm for the  JSP introduced by Taillard. In this paper, we discuss various shortcomings of this measure  and develop a new model of problem di#culty that corrects these deficiencies. We show  that Taillard's algorithm can be modeled with high fidelity as a simple variant of a straightforward  random walk. The random walk model accounts for nearly all of the variability  in the cost required to locate both optimal and sub-optimal solutions to random JSPs,  and provides an explanation for differences in the di#culty of random versus structured  JSPs. Finally, we discuss and empirically substantiate two novel predictions regarding tabu  search algorithm behavior. First, the method for constructing the initial solution is highly  unlikely to impact the performance of tabu search. Second, tabu tenure should be selected  to be as small as possible while simultaneously avoiding search stagnation; values larger  than necessary lead to significant degradations in performance.",
            "group": 1522,
            "name": "10.1.1.60.9407",
            "keyword": "",
            "title": "Linking Search Space Structure, Run-Time Dynamics, and Problem Difficulty: A Step Toward Demystifying Tabu Search"
        },
        {
            "abstract": "This paper investigates a hybridisation of the  very large neighbourhood search approach with  local search methods to address examination  timetabling problems. In this paper, we describe  a 2 phase approach. The first phase employs  \"multi start\" to carry out search upon a very  large neighbourhood of solutions using graph  theoretical algorithms implemented on an  improvement graph. The second phase makes  further improvements by utilising a local search  method. We present experimental results which  show that this combined approach compares  favourably with other algorithms on the standard  benchmark problems.",
            "group": 1523,
            "name": "10.1.1.60.9552",
            "keyword": "Timetabling Problem",
            "title": "A Multi-start Very Large Neighbourhood Search Approach with Local Search Methods for Examination Timetabling"
        },
        {
            "abstract": "Evolution has produced organisms whose locomotive agility and adaptivity mock the di#culty faced by robotic scientists. The problem of locomotion, which nature has solved so well, is surprisingly complex and di#cult. We explore the ability of an artificial eight-legged arachnid, or animat, to autonomously learn a locomotive gait in a three-dimensional environment. We take a physics-based approach at modeling the world and the virtual body of the animat. The arachnid-like animat learns muscular control functions using simulated annealing techniques, which attempts to maximize forward velocity and minimize energy expenditure. We experiment with varying the weight of these parameters and the resulting locomotive gaits. We perform two experiments in which the first is a naive physics model of the body and world which uses point-masses and idealized joints and muscles. The second experiment is a more realistic simulation using rigid body elements with distributed mass, friction, motors, and mechanical joints. By emphasizing physical aspects we wish to minimize, a number of interesting gaits emerge.",
            "group": 1524,
            "name": "10.1.1.60.9742",
            "keyword": "artificial liferoboticslocomotionanimatspidersimulated annealingphysics modeling",
            "title": "Artificial Spider: Eight-Legged Arachnid and Autonomous"
        },
        {
            "abstract": "Over the last decade and a half, tabu search algorithms for machine scheduling have gained a near-mythical reputation by consistently equaling or establishing state-of-the-art performance levels on a range of academic and real-world problems. Yet, despite these successes, remarkably little research has been devoted to developing an understanding of why tabu search is so e#ective on this problem class. In this paper, we report results that provide significant progress in this direction. We consider Nowicki and Smutnicki's i-TSAB tabu search algorithm, which represents the current state-of-the-art for the makespan-minimization form of the classical jobshop scheduling problem. Via a series of controlled experiments, we identify those components of i-TSAB that enable it to achieve state-of-the-art performance levels. In doing so, we expose a number of misconceptions regarding the behavior and/or benefits of tabu search and other local search metaheuristics for the job-shop problem. Our results also serve to focus future research, by identifying those specific directions that are most likely to yield further improvements in performance.",
            "group": 1525,
            "name": "10.1.1.60.9971",
            "keyword": "",
            "title": "Deconstructing Nowicki and Smutnicki's i-TSAB Tabu Search Algorithm for the Job-Shop Scheduling Problem"
        },
        {
            "abstract": "In this paper we consider a new class of search and optimization  algorithms inspired by molecular dynamics simulations in physics.",
            "group": 1526,
            "name": "10.1.1.61.4",
            "keyword": "",
            "title": "Constrained Molecular Dynamics as a Search"
        },
        {
            "abstract": "We define a new method for global optimization, the Homotopy Optimization  Method (HOM). This method di#ers from previous homotopy and continuation  methods in that its aim is to find a minimizer for each of a set of values  of the homotopy parameter, rather than to follow a path of minimizers. We  define a second method, called HOPE,byallowingHOM to follow an ensemble of  points obtained by perturbation of previous ones. We relate this new method  to standard methods such as simulated annealing and show under what circumstances  it is superior. We present results of extensive numerical experiments  demonstrating performance of HOM and HOPE.",
            "group": 1527,
            "name": "10.1.1.61.557",
            "keyword": "SAND2005-7495",
            "title": "Sandia Report"
        },
        {
            "abstract": "There is a perception that teaching space in universities is a rather  scarce resource. However, some studies have revealed that in many institutions  it is actually chronically under-used. Often, rooms are occupied only half the  time, and even when in use they are often only half full. This is usually measured  by the \"utilisation\" which is basically the percentage of available 'seat-hours'  that are employed. In real institutions, this utilisation can often takes values as  low as 20-40%.",
            "group": 1528,
            "name": "10.1.1.61.636",
            "keyword": "",
            "title": "Towards Improving the Utilisation of University Teaching Space"
        },
        {
            "abstract": "A fundamental issue in real-world systems, such as sensor  networks, is the selection of observations which  most effectively reduce uncertainty. More specifically,  we address the long standing problem of nonmyopically  selecting the most informative subset of variables in a  graphical model. We present the first efficient randomized  algorithm providing a constant factor (1-1/e-#)  approximation guarantee for any # > 0 with high confidence. The",
            "group": 1529,
            "name": "10.1.1.61.1560",
            "keyword": "",
            "title": "Near-optimal Nonmyopic Value of Information in Graphical Models"
        },
        {
            "abstract": "The Knowledge-Based Machine Translation paradigm requires a comprehensive  analysis of input texts into an unambiguous machine-tractable representation of  the propositional and meta-propositional meaning of that text, for which we use a  particular framework referred to as ontological semantics. The work presented  here begins with a definition of a representation language for lexical semantic  specification (and syntax/semantics interface) to support such an analysis, as well  as a generalized algorithm for building the meaning representation from these lexical  semantic specifications, utilizing the ontology and a syntactic parse as knowledge  sources. The core of the algorithm is an algorithm for semantic constraint  satisfaction and relaxation, involving finding the best path over the ontology between  a candidate filler of a relation and semantic constraints on that relation. The  ontology is viewed as a multi-dimensional graph, with distinct topologies in each  dimension reflecting specific semantic relations between nodes (representing concepts)  , where weights or arc distance reflects strength of semantic relatedness in  context (where the path-so-far context is maintained in a state transition table).",
            "group": 1530,
            "name": "10.1.1.61.1683",
            "keyword": "",
            "title": "An Ontological-Semantic Framework for Text Analysis"
        },
        {
            "abstract": "This paper presents a new sampling algorithm for approximating functions  of variables representable as undirected graphical models of arbitrary  connectivity with pairwise potentials, as well as for estimating the  notoriously difficult partition function of the graph. The algorithm fits  into the framework of sequential Monte Carlo methods rather than the  more widely used MCMC, and relies on constructing a sequence of intermediate  distributions which get closer to the desired one. While the  idea of using \"tempered\" proposals is known, we construct a novel sequence  of target distributions where, rather than dropping a global temperature  parameter, we sequentially couple individual pairs of variables  that are, initially, sampled exactly from a spanning tree of the variables. We present",
            "group": 1531,
            "name": "10.1.1.61.2667",
            "keyword": "",
            "title": "Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise"
        },
        {
            "abstract": "In this work we focus on efficient heuristics for  solving a class of stochastic planning problems  that arise in a variety of business, investment, and  industrial applications. The problem is best described  in terms of future buy and sell contracts. By buying",
            "group": 1532,
            "name": "10.1.1.61.3537",
            "keyword": "",
            "title": "A Clustering Approach to Solving Large Stochastic Matching Problems"
        },
        {
            "abstract": "... This paper presents a survey of the latest research motivated by this recognition. The presentation is focused on multi-stage applications of advanced local search techniques on the VRPTW. Multi-stage algorithms optimize the number of vehicles and travel time independently in order to ensure that the search is directed towards the achievement of the primary objective. Basic features of these algorithms, as well as hybridization strategies are described. For most algorithms, experimental results on Solomon's benchmark test problems are provided and analyzed",
            "group": 1533,
            "name": "10.1.1.61.5490",
            "keyword": "",
            "title": "Advanced Multi-Stage Local Search Applications to Vehicle Routing Problem with Time Windows: A Review"
        },
        {
            "abstract": "For the first time we find Boolean functions on 9 variables having nonlinearity 241, that remained as an open question in literature for almost three decades. Such functions are discovered using a suitably modified steepest-descent based iterative heuristic search in the class of rotation symmetric Boolean functions (RSBFs). This shows that there exist Boolean functions on n (odd) variables having nonlinearity > 2^(n-1) - 2^((n-1)/2) if and only if n > 7. Using the same search method, we also find several other important functions and we study the autocorrelation, propagation characteristics and resiliency of the RSBFs (using proper affine transformations, if required). The results show that it is possible to get balanced Boolean functions on n = 10 variables having autocorrelation spectra with maximum absolute value < 2^(n/2), which was not known earlier. In certain cases the functions can be affinely transformed to get first order propagation characteristics. We also obtain 10-variable functions having first order resiliency and nonlinearity 492 which was posed as an open question in Crypto 2000.",
            "group": 1534,
            "name": "10.1.1.61.5629",
            "keyword": "Boolean FunctionsCombinatorial ProblemsCryptographyHeuristic SearchNonlinearityRotational Symmetry",
            "title": "There exist Boolean functions on n (odd) variables having nonlinearity > 2^(n-1) - 2^((n-1)/2) if and only if n>7"
        },
        {
            "abstract": "The ant colony algorithm is a new technique for combinatorial optimization borrowed from swarm intelligence. This paper outlines an ant colony algorithm to compute the optimal order of restoring sections in a power distribution system. Restoration of distribution feeders after long interruptions creates cold load pickup conditions due to loss of diversity among the loads. The distribution system load may have to be restored step-by-step using sectionalizing switches under such conditions to prevent overheating of substation transformer. The restoration time is dependent on the order in which sections are restored. Results obtained using this method for two test cases are presented including a comparison with the simulated annealing algorithm.",
            "group": 1535,
            "name": "10.1.1.61.6512",
            "keyword": "Distribution system restorationcold load pickupant colony optimization. COLD LOAD PICKUP",
            "title": "Ant Algorithms for the Optimal Restoration of Distribution Feeders During Cold Load Pickup"
        },
        {
            "abstract": "  Efficient representations and solutions for large structured decision problems with continuous and discrete variables are among the important challenges faced by the designers of automated decision support systems. In this work, we describe a novel hybrid factored Markov decision process (MDP) model that allows for a compact representation of these problems, and a hybrid approximate linear programming (HALP) framework that permits their efficient solutions. The central idea of HALP is to approximate the optimal value function of an MDP by a linear combination of basis functions and optimize its weights by linear programming. We study both theoretical and practical aspects of this approach, and demonstrate its scale-up potential on several hybrid optimization problems",
            "group": 1536,
            "name": "10.1.1.61.8888",
            "keyword": "TABLE OF CONTENTS PREFACE........................................... ix",
            "title": "Planning in Hybrid Structured Stochastic Domains"
        },
        {
            "abstract": "Efficient representations and solutions for large decision problems with continuous and  discrete variables are among the most important challenges faced by the designers of automated  decision support systems. In this paper, we describe a novel hybrid factored Markov  decision process (MDP) model that allows for a compact representation of these problems,  and a new hybrid approximate linear programming (HALP) framework that permits their  efficient solutions. The central idea of HALP is to approximate the optimal value function  by a linear combination of basis functions and optimize its weights by linear programming.",
            "group": 1537,
            "name": "10.1.1.61.9846",
            "keyword": "",
            "title": " Solving Factored MDPs with Hybrid State and Action Variables"
        },
        {
            "abstract": "In this survey we discuss different state-of-the-art approaches  of combining exact algorithms and metaheuristics to solve combinatorial  optimization problems. Some of these hybrids mainly aim at providing optimal  solutions in shorter time, while others primarily focus on getting better  heuristic solutions. The two main categories in which we divide the approaches  are collaborative versus integrative combinations. We further classify  the different techniques in a hierarchical way. Altogether, the surveyed  work on combinations of exact algorithms and metaheuristics documents the  usefulness and strong potential of this research direction.",
            "group": 1538,
            "name": "10.1.1.61.9970",
            "keyword": "",
            "title": "Combining Metaheuristics and Exact Algorithms in Combinatorial Optimization: A Survey and Classification"
        },
        {
            "abstract": "We consider a panel of experts asked to assign probabilities to events, both logically simple and complex. The events evaluated by different experts are based on overlapping sets of variables but may otherwise be distinct. The union of all the judgments will likely be probabilistic incoherent. We address the problem of revising the probability estimates of the panel so as to produce a coherent set that best represents the group's expertise.",
            "group": 1539,
            "name": "10.1.1.62.411",
            "keyword": "",
            "title": "Aggregating Disparate Estimates of Chance"
        },
        {
            "abstract": "In the past two decades, the simulated annealing technique has been considered as a powerful approach to handle many NP-hard optimization problems in VLSI designs. Recently, a new Monte Carlo and optimization technique, named simulated tempering, was invented and has been successfully applied to many scientific problems, from random field Ising modeling to the traveling salesman problem. It is designed to overcome the drawback in simulated annealing  when the problem has a rough energy landscape with many local minima separated by high energy barriers. In this paper, we have successfully applied a version of relaxed simulated tempering to slicing floorplan design with consideration of both area and wirelength optimization. Good experimental results were obtained.",
            "group": 1540,
            "name": "10.1.1.62.544",
            "keyword": "",
            "title": "Relaxed Simulated Tempering for VLSI Floorplan Designs"
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN). Given a netlist, we first compute a K-way partition of the nets based on the HDN representation, and then transform a K-way net partition into a K-way module partitioning solution. The main contribution of our work is the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions, while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding factor between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present efficient solutions to the K-MC problem based on network flow computation. Extensive experimental results show that our algorithm consistently outperforms the conventional K-FM partitioning algorithm by a significant margin.",
            "group": 1541,
            "name": "10.1.1.62.613",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "Acyclic partitioning on combinational boolean networks has wide range of applications, from multiple FPGA chip partitioning to parallel circuit simulation. In this paper, we present two efficient algorithms for the acyclic multi-way partitioning. One is a generalized FMbased algorithm. The other is based on the theory of maximum fanout-free cone (MFFC) decomposition. The acyclic FM-algorithm usually results in larger cut-size, as expected, compared to the undirected FM-algorithm due to the acyclic constraint. To our surprise, however, the MFFC-based acyclic partitioning algorithm consistently produces smaller (50% on average) cut-sized solutions than the conventional FM-algorithm. This result suggests that considering signal directions during the process can lead to very natural circuit decomposition and clustering, which in turn results in better partitioning solutions. We have also implemented parallel gate level simulators in Maisie and applied our partitioning algorithms to evaluate their impact on circuit simulation.",
            "group": 1542,
            "name": "10.1.1.62.640",
            "keyword": "",
            "title": "Acyclic Multi-Way Partitioning of Boolean Networks"
        },
        {
            "abstract": "As microprocessor technology continues to scale into the nanometer regime, recent studies show that interconnect delay will be a limiting factor for performance, and multiple cycles will be necessary to communicate global signals across the chip. Thus, longer interconnects need to be pipelined, and the impact of the extra latency along wires needs to be considered during early micro-architecture design exploration. In this paper, we address this problem and make the following contributions: (1) a floorplan-driven micro-architecture evaluation methodology considering interconnect pipelining at a given target frequency by selectively optimizing architecture level critical paths. (2) use of micro-architecture performance sensitivity models to weight micro-architectural critical paths during floorplanning and optimize them for higher performance. (3) a methodology to study the impact of frequency scaling on micro-architecture performance with consideration of interconnect pipelining.",
            "group": 1543,
            "name": "10.1.1.62.654",
            "keyword": "IPC cycle time",
            "title": "Microarchitecture Evaluation With Floorplanning"
        },
        {
            "abstract": "In this paper, a simulated-annealing-based method called Filter Simulated Annealing (FSA) method is proposed to deal with the constrained global optimization problem. The considered problem is reformulated so as to take the form of optimizing two functions, the objective function and the constraint violation function. Then, the FSA method is applied to solve the reformulated problem. The FSA method invokes a multi-start diversification scheme in order to achieve an efficient exploration process. To deal with the considered problem, a filter-set-based procedure is built in the FSA structure. Finally, an intensification scheme is applied as a final stage of the proposed method in order to overcome the slow convergence of SA-based methods. The computational results obtained by the FSA method are promising and show a superior performance of the proposed method, which is a point-to-point method, against population-based methods.",
            "group": 1544,
            "name": "10.1.1.62.1474",
            "keyword": "Key wordsConstrained global optimizationMetaheuristicsSimulated annealingFilter SetApproximate descent direction",
            "title": "Derivative-free filter simulated annealing method for constrained continuous global optimization"
        },
        {
            "abstract": "The evolution of a continuous time recurrent neural network central pattern generation for walking is characterized and found to proceed in two phases. The first phase spans the beginning of the search through the generation at which a \u201cbreakthrough\u201d individual is discovered. The second phase proceeds from that generation forward. The first phase is most quickly completed if each succeeding population is as random as possible. Hence GA searches performed at lower mutation variances require more generations to discover a breakthrough individual than higher mutation variances searches. In the second phase the best fitness in the population most rapidly increases at low mutation variances. The role of parameter space structure in these trends will be examined.",
            "group": 1545,
            "name": "10.1.1.62.1649",
            "keyword": "",
            "title": "Evolving walking: The anatomy of an evolutionary search"
        },
        {
            "abstract": "Keywords: NP-complete problems; complexity; traveling salesman problem; search phase transitions; finite size scaling; easy and hard instances",
            "group": 1546,
            "name": "10.1.1.62.2258",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract-Parallel algorithms with quality equivalent to the simu-lated annealing placement algorithm for standard cells [23] are pre-sented. The first, called heuristic spanning, creates parallelism by simultaneously investigating different areas of the plausible combina-torial search space. It is used to replace the high temperature portion of simulated annealing. The low temperature portion of Simulated An-nealing is sped up by a technique called section annealing, in which placement is geographically divided and the pieces are assigned to sep-arate processors. Each processor generates Simulated Annealing-style moves for the cells in its area, and communicates the moves to other processors as necessary. Heuristic spanning and section annealing are shown, experimentally, to converge to the same final cost function as regular simulated annealing. These approaches achieve significant speed-up over uniprocessor simulated annealing, giving high quality VLSI placement of standard cells in a short period of time. I.",
            "group": 1547,
            "name": "10.1.1.62.2573",
            "keyword": "",
            "title": "Parallel Standard Cell Placement Algorithms with Quality Equivalent to Simulated Annealing"
        },
        {
            "abstract": "When no knowledge is available on the structure of the fitness landscape, effectively the fitness function is a black box. Search in this scenario is not well understood. In this paper we introduce a new theoretical approach to study search algorithms in this scenario. This is based on a new definition of the concept of landscape \u2013 the information landscape \u2013 which allows us to evaluate the quantity of information embedded in a landscape. We derive a linear approximation to the performance of an algorithm based on this notion. This leads to the definition of the concept of performance landscape, which allows us to evaluate the quality of the information in a landscape with respect to an algorithm\u2019s search behaviour. We show that these two landscapes and their simple interaction allow us to better understand the general behaviour of search algorithms in the black box scenario. We also derive a new simple measure of problem difficulty. In experiments with a large variety of test problems and different versions of genetic algorithm we provide strong evidence supporting this framework. 1",
            "group": 1548,
            "name": "10.1.1.62.2664",
            "keyword": "",
            "title": "Information and Performance Landscapes"
        },
        {
            "abstract": "Evolution has produced organisms whose locomotive agility and adaptivity mock the difficulty faced by robotic scientists. The problem of locomotion, which nature has solved so well, is surprisingly complex and difficult. We explore the ability of an artificial eight-legged arachnid, or animat, to autonomously learn a locomotive gait in a three-dimensional environment. We take a physics-based approach at modeling the world and the virtual body of the animat. The arachnid-like animat learns muscular control functions using simulated annealing techniques, which attempts to maximize forward velocity and minimize energy expenditure. We experiment with varying the weight of these parameters and the resulting locomotive gaits. We perform two experiments in which the first is a naive physics model of the body and world which uses point-masses and idealized joints and muscles. The second experiment is a more realistic simulation using rigid body elements with distributed mass, friction, motors, and mechanical joints. By emphasizing physical aspects we wish to minimize, a number of interesting gaits emerge.",
            "group": 1549,
            "name": "10.1.1.62.3034",
            "keyword": "artificial liferoboticslocomotionanimatspidersimulated annealingphysics modeling",
            "title": "Artificial spider: eight-legged arachnid and autonomous learning of locomotion"
        },
        {
            "abstract": "",
            "group": 1550,
            "name": "10.1.1.62.3228",
            "keyword": "",
            "title": "The way forward for unifying dynamic test case generation: The optimisation-based approach"
        },
        {
            "abstract": "UFR de math\u00e9matiques et d\u2019informatique",
            "group": 1551,
            "name": "10.1.1.62.3404",
            "keyword": "Computer GoMonte-CarloReinforcement Learning",
            "title": "Monte-Carlo Go Reinforcement Learning Experiments"
        },
        {
            "abstract": "Heuristic search methods have been applied to a wide variety of optimisation problems. A central element of these algorithms \u2019 success is the correct choice of values for their control parameters. To tune these settings, the use of specialists \u2019 knowledge and experience are often required. In this thesis, we first formalise the problem of parameter adaptation in heuristic search. Thereafter, we propose an automated mechanism, i.e. a method that reduces the strong dependency on experts, for choosing the best performing algorithm among several heuristic search approaches and optimis-ing its parameters. The novel Multiple Algorithms \u2019 Parameter Adaptation Algorithm (MAPAA) is based on Population-Based Incremental Learning, a method that combines the concepts of Competitive Learning and Genetic Algorithms. Addressing specific characteristics of the adaptation scenario, we extend the basic approach to deal with more versatile search spaces and to run successfully for small populations and few generational cycles. All newly introduced techniques are analysed in detail, and the efficiency and robustness of the MAPAA is studied and proven in several applications. I Acknowledgements I would like to express my deepest gratitude to my supervisor Professor Edward Tsang. He has been a remarkable mentor. Only his guidance, en-couragement and seemingly infinite patience have made my research work possible.",
            "group": 1552,
            "name": "10.1.1.62.3764",
            "keyword": "Acknowledgements",
            "title": "Summary"
        },
        {
            "abstract": "Software testing is costly, labour-intensive, and time-consuming. For most practical systems it will not be possible to perform \u2018exhaustive testing\u2019. Test sets must be effective (i.e. they reveal faults) but also easily generated (i.e. the process of generation must be efficient). We generally aim to develop small sets with high fault detection ability. Systematically generating effective test-data is one of the most interesting and practically relevant topics in the testing domain. Modern testing requires faults to be discovered at the earliest possible stage, i.e. specification or architecture design stage rather than the coding stage, because the cost of fixing an error increases with the time between its introduction and detection. We need to generate test cases to exercise our high-level models. Again, as with all testing, we wish to develop effective test sets, and to do so efficiently. One means of capturing high-level behaviour of systems is provided by the Matlab/Simulink toolset. Matlab/Simulink is popularly used in embedded systems engineering as an architectural-level design notation. Engineers like using it, finding it intuitively appealing. In",
            "group": 1553,
            "name": "10.1.1.62.5100",
            "keyword": "",
            "title": "Automatic Test-Data Generation for Testing Simulink Models"
        },
        {
            "abstract": "reactions. Previous hybridization models have focused on macroscopic reactions between two DNA strands at the sequence level. Here, we propose a novel population-based Monte Carlo algorithm that simulates a microscopic model of reacting DNA molecules. The algorithm uses two essential thermodynamic quantities of DNA molecules: the binding energy of bound DNA strands and the entropy of unbound strands. Using this evolutionary Monte Carlo method, we obtain a minimum free energy configuration in the equilibrium state. We applied this method to a logical reasoning problem and compared the simulation results with the experimental results of the wet-lab DNA experiments performed subsequently. Our simulation predicted the experimental results quantitatively. \u00a9 2007 Elsevier B.V. All rights reserved.",
            "group": 1554,
            "name": "10.1.1.62.5248",
            "keyword": "DNA computingEvolutionary algorithmMonte Carlo methodDNA theorem proving",
            "title": "+Model BIO-2779; No. of Pages 7 ARTICLE IN PRESS"
        },
        {
            "abstract": "We propose a two-stage simulated annealing method. While most previous work has focused on ad hoc constant starting temperatures for the low temperature annealing phase, this paper presents a more formal method for start-ing temperature determination in two-stage simulated annealing systems. We have successfully applied our method to three optimization problems using both classic and adaptive schedules. We also briejly discuss an alterna-tive stop criterion that experimentally reduces the running time up to an additional ten percent in our problem suite. 1.",
            "group": 1555,
            "name": "10.1.1.62.5683",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": null,
            "group": 1556,
            "name": "10.1.1.62.6097",
            "keyword": "",
            "title": "MIC2005. The 6th Metaheuristics International Conference 361 An Analysis of the Hardness of TSP Instances for Two High-performance Algorithms"
        },
        {
            "abstract": "Recent years have seen growing interest in the problem of super-resolution restoration of video sequences. Whereas in the traditional single image restoration problem only a single input image is available for processing, the task of reconstructing super-resolution images from multiple undersampled and degraded images can take advantage of the additional spatiotemporal data available in the image sequence. In particular, camera and scene motion lead to frames in the source video sequence containing similar, but not identical information. The additional information available in these frames make possible reconstruction of visually superior frames at higher resolution than that of the original data. In this paper we review the current state of the art and identify promising directions for future research.",
            "group": 1557,
            "name": "10.1.1.62.6195",
            "keyword": "",
            "title": "Executive Summary"
        },
        {
            "abstract": "Abstract- The portfolio optimization problem uses mathematical approaches to model stock exchange investments. Its aim is to find an optimal set of assets to invest on, as well as the optimal investments for each asset. In the present work, the problem is treated as a multiobjective optimization problem. Three well-known optimization techniques greedy search, simulated annealing and ant colony optimization are adapted to this multiobjective context. Pareto fronts for five stock indexes are collected, showing the different behaviors of the algorithms adapted. Finally, the results are discussed. 1",
            "group": 1558,
            "name": "10.1.1.62.6436",
            "keyword": "",
            "title": "A multiobjective approach to the portfolio optimization problem"
        },
        {
            "abstract": "The information provided is the sole responsibility of the authors and does not necessarily reflect the opinion of the members of IRIDIA. The authors take full responsibility for any copyright breaches that may result from publication of this paper in the IRIDIA \u2013 Technical Report Series. IRIDIA is not responsible for any use that might be made of data appearing in this publication. On the Estimation of the Expected Performance of a Metaheuristic on a Class of Instances",
            "group": 1559,
            "name": "10.1.1.62.7136",
            "keyword": "",
            "title": "Revision history:"
        },
        {
            "abstract": "In an attempt to ensure good-quality printouts of our technical reports, from the supplied PDF files, we process to PDF using Acrobat Distiller. We encourage our authors to use outline fonts coupled with embedding of the used subset of all fonts (in either Truetype or Type 1 formats) except for the standard Acrobat typeface families of Times, Helvetica (Arial), Courier and Symbol. In the case of papers prepared using TEX or LATEX we endeavour to use subsetted Type 1 fonts, supplied by Y&Y Inc., for the Computer Modern, Lucida Bright and Mathtime families, rather than the public-domain Computer Modern bitmapped fonts. Note that the Y&Y font subsets are embedded under a site license issued by Y&Y Inc. For further details of site licensing and purchase of these fonts visit",
            "group": 1560,
            "name": "10.1.1.62.7271",
            "keyword": "",
            "title": "Pareto-Based Optimization for Multi-objective Nurse Scheduling * Corresponding author"
        },
        {
            "abstract": "We develop a simulated annealing technique to jointly optimize a distributed quantization structure meant to maximize the asymptotic error exponent of a downstream classifier or detector. This distributed structure sequentially processes an input vector and exploits broadcasts to improve the best possible error exponents. The annealing approach is a robust technique that avoids local maxima and is easily tailored to a broadcast quantizer\u2019s structural constraints. 1.",
            "group": 1561,
            "name": "10.1.1.62.7599",
            "keyword": "",
            "title": "Joint Optimization of Distributed Broadcast Quantization Systems for Classification"
        },
        {
            "abstract": "graduated in 2003 and obtained the Master\u2019s degree",
            "group": 1562,
            "name": "10.1.1.62.7749",
            "keyword": "",
            "title": "Representation Database. OPTIMIZATION METHODS FOR AREA AGGREGATION Abstract IN LAND COVER MAPS"
        },
        {
            "abstract": "",
            "group": 1563,
            "name": "10.1.1.62.8631",
            "keyword": "",
            "title": "1 Introduction On the Interplay Between Morphological, Neural, and Environmental Dynamics: A Robotic Case Study"
        },
        {
            "abstract": "Abstract-One way to alleviate the heavy computation required by simulated annealing placement algorithms is to replace a significant fraction of the higher or middle temperatures with a faster heuristic, and then follow it with simulated annealing. A crucial issue in this ap-proach is the determination of the starting temperature for the simu-lated annealing phase-a temperature should be chosen that causes an appropriate amount of optimization to he done, but makes good use of the structure provided by the heuristic. This paper presents a method for measuring the temperature of an existing placement. The approach is based on the measurement of the probability distribution of the change in cost function, P(AC), and makes the assumption that the placement is in simulated annealing equilibrium at some temperature. The temperature of placements produced both by a simulated anneal-ing and a min-cut placement algorithm are measured, and good agree-ment with known temperatures is obtained. The P ( A C) distribution",
            "group": 1564,
            "name": "10.1.1.62.8702",
            "keyword": "",
            "title": "Temperature measurement and equilibrium dynamics of simulated annealing placements"
        },
        {
            "abstract": "Stochastic simulations are able to capture the fine grain behaviour and randomness of outcome of biological networks not captured by deterministic techniques. As such they are becoming an increasingly important tool in the biological community. However, current efforts in the stochastic simulation of biological networks are hampered by two main problems: firstly the lack of complete knowledge of kinetic parameters; and secondly the computational cost of the simulations. In this paper we investigate these problems using the framework of stochastic Petri nets. We present a new stochastic Petri net simulation tool NASTY which allows large numbers of stochastic simulations to be carried out in parallel. We then begin to address the important problem of incomplete knowledge of kinetic parameters by developing a distributed genetic algorithm, based on NASTY\u2019s simulation engine, to parameterise stochastic networks. Our algorithm is able to successfully estimate kinetic parameters to replicate a systems behaviour and we illustrate this by presenting a case study in which the kinetic parameters are derived for a stochastic model of the stress response pathway in the bacterium E.coli.",
            "group": 1565,
            "name": "10.1.1.62.9231",
            "keyword": "Petri netsGenetic AlgorithmsSystems BiologyKinetic parametersStochastic simulation",
            "title": "Newcastle upon Tyne, NE1 7RU, UK. Automatic Parameterisation of Stochastic Petri Net Models of Biological Networks Abstract"
        },
        {
            "abstract": "Abstract. This paper deals with Automated Test Pattern Generation (ATPG) for large synchronous sequential circuits and describes a new approach based on Simulated Annealing. Simulation-based ATPG tools have several advantages with respect to deterministic and symbolic ones, especially because they can deal with large circuits. A prototypical system named SAARA is used to assess the effectiveness of the Simulated Annealing approach in terms of test quality and CPU time requirements. Results are reported, showing that SAARA is able to deal with large sequential circuits. A comparison with a state-of-the-art ATPG tool based on a Genetic Algorithm shows that SAARA generally improves the attained results in terms of fault coverage. 1.",
            "group": 1566,
            "name": "10.1.1.62.9344",
            "keyword": "Simulated AnnealingTest Pattern Generation",
            "title": "SAARA: a Simulated Annealing Algorithm for Test Pattern Generation for Digital Circuits"
        },
        {
            "abstract": "A central challenge in learning probabilistic graphical models is dealing with domains that involve hidden variables. The common approach for learning model parameters in such domains is the expectation maximization (EM) algorithm. This algorithm, however, can easily get trapped in suboptimal local maxima. Learning the model structure is even more challenging. The structural EM algorithm can adapt the structure in the presence of hidden variables, but usually performs poorly without prior knowledge about the cardinality and location of the hidden variables. In this work, we present a general approach for learning Bayesian networks with hidden variables that overcomes these problems. The approach builds on the information bottleneck framework of Tishby et al. (1999). We start by proving formal correspondence between the information bottleneck objective and the standard parametric EM functional. We then use this correspondence to construct a learning algorithm that combines an information-theoretic smoothing term with a continuation procedure. Intuitively, the algorithm bypasses local maxima and achieves superior solutions by following a continuous path from a solution of, an easy and smooth, target function, to a solution of the desired likelihood function. As we show, our algorithmic framework allows learning of the parameters as well as the structure of a network. In addition, it also allows us to introduce new hidden variables during model selection and learn their cardinality. We demonstrate the performance of our procedure on several challenging real-life data sets.",
            "group": 1567,
            "name": "10.1.1.63.317",
            "keyword": "Bayesian networkshidden variablesinformation bottleneckcontinuationvariational",
            "title": "Learning hidden variable networks: The information bottleneck approach"
        },
        {
            "abstract": "The dominant motivational paradigm in embodied AI so far is based on the classical behaviorist approach of reward and punishment. The paper introduces a new principle based on \u2019flow theory\u2019. This new, \u2018autotelic\u2019, principle proposes that agents can become self-motivated if their target is to balance challenges and skills. The paper presents an operational version of this principle and argues that it enables a developing robot to self-regulate his development. 1 1",
            "group": 1568,
            "name": "10.1.1.63.421",
            "keyword": "",
            "title": "2004b) The Autotelic Principle"
        },
        {
            "abstract": "In this thesis, we present new methods for solving nonlinear optimization problems. These problems are di cult to solve because the nonlinear constraints form feasible regions that are di cult to nd, and the nonlinear objectives contain local minima that trap descent-type search methods. In order to nd good solutions in nonlinear optimization, we focus on the following two key issues: how to handle nonlinear constraints and how to escape from local minima. We use a Lagrange-multiplier-based formulation to handle nonlinear constraints, and develop Lagrangian methods with dynamic control to provide faster and more robust convergence. We extend the traditional Lagrangian theory for the continuous space to the To my wife Lei, my parents, and my son Charles discrete space and develop e cient discrete Lagrangian methods. To overcome local minima, we design a new trace-based global-search method that relies on an external traveling trace to pull a search trajectory out of a local optimum in a continuous fashion without having to restart the search from a new starting point. Good starting points identi ed in the global search are used in the local search to identify true local optima. By combining these new methods, we develop a prototype, called Novel (Nonlinear Optimization Via External Lead),",
            "group": 1569,
            "name": "10.1.1.63.462",
            "keyword": "",
            "title": "DEDICATION GLOBAL SEARCH METHODS FOR SOLVING NONLINEAR OPTIMIZATION PROBLEMS"
        },
        {
            "abstract": "Abstract. A standard problem within universities is that of Teaching Space Allocation; the assignment of rooms and times to various teaching activities. The focus is usually on courses that are expected to fit into one room. However, it can also happen that the course will need to be broken up, or \u201csplit\u201d, into multiple sections. A lecture might be too large to fit into any one room. Another common example is that the course corresponds to seminars or tutorials, and although hundreds of students are enrolled, each individual class, or event, should be just tens of students in order to meet student and institutional preferences. Typically, decisions as to how to split courses need to be made within the context of limited space requirements. Institutions do not have an unlimited number of teaching rooms, and need to effectively use those that they do have. The efficiency of space usage is usually measured by the overall \u201cutilisation \u201d which is basically the fraction of the available seat-hours that are actually used. A multi-objective optimisation problem naturally arises; with a trade-off between satisfying preferences on splitting, a desire to increase utilisation, and also to satisfy other constraints such as those based on event location, and timetabling conflicts. In this paper we explore such trade-off surfaces. The explorations themselves are based on a local search method we introduce that attempts to optimise the space utilisation by means of a \u201cdynamic splitting \u201d strategy. The local moves are designed to improve utilisation and the satisfaction of other constraints, but are also allowed to split, and un-split, courses so as to simultaneously meet the splitting objectives. 1",
            "group": 1570,
            "name": "10.1.1.63.1031",
            "keyword": "",
            "title": "The teaching space allocation problem with splitting"
        },
        {
            "abstract": "This paper addresses the problem of real-time 3D modelbased tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the properties of these two sensor systems and show that this leads to some non-trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly combining the pose estimates they produce. Further we show how on-line learning can be used to improve the performance of feature tracking. Finally, to aid real-time performance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combination of these techniques results in a system which is capable of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50 \u25e6 camera shake at 6Hz. 1.",
            "group": 1571,
            "name": "10.1.1.63.1423",
            "keyword": "",
            "title": "Fusing points and lines for high performance tracking"
        },
        {
            "abstract": "Delay estimation of a code-spread code-division multiple-access (CS-CDMA) system using low-rate maximum free distance (MFD) convolutional codes for bandwidth expansion is studied. The MFD codes used, have a repetitive structure, which is utilized for non-data-aided synchronization. The repetition pattern of the code is optimized for good delay estimation properties, using an optimization criterion based on the Cram\u00e9r-Rao bound for the delay estimation problem. An approximate maximum-likelihood estimator that jointly estimates the channel gain, the codewords and the delay is proposed. A corresponding estimator for direct-sequence CDMA (DS-CDMA) is also derived. The performance, in terms of estimation error, outlier rate and bit-error rate, for both the systems are compared. 1",
            "group": 1572,
            "name": "10.1.1.63.1595",
            "keyword": "",
            "title": "Synchronization of Code-Spread CDMA Systems"
        },
        {
            "abstract": "of the requirements for the degree",
            "group": 1573,
            "name": "10.1.1.63.2345",
            "keyword": "",
            "title": "c \u25cb Copyright by"
        },
        {
            "abstract": "Abstract. A standard problem within universities is that of Teaching Space Allocation (TSA); the assignment of rooms and times to various teaching activities. The focus is usually on courses that are that are expected to fit into one room (and we have studied this case elsewhere [3]). However, it can also happen that the course will need to be broken up, or \u201csplit\u201d, into multiple sections. A lecture might be too large to fit into any one room. Another common example is that the course corresponds to seminars or tutorials, and although hundreds of students are enrolled, each individual class, or event, should be just tens of students in order to meet student and institutional preferences. Typically, decisions as to how to split courses need to be made within the context of limited space requirements: institutions do not have an unlimited number of teaching rooms, and need to effectively use those that they do have. The efficiency of space usage in usually measured by the overall \u201cutilisation \u201d which is basically the fraction of the available seat-hours that are actually used. A multi-objective optimisation problem naturally arises; with a trade-off between satisfying preferences on splitting, a desire to increase utilisation, and also to satisfy other constraints such as those based on event location, and timetabling conflicts. In this paper we explore such trade-off surfaces. The explorations themselves are based on a local search method we introduce that optimises the space utilisation by means of a \u201cdynamic splitting \u201d strategy. The local moves are designed to improve utilisation and the satisfaction of other constraints, but are also allowed to split, and un-split, courses so as to simultaneously meet the splitting objectives. 1",
            "group": 1574,
            "name": "10.1.1.63.2597",
            "keyword": "",
            "title": "The teaching space allocation problem with splitting"
        },
        {
            "abstract": "Self-Assembly is a powerful autopoietic mechanism ubiquitous throughout the natural world. It may be found at the molecular scale and also at astronomical scales. Self-assembly power lays in the fact that it is a distributed, not-necessarily synchronous, control mechanism for the bottom-up manufacture of complex systems. Control of the assembly process is shared across a myriad of elemental components, none of which has either the storage or the computation capabilities to know and follow a master plan for the assembly of the intended system. In this paper we present an evolutionary algorithm which is capable of programming the so called \u201cWang Tiles \u201d for the self-assembly of two-dimensional squares. 1",
            "group": 1575,
            "name": "10.1.1.63.2673",
            "keyword": "",
            "title": "Automated Tile Design for Self-Assembly Conformations"
        },
        {
            "abstract": "In this paper we provide a detailed and comprehensive survey of proposed approaches for network design, charting the evolution of models and techniques for the automatic planning of cellular wireless services. These problems present themselves as a trade-off between commitment to infrastructure and quality of service, and have become increasingly complex with the advent of more sophisticated protocols and wireless architectures. Consequently these problems are receiving increased attention from researchers in a variety of fields who adopt a wide range of models, assumptions and methodologies for problem solution. We seek to unify this dispersed and fragmented literature by charting the evolution of centralised planning for cellular systems.",
            "group": 1576,
            "name": "10.1.1.63.2916",
            "keyword": "",
            "title": "Evolution of planning for wireless communication systems"
        },
        {
            "abstract": "Mean field annealing (MFA) algorithm, proposed for solving combinatorial optimization problems, combines the characteristics of neural networks and simulated annealing. Previous works on MFA resulted with successful mapping of the algorithm to some classic optimization problems such as traveling salesperson problem, scheduling problem, knapsack problem and graph partitioning problem. In this paper, MFA is formulated for the circuit partitioning problem using the so called net-cut model. Hence, the deficiencies of using the graph representation for electrical circuits are avoided. An efficient implementation scheme, which decreases the complexity of the proposed algorithm by asymptotical factors is also developed. Comparative performance analysis of the proposed algorithm with two well-known heuristics, simulated annealing and Kernighan-Lin, indicates that MFA is a successful alternative heuristic for the circuit partitioning problem.",
            "group": 1577,
            "name": "10.1.1.63.3618",
            "keyword": "Mean field annealingcircuit partitioningnet-cut model",
            "title": "Circuit partitioning using mean field annealing"
        },
        {
            "abstract": "Abstract \u2014 This paper presents a two-pass algorithm for estimating motion vectors from image sequences. In the proposed algorithm, the motion estimation is formulated as a problem of obtaining the maximum a posteriori in the Markov random field (MAP-MRF). An optimization method based on the mean field theory (MFT) is opted to conduct the MAP search. The estimation of motion vectors is modeled by only two MRF\u2019s, namely, the motion vector field and unpredictable field. Instead of utilizing the line field, a truncation function is introduced to handle the discontinuity between the motion vectors on neighboring sites. In this algorithm, a \u201cdouble threshold \u201d preprocessing pass is first employed to partition the sites into three regions, whereby the ensuing MFT-based pass for each MRF is conducted on one or two of the three regions. With this algorithm, no significant difference exists between the block-based and pixel-based MAP searches any more. Consequently, a good compromise between precision and efficiency can be struck with ease. To render our algorithm more resilient against noises, the mean absolute difference instead of mean square error is selected as the measure of difference, which is more reliable according to the knowledge of robust statistics. This is supported by our experimental results from both synthetic and real-world image sequences. The proposed two-pass algorithm is much faster than any other MAP-MRF motion estimation method reported in the literature so far. Index Terms\u2014Image processing, Markov random field, motion estimation, object detection, video coding.",
            "group": 1578,
            "name": "10.1.1.63.4517",
            "keyword": "",
            "title": "An Efficient Two-Pass MAP-MRF Algorithm for Motion Estimation Based on Mean Field Theory"
        },
        {
            "abstract": "and upper mantle",
            "group": 1579,
            "name": "10.1.1.63.4607",
            "keyword": "",
            "title": "Monte-Carlo inversion for a global shear velocity model of the crust and upper mantle, Geophys"
        },
        {
            "abstract": " ",
            "group": 1580,
            "name": "10.1.1.63.4710",
            "keyword": "",
            "title": "  Variable Precision Analysis for FPGA Synthesis"
        },
        {
            "abstract": "Abstract- Evolutionary computation courses have been offered by a wide range of departmentsJschoo1s to stu-dents with many different backgrounds. This paper de-scribes three postgraduate courses with significant evo-lutionary computation components offered by the Uni-",
            "group": 1581,
            "name": "10.1.1.63.4736",
            "keyword": "",
            "title": "How Does Evolutionary Computation Fit Into IT Postgraduate Teaching"
        },
        {
            "abstract": "In this thesis we present new methods for the automated design of new heuristics in knowledge-lean applications and for finding heuristics that can be generalized to unlearned test cases. These applications lack domain knowledge for credit assignment; hence, operators for composing new heuristics are generally model free, domain independent, and syntactic in nature. The operators we have used are genetics based; examples of which include mutation and crossover. Learning is based on a generate-and-test paradigm that maintains a pool of competing heuristics, tests them to a limited extent, creates new ones from those that perform well in the past, and prunes poor ones from the pool. We have studied four important issues in learning better heuristics: (a) partitioning of a problem domain into smaller subsets, called subdomains, so that performance values within each subdomain can be evaluated statistically, (b) anomalies in performance evaluation within a subdomain, (c) rational scheduling of limited computational resources in testing candidate heuristics in single-objective as well as multi-objective learning, and (d) nding heuristics that can be generalized to unlearned subdomains. We show experimental results in learning better heuristics for (a) process placement for distributed-memory multicomputers, (b) node decomposition in a branch-and-bound search, (c) generation of test patterns in VLSI circuit testing, (d) VLSI cell placement and routing, and (e) blind equalization.  ",
            "group": 1582,
            "name": "10.1.1.63.4937",
            "keyword": "iv",
            "title": "Automated Design of Knowledge-Lean Heuristics: Learning, Resource Scheduling, and Generalization"
        },
        {
            "abstract": "[11]) demonstrate the discovery of putative disease subtypes from gene expression data. The underlying computational problem is to partition the set of sample tissues into statistically meaningful classes. In this paper we present a novel approach to class discovery and develop automatic analysis methods. Our approach is based on statistically scoring candidate partitions according to the overabundance of genes that separate the different classes. Indeed, in biological datasets, an overabundance of genes separating known classes is typically observed. we measure overabundance against a stochastic null model. This allows for highlighting subtle, yet meaningful, partitions that are supported on a small subset of the genes. Using simulated annealing we explore the space of all possible partitions of the set of samples, seeking partitions with statistically significant overabundance of differentially expressed genes. We demonstrate the performance of our methods on synthetic data, where we recover planted partitions. Finally, we turn to tumor expression datasets, and show that we find several highly pronounced partitions. 1.",
            "group": 1583,
            "name": "10.1.1.63.4987",
            "keyword": "",
            "title": "Agilent Laboratories"
        },
        {
            "abstract": "Abstract-- Location area (LA) planning plays an important role in cellular networks because of the trade-off caused by paging and registration signaling. The upper bound on the size of an LA is the service area of a mobile switching center (MSC). In that extreme case, the cost of paging is at its maximum, but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal, but the registration cost is the largest. In general, the most important component of these costs is the load on the signaling resources. Between the extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we try to find an optimal method for determining the location areas. For that purpose, we use the available network information to formulate a realistic optimization problem. We propose an algorithm based on simulated annealing (SA) for the solution of the resulting problem. Then, we investigate the quality of the SA technique by comparing its results to greedy search and random generation methods.",
            "group": 1584,
            "name": "10.1.1.63.5656",
            "keyword": "Index terms\u2014Location AreaCellular NetworksSimulated",
            "title": "Location Area Planning in Cellular Networks Using Simulated Annealing *"
        },
        {
            "abstract": "In this paper, we propose a new probabilistic sampling procedure and its application in simulated annealing (SA). The new procedure uses Bayesian analysis to evaluate samples made already and draws the next sample based on a density function constructed through Bayesian analysis. After integrating our procedure in SA, we apply it to solve a set of optimization benchmarks. Our results show that our proposed procedure, when used in SA, is very effective in generating highquality samples that are more reliable and robust in leading to global solutions. 1",
            "group": 1585,
            "name": "10.1.1.63.9215",
            "keyword": "",
            "title": "Data sampling using Bayesian analysis and its applications in simulated annealing"
        },
        {
            "abstract": "Previous papers in this series of statistical mechanics of neocortical interactions (SMNI) have detailed a development from the relatively microscopic scales of neurons up to the macroscopic scales as recorded by electroencephalography (EEG), requiring an intermediate mesocolumnar scale to be developed at the scale of minicolumns ( \u2248 10 2 neurons) and macrocolumns ( \u2248 10 5 neurons). Opportunity was taken to view SMNI as sets of statistical constraints, not necessarily describing specific synaptic or neuronal mechanisms, on neuronal interactions, on some aspects of short-term memory (STM), e.g., its capacity, stability, and duration. A recently developed C-language code, PATHINT, provides a non-Monte Carlo technique for calculating the dynamic evolution of arbitrary-dimension (subject to computer resources) nonlinear Lagrangians, such as derived for the two-variable SMNI problem. Here, PATHINT is used to explicitly detail the evolution of the SMNI constraints on STM.",
            "group": 1586,
            "name": "10.1.1.63.9433",
            "keyword": "",
            "title": "Statistical mechanics of neocortical interactions: Path-integral evolution of short-term memory"
        },
        {
            "abstract": "A new mapping heuristic is developed, based on the recently proposed Mean Field Annealing (MFA) algorithm. An efficient implementation scheme, which decreases the complexity of the proposed algorithm by asymptotical factors, is also given. Performance of the proposed MFA algorithm is evaluated in comparison with two wellknown heuristics; Simulated Annealing and Kernighan-Lin. Results of the experiments indicate that MFA can be used as an alternative heuristic for solving the mapping problem. Inherent parallelism of MFA is exploited by designing an efficient parallel algorithm for the proposed MFA heuristic. ",
            "group": 1587,
            "name": "10.1.1.63.9577",
            "keyword": "",
            "title": "A NEW MAPPING HEURISTIC BASED ON MEAN FIELD ANNEALING"
        },
        {
            "abstract": "computational technique for finding optimal solutions to combinatorial problems for which the combinatorial explosion phenomenon rules out the possibility of systematically examining each alternative. It is currently being applied to the practical problem of optimizing the physical design of computer circuitry, and to the theoretical problems of resolving patterns of auditory and visual stimulation into meaningful arrangements of phonemes and three-dimensional objects. Grammatical parsing-- resolving unanalysed linear sequences of words into meaningful grammatical structures-- can be regarded as a perception problem logically analogous to",
            "group": 1588,
            "name": "10.1.1.63.9590",
            "keyword": "",
            "title": "I. Simulated annealing (e.g."
        },
        {
            "abstract": "In embedded multiprocessors cache partitioning is a known technique to eliminate inter-task cache conflicts, so to increase predictability. On such systems, the partitioning ratio is a parameter that should be tuned to optimize performance. In this paper we propose a Simulated Annealing (SA) based heuristic to determine the cache partitioning ratio that maximizes an application\u2019s throughput. In its core, the SA method iterates many times over many partitioning ratios, checking the resulted throughput. Hence the throughput of the system has to be estimated very fast, so we utilize a light simulation strategy. The light simulation derives the throughput from tasks \u2019 timings gathered off-line. This is possible because in an environment where tasks don\u2019t interfere with each other, their performance figures can be used in any possible combination. An application of industrial relevance (H.264 decoder) running on a parallel homogeneous platform is used to demonstrate the proposed method. For the H.264 application 9 % throughput improvement is achieved when compared to the throughput obtained using methods of partitioning for the least number of misses. This is a significant improvement as it represents 45 % from the theoretical throughput improvement achievable when assuming an infinite cache. 1.",
            "group": 1589,
            "name": "10.1.1.63.9670",
            "keyword": "",
            "title": "Throughput optimization via cache partitioning for embedded multiprocessors"
        },
        {
            "abstract": "The application of robots in critical missions in hazardous environments requires the development of reliable or fault tolerant manipulators. In this paper, we define fault tolerance as the ability to continue the performance of a task after immobilization of a joint due to failure. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical design procedure can be used, as is illustrated through an example. 1",
            "group": 1590,
            "name": "10.1.1.64.116",
            "keyword": "",
            "title": "Mapping tasks into fault tolerant manipulators"
        },
        {
            "abstract": "To exploit the vast data obtained from high throughput molecular biology, a variety of modelling and analysis techniques must be fully utilised. In this thesis, Petri nets are investigated within the context of computational systems biology, with the specific focus of facilitating the creation and analysis of models of biological pathways. The analysis of qualitative models of genetic networks using safe Petri net techniques was investigated with particular reference to model checking. To exploit existing model repositories a mapping was presented for the automatic translation of models encoded in the Systems Biology Markup Language (SBML) into the Petri Net framework. The mapping is demonstrated via the conversion and invariant analysis of two published models of the glycolysis pathway. Dynamic stochastic simulations of biological systems suffer from two problems: computational cost; and lack of kinetic parameters. A new stochastic Petri net simulation tool, NASTY was developed which addresses the prohibitive real-time",
            "group": 1591,
            "name": "10.1.1.64.468",
            "keyword": "",
            "title": "Modelling Bacterial Regulatory Networks with Petri Nets"
        },
        {
            "abstract": null,
            "group": 1592,
            "name": "10.1.1.64.1008",
            "keyword": "",
            "title": "Machine Learning for Automation and Full Documentation \u2022 Asymptotic Results are Irrelevant for Optimization 21.2 Reactive Search Applied to Tabu Search............... 21-3 Prohibition-Based Diversification: Tabu Search \u2022 Reaction"
        },
        {
            "abstract": "Abstract-- Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage. I.",
            "group": 1593,
            "name": "10.1.1.64.1205",
            "keyword": "",
            "title": "Power Efficient Organization of Wireless Sensor Networks"
        },
        {
            "abstract": "Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm\u2019s empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods.",
            "group": 1594,
            "name": "10.1.1.64.1853",
            "keyword": "",
            "title": "An evolutionary algorithm that constructs recurrent neural networks"
        },
        {
            "abstract": "Abstract. Stemmatology studies relations among different variants of a text that has been gradually altered as a result of imperfectly copying the text over and over again. Applications are mainly in humanities, especially textual criticism, but the methods can be used to study the evolution of any symbolic objects, including chain letters and computer viruses.We propose an algorithm for stemmatic analysis based on a minimum-information criterion and stochastic tree optimization. Our approach is related to phylogenetic reconstruction criteria such as maximum parsimony and maximum likelihood, and builds upon algorithmic techniques developed for bioinformatics. Unlike many earlier methods, the proposed method does not require significant preprocessing of the data but rather, operates directly on aligned text files. We demonstrate our method on a realworld experiment involving all 52 known variants of the legend of St. Henry of Finland, and provide the first computer-generated family tree of the legend. The obtained tree of the variants is supported to a large extent by results obtained with more traditional methods, and identifies a number of previously unrecognized relations. 1",
            "group": 1595,
            "name": "10.1.1.64.2113",
            "keyword": "",
            "title": "A compression-based method for stemmatic analysis, 2006. Full version available at http://www.cs.helsinki.fi/teemu.roos/pub/ecai06 full.pdf"
        },
        {
            "abstract": "Concerning the simulation of neural networks with arbitrary topology on distributed memory multiprocessor systems, we introduce our approach to an automatic determination of a near-optimal mapping of neural networks onto a given multiprocessor. Our approach is based on stochastic local search. We propose a decomposition of the mapping into a network partitioning step followed by a placement of partitions onto processors. 1",
            "group": 1596,
            "name": "10.1.1.64.3688",
            "keyword": "",
            "title": "How to find a near-optimal mapping of neural networks onto message passing multicomputers"
        },
        {
            "abstract": "Abstract\u2014In this paper, we develop a joint Network Coding (NC)-channel coding error-resilient sensor-network approach that performs In-Network Processing based on channel code Design (INPoD). INPoD represents a major development of an underlying framework for designing Code-on-Network-Graph (CNG). CNG (and hence INPoD) maps variable nodes of Low Density Parity Check (LDPC) codes onto sensor nodes, and consequently translates check equations (used in linear algebraic LDPC codes) into in-network processing. INPoD/CNG codes not only improve capacity, but are also resilient to errors in noisy environments. In absence of INPoD, basic CNG employs standard LDPC codes while assuming the underlying sensor network is capable of supporting the degree distribution dictated by these codes. In practice, however, we usually have the network topology pre-determined by the placement of the sensors, and hence we are constrained to map a code onto a given topology. In this paper, we specifically address this problem, and propose the INPoD framework, which enables the use of LDPC design tools in the design of CNG for a given sensor network topology. We formulate the design of CNG, as a convex optimization problem, which determines the best codes to be used, given the underlying network connectivity and channel conditions. Specifically, we use density evolution to design degree distributions which controls the performance of the joint NCchannel code CNG. We also give a code construction algorithm, which achieves a designed degree distribution. We show that well-designed INPoD provide gains of 1.5 to 2.5 dB, when compared with the best known erasure codes \u2013 LT codes.",
            "group": 1597,
            "name": "10.1.1.64.3726",
            "keyword": "Index Terms\u2014Low Density Parity Check CodesLT codesWireless Sensor NetworksNetwork Coding",
            "title": "INPoD: In-Network Processing over Sensor Networks based on Code Design"
        },
        {
            "abstract": "Modern distributed information systems are gaining an increasing importance in our every day lives. As access to networked applications become omnipresent through PC\u2019s, hand-held and wireless devices, more and more economical,",
            "group": 1598,
            "name": "10.1.1.64.3946",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "This highly interdisciplinary project extends previous work in combat modeling and in control-theoretic descriptions of decision-making human factors in complex activities. A previous paper has established the first theory of the statistical mechanics of combat (SMC), developed using modern methods of statistical mechanics, baselined to empirical data gleaned from the National Training Center (NTC). This previous project has also established a JANUS(T)-NTC computer simulation/wargame of NTC, providing a statistical \u2018\u2018what-if \u2019\u2019 capability for NTC scenarios. This mathematical formulation is ripe for control-theoretic extension to include human factors, a methodology previously developed in the context of teleoperated vehicles. Similar NTC scenarios differing at crucial decision points will be used for data to model the influence of decision making on combat. The results may then be used to improve present human factors and C 2 algorithms in computer simulations/wargames. Our approach is to \u2018\u2018subordinate\u2019 \u2019 the SMC nonlinear stochastic equations, fitted to NTC scenarios, to establish the zeroth order description of that combat. In practice, an equivalent mathematical-physics representation is used, more suitable for numerical and formal work, i.e., a Lagrangian representation. Theoretically, these equations are nested within a larger set of nonlinear stochastic operator-equations which include C 3 human factors, e.g., supervisory decisions. In this study, we propose to perturb this operator theory about the SMC zeroth order set of equations. Then, subsets of scenarios fit to zeroth order, originally considered to be similarly degenerate, can be further split perturbatively to distinguish C 3 decision-making influences. New methods of Very Fast Simulated Re-Annealing (VFSR), developed in the previous project, will be used for fitting these models to empirical data.",
            "group": 1599,
            "name": "10.1.1.64.4338",
            "keyword": "",
            "title": "and"
        },
        {
            "abstract": "Abstract With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp, SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wireexposed architectures. In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, much analysis is needed to adapt a stream program to a parallel stream processor. We describe fission and fusion transformations that can be used to adjust the granularity of a stream graph, a layout algorithm for mapping a stream graph to a given network topology, and a scheduling algorithm for generating a fine-grained static communication pattern for each computational element. We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing optimizations. Using the cycle-accurate Raw simulator, we demonstrate that these optimizations can improve performance by up to 145%. We consider this work to be a first step towards a portable programming model for communication-exposed architectures. n 1 Introduction As we approach the billion-transistor era, a number of emerging architectures are addressing the wire delay problem by replicating the basic processing unit and exposing the communication between units to a software layer (e.g., Raw [24], SmartMemories [13], TRIPS [19]). These machines are especially well-suited for streaming applications that have regular communication patterns and widespread parallelism.",
            "group": 1600,
            "name": "10.1.1.64.5200",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "The world-wide increasing demand for energy is one of the key challenges of our century. However, this demand is in conflict to the Kyoto protocols of 1997 and 2001 that contain an international agreement for reducing green house gases. Energy generation concepts involving alternative energy sources and innovative technologies with high thermodynamical efficiencies are needed in order to address these issues. Gas turbines are one of the key energy producing devices of our generation. Improved designs of gas turbine components are necessary in order to address the increasing demands of high performance and reduced emissions. The design of gas turbine components is a complex and time-consuming engineering task that involves meeting of several design objectives and constraints. This task is usually addressed in an iterative process. Advances in domain knowledge and in areas such as information technology offer the possibility of accelerating and improving this design cycle through automated optimization procedures.",
            "group": 1601,
            "name": "10.1.1.64.5393",
            "keyword": "",
            "title": "citizen of Germany accepted on the recommendation of"
        },
        {
            "abstract": "A quantum algorithm for general combinatorial search that uses the underlying structure of the search space to increase the probability of finding a solution is presented. This algorithm shows how coherent quantum systems can be matched to the underlying structure of abstract search spaces, and is analytically simpler than previous structured search methods. The algorithm is evaluated empirically with a variety of search problems, and shown to be particularly effective for searches with many constraints. Furthermore, the algorithm provides a simple framework for utilizing search heuristics. It also exhibits the same phase transition in search difficulty as found for sophisticated classical search methods, indicating it is effectively using the problem structure. 1",
            "group": 1602,
            "name": "10.1.1.64.6087",
            "keyword": "",
            "title": "A framework for structured quantum search"
        },
        {
            "abstract": "Department of Computer Science. University of York,",
            "group": 1603,
            "name": "10.1.1.64.6661",
            "keyword": "",
            "title": "An automated framework for structural test-data generation"
        },
        {
            "abstract": "circuit partitioning algorithms employ the locking mechanism, which enforces each cell to move exactly once per pass. In this paper, we propose two novel approaches for multiway circuit partitioning to overcome this limitation. Our approaches allow each cell to move more than once. Our first approach still uses the locking mechanism but in a relaxed way. It introduces the phase concept such that each pass can include more than one phase, and a phase can include at most one move of each cell. Our second approach does not use the locking mechanism at all. It introduces the mobility concept such that each cell can move as freely as allowed by its mobility. Each approach leads to KL-based generic algorithms whose parameters can be set to obtain algorithms with different performance characteristics. We generated three versions of each generic algorithm and evaluated them on a subset of common benchmark circuits in comparison with Sanchis\u2019 algorithm (FMS) and the simulated annealing algorithm (SA). Experimental results show that our algorithms are efficient, they outperform FMS significantly, and they perform comparably to SA. Our algorithms perform relatively better as the number of parts in the partition increases as well as the density of the circuit decreases. This paper also provides guidelines for good parameter settings for the generic algorithms. Index Terms \u2014 Iterative improvement, Kernighan\u2013Linbased algorithms, move-based partitioning, multiway circuit partitioning, relaxed locking, very large scale integration (VLSI). I.",
            "group": 1604,
            "name": "10.1.1.64.6682",
            "keyword": "",
            "title": "Two Novel Multiway Circuit Partitioning Algorithms Using Relaxed Locking"
        },
        {
            "abstract": "",
            "group": 1605,
            "name": "10.1.1.64.6826",
            "keyword": "",
            "title": "Simulated Annealing For The Optimization OfChemical Batch Production Processes1"
        },
        {
            "abstract": "Scheduling a streaming application on high-performance computing (HPC) resources has to be sensitive to the computation and communication needs of each stage of the application dataflow graph to ensure QoS criteria such as latency and throughput. Since the grid has evolved out of traditional high-performance computing, the tools available for scheduling are more appropriate for batch-oriented applications. Our scheduler, called Streamline, considers the dynamic nature of the grid and runs periodically to adapt scheduling decisions using application requirements (per-stage computation and communication needs), application constraints (such as co-location of stages), and resource availability. The performance of Streamline is compared with an Optimal placement, Simulated Annealing (SA) approximations, and E-Condor, a streaming grid scheduler built using Condor. For kernels of streaming applications, we show that Streamline performs close to the Optimal and SA algorithms, and an order of magnitude better than E-Condor under non-uniform load conditions. We also conduct scalability studies showing the advantage of Streamline over other approaches. 1.",
            "group": 1606,
            "name": "10.1.1.64.7201",
            "keyword": "",
            "title": "Streamline: A Scheduling Heuristic for Streaming Applications on the Grid"
        },
        {
            "abstract": "Seismic data provide detailed information about subsurface structures. Reflection events visible in the seismic data are known as horizons, and indicate boundaries between different rock layers. A fault is a surface along which one side of rock layers has moved relative to the other in a direction parallel to the surface. Faults are recognized in seismic data by discontinuities of horizons. Fault interpretation is an important but time-consuming task of seismic interpretations. It is least supported by automatic tools due to seismic data distortions near fault regions. In this paper, we present an automatic method for correlations of horizons across faults in 3d seismic data. As automating horizons correlations using only seismic data features is not feasible, we find optimal correlations by imposing geological constraints. We formulate the correlation task as a non-rigid continuous point matching between the two sides of the fault. A fault surface is identified and seismic features on both sides of the fault are gathered. One side of the fault serves as the floating image while the other side is the reference image. A fault displacement model is constructed by performing initial discrete match of some prominent regions. Then the simulated annealing optimization technique is used to perform continuous point matching between the two sides according to seismic feature similarity and the fault model. A possible multiresolution scheme for the simulated annealing optimization is discussed. The method was applied to real 3D seismic data, and has shown to produce geologically acceptable results. Key Words: automatic seismic interpretation, model-based correspondence analysis. 1",
            "group": 1607,
            "name": "10.1.1.64.8203",
            "keyword": "",
            "title": "A model-based approach to automatic 3d seismic horizons correlations across faults"
        },
        {
            "abstract": "Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which itis impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three di erent classes of randomly generated Boolean Satis ability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may bevery large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may becomputationally expensive ifthe local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms. 1.",
            "group": 1608,
            "name": "10.1.1.64.8380",
            "keyword": "",
            "title": "When Gravity Fails: Local Search Topology"
        },
        {
            "abstract": "Simulation of logic designs is a very important part of the VLSI-design process. The increasing size of the designs requires more e cient simulation strategies to accelerate the simulation process. Parallel logic simulation seems to be a promising approach in this direction. This paper describes the ba-sic principles of parallel logic simulation, discusses di erent approaches, and surveys the research done in this eld so far. 1",
            "group": 1609,
            "name": "10.1.1.64.8646",
            "keyword": "",
            "title": "A Survey on Parallel Logic Simulation"
        },
        {
            "abstract": "problems",
            "group": 1610,
            "name": "10.1.1.64.8860",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": " ",
            "group": 1611,
            "name": "10.1.1.64.9532",
            "keyword": "",
            "title": "Ant Colony Optimization -- Artificial Ants as a Computational Intelligence Technique"
        },
        {
            "abstract": "In an attempt to ensure good-quality printouts of our technical reports, from the supplied PDF files, we process to PDF using Acrobat Distiller. We encourage our authors to use outline fonts coupled with embedding of the used subset of all fonts (in either Truetype or Type 1 formats) except for the standard Acrobat typeface families of Times, Helvetica (Arial), Courier and Symbol. In the case of papers prepared using TEX or LATEX we endeavour to use subsetted Type 1 fonts, supplied by Y&Y Inc., for the Computer Modern, Lucida Bright and Mathtime families, rather than the public-domain Computer Modern bitmapped fonts. Note that the Y&Y font subsets are embedded under a site license issued by Y&Y Inc. For further details of site licensing and purchase of these fonts visit",
            "group": 1612,
            "name": "10.1.1.64.9900",
            "keyword": "",
            "title": "A Scatter Search for the Nurse Rostering Problem"
        },
        {
            "abstract": "In this paper, we propose a new dispatching rule and a set of local search algorithms based on the \u00aeltered beam search, GRASP and simulated annealing methodologies to construct short-term observation schedules of space mission projects, mainly for NASA's Hubble Space Telescope (HST). The main features of generating short-term observations of HST are state dependent set up times, user speci\u00aeed deadlines, visibility windows of the targets and the priorities assigned to the observations. The objective of HST scheduling is to maximize the scienti\u00aec return. We have tested the relative performances of the proposed algorithms including the nearest neighbor rule both in objective function value and computational time aspects by utilizing a full-factorial experimental design.",
            "group": 1613,
            "name": "10.1.1.65.99",
            "keyword": "Space mission projectsschedulinglocal search algorithms",
            "title": "Generating short-term observation schedules for space mission projects"
        },
        {
            "abstract": "Simulated annealing is a probabilistic algorithm that has shown some promise when applied to combinatorially NP-hard problems. One advantage of the simulated annealing algorithm is that it is based on an analogy with statistical mechanics which is not problem-specific. How-ever, any implementation of the algorithm for a given problem requires that several specific choices be made. The success or failure of the proce-dure may depend on these choices. In this study we explore the effect of choice of neighborhood size on the algorithm\u2019s performance when applied to the travelling salesman problem.",
            "group": 1614,
            "name": "10.1.1.65.210",
            "keyword": "Key Words and PhrasesSimulated Annealingtravelling salesman",
            "title": "t AMERICAN JOURNAL OF MATHEMATICAL AND MANAGEMENT SCIENCES CopyllgMQ ISM by American Sclonur Pmr, Inc. NEIGHBORHOOD SIZE IN THE SIMULATED ANNEALING ALGORITHM"
        },
        {
            "abstract": "Abstract: A common weakness of local search metaheuristics, such as Simulated Annealing, in solving combinatorial optimisation problems, is the necessity of setting a certain number of parameters. This tends to generate a significant increase in the total amount of time required to solve the problem and often requires a high level of experience from the user. This paper is motivated by the goal of overcoming this drawback by employing &quot;parameter-free &quot; techniques in the context of automatically solving course timetabling problems. We employ local search techniques with &quot;straightforward &quot; parameters, i.e. ones that an inexperienced user can easily understand. In particular, we present an extended variant of the &quot;Great Deluge &quot; algorithm, which requires only two parameters (which can be interpreted as search time and an estimation of the required level of solution quality). These parameters affect the performance of the algorithm so that a longer search provides a better result- as long as we can intelligently stop the approach from converging too early. Hence, a user can choose a balance between processing time and the quality of the solution. The proposed method has been tested on a range of university course timetabling problems and the results were evaluated within an International Timetabling Competition. The effectiveness of the proposed technique has been confirmed by a high level of quality of results. These results represented the third overall average rating among 21 participants and the best solutions on 8 of the 23 test problems.",
            "group": 1615,
            "name": "10.1.1.65.234",
            "keyword": "Combinatorial optimisationmetaheuristiclocal searchtimetabling",
            "title": "A time-predefined approach to course timetabling"
        },
        {
            "abstract": "In the vehicle routing problem with stochastic demands a vehicle has to serve a set of customers whose exact demand is known only upon arrival at the customer\u2019s location. The objective is to find a permutation of the customers (an a priori tour) that minimizes the expected distance traveled by the vehicle. Since the objective function is computationally demanding, effective approximations of it could improve the algorithms\u2019 performance. For the problem under study, we show that a good choice is using the length of the a priori tour as a fast approximation of the objective, to be used in the local search of the several metaheuristics analyzed. We also show that for the instances tested, our metaheuristics find better solutions with respect to a known effective heuristic and with respect to solving the problem as two related deterministic problems.",
            "group": 1616,
            "name": "10.1.1.65.578",
            "keyword": "",
            "title": "Metaheuristics for the vehicle routing problem with stochastic demands"
        },
        {
            "abstract": "How to recognise different types of shape from quite a long way away i Random Structures and Evolution of Biopolymers",
            "group": 1617,
            "name": "10.1.1.65.1002",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Creating a new FPGA is a challenging undertaking because of the significant effort that must be spent on circuit design, layout and verification. It currently takes approximately 50 to 200 person years from architecture definition to tape-out for a new FPGA family. Such a lengthy development time is necessary because the process is primarily done manually. Simplifying and shortening the design process would be advantageous since it could reduce the time to market for new FPGAs while also enhancing architecture explorations. One way to accomplish this is through automation and, in this paper, we describe our efforts to automate the entire process by making use of a previously developed set of tools that assist in the creation of the repeatable FPGA tile [25]. Our aim is to demonstrate the feasibility of a CAD flow that uses an input FPGA architecture description to generate a layout that can be sent for fabrication. We prove the feasibility of this proposition by actually designing and fabricating a complete FPGA. Initial functional testing of the FPGA appears promising but is inconclusive at this time. Through this architecture to layout process, we investigate the issues that are faced in the architecture selection, circuit design, layout and verification of such an automatically produced FPGA. We found that there are significant savings in design time. As well, we demonstrate that we can produce a layout using automated tools that is only 36 % larger than a commercial FPGA device layout. Given the significant time savings and the relatively minor area penalty, we feel that this work demonstrates that automated layout of FPGAs is practical and advantageous.",
            "group": 1618,
            "name": "10.1.1.65.1411",
            "keyword": "Categories and Subject Descriptors B.7.2 [Integrated CircuitsDesign Aids \u2013 layoutplacement and routingverification B.7.1 [Integrated CircuitsTypes and Design Styles \u2013 gate arrays General Terms DesignVerification Keywords FPGAPLDprogrammable logicautomatic layout",
            "title": "Design, Layout and Verification of an FPGA using Automated Tools"
        },
        {
            "abstract": "Abstract. Satisfiability is a class of NP-complete problems that model a wide range of real-world applications. These problems are difficult to solve because they have many local minima in their search space, often trapping greedy search methods that utilize some form of descent. In this paper, we propose a new discrete Lagrange-multiplier-based global-search method (DLM) for solving satisfiability problems. We derive new approaches for applying Lagrangian methods in discrete space, we show that an equilibrium is reached when a feasible assignment to the original problem is found and present heuristic algorithms to look for equilibrium points. Our method and analysis provides a theoretical foundation and generalization of local search schemes that optimize the objective alone and penalty-based schemes that optimize the constraints alone. In contrast to local search methods that restart from a new starting point when a search reaches a local trap, the Lagrange multipliers in DLM provide a force to lead the search out of a local minimum and move it in the direction provided by the Lagrange multipliers. In contrast to penalty-based schemes that rely only on the weights of violated constraints to escape from local minima, DLM also uses the value of an objective function (in this case the number of violated constraints) to provide further guidance. The dynamic shift in emphasis between the objective and the constraints, depending on their relative values, is the key of Lagrangian",
            "group": 1619,
            "name": "10.1.1.65.1657",
            "keyword": "",
            "title": "A discrete lagrangian-based global-search method for solving satisfiability problems"
        },
        {
            "abstract": "In current change management tools, the actual changes occur outside the tool. In contrast, Infuse concentrates on the actual change process and provides facilities for both managing and coordinating source changes. Infuse provides facilities for automatically structuring the cooperation among programmers, propagating changes, and determining the consistency of changes, and provides a basis for negotiating the resolution of conflicting changes and for iterating over a set of changes. 1.",
            "group": 1620,
            "name": "10.1.1.65.2178",
            "keyword": "",
            "title": "Infuse: a tool for automatically managing and coordinating source changes in large systems"
        },
        {
            "abstract": "",
            "group": 1621,
            "name": "10.1.1.65.3081",
            "keyword": "VLSI circuit designCell placement problemField programmable gate arrayMean field annealingNeural-network algorithms",
            "title": "Contributed article"
        },
        {
            "abstract": "Abstract \u2014 Circuit placement has a large impact on all aspects of performance; speed, power consumption, reliability, and cost are all affected by the physical locations of interconnected transistors. The placement problem is NP-Complete for even simple metrics. In this paper, we apply techniques developed by the Operations Research (OR) community to the placement problem. Using an Integer Programming (IP) formulation and by applying a \u201cbranch-and-price \u201d approach, we are able to optimally solve placement problems that are an order of magnitude larger than those addressed by traditional methods. Our results show that suboptimality is rampant on the small scale, and that there is merit in increasing the size of optimization windows used in detail placement. I.",
            "group": 1622,
            "name": "10.1.1.65.3524",
            "keyword": "",
            "title": "Optimal Placement by Branch-and-Price"
        },
        {
            "abstract": "The compact and harmonious layout of ads and text is a fundamental and costly step in the production of commercial telephone directories ( \u00a8Yellow Pages\u00a8). We formulate a canonical version of Yellow-Pages pagination and layout (YPPL) as an optimization problem in which the task is to position ads and text-stream segments on sequential pages so as to minimize total page length and maximize certain layout aesthetics, subject to constraints derived from page-format requirements and positional relations between ads and text. We present a heuristic-search approach to the YPPL problem. Our algorithm has been applied to a sample of real telephone-directory data, and produces solutions that are significantly shorter and better than the published ones.",
            "group": 1623,
            "name": "10.1.1.65.4520",
            "keyword": "",
            "title": "Harvard"
        },
        {
            "abstract": "",
            "group": 1624,
            "name": "10.1.1.65.5088",
            "keyword": "",
            "title": "Simulated Annealing for Graph Bisection"
        },
        {
            "abstract": "Abstract. Fitness landscapes have proven to be a valuable concept in evolutionary biology, combinatorial optimization, and the physics of disordered systems. A fitness landscape is a mapping from a configuration space into the real numbers. The configuration space is equipped with some notion of adjacency, nearness, distance or accessibility. Landscape theory has emerged as an attempt to devise suitable mathematical structures for describing the \u201cstatic \u201d properties of landscapes as well as their influence on the dynamics of adaptation. In this review we focus on the connections of landscape theory with algebraic combinatorics and random graph theory, where exact results are available.",
            "group": 1625,
            "name": "10.1.1.65.5204",
            "keyword": "Key words. fitness landscapegenotype phenotype maprandom graphs",
            "title": "Combinatorial landscapes"
        },
        {
            "abstract": "Until recently it was widely considered that value function-based reinforcement learning methods were the only feasible way of solving general stochastic optimal control problems. Unfortunately, these approaches are inapplicable to real-world problems with continuous, high-dimensional and partiallyobservable properties such as motor control tasks. While policy-gradient reinforcement learning methods suggest a suitable approach to such tasks, they suffer from typical parametric learning issues such as model selection and catastrophic forgetting. This thesis investigates the application of policy-gradient learning to a range of simulated motor learning tasks and introduces the use of local factored policies to enable incremental learning in tasks of unknown complexity. Acknowledgments Nicola Slade, for her unwavering support and unquestioning commitment. My parents, for their encouragement and endurance.",
            "group": 1626,
            "name": "10.1.1.65.7166",
            "keyword": "Contents",
            "title": "Policy-gradient learning for motor control"
        },
        {
            "abstract": "Load balancing by redundant decomposition and mapping",
            "group": 1627,
            "name": "10.1.1.65.7509",
            "keyword": "Redundant domain decompositionMappingGraph based parallel process/processor modelling 0. Background Experience gained in the CAMAS projec",
            "title": "VENERATION"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. I authorize the University of Waterloo to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize the University of Waterloo to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. ii The University of Waterloo requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date. iii Abstract One of the most challenging tasks in cartography is the labeling of maps-- attaching text to geographic features. Many of the issues become simpler when the features are points, for example cities on a large-scale map, because we expect the text to be placed horizontally and close to the associated point. We want labels that do not overlap and are large enough to be readable. Even simple formulations of this problem are NP-complete. One such formulation is the point-feature label placement problem: given a set of points in the plane, and an axis-parallel rectangular label associated with each point, place each label with one corner at the associated point such that no two labels overlap. This problem is known to be NP-complete. Modeling each label as a fixed rectangle in this way is quite limiting. Researchers have considered approximation algorithms where each label can be scaled. In this thesis, we propose an alternative formulation to the map labeling problem. We introduce the use of elastic labels, where each label is a rectangle with fixed area, but varying in height and width. Then, we define the elastic labeling problem as determining the choice of height and width of each label, and the corner of the label to place at the associated point, so that no two labels overlap. This problem is useful when the goal of placing a label at a given point is to associate some text, consisting of more than one word, with the point. In this case we can write the specified text inside the label using one, two, or more rows, as long as the label is placed at the specified point.",
            "group": 1628,
            "name": "10.1.1.65.7592",
            "keyword": "",
            "title": "Map Labeling Problems"
        },
        {
            "abstract": "One of the major costs in a software project is the construction of test-data. This paper outlines a generalised test-case data generation framework based on optimisation techniques. The framework can incorporate a number of testing criteria, for both functional and non-functional properties. Application of the optimisation framework to testing specification failures and exception conditions is illustrated. The results of a number of small case studies are presented and show the efficiency and effectiveness of this dynamic optimisation-base approach to generating test-data. 1.1 Keywords Automatic test-case generation, software testing, formal specifications, exception conditions, optimisation techniques, simulated annealing. 2",
            "group": 1629,
            "name": "10.1.1.65.7743",
            "keyword": "",
            "title": "Automated program flaw finding using simulated annealing"
        },
        {
            "abstract": "Application of parallel computing to stochastic parameter",
            "group": 1630,
            "name": "10.1.1.65.7932",
            "keyword": "OptimizationModelHydrologyBird migrationOctaveMessage passing interface",
            "title": "estimation in environmental models"
        },
        {
            "abstract": "not be interpreted as representing the official policies, either expressed or implied, of the funding agencies. There exists a need for manipulators that are more flexible and reliable than the current fixed configuration manipulators. Indeed, robot manipulators can be easily reprogrammed to per-form different tasks, yet the range of tasks that can be performed by a manipulator is limited by its mechanical structure. In remote and hazardous environments, such as a nuclear facil-ity or a space station, the range of tasks that may need to be performed often exceeds the capabilities of a single manipulator. Moreover, it is essential that critical tasks be executed reliably in these environments. To address this need for a more flexible and reliable manipulator, we propose the concept of a rapidly deployable fault tolerant manipulator system. Such a system combines a Reconfig-urable Modular Manipulator System (RMMS) with support software for rapid program-ming, trajectory planning, and control. This allows the user to rapidly configure a fault tolerant manipulator custom-tailored for a given task. This thesis investigates all aspects",
            "group": 1631,
            "name": "10.1.1.65.8769",
            "keyword": "",
            "title": "An agent-based approach to the design of rapidly deployable fault tolerant manipulators"
        },
        {
            "abstract": " ",
            "group": 1632,
            "name": "10.1.1.65.8945",
            "keyword": "",
            "title": "Simulation-Based Search for Hybrid System Control and Analysis"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii This dissertation explores approaches to the satisfiability problem, focusing on local search methods. The research endeavours to better understand how and why some local search methods are effective. At the root of this understanding are a set of metrics that characterize the behaviour of local search methods. Based on this understanding, two new local search methods are proposed and tested, the first, SDF, demonstrating the value of the insights drawn from the metrics, and the second, ESG, achieving state-of-the-art performance and generalizing the approach to arbitrary 0-1 integer linear programming problems. This generality is demonstrated by applying ESG to combinatorial auction winner determination. Further augmentations to local search are proposed and examined, exploring hybrids that incorporate aspects of backtrack search methods. iii Acknowledgements I owe thanks to many people for the completion of this dissertation. \u2022 To Jim Linders and Fakhri Karray for the opportunities they afforded me. \u2022 To my committee members, Nancy Day and Peter van Beek, for their insights. \u2022 To Bart Selman and John Thistle, for kindly agreeing to act as examiners. \u2022 To Rob Holte and everyone at the University of Alberta. \u2022 To my advisor, Dale Schuurmans, with whom it has been a pleasure and privilege to work (albeit often in the wee hours). \u2022 To my schoolmates, colleagues, and friends for their comments, criticism, and fun: Relu, Ali, Dana, Fuchun, and Paul (Remember: Aim high... fall hard).",
            "group": 1633,
            "name": "10.1.1.65.9422",
            "keyword": "Contents",
            "title": "Augmenting Local Search for Satisfiability"
        },
        {
            "abstract": "Abstract. This work compares different metaheuristics techniques applied to an important problem in natural language: tagging. Tagging amounts to assigning to each word in a text one of its possible lexical categories (tags) according to the context in which the word is used (thus it is a disambiguation task). Specifically, we have applied a classic genetic algorithm (GA), a CHC algorithm, and a Simulated Annealing (SA). The aim of the work is to determine which one is the most accurate algorithm (GA, CHC or SA), which one is the most appropriate encoding for the problem (integer or binary) and also to study the impact of parallelism on each considered method. The work has been highly simplified by the use of MALLBA, a library of search techniques which provides generic optimization software skeletons able to run in sequential, LAN and WAN environments. Experiments show that the GA with the integer encoding provides the more accurate results. For the CHC algorithm, the best results are obtained with binary coding and a parallel implementation. SA provides less accurate results than any of the evolutionary algorithms. 1",
            "group": 1634,
            "name": "10.1.1.66.923",
            "keyword": "",
            "title": "Metaheuristics for Natural Language Tagging"
        },
        {
            "abstract": " ",
            "group": 1635,
            "name": "10.1.1.66.1257",
            "keyword": "",
            "title": "Place and Route Techniques for FPGA Architecture Advancement"
        },
        {
            "abstract": "One of the major costs in a software project is the construction of test-data. This paper outlines a generalised test-case data generation framework based on optimisation techniques. This framework can incorporate a number of testing criteria unifying both functional and non-function testing. Application of the optimisation based approach are given for worst-case execution time, specification conformance, structural coverage and exception condition testing. The results of a number of small example case studies are presented and show the efficacy of the approach. 1",
            "group": 1636,
            "name": "10.1.1.66.2259",
            "keyword": "",
            "title": "The way forward for unifying dynamic test case generation: The optimisation-based approach"
        },
        {
            "abstract": "including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii FPGAs have become quite popular for implementing digital circuits and systems because of reduced costs and fast design cycles. This has led to increased complexity of FPGAs, and with technology scaling, many new challenges have come up for the FPGA industry, leakage power being one of the key challenges. The current generation FPGAs are being implemented in 90nm technology, therefore, managing leakage power in deepsubmicron FPGAs has become critical for the FPGA industry to remain competitive in the semiconductor market and to enter the mobile applications domain. In this work an analytical state dependent leakage power model for FPGAs is developed, followed by dual-Vt based designs of the FPGA architecture for reducing leakage power. The leakage power model computes subthreshold and gate leakage in FPGAs, since",
            "group": 1637,
            "name": "10.1.1.66.3267",
            "keyword": "",
            "title": "AUTHOR\u2019S DECLARATION FOR ELECTRONIC SUBMISSION OF THESIS"
        },
        {
            "abstract": "Abstract\u2014Location area (LA) planning plays an important role in cellular networks because of the tradeoff caused by paging and registration signalling. The upper boundary for the size of an LA is the service area of a mobile services switching center (MSC). In that extreme case, the cost of paging is at its maximum but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal but the cost of registration is the largest. Between these extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we seek to determine the location areas in an optimum fashion. Cell to switch assignments are also determined to achieve the minimization of the network cost. For that purpose, we use the available network information to formulate a realistic optimization problem, and propose an algorithm based on simulated annealing (SA) for its solution. Then, we investigate the quality of the SA-based technique by comparing it to greedy search, random generation methods, and a heuristic algorithm. Index Terms\u2014Location area (LA), location management, location tracking, location update, paging area, simulated annealing. I.",
            "group": 1638,
            "name": "10.1.1.66.4015",
            "keyword": "",
            "title": "Location area planning and cell-to-switch assignment in cellular networks"
        },
        {
            "abstract": "Abstract: We address an important problem of a manufacturing system. The system procures raw materials from outside suppliers in a lot and processes them to produce finished goods. It proposes an ordering policy for raw materials to meet the requirements of a production facility. In return, this facility has to deliver finished products demanded by external buyers at fixed time intervals. First, a general cost model is developed considering both raw materials and finished products. Then this model is used to develop a simulated annealing approach to determining an optimal ordering policy for procurement of raw materials and also for the manufacturing batch size to minimize the total cost for meeting customer demands in time. The solutions obtained were compared with those of traditional approaches. Numerical examples are presented.",
            "group": 1639,
            "name": "10.1.1.66.4153",
            "keyword": "Inventoryprocurementperiodic deliveryoptimum order quantityheuristicsimulated",
            "title": "SIMULATED ANNEALING AND JOINT MANUFACTURING BATCH-SIZING"
        },
        {
            "abstract": "Real-time control algorithms are designed based on the characteristics of the controlled plants and they require good performance without delays. However, digital control implementation typically introduces delays and jitters due to insufficient CPU processing power and the limitations of the real-time scheduling method used. This can degrade the system performance or even make it unstable. In this paper we propose an integrated approach for control design and real-time scheduling, suitable for both discrete-time and continuous-time controllers. It guarantees system performance by accepting a certain minimum value of jitter for control tasks and feasibly schedules them together with other tasks in the system. Results from comparison with other approaches from real-time and control theory domains underline the effectiveness of our method. 1",
            "group": 1640,
            "name": "10.1.1.66.4449",
            "keyword": "",
            "title": "Real-Time Control Design for Flexible Scheduling using Jitter Margin"
        },
        {
            "abstract": "This paper presents a comprehensive survey on global routing research over about the last two decades, with an emphasis on the problems of simultaneously routing multiple nets in VLSI circuits under various design styles. The survey begins with a coverage of traditional approaches such as sequential routing and rip-up-and-reroute, and then discusses multicommodity flow based methods, which have attracted a good deal of attention recently. The family of hierarchical routing techniques and several of its variants are then overviewed, in addition to other techniques such as move-based heuristics and iterative deletion. While many traditional techniques focus on the conventional ob-jective of managing congestion, newer objectives have come into play with the advances in VLSI technology. Specifically, the focus of global routing has shifted so that it is important to augment the congestion objective with metrics for timing and crosstalk. In the later part of this paper, we summarize the recent progress in these directions. Finally, the survey concludes with a summary of",
            "group": 1641,
            "name": "10.1.1.66.4927",
            "keyword": "",
            "title": "A Survey on Multi-Net Global Routing for Integrated Circuits"
        },
        {
            "abstract": "an Internet-based System Modeling Environment by",
            "group": 1642,
            "name": "10.1.1.66.5569",
            "keyword": "",
            "title": "Certified by..................................,...("
        },
        {
            "abstract": " ",
            "group": 1643,
            "name": "10.1.1.66.5769",
            "keyword": "",
            "title": "System Support for Distributed 3D Real-Time Rendering on Commodity Clusters"
        },
        {
            "abstract": "The problem of multiple sequence alignment is one of the most important problems in computational biology. In this paper we present a new method that simultaneously performs multiple sequence alignment and phylogenetic tree inference for large input data sets. We describe a parallel implementation of our method that utilises simulated annealing metaheuristic to find locally optimal phylogenetic trees in reasonable time. To validate the method, we perform a set of experiments with synthetic as well as real\u2013life data. 1",
            "group": 1644,
            "name": "10.1.1.66.6823",
            "keyword": "",
            "title": "Parallel multiple sequence alignment with decentralized cache support"
        },
        {
            "abstract": "This work presents a formulation developed to minimize the volume of concrete in reinforced concrete building grillages. An association of the displacement method with optimization techniques seeks to obtain the cross-sectional dimensions which lead to the smallest volume of concrete, attending to the ultimate loads (failure) and service loads (deflections). The constraints imposed in the formulation of the problem related to the limitation of the displacements include the effects of instantaneous and long-term deflections, considering an equivalent inertia with the contribution of concrete between cracking (Branson). The determination of the minimum height to each cross-section due to the flexural strength can be performed by fixing neutral axis position or by the maintenance of the section as underreinforced. Due to the relative complexity of the formulation as well as to the existence of local minimas, even for a small number of variables, a stochastic method was chosen, being implemented the Simulated Annealing. In order to verify the efficiency of the proposed procedure, some of the analyzed structures are presented, as well as the results obtained from the implementation of the proposed formulation.",
            "group": 1645,
            "name": "10.1.1.66.7072",
            "keyword": "simulated annealinggrillagesreinforced concreteweight optimization",
            "title": "Stochastic Optimization Applied to R-C Building Grillages"
        },
        {
            "abstract": "Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test-data to achieve 100 % coverage of a given structural coverage metric is labour intensive and expensive. This paper presents an approach to automate the generation of such test-data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test-data. The same approach can be be generalised to solve other test-data generation problems. Three such applications are discussed { boundary value analysis, assertion/run-time exception testing and component re-use testing. Aprototype tool-set has been developed to facilitate the automatic generation of test-data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the e ciency and e ectiveness of this approach. 1",
            "group": 1646,
            "name": "10.1.1.66.7212",
            "keyword": "",
            "title": "An automated framework for structural test-data generation"
        },
        {
            "abstract": "Abstract- Partitioning unstructured graphs is central to the parallel",
            "group": 1647,
            "name": "10.1.1.66.7460",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "",
            "group": 1648,
            "name": "10.1.1.66.7705",
            "keyword": "",
            "title": "An automated framework for structural test-data generation"
        },
        {
            "abstract": "Abstract. This paper shows how Markovian segmentation algorithms used to solve well known computer vision problems such as motion estimation, motion detection and stereovision can be significantly accelerated when implemented on programmable graphics hardware. More precisely, this contribution exposes how the parallel abilities of a standard Graphics Processing Unit (usually devoted to image synthesis) can be used to infer the labels of a label field. The computer vision problems addressed in this paper are solved in the maximum a posteriori (MAP) sense with an optimization algorithm such as ICM or simulated annealing. To do so, the fragment processor is used to update in parallel every labels of the segmentation map while rendering passes and graphics textures are used to simulate optimization iterations. Results show that impressive acceleration factors can be reached, especially when the size of the scene, the number of labels or the number of iterations is large. Hardware results have been obtained with programs running on a mid-end affordable graphics card. 1",
            "group": 1649,
            "name": "10.1.1.66.7772",
            "keyword": "",
            "title": "Markovian Energy-Based Computer Vision Algorithms on Graphics Hardware"
        },
        {
            "abstract": "This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved by hBOA; nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested classes of minimum vertex cover problems.",
            "group": 1650,
            "name": "10.1.1.66.8855",
            "keyword": "simulated annealing",
            "title": "Hybrid Evolutionary Algorithms on Minimum Vertex Cover for Random Graphs"
        },
        {
            "abstract": "A new Mean Field Annealing (MFA) formulation is proposed for the mapping problem for mesh-connected architectures. The proposed MFA heuristic exploits the conventional routing scheme used in mesh interconnection topologies to introduce an e cient encoding scheme. An e cient implementation scheme which decreases the complexity ofthe proposed algorithm by asymptotical factors is also developed. Experimental results also show that the proposed MFA heuristic approaches the speed performance of the fast Kernighan-Lin heuristic while approaching the solution quality ofthepowerful simulated annealing heuristic. ",
            "group": 1651,
            "name": "10.1.1.66.9573",
            "keyword": "",
            "title": "An Efficient Mapping Heuristic for Mesh-Connected Parallel Architectures Based On Mean Field Annealing"
        },
        {
            "abstract": "The requirements for tracking in augmented reality environments are stringent because of the need to register real and computer-generated virtual objects. Driven by the need to track real objects within these environments, we propose two algorithms to distribute markers on complex rigid objects. The proposed algorithms employ an optimization technique with a spherical or cylindrical intermediary surface. The validity and effectiveness of the algorithms are tested heuristically by simulation.  ",
            "group": 1652,
            "name": "10.1.1.66.9884",
            "keyword": "",
            "title": "Marker Mapping Techniques for Augmented Reality Visualization "
        },
        {
            "abstract": "This paper presents a genetic approach to the problem of map topological clustering. Maps are symbolically represented as graphs whose vertices are landmarks in the environment. Clustering is performed according to a fitness function which takes functional requirements into account.",
            "group": 1653,
            "name": "10.1.1.67.139",
            "keyword": "ClusteringFitness functionGenetic algorithmsLearningMaps",
            "title": "Pattern Recognition Letters Topological clustering of maps using a genetic algorithm"
        },
        {
            "abstract": "Dynamic programming (DP) is a fast, elegant method for solving many one-dimensional optimisation problems but, unfortunately, most problems in image analysis, such as restoration and warping, are two-dimensional. We consider three generalisations of DP. The first is iterated dynamic programming (IDP), where DP is used to recursively solve each of a sequence of one-dimensional problems in turn, to find a local optimum. A second algorithm is an empirical, stochastic optimiser, which is implemented by adding progressively less noise to IDP. The final approach replaces DP by a more computationally intensive Forward-Backwards Gibbs Sampler, and uses a simulated annealing cooling schedule. Results are compared with existing pixel-by-pixel methods of iterated conditional modes (ICM) and simulated annealing to restore a synthetic aperture radar (SAR) image and are also used to warp a pulsed-field electrophoresis gel in order to align with a reference image. We find that IDP and its stochastic variant outperform the remaining algorithms. Key words: Forward-Backwards Gibbs Sampler, image restoration, image warping, Markov random field, pulsed-field gel electrophoresis, simulated annealing, synthetic aperture radar. 1",
            "group": 1654,
            "name": "10.1.1.67.142",
            "keyword": "",
            "title": "Two-dimensional generalisations of dynamic programming for image analysis"
        },
        {
            "abstract": "In this thesis, we show how an Extended Guided Local Search can be applied to a set of problems and show that the extensions can improve its performance. We show how an aspiration criterion can be added to Guided Local Search to improve its performance for some problem types and parameter settings. We then demonstrate how, by making an occasional random move, the performance of Guided Local Search can be further improved for some problems and parameter settings. For both extensions, we make use of search monitors to attempt to analyse when and why each extension succeeds or fails. Finally, we combine the extensions and compare the resulting Extended Guided Local Search with some state-of-the-art algorithms for the different problem types, we have used for our experiments. 2",
            "group": 1655,
            "name": "10.1.1.67.651",
            "keyword": "",
            "title": "Acknowledgements"
        },
        {
            "abstract": "Abstract. Nature-inspired algorithms such as genetic algorithms, particle swarm optimisation and ant colony algorithms have successfully solved computer science problems of search and optimisation. The initial implementations of these techniques focused on static problems solved on single machines. These have been extended",
            "group": 1656,
            "name": "10.1.1.67.1030",
            "keyword": "",
            "title": "A Roadmap of Nature-Inspired Systems Research and Development"
        },
        {
            "abstract": "ter verkrijging van de graad van doctor aan de Universiteit van Amsterdam, op gezag van de Rector Magnificus prof. dr J. J. M. Franse ten overstaan van een door het College voor Promoties ingestelde commissie in het openbaar te verdedigen in de Aula der Universiteit op dinsdag 24 februari 1998 te 15.00 uur door",
            "group": 1657,
            "name": "10.1.1.67.1238",
            "keyword": "Printed at PrintPartners IpskampEnschedeThe Netherlands. Contents",
            "title": "1.1.2 This Chapter............................ 2"
        },
        {
            "abstract": "ter verkrijging van de graad van doctor aan de Universiteit van Amsterdam, op gezag van de Rector Magnificus prof. dr J. J. M. Franse ten overstaan van een door het College voor Promoties ingestelde commissie in het openbaar te verdedigen in de Aula der Universiteit op dinsdag 24 februari 1998 te 15.00 uur door",
            "group": 1658,
            "name": "10.1.1.67.1238",
            "keyword": "Printed at PrintPartners IpskampEnschedeThe Netherlands. Contents",
            "title": "1.1.2 This Chapter............................ 2"
        },
        {
            "abstract": "Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test-data to achieve 100 % coverage of a given structural coverage metric is labour intensive and expensive. This paper presents an approach to automate the generation of such test-data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test-data. The same approach can be generalised to solve other test-data generation problems. Three such applications are discussed { boundary value analysis, assertion/run-time exception testing and component re-use testing. Aprototype tool-set has been developed to facilitate the automatic generation of test-data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the e ciency and e ectiveness of this approach. 1",
            "group": 1659,
            "name": "10.1.1.67.1350",
            "keyword": "",
            "title": "An automated framework for structural test-data generation"
        },
        {
            "abstract": "Software",
            "group": 1660,
            "name": "10.1.1.67.2132",
            "keyword": "",
            "title": "A Search-Based Automated Test-Data Generation Framework for Safety Critical Software"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii Non-linear programming has been extensively used in wireless telecom-munication systems design. An important criterion in optimization is the minimization of mean square error. This thesis examines two applications: peak to average power ratio (PAPR) reduction in orthogonal frequency di-vision multiplexing (OFDM) systems and wireless airtime traffic estimation. These two applications are both of interests to wireless service providers. PAPR reduction is implemented in the handheld devices and low complexity is a major objective. On the other hand, exact traffic prediction can save",
            "group": 1661,
            "name": "10.1.1.67.2878",
            "keyword": "",
            "title": "Application of Non-linear Optimization Techniques in Wireless Telecommunication Systems"
        },
        {
            "abstract": "The linear ordering problem is an\u00c6\u00c8-hard problem that arises in a variety of applications. Due to its interest in practice, it has received considerable attention and a variety of algorithmic approaches to its solution have been proposed. In this paper we give a detailed search space analysis of available LOP benchmark instance classes that have been used in various researches. The large fitness-distance correlations observed for many of these instances suggest that adaptive restart algorithms like iterated local search or memetic algorithms, which iteratively generate new starting solutions for a local search based on previous search experience, are promising candidates for obtaining high performing algorithms. We therefore experimentally compared two such algorithms and the final experimental results suggest that, in particular, the memetic algorithm is the new state-of-the-art approach to the LOP. \ufffd\ufffd\ufffd\u00d2\ufffd\ufffd\ufffd",
            "group": 1662,
            "name": "10.1.1.67.2955",
            "keyword": "",
            "title": "The linear ordering problem: instances, search space analysis and algorithms"
        },
        {
            "abstract": "Abstract: In this paper, we deal with two important issues in relation to modular reconfigurable manipulators, namely, the determination of the modular assembly configuration optimally suited to perform a specific task and the synthesis of fault tolerant systems. We present a numerical approach yielding an assembly configuration that satisfies four kinematic task requirements: reachability, joint limits, obstacle avoidance and measure of isotropy. Further, because fault tolerance is a must in critical missions that may involve high costs if the mission were to fail due to a failure in the manipulator system, we address the property of fault tolerance in more detail. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical solution procedure can be used, as is illustrated through an example. 1",
            "group": 1663,
            "name": "10.1.1.67.3370",
            "keyword": "",
            "title": "Synthesis Methodology for Task Based Reconfiguration of Modular Manipulator Systems"
        },
        {
            "abstract": "Abstract \u2014 We describe a novel shape formation algorithm for ensembles of 2-dimensional lattice-arrayed modular robots, based on the manipulation of regularly shaped voids within the lattice (\u201choles\u201d). The algorithm is massively parallel and fully distributed. Constructing a goal shape requires time proportional only to the complexity of the desired target geometry. Construction of the shape by the modules requires no global communication nor broadcast floods after distribution of the target shape. Results in simulation show 97.3 % shape compliance in ensembles of approximately 60,000 modules, and we believe that the algorithm will generalize to 3D and scale to handle millions of modules. This paper is submitted to Invited Session: New Trends in Modular Robotics. I.",
            "group": 1664,
            "name": "10.1.1.67.3761",
            "keyword": "",
            "title": "Scalable shape sculpting via hole motion: Motion planning in lattice-constrained modular robots"
        },
        {
            "abstract": "A paradigm of statistical mechanics of financial markets (SMFM) is fit to multivariate financial markets using Adaptive Simulated Annealing (ASA), a global optimization algorithm, to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta are thereby derived and used as technical indicators in a recursive ASA optimization process to tune trading rules. These trading rules are then used on out-of-sample data, to demonstrate that they can profit from the SMFM model, to illustrate that these markets are likely not efficient. This methodology can be extended to other systems, e.g., electroencephalography. This approach to complex systems emphasizes the utility of blending an intuitive and powerful mathematical-physics formalism to generate indicators which are used by AI-type rule-based models of management.   ",
            "group": 1665,
            "name": "10.1.1.67.4262",
            "keyword": "",
            "title": "Canonical momenta indicators of financial markets and neocortical EEG"
        },
        {
            "abstract": " This paper addresses the optimization of pseudo-random planar point patterns for invariant-based identification or indexing. This is a novel problem and is formulated here as the maximization of the spacing of all the invariants when considered as points in a space. The task is of formidable complexity and a stochastic approximation strategy is proposed that yields interesting results.",
            "group": 1666,
            "name": "10.1.1.67.4548",
            "keyword": "",
            "title": "Optimizing Random Patterns for Invariants-based Identification"
        },
        {
            "abstract": "The powerful techniques of modern nonlinear statistical mechanics are used to compare battalion-scale combat computer models (including simulations and wargames) to exercise data. This is necessary if large-scale combat computer models are to be extrapolated with confidence to develop battle-management, C 3 and procurement decision-aids, and to improve training. This modeling approach to battalion-level missions is amenable to reasonable algebraic and/or heuristic approximations to drive higher-echelon computer models. Each data set is fit to several candidate short-time probability distributions, using methods of \u2018\u2018very fast simulated re-annealing\u2019 \u2019 with a Lagrangian (time-dependent algebraic cost-function) derived from nonlinear stochastic rate equations. These candidate mathematical models are further tested by using path-integral numerical techniques we have dev eloped to calculate long-time probability distributions spanning the combat scenario. We hav e demonstrated proofs of principle, that battalion-level combat exercises can be well represented by the computer simulation JANUS(T), and that modern methods of nonlinear nonequilibrium statistical mechanics can well model these systems. Since only relatively simple drifts and diffusions were required, in larger systems, e.g., at brigade and division levels, it might be possible to \u2018\u2018absorb\u2019 \u2019 other important variables (C 3, human factors, logistics, etc.) into more nonlinear mathematical forms. Otherwise, this battalion-level model should be supplemented with a \u2018\u2018tree\u2019 \u2019 of branches corresponding to estimated values of these variables.",
            "group": 1667,
            "name": "10.1.1.67.4848",
            "keyword": "",
            "title": "Mathematical comparison of combat computer models to exercise data"
        },
        {
            "abstract": "",
            "group": 1668,
            "name": "10.1.1.67.6177",
            "keyword": "",
            "title": "Non-Sequential Tool Interaction Strategies for Sea-of-Gates Layout Synthesis"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We propose two new methods, one based on a discrete form of gradient descent, the other on simulated annealing, and report on a series of empirical tests comparing these and the other known algorithms for the problem. Based on this study, the first to be conducted, we identify the best approaches as a function of available computation time.",
            "group": 1669,
            "name": "10.1.1.67.6618",
            "keyword": "CR CategoriesH.5.2 [Information Interfaces and PresentationUser Interfaces\u2014screen design. 2.1 [Artificial IntelligenceApplications and Expert Systems\u2014cartography. I.3.5 [Computer GraphicsComputational Geometry and Object Modeling\u2014geometric algorithmslanguagesand systems. General Termsalgorithmsexperimentation. Additional Key Words and Phraseslabel placementautomated cartographystochastic methodssimulated annealingheuristic",
            "title": "An empirical study of algorithms for point-feature label placement"
        },
        {
            "abstract": "The application of robots in critical missions in hazardous environments requires the development of reliable or fault tolerant manipulators. In this paper, we define fault tolerance as the ability to continue the performance of a task after immobilization of a joint due to failure. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical design procedure can be used, as is illustrated through an example. 1",
            "group": 1670,
            "name": "10.1.1.67.6945",
            "keyword": "",
            "title": "Mapping tasks into fault tolerant manipulators"
        },
        {
            "abstract": "Dank an alle, die zum Gelingen dieser Arbeit beigetragen haben:",
            "group": 1671,
            "name": "10.1.1.67.8416",
            "keyword": "",
            "title": "von"
        },
        {
            "abstract": "In practical portfolio choice models risk is often defined as VaR, expected shortfall, maximum loss, Omega function, etc. and is computed from simulated future scenarios of the portfolio value. It is well known that the minimization of these functions can not, in general, be performed with standard methods. We present a multi-purpose data-driven optimization heuristic capable to deal efficiently with a variety of risk functions and practical constraints on the positions in the portfolio. The efficiency and robustness of the heuristic is illustrated by solving a collection of real world portfolio optimization problems using different risk functions such asVaR, expected shortfall, maximum loss and Omega function with the same algorithm.",
            "group": 1672,
            "name": "10.1.1.67.8563",
            "keyword": "Key wordsPortfolio optimizationHeuristic optimizationThreshold acceptingDownside risk",
            "title": "A Data-Driven Optimization Heuristic for Downside Risk Minimization Abstract"
        },
        {
            "abstract": "In this paper, we deal with two important issues in relation to modular recon gurable manipulators, namely, the determination of the modular assembly con guration optimally suited to perform a speci c task and the synthesis of fault tolerant systems. We present a numerical approach yielding an assembly con guration that satis es four kinematic task requirements: reachability, joint limits, obstacle avoidance and measure of isotropy. Further, because critical missions may involve high costs if the mission were to fail due to a failure in the manipulator system, we address the property of fault tolerance in more detail. We prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. For general purpose manipulators two redundant degrees-of-freedom are needed for every order of fault tolerance. However, we show that only one degree ofredundancy is su cient for task speci c fault tolerance. 1",
            "group": 1673,
            "name": "10.1.1.67.9310",
            "keyword": "",
            "title": "Design of Modular Fault Tolerant Manipulators"
        },
        {
            "abstract": "Abstract. Embedded systems increasingly encompass both dependability and responsiveness requirements. While sophisticated techniques exist, on a discrete basis, for both fault-tolerance (FT) and real-time (RT), the composite considerations for FT+RT are still evolving. Obviously the different objectives needed for FT and RT make composite optimization hard. In this paper, the proposed Multi Variable Optimization (MVO) process develops integrated FT+RT considerations. We introduce dependability as an initial optimization criteria by confining error propagation probability, i.e., limiting the interactions. Subsequently, quantification of interactions together with RT optimization by minimizing scheduling length is developed. A simulated annealing approach is utilized to find optimized solutions. We provide experimental results for our approach, showing significant design improvements over contemporary analytical initial feasibility solutions. 1",
            "group": 1674,
            "name": "10.1.1.67.9366",
            "keyword": "",
            "title": "A Multi Variable Optimization Approach for the Design of Integrated Dependable Real-Time Embedded Systems"
        },
        {
            "abstract": "  This article provides an overview of state-of-the-art technologies relevant to dynamic transportation planning problems that involve the reactive routing nnd scheduling of a fleet of vehicles in response to dynamically changing transportation demands. Specifically, we focus on a new class of complex transportation planning problems, which we refer to as the \"Dynamic Dial-A-Ride Problem with Multiple Acceptable Destinations and/or Origins\" (D-DARP-MADO). While this class of dynamic problems is representative of a number of practical transportation problems, it does not appear to have been the object of prior studies. This is not to say that techniques proposed for simpler routing and scheduling problems cannot be brought to bear on this problem. To the contrary, our survey shows that a number of techniques developed",
            "group": 1675,
            "name": "10.1.1.67.9524",
            "keyword": "",
            "title": "Models and Techniques of Dynamic Demand-Responsive Transportation Planning"
        },
        {
            "abstract": "We propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner. We focus on problems specified as a Boolean formula, i.e., on SAT instances. Sampling for SAT problems has been shown to have interesting connections with probabilistic reasoning, making practical sampling algorithms for SAT highly desirable. The best current approaches are based on Markov Chain Monte Carlo methods, which have some practical limitations. Our approach exploits combinatorial properties of random parity (XOR) constraints to prune away solutions near-uniformly. The final sample is identified amongst the remaining ones using a state-of-the-art SAT solver. The resulting sampling distribution is provably arbitrarily close to uniform. Our experiments show that our technique achieves a significantly better sampling quality than the best alternative. 1",
            "group": 1676,
            "name": "10.1.1.68.213",
            "keyword": "",
            "title": "B.: Near-uniform sampling of combinatorial spaces using XOR constraints"
        },
        {
            "abstract": "Abstract This article proposes a complete parallel relational optimization methodology based on randomized search strategies from the theoretical basics to the experimental validation. This methodology is based on a survey-like analysis of related search techniques. We defend why randomized search strategies are an effective technique, both in output quality and optimization effort, for complex parallel query optimization. The traditional techniques are tuned and parallized versions are developed. Furthermore we describe how the transformation rules of the search strategy could interact with the resource allocation and present an allocation model. A series of experiments performed on a 100 relation database with 18 randomly chosen queries demonstrate that an excellent tradeoff between optimization time and optimization quality can be achieved. Key words: Parallel databases, parallel query optimization, randomized search strategies. 1 Introduction Modern database applications, such as data mining and decision support pose several new challenges to parallel relational query optimization and processing [1, 2]. The complexity of queries against the database size rises significantly compared to traditional systems. Typically the number of joins and the relation size [3] are the 1",
            "group": 1677,
            "name": "10.1.1.68.696",
            "keyword": "",
            "title": "The Use of Randomized Search Strategies for Complex Parallel Relational Query Optimization"
        },
        {
            "abstract": "%T Very fast simulated re-annealing",
            "group": 1678,
            "name": "10.1.1.68.918",
            "keyword": "",
            "title": "U.S. Army Concepts Analysis Agency"
        },
        {
            "abstract": "Given a protein's sequence, one may try to predict its structure by reference to basic physics or even by searching against some more pragmatic quasi-energy or score function. Threading attempts to solve what should be a simpler problem \u2013 looking through the set of currently known structures and identifying the ones which are most likely to be appropriate for the sequence of interest. Unlike pure sequence-based methods, the calculations should use known structural information. It remains to be seen if threading will be obsolesced by the best sequence based methods or the newest approaches which do not even use a template structure. 2 1.",
            "group": 1679,
            "name": "10.1.1.68.2071",
            "keyword": "protein structure predictionsequence alignmentforce fieldsequence to structure alignmentalignment quality",
            "title": "Title: Protein Threading Manuscript version: 15 Nov. 03 For submission to &quot;The Proteomics Handbook&quot;"
        },
        {
            "abstract": "mesoscopic neural networks based on",
            "group": 1680,
            "name": "10.1.1.68.3856",
            "keyword": "",
            "title": "statistical mechanics"
        },
        {
            "abstract": "Modern servers access large volumes of data while running commercial workloads. The data is typically spread among several storage devices (e.g. disks). Carefully placing the data across the storage devices can minimize costly remote accesses and improve performance. We propose the use of simulated annealing to arrive at an effective layout of data on disk. The proposed technique considers the configuration of the system and the cost of data movement. An initial layout globally optimized across all queries, shows speedups of up to 13 % for a group of DSS queries and up to 6 % for selected OLTP queries. This technique can be re-applied at run-time to further improve performance beyond the initial, globally optimized data layout. This scheme monitors architecture parameters to prevent optimizations of multiple operations to conflict with each other. Such a dynamic reorganization results in speedups of up to 23 % for the DSS queries and up to 10 % for the OLTP queries. 1.",
            "group": 1681,
            "name": "10.1.1.68.4247",
            "keyword": "",
            "title": "Improving Server Performance on Transaction Processing Workloads by Enhanced Data Placement"
        },
        {
            "abstract": "I. Introduction: Curve fitting or fitting a statistical/mathematical model to data finds its application in almost all empirical sciences- viz. physics, chemistry, zoology, botany, environmental sciences, economics, etc. It has four objectives: the first, to describe the observed (or experimentally obtained) dataset by a statistical/mathematical formula; the second, to estimate the parameters of the formula so obtained and interpret them so that",
            "group": 1682,
            "name": "10.1.1.68.4574",
            "keyword": "",
            "title": "Performance of Differential Evolution Method in Least Squares Fitting of Some Typical Nonlinear Curves"
        },
        {
            "abstract": "Simulated tempering and swapping are two families of sampling algorithms in which a parameter representing temperature varies during the simulation. The hope is that this will overcome bottlenecks that cause sampling algorithms to be slow at low temperatures. Madras and Zheng demonstrate that the swapping and tempering algorithms allow efficient sampling from the low-temperature mean-field Ising model, a model of magnetism, and a class of symmetric bimodal distributions [10]. Local Markov chains fail on these distributions due to the existence of bad cuts in the state space. Bad cuts also arise in the \u00a2-state Potts model, another fundamental model for magnetism that generalizes the Ising model. Glauber (local) dynamics and the Swendsen-Wang algorithm have been shown to be prohibitively slow for sampling from the Potts model at some temperatures [1, 2, 6]. It is reasonable to ask whether tempering or swapping can overcome the bottlenecks that cause these algorithms to converge slowly on the Potts model. We answer this in the negative, and give the first example demonstrating that tempering can mix slowly. We show this for the 3-state ferromagnetic Potts model on the complete graph, known as the mean-field model. The slow convergence is caused by a first-order (discontinuous) phase transition in the underlying system. Using this insight, we define a variant of the swapping algorithm that samples efficiently from a class of bimodal distributions, including the mean-field Potts model. 1",
            "group": 1683,
            "name": "10.1.1.68.4809",
            "keyword": "",
            "title": "Abstract Torpid Mixing of Simulated Tempering on the Potts Model"
        },
        {
            "abstract": "Abstract-- Widespread use of wireless devices presents new challenges for network operators, who need to provide service to ever larger numbers of mobile end users, while ensuring Quality-of-Service guarantees. In this paper we describe a new distributed routing algorithm that performs dynamic load-balancing for wireless access networks. The algorithm constructs a load-balanced backbone tree, which simplifies routing and avoids per-destination state for routing and per-flow state for QoS reservations. We evaluate the performance of the algorithm using several metrics including adaptation to mobility, degree of load-balance, bandwidth blocking rate, and convergence speed. We find that the algorithm achieves better network utilization by lowering bandwidth blocking rates than other methods. I.",
            "group": 1684,
            "name": "10.1.1.68.4915",
            "keyword": "",
            "title": "Load-Balancing Routing for Wireless Access Networks"
        },
        {
            "abstract": "The field of structural biology is becoming increasingly important as new technological developments facilitate the collection of data on the atomic structures of proteins and nucleic acids. The solid-state NMR method is a relatively new biophysical technique that holds particular promise for determining the structures of peptides and proteins that are located within the cell membrane. This method",
            "group": 1685,
            "name": "10.1.1.68.5018",
            "keyword": "",
            "title": "doi:10.1016/j.bulm.2004.03.006 Mathematical Aspects of Protein Structure Determination with NMR Orientational Restraints"
        },
        {
            "abstract": "Abstract In this work we investigate the effects of the parallelization of a local search algorithm for MAX-SAT. The variables of the problem are divided in o / subsets and local search is applied to each of them in parallel, supposing that variables belonging to other subsets remain unchanged. We show empirical evidence for the existence of a critical level of parallelism which leads to the best performance. This result allows to improve local search and adds new elements to the investigation of criticality and parallelism in combinatorial optimization problems.",
            "group": 1686,
            "name": "10.1.1.68.5475",
            "keyword": "",
            "title": "\\Lambda DEIS Universit`a degli Studi di Bologna Viale Risorgimento, 2- Bologna (Italia)"
        },
        {
            "abstract": "Abstract\u2014We compare Genetic Algorithms (GA) with a functional search method, Very Fast Simulated Reannealing (VFSR), that not only is efficient in its search strategy, but also is statistically guaranteed to find the function optima. GA previously has been demonstrated to be competitive with other standard Boltzmann-type simulated annealing techniques. Presenting a suite of six standard test functions to GA and VFSR codes from previous studies, without any additional fine tuning, strongly suggests that VFSR can be expected to be orders of magnitude more efficient than GA. Genetic Algorithms & VFSR-2- Ingber &Rosen 1.",
            "group": 1687,
            "name": "10.1.1.68.5768",
            "keyword": "",
            "title": "Mathematical and Computer Modelling, 16(11) 1992, 87-100. GENETIC ALGORITHMS AND VERY FAST SIMULATED REANNEALING: A COMPARISON"
        },
        {
            "abstract": "Abstract: In this paper, we deal with two important issues in relation to modular reconfigurable manipulators, namely, the determination of the modular assembly configuration optimally suited to perform a specific task and the synthesis of fault tolerant systems. We present a numerical approach yielding an assembly configuration that satisfies four kinematic task requirements: reachability, joint limits, obstacle avoidance and measure of isotropy. Further, because fault tolerance is a must in critical missions that may involve high costs if the mission were to fail due to a failure in the manipulator system, we address the property of fault tolerance in more detail. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical solution procedure can be used, as is illustrated through an example. 1",
            "group": 1688,
            "name": "10.1.1.68.6510",
            "keyword": "",
            "title": "Synthesis Methodology for Task Based Reconfiguration of Modular Manipulator Systems"
        },
        {
            "abstract": "by",
            "group": 1689,
            "name": "10.1.1.68.8245",
            "keyword": "Mechanical Engineering",
            "title": "By"
        },
        {
            "abstract": "Abstract: In this paper, we deal with two important issues in relation to modular reconfigurable manipulators, namely, the determination of the modular assembly configuration optimally suited to perform a specific task and the synthesis of fault tolerant systems. We present a numerical approach yielding an assembly configuration that satisfies four kinematic task requirements: reachability, joint limits, obstacle avoidance and measure of isotropy. Further, because fault tolerance is a must in critical missions that may involve high costs if the mission were to fail due to a failure in the manipulator system, we address the property of fault tolerance in more detail. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical solution procedure can be used, as is illustrated through an example. 1",
            "group": 1690,
            "name": "10.1.1.68.8262",
            "keyword": "",
            "title": "Synthesis Methodology for Task Based Reconfiguration of Modular Manipulator Systems"
        },
        {
            "abstract": "Previous papers in this series of statistical mechanics of neocortical interactions (SMNI) have detailed a development from the relatively microscopic scales of neurons up to the macroscopic scales as recorded by electroencephalography (EEG), requiring an intermediate mesocolumnar scale to be developed at the scale of minicolumns ( \u2248 10 2 neurons) and macrocolumns ( \u2248 10 5 neurons). Opportunity was taken to view SMNI as sets of statistical constraints, not necessarily describing specific synaptic or neuronal mechanisms, on neuronal interactions, on some aspects of short-term memory (STM), e.g., its capacity, stability, and duration. Arecently developed C-language code, PATHINT, provides a non-Monte Carlo technique for calculating the dynamic evolution of arbitrary-dimension (subject to computer resources) nonlinear Lagrangians, such as derived for the two-variable SMNI problem. Here, PATHINT is used to explicitly detail the evolution of the SMNI constraints on STM.",
            "group": 1691,
            "name": "10.1.1.68.8616",
            "keyword": "",
            "title": "Statistical mechanics of neocortical interactions: Path-integral evolution of short-term memory"
        },
        {
            "abstract": "Some apparently powerful algorithms for automatic label placement on maps use heuristics that capture considerable cartographic expertise but are hampered by provably inefficient methods of search and optimization. On the other hand, no approach to label placement that is based on an efficient optimization technique has been applied to the production of general cartographic maps \u2014 those with labeled point, line, and area features \u2014 and shown to generate labelings of acceptable quality. We present an algorithm for label placement that achieves the twin goals of practical efficiency and high labeling quality by combining simple cartographic heuristics with effective stochastic optimization techniques.",
            "group": 1692,
            "name": "10.1.1.68.9845",
            "keyword": "",
            "title": "A General Cartographic Labeling Algorithm"
        },
        {
            "abstract": "It is proposed to apply modern methods of nonlinear nonequilibrium statistical mechanics to develop software algorithms that will optimally respond to targets within short response times with minimal computer resources. This Statistical Mechanics Algorithm for Response to Targets (SMART) can be developed with a view tow ards its future implementation into a hardwired Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed of response to targets (SMART SAM).  ",
            "group": 1693,
            "name": "10.1.1.69.225",
            "keyword": "",
            "title": "Statistical mechanics algorithm for response to targets (SMART)"
        },
        {
            "abstract": "Simulated Annealing (SA) procedures can potentially yield near-optimal solutions to many difficult combinatorial optimization problems, though often at the expense of intensive computational efforts. The single most significant source of inefficiency in SA search is the inherent stochasticity of the procedure, typically requiring that the procedure be rerun a large number of times before a near-optimal solution is found. This paper describes a mechanism that attempts to learn the structure of the search space over multiple SA runs on a given problem. Specifically, probability distributions are dynamically updated over multiple runs to estimate at different checkpoints how promising a SA run appears to be. Based on this mechanism, two types of criteria are developed that aim at increasing search efficiency: (1) a cutoff criterion used to determine when to abandon unpromising runs and (2) restart criteria used to determine whether to start a fresh SA run or restart search in the middle of an earlier run. Experimental results obtained on a class of complex job shop scheduling problems show (1) that SA can produce high quality solutions for this class of problems, if run a large number of times, and (2) that our learning mechanism can significantly reduce the computation time required to find high quality solutions to these problems. The results also indicate that, the closer one wants to be to the optimum, the larger the speedups. Similar results obtained on a smaller set of benchmark Vehicle Routing Problems with Time Windows (VRPTW) suggest that our learning mechanisms should help improve the efficiency of SA in a number of different domains.",
            "group": 1694,
            "name": "10.1.1.69.774",
            "keyword": "Key WordsSimulated annealinglearningscheduling",
            "title": "Learning to Recognize (Un)Promising Simulated Annealing Runs: Efficient Search Procedures for Job Shop Scheduling and Vehicle Routing"
        },
        {
            "abstract": "Traditional bottom-up models of visual processing assume that figure-ground organization precedes object recognition. This assumption seems logically necessary: How can object recognition occur before a region is labeled as figure? However, some behavioral studies find that familiar regions are more likely to be labeled figure than less familiar regions, a-problematic finding for bottom-up models. An interactive account is proposed in which figure-ground processes receive top-down input from object representations in a hierarchical system. A graded, interactive computational model is presented that accounts for behavioral results in which familiarity effects are found. The interactive model offers an alternative conception of visual processing to bottom-up models. In a typical visual scene multiple objects partially occlude one another, which makes object recognition a computation-ally complex task. Traditional information-processing theo-ries of visual perception have suggested that prior to object representation and recognition, an earlier stage of perceptual organization occurs to determine which features, locations, or surfaces most likely belong together (for examples, see",
            "group": 1695,
            "name": "10.1.1.69.980",
            "keyword": "",
            "title": "Figure-ground organization and object recognition processes: An interactive account"
        },
        {
            "abstract": "In this paper, we present improvements to recursive bisection based placement. In contrast to prior work, our horizontal cut lines are not restricted to row boundaries; this avoids a \u201cnarrow region\u201d problem. To support these new cut line positions, a dynamic programming based legalization algorithm has been developed. The combination of these has improved the stability and lowered the wire lengths produced by our Feng Shui placement tool. On benchmarks derived from industry partitioning examples, our results are close to those of the annealing based tool Dragon, while taking only a fraction of the run time. On synthetic benchmarks, our wire lengths are nearly 23 % better than those of Dragon. For both benchmark suites, our results are substantially better than those of the recursive bisection based tool Capo and the analytic placement tool Kraftwerk. 1.",
            "group": 1696,
            "name": "10.1.1.69.1108",
            "keyword": "",
            "title": "Fractional cut: Improved recursive bisection placement"
        },
        {
            "abstract": "This paper explores how communication can be understood as an adaptation by agents to their environment. We model agents as recurrent neural networks. After arguing against systems which use discrete symbols to evolve communication, we supply our agents with a number of continuous communications channels. The agents use these channels to initiate real-valued signals which propagate through the environment, decaying over distance, perhaps being perturbed by environmental noise. Initially, the agents \u2019 signals appear random; over time, a structure emerges as the agents learn to communicate task-specific information about their environment. We demonstrate how different communication schemes can evolve for a task, and then discover a commonality between the schemes in terms of information passed between agents. From this we discuss what it means to communicate, and describe how a semantics emerges in the agents \u2019 signals relative to their task domain. Key Words: communication; evolutionary algorithms; autonomous agents; neural networks",
            "group": 1697,
            "name": "10.1.1.69.1737",
            "keyword": "",
            "title": "The evolution of communication in adaptive agents"
        },
        {
            "abstract": "A series of papers has developed a statistical mechanics of neocortical interactions (SMNI), deriving aggregate behavior of experimentally observed columns of neurons from statistical electrical-chemical properties of synaptic interactions. While not useful to yield insights at the single neuron level, SMNI has demonstrated its capability in describing large-scale properties of short-term memory and electroencephalographic (EEG) systematics. The necessity of including nonlinear and stochastic structures in this development has been stressed. Sets of EEG and evoked potential data were fit, collected to investigate genetic predispositions to alcoholism and to extract brain \u201csignatures\u201d of short-term memory. Adaptive Simulated Annealing (ASA), a global optimization algorithm, was used to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta indicators (CMI) are thereby derived for individual\u2019s EEG data. The CMI give better signal recognition than the raw data, and can be used to advantage as correlates of behavioral states. These results give strong quantitative support for an accurate intuitive picture, portraying neocortical interactions as having common algebraic or physics mechanisms that scale across quite disparate spatial scales and functional or behavioral phenomena, i.e., describing interactions among neurons, columns of neurons, and regional masses of neurons. This paper adds to these previous investigations two important aspects, a description of how the CMI may be used in source localization, and calculations using previously ASA-fitted parameters in out-of-sample data.",
            "group": 1698,
            "name": "10.1.1.69.2231",
            "keyword": "EEGSimulated AnnealingStatistical Mechanics Statistical Mechanics of Neocortical...-2- Lester Ingber",
            "title": "Statistical mechanics of neocortical interactions: Training and testing canonical momenta indicators of EEG"
        },
        {
            "abstract": null,
            "group": 1699,
            "name": "10.1.1.69.2708",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": "In this paper, we deal with two important issues in relation to modular recon gurable manipulators, namely, the determination of the modular assembly con guration optimally suited to perform a speci c task and the synthesis of fault tolerant systems. We present a numerical approach yielding an assembly con guration that satis es four kinematic task requirements: reachability, joint limits, obstacle avoidance and measure of isotropy. Further, because critical missions may involve high costs if the mission were to fail due to a failure in the manipulator system, we address the property of fault tolerance in more detail. We prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. For general purpose manipulators two redundant degrees-of-freedom are needed for every order of fault tolerance. However, we show that only one degree ofredundancy is su cient for task speci c fault tolerance. 1",
            "group": 1700,
            "name": "10.1.1.69.3462",
            "keyword": "",
            "title": "Design of Modular Fault Tolerant Manipulators"
        },
        {
            "abstract": "Abstract: This paper presents an approach to correct chromatic distortion within an image (vignetting) and to compensate for color response differences among similar cameras which equip a team of robots, based on Evolutionary Algorithms. Our black-box approach does not make assumptions concerning the physical/geometrical roots of the distortion, and the efficient implementation is suitable for real time applications on resource constrained platforms. 1",
            "group": 1701,
            "name": "10.1.1.69.3516",
            "keyword": "Camera modelingcolor correctionevolutionary algorithmsreal time",
            "title": "REAL-TIME INTER- AND INTRA- CAMERA COLOR MODELING AND CALIBRATION FOR RESOURCE CONSTRAINED ROBOTIC PLATFORMS"
        },
        {
            "abstract": "",
            "group": 1702,
            "name": "10.1.1.69.3960",
            "keyword": "Supply chain optimisationvehicle routing problemant colony optimisation",
            "title": "Ant Colony Optimization for vehicle routing in advanced logistics systems"
        },
        {
            "abstract": "Abstract. This paper presents a computational model that segments images based on the textural properties of object surfaces. The proposed Coupled-Membrane model applies the weak membrane approach to an image Wl(O ' derived from the power responses of a family of selfsimilar quadrature Gabor wavelets. While segmentation breaks are allowed in x and y only, coupling is introduced to in all 4 dimensions. The resulting spatial and spectral diffusion prevents minor variations in local textures from producing segmentation boundaries. Experiments showed that the model is adequate in segmenting a class of synthetic and natural texture images. 1",
            "group": 1703,
            "name": "10.1.1.69.4534",
            "keyword": "",
            "title": "Texture Segmentation by Minimizing Vector-Valued Energy Functionals: The Coupled-Membrane Model"
        },
        {
            "abstract": "Many problems that are treated by genetic algorithms belong to the class of NP-complete problems. The vantage of genetic algorithms when being applied to such kind of problems lies in the ability to search through the solution space in a broader sense than other heuristic methods that are based upon neighborhood search methods. Nevertheless, also genetic algorithms are frequently faced with a problem which, at least in its impact, is quite similar to the problem of stagnating in a local but not global solution what typically occurs when applying neighborhood based searches to hard problems with multimodal solution spaces. This drawback, called premature convergence in the terminology of genetic algorithms, occurs when the population of a genetic algorithm reaches such a suboptimal state that the genetic operators can no longer produce offspring that outperform their parents. During the last decades plenty of work has been investigated to introduce new coding standards and operators in order to overcome this essential handicap of genetic algorithms. As these coding standards and the belonging operators are rather problem specific in general we try to take a different approach and look upon the concepts of genetic algorithms as an artificial self organizing process in a bionically inspired generic way in order to improve the global convergence behaviour of genetic algorithms independently of the actually employed implementation. In doing so we have introduced an advanced selection model for genetic algorithms that allows adaptive",
            "group": 1704,
            "name": "10.1.1.69.4835",
            "keyword": "",
            "title": "Michael Affenzeller * genetic algorithms, evolution strategies, selective pressure A GENERIC EVOLUTIONARY COMPUTATION APPROACH BASED UPON GENETIC ALGORITHMS AND EVOLUTION STRATEGIES"
        },
        {
            "abstract": "evolutionary annealing-simplex algorithm for global",
            "group": 1705,
            "name": "10.1.1.69.4911",
            "keyword": "evolutionary algorithmssimulated annealing",
            "title": "optimisation of"
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, togenerate prices and derivatives commonly used by option traders. Ke ywords: options; eurodollar; volatility; path integral; optimization; statistical mechanics",
            "group": 1706,
            "name": "10.1.1.69.4914",
            "keyword": "High-resolution path-integral...-2- Lester Ingber",
            "title": "High-resolution path-integral development of financial options"
        },
        {
            "abstract": "This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difficult partition function of the graph. The algorithm fits into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using \u201ctempered \u201d proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs. 1",
            "group": 1707,
            "name": "10.1.1.69.5002",
            "keyword": "",
            "title": "Hot Coupling: a particle approach to inference and normalization on pairwise undirected graphs"
        },
        {
            "abstract": "Many apparently compelling techniques for automatic label placement use sophisticated heuristics for capturing cartographic knowledge, but, as noted by Zoraster (1991), also use inferior optimization strategies for nding good tradeo s between the variety of competing concerns involved in typical labeling problems. These techniques use procedural methods or",
            "group": 1708,
            "name": "10.1.1.69.5281",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Quantum Cellular Automata (QCA) has been proposed as an alternative architecture to CMOS and in principle should permit the implementation of ultra lower-power, nano-scale logic circuitry working at teraflop frequency. QCA is based on a new paradigm for encoding binary logic into electronic circuitry, where binary 1s and 0s are mapped to spatial configurations of electrons rather than magnitudes of electronic currents. The layout rules for QCA based circuits are radically different from those of CMOS based circuits, and design automation tools for QCA circuit layout are hard to find. This paper discusses the first automatic global placement algorithm for QCA-based circuits. We divide the QCA global placement process into zone partitioning and zone placement, and identify the constraints and objectives that are unique to QCA-based circuits as opposed to the conventional CMOS VLSI. 1.",
            "group": 1709,
            "name": "10.1.1.69.6154",
            "keyword": "",
            "title": "Global placement for quantum-dot cellular automata based circuits"
        },
        {
            "abstract": "Scheduling problems are often modeled as resource constrained problems in which critical resource assignments to tasks are known and the best assignment of resource time must be made subject to these constraints. Generalization to resource scheduling, where resource assignments are chosen concurrently with times results in a problem which is much more di\u00b1cult. A simpli\u00afed model of the general resource scheduling model is possible, however, in which tasks must be assigned a single primary resource, subject to constraints resulting from preassignment of secondary, or auxiliary, resources. This paper describes extensions and enhancements of Tabu Search for the special case of the resource scheduling problem described above. The class of problems is further restricted to those where it is reasonable to enumerate both feasible time and primary resource assignments. Potential applications include shift oriented production and manpower scheduling problems as well as course scheduling where classrooms (instructors) are primary and instructors (rooms) and students are secondary resources. The underlying model is a type of quadratic multiple choice problem which we call multiple choice quadratic vertex packing (MCQVP). Results for strategic oscillation and biased candidate sampling strategies are shown for reasonably sized real and randomly generated, synthetic, problem instances. These strategies are compared with other variations using consistent measures of solution time and quality developed for this study. 1",
            "group": 1710,
            "name": "10.1.1.69.6510",
            "keyword": "",
            "title": "TABU SEARCH FOR A CLASS OF SCHEDULING PROBLEMS (Preprint)"
        },
        {
            "abstract": "In the vehicle routing problem with stochastic demands a vehicle has to serve a set of customers whose exact demand is known only upon arrival at the customer\u2019s location. The objective is to find a permutation of the customers (an a priori tour) that minimizes the expected distance traveled by the vehicle. Since the objective function is computationally demanding, effective approximations of it could improve the algorithms \u2019 performance. We show that a good choice is using the length of the a priori tour as a fast approximation of the objective, to be used in the local search of the several metaheuristics analyzed. We also show that for the instances tested, our metaheuristics find better solutions with respect to a known effective heuristic and with respect to solving the problem as two related deterministic problems. ",
            "group": 1711,
            "name": "10.1.1.69.6830",
            "keyword": "",
            "title": "Metaheuristics for the vehicle routing problem with stochastic demands"
        },
        {
            "abstract": null,
            "group": 1712,
            "name": "10.1.1.69.7737",
            "keyword": "1.3 CONNECTIONS TO STATISTICAL PHYSICS............ viii",
            "title": "1.2 SATISFIABILITY AND HARD-PROBLEM INSTANCES....... iv"
        },
        {
            "abstract": "Planning by Rewriting (PbR) is a paradigm for efficient high-quality planning that exploits declarative plan rewriting rules and efficient local search techniques to transform an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition to addressing planning efficiency and plan quality, PbR offers a new anytime planning algorithm. The plan rewriting rules can be either specified by a domain expert or automatically learned. We describe a learning approach based on comparing initial and optimal plans that produces rules competitive with manually-specified ones. PbR is fully implemented and has been applied to several existing domains. The experimental results show that the PbR approach provides significant savings in planning effort while generating high-quality plans. 1",
            "group": 1713,
            "name": "10.1.1.69.8019",
            "keyword": "",
            "title": "Plan Optimization by Plan Rewriting"
        },
        {
            "abstract": "Keywords: IEEE Transactions on Vehicular Technology basic structural model, autoregressive integrated moving average, Kalman filter, mean absolute percentage error",
            "group": 1714,
            "name": "10.1.1.69.8390",
            "keyword": "basic structural modelautoregressive integrated moving averageKalman filtermean absolute",
            "title": "1 2"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii The goal of this work is to understand the application of the evolutionary programming approach to the problem of quantum circuit design. This problem is motivated by the following observations: \u2022 In order to keep up with the seemingly insatiable demand for computing power our computing devices will continue to shrink, all the way down to the atomic scale, at which point they become quantum mechanical systems. In fact, this event, known as Moore\u2019s Horizon, is likely to occur in less than 25 years. \u2022 The recent discovery of several quantum algorithms which can solve some interesting problems more efficiently than any known classical algorithm. \u2022 While we are not yet certain that quantum computers will ever be practical to build,",
            "group": 1715,
            "name": "10.1.1.69.8605",
            "keyword": "",
            "title": "On the evolutionary design of quantum circuits"
        },
        {
            "abstract": "not be interpreted as representing the official policies, either expressed or implied, of the funding agencies. There exists a need for manipulators that are more flexible and reliable than the current fixed configuration manipulators. Indeed, robot manipulators can be easily reprogrammed to per-form different tasks, yet the range of tasks that can be performed by a manipulator is limited by its mechanical structure. In remote and hazardous environments, such as a nuclear facil-ity or a space station, the range of tasks that may need to be performed often exceeds the capabilities of a single manipulator. Moreover, it is essential that critical tasks be executed reliably in these environments. To address this need for a more flexible and reliable manipulator, we propose the concept of a rapidly deployable fault tolerant manipulator system. Such a system combines a Reconfig-urable Modular Manipulator System (RMMS) with support software for rapid program-ming, trajectory planning, and control. This allows the user to rapidly configure a fault tolerant manipulator custom-tailored for a given task. This thesis investigates all aspects",
            "group": 1716,
            "name": "10.1.1.69.8674",
            "keyword": "",
            "title": "An agent-based approach to the design of rapidly deployable fault tolerant manipulators"
        },
        {
            "abstract": "A series of papers has developed a statistical mechanics of neocortical interactions (SMNI), deriving aggregate behavior of experimentally observed columns of neurons from statistical electrical-chemical properties of synaptic interactions. While not useful to yield insights at the single neuron level, SMNI has demonstrated its capability in describing large-scale properties of short-term memory and electroencephalographic (EEG) systematics. The necessity of including nonlinear and stochastic structures in this development has been stressed. Sets of EEG and evoked potential data were fit, collected to investigate genetic predispositions to alcoholism and to extract brain \u201csignatures \u201d of short-term memory. Adaptive Simulated Annealing (ASA), a global optimization algorithm, was used to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta indicators (CMI) are thereby derived for individual\u2019s EEG data. The CMI give better signal recognition than the raw data, and can be used to advantage as correlates of behavioral states. These results give strong quantitative support for an accurate intuitive picture, portraying neocortical interactions as having common algebraic or physics mechanisms that scale across quite disparate spatial scales and functional or behavioral phenomena, i.e., describing interactions among neurons, columns of neurons, and regional masses of neurons.",
            "group": 1717,
            "name": "10.1.1.69.8738",
            "keyword": "Statistical Mechanics of Neocortical...-2- Lester Ingber",
            "title": "Statistical mechanics of neocortical interactions: Constraints on 40 Hz models of shortterm memory"
        },
        {
            "abstract": "Management decisions involving groundwater supply and remediation often rely on optimization techniques to determine an effective strategy. We introduce several derivative-free sampling methods for solving constrained optimization problems that have not yet been considered in this field, and we include a genetic algorithm for completeness. Two well-documented community problems are used for illustration purposes: a groundwater supply problem and a hydraulic capture problem. The community problems were found to be challenging applications due to the objective functions being nonsmooth, nonlinear, and having many local minima. Because the results were found to be sensitive to initial conditions for some methods, guidance is provided in selecting initial conditions for these problems that improve the likelihood of achieving significant reductions in the objective function to be minimized. In addition, we suggest some potentially fruitful areas for future research.",
            "group": 1718,
            "name": "10.1.1.69.8974",
            "keyword": "",
            "title": "A Comparison of Derivative-Free Optimization Methods for Groundwater Supply and Hydraulic Capture Community Problems"
        },
        {
            "abstract": "Xpress-MP Revised translation from the French language edition of:",
            "group": 1719,
            "name": "10.1.1.69.9634",
            "keyword": "",
            "title": "Published by Dash Optimization Ltd. www.dashoptimization.com Published by: Dash Optimization Ltd. Blisworth House"
        },
        {
            "abstract": "Abstract\u2014The choice of a good annealing schedule is necessary for good performance of simulated annealing for channel routing and other combinatorial optimization problems. In this paper, we propose a new means of controlling annealing temperatures by posing the simulated annealing task as an optimal control problem and applying the techniques of reinforcement learning. Although many means of automating control of annealing temperatures have been proposed, this technique requires no specific knowledge of the problem and provides a natural means of expressing time versus quality tradeoffs. Index Terms\u2014learning systems, optimization methods, routing, simulated annealing.",
            "group": 1720,
            "name": "10.1.1.70.106",
            "keyword": "S",
            "title": ""
        },
        {
            "abstract": "Two global optimization algorithms, namely Genetic Algorithm (GA) and Simulated Annealing (SA), have been applied to the aerodynamic shape optimization of transonic cascades; the objective being the redesign of an existing turbomachine airfoil to improve its performance by minimizing the total pressure loss while satisfying a number of constraints. This is accomplished by modifying the blade camber line; keeping the same blade thickness distribution, mass flow rate and the same flow turning. The objective is calculated based on an Euler solver and the blade camber line is represented with non-uniform rational B-splines (NURBS). The SA and GA methods were first assessed for known test functions and their performance in optimizing the blade shape for minimum loss is then demonstrated on a transonic turbine cascade where it is shown to produce a significant reduction in total pressure loss by eliminating the passage shock. 1.",
            "group": 1721,
            "name": "10.1.1.70.234",
            "keyword": "",
            "title": "Global Optimization Methods for the Aerodynamic Shape Design of Transonic Cascades"
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, to generate prices and derivatives commonly used by option traders. Ke ywords: options; eurodollar; volatility; path integral; optimization; statistical mechanics",
            "group": 1722,
            "name": "10.1.1.70.619",
            "keyword": "High-resolution path-integral...-2- Lester Ingber",
            "title": "High-resolution path-integral development of financial options"
        },
        {
            "abstract": "Swarm intelligence is a relatively new approach to problem solving that takes inspiration from the social behaviors of insects and of other animals. In particular, ants have inspired a number of methods and techniques among which the most studied and the most successful is the general purpose optimization technique known as ant colony optimization. Ant colony optimization (ACO) takes inspiration from the foraging behavior of some ant species. These ants deposit pheromone on the ground in order to mark some favorable path that should be followed by other members of the colony. Ant colony optimization exploits a similar mechanism for solving optimization problems. From the early nineties, when the first ant colony optimization algorithm was proposed, ACO attracted the attention of increasing numbers of researchers and many successful applications are now available. Moreover, a substantial corpus of theoretical results is becoming available that provides useful guidelines to researchers and practitioners in further applications of ACO. The goal of this article is to introduce ant colony optimization and to survey its most notable applications. Section I provides some background information on the foraging behavior of ants. Section II describes ant colony optimization and its main variants. Section III surveys the most notable theoretical results concerning ACO, and Section IV illustrates some of its most successful applications. Section V highlights some currently active research topics, and Section VI provides an overview of some other algorithms that, although not directly related to ACO, are nonetheless inspired by the behavior of ants. Section VII concludes the article.",
            "group": 1723,
            "name": "10.1.1.70.1052",
            "keyword": "",
            "title": "Ant Colony Optimization \u2013 Artificial Ants as a Computational Intelligence Technique"
        },
        {
            "abstract": "The relevance of tuning the parameters of metaheuristics. A case study: The vehicle routing problem with stochastic demand",
            "group": 1724,
            "name": "10.1.1.70.2059",
            "keyword": "",
            "title": "et de D\u00e9veloppements en Intelligence Artificielle"
        },
        {
            "abstract": "Abstract \u2014 We present a method for performing mode classification of real-time streams of GPS surface position data. Our approach has two parts: an algorithm for robust, unconstrained fitting of hidden Markov models (HMMs) to continuousvalued time series, and SensorGrid technology that manages data streams through a series of filters coupled with a publish/subscribe messaging system. The SensorGrid framework enables strong connections between data sources, the HMM time series analysis software, and users. We demonstrate our approach through a web portal environment through which users can easily access data from the SCIGN and SOPAC GPS networks in Southern California, apply the analysis method, and view results. Ongoing real-time mode classifications of streaming GPS data are displayed in a map-based visualization interface. I.",
            "group": 1725,
            "name": "10.1.1.70.2126",
            "keyword": "",
            "title": "Analysis of streaming GPS measurements of surface displacement through a web services environment"
        },
        {
            "abstract": "Deriving algorithms to solve problems is a main activity in computer science. Elaborate techniques are often devised to efficiently handle specific problems. This paper proposes the use of a general method of solving problems, called the Iterative Multi-Agent (IMA) Method, which assumes little about the specific problem at hand. In IMA the problem is divided into overlapping sub-problems, and an agent is assigned the task of solving each of them. The agents iterate over the solutions to the sub-problems in the hope of finding a solution that satisfies the whole problem. If noticing a clash with another part of the problem that was solved by another expert, each expert revises its solution, hoping the new one will remove the clash. This is repeated unless the problem is solved with no clashes, or when a specific number of iterations are performed. In IMA no central authority knows about the whole problem, or even how each agent's work may affect other agents ' work. By design, IMA agents can run in parallel, solving the often hard problem of coming up with a parallel algorithm. The agents need minimum amounts of communication and synchronisation, making the method scalable on hardware systems with many processors. To demonstrate IMA's adaptability, in this paper we tackle the problem of extracting rules by pruning (removing) the condition attributes",
            "group": 1726,
            "name": "10.1.1.70.3747",
            "keyword": "",
            "title": "ISBN 0-7731-0482-8 Problem Solving with Limited Knowledge: Pruning Attributes in Classification Rules"
        },
        {
            "abstract": "We present a system that can evolve the morphology and the controller of virtual walking and block-throwing creatures (catapults) using a genetic algorithm. The system is based on Sims \u2019 work, implemented as a flexible platform with an off-the-shelf dynamics engine. Experiments aimed at evolving Sims-type walkers resulted in the emergence of various realistic gaits while using fairly simple objective functions. Due to the flexibility of the system, drastically different morphologies and functions evolved with only minor modifications to the system and objective function. For example, various throwing techniques evolved when selecting for catapults that propel a block as far as possible. Among the strategies and morphologies evolved, we find the dropkick strategy, as well as the systematic invention of the principle behind the wheel when allowing mutations to the projectile. Key Words Artificial life, genetic algorithm, catapults, walkers, dynamics simulator, co-evolution.",
            "group": 1727,
            "name": "10.1.1.70.4565",
            "keyword": "1",
            "title": "Evolving Virtual Creatures and Catapults"
        },
        {
            "abstract": "Abstract \u2014 Deep submicron technology scaling has two major ramifications on the design process. First, reduced feature size significantly increases wire delay, thus resulting in critical paths being dominated by global interconnect rather than gate delays. Second, ultra-high level of integration mandates design of systems-on-chip that encompass numerous design blocks of decreased functional granularity and increased communication demands. The convergence of these two factors emphasizes the importance of the on-chip bus network as one of the crucial high-performance enablers for future systems-on-chip. We have developed an on-chip bus network design methodology and corresponding set of tools which, for the first time, close the synthesis loop between system and physical design. The approach has three components: a communication profiler, a bus network designer, and a fast approximate floorplanner. The communication profiler collects run-time information about the traffic between system cores. The bus network design component optimizes the bus network structure by coordinating information from the other two components. The floorplanner aims at creating a feasible floorplan; it also sends feedback about the most constrained parts of the network. We demonstrate the effectiveness of our bus network design approach on a number of multi-core designs. Index Terms \u2014 Bus network design, latency, system synthesis, on-chip communication.",
            "group": 1728,
            "name": "10.1.1.70.4822",
            "keyword": "",
            "title": "IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, NO., DATE 1 Latency-Guided On-Chip Bus Network Design"
        },
        {
            "abstract": "Simulated tempering and swapping are two families of sampling algorithms in which a parameter representing temperature varies during the simulation. The hope is that this will overcome bottlenecks that cause sampling algorithms to be slow at low temperatures. Madras and Zheng demonstrate that the swapping and tempering algorithms allow efficient sampling from the low-temperature mean-field Ising model, a model of magnetism, and a class of symmetric bimodal distributions [10]. Local Markov chains fail on these distributions due to the existence of bad cuts in the state space. Bad cuts also arise in the \u00d5-state Potts model, another fundamental model for magnetism that generalizes the Ising model. Glauber (local) dynamics and the Swendsen-Wang algorithm have been shown to be prohibitively slow for sampling from the Potts model at some temperatures [1, 2, 6]. It is reasonable to ask whether tempering or swapping can overcome the bottlenecks that cause these algorithms to converge slowly on the Potts model. We answer this in the negative, and give the first example demonstrating that tempering can mix slowly. We show this for the 3-state ferromagnetic Potts model on the complete graph, known as the mean-field model. The slow convergence is caused by a first-order (discontinuous) phase transition in the underlying system. Using this insight, we define a variant of the swapping algorithm that samples efficiently from a class of bimodal distributions, including the mean-field Potts model. 1",
            "group": 1729,
            "name": "10.1.1.70.4952",
            "keyword": "",
            "title": "Torpid Mixing of Simulated Tempering on the Potts Model"
        },
        {
            "abstract": "",
            "group": 1730,
            "name": "10.1.1.70.5477",
            "keyword": "",
            "title": "Dvi file name: &quot;ccreview.dvi&quot;. This PostScript file was produced on host &quot;shakespeare.rutgers.edu&quot;."
        },
        {
            "abstract": "Belief networks (or probabilistic networks) and neural networks are two forms of network representations that have been used in the development ofintelligent systems in the eld of arti cial intelligence. Belief networks provide a concise representation of general probability distributions over a set of random variables, and facilitate exact cal-culation of the impact of evidence on propositions of interest. Neural networks, which represent parameterized algebraic combinations of nonlinear activation functions, have found widespread use as models of real neural systems and as function approximators because of their amenability to simple training algorithms. Furthermore, the simple, local nature of most neural network training algorithms provides a certain biological plausibility and allows for a massively parallel implementation. In this paper, we show that similar local learning algorithms can be derived for belief networks, and that these learning algorithms can operate using only information that is directly available from the normal, inferential processes of the networks. This removes the main obstacle pre-venting belief networks from competing with neural networks on the above-mentioned tasks. The precise, local, probabilistic interpretation of belief networks also allows them to be partially or wholly constructed by humans; allows the results of learning to be easily understood; and allows them to contribute to rational decision-making in awell-de ned way.",
            "group": 1731,
            "name": "10.1.1.70.6378",
            "keyword": "",
            "title": "Adaptive probabilistic networks"
        },
        {
            "abstract": "D\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle",
            "group": 1732,
            "name": "10.1.1.70.6863",
            "keyword": "Markovian segmentationGraphics HardwareGPUParameter estimation 2",
            "title": "Markovian segmentation and parameter estimation on graphics hardware"
        },
        {
            "abstract": "Abstract\u2014A series of papers has developed a statistical mechanics of neocortical interactions (SMNI), deriving aggregate behavior of experimentally observed columns of neurons from statistical electrical-chemical properties of synaptic interactions. While not useful to yield insights at the single neuron level, SMNI has demonstrated its capability in describing large-scale properties of short-term memory and electroencephalographic (EEG) systematics. The necessity of including nonlinear and stochastic structures in this development has been stressed. Sets of EEG and evoked potential data were fit, collected to investigate genetic predispositions to alcoholism and to extract brain \u201csignatures \u201d of short-term memory. Adaptive Simulated Annealing (ASA), a global optimization algorithm, was used to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta indicators (CMI) are thereby derived for individual\u2019s EEG data. The CMI give better signal recognition than the raw data, and can be used to advantage as correlates of behavioral states. These results give strong quantitative support for an accurate intuitive picture, portraying neocortical interactions as having common algebraic or physics mechanisms that scale across quite disparate spatial scales and functional or behavioral phenomena, i.e., describing interactions among neurons, columns of neurons, and regional masses of neurons. This paper adds to these previous investigations two important aspects, a description of how the CMI may be used in source localization, and calculations using previously ASA-fitted parameters in out-of-sample data.",
            "group": 1733,
            "name": "10.1.1.70.6954",
            "keyword": "EEGSimulated AnnealingStatistical Mechanics Statistical Mechanics of Neocortical...-2- Lester Ingber",
            "title": "Statistical mechanics of neocortical interactions: Training and testing canonical momenta indicators of EEG,\u2019\u2019 Mathl. Computer Modelling 27"
        },
        {
            "abstract": "This is a preliminary version of a chapter that appeared in the book Local Search in Combinatorial Optimization, E. H. L. Aarts and J. K. Lenstra (eds.), John Wiley and Sons, London, 1997, pp. 215-310. The traveling salesman problem (TSP) has been an early proving ground for many approaches to combinatorial optimization, including classical local optimization techniques as well as many of the more recent variants on local optimization, such as simulated annealing, tabu search, neural networks, and genetic algorithms. This chapter discusses how these various approaches have been adapted to the TSP and evaluates their relative success in this perhaps atypical domain from both a",
            "group": 1734,
            "name": "10.1.1.70.7639",
            "keyword": "",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "The Mirrored Traveling Tournament Problem (mTTP) is an optimization problem that represents certain types of sports timetabling, where the objective is to minimize the total distance traveled by the teams. This work proposes the use of hybrid heuristic to solve the mTTP, using an evolutionary algorithm in association with the metaheuristic Simulated Annealing. It suggests the use of Genetic Algorithm with a compact genetic codification in conjunction with an algorithm to expand the code. The validation of the results will be done in benchmark problems available in literature and real benchmark problems, e.g. Brazilian Soccer Championship. ",
            "group": 1735,
            "name": "10.1.1.70.8366",
            "keyword": "",
            "title": " Mirrored Traveling Tournament Problem: An Evolutionary Approach"
        },
        {
            "abstract": "In this paper we consider the problem of finding the efficient frontier associated with the standard mean-variance portfolio optimisation model. We extend the standard model to include cardinality constraints that limit a portfolio to have a specified number of assets, and to impose limits on the proportion of the portfolio held in a given asset (if any of the asset is held). We illustrate the differences that arise in the shape of this efficient frontier when such constraints are present. We present three heuristic algorithms based upon genetic algorithms, tabu search and simulated annealing for finding the cardinality constrained efficient frontier. Computational results are presented for five data sets involving up to 225 assets.",
            "group": 1736,
            "name": "10.1.1.70.8715",
            "keyword": "portfolio optimisationefficient frontier",
            "title": "1 The Management School Heuristics for cardinality constrained portfolio optimisation"
        },
        {
            "abstract": "Abstract. A standard problem within universities is that of Teaching Space Allocation; the assignment of rooms and times to various teaching activities. The focus is usually on courses that are expected to fit into one room. However, it can also happen that the course will need to be broken up, or \u201csplit\u201d, into multiple sections. A lecture might be too large to fit into any one room. Another common example is that the course corresponds to seminars or tutorials, and although hundreds of students are enrolled, each individual class, or event, should be just tens of students in order to meet student and institutional preferences. Typically, decisions as to how to split courses need to be made within the context of limited space requirements. Institutions do not have an unlimited number of teaching rooms, and need to effectively use those that they do have. The efficiency of space usage is usually measured by the overall \u201cutilisation \u201d which is basically the fraction of the available seat-hours that are actually used. A multi-objective optimisation problem naturally arises; with a trade-off between satisfying preferences on splitting, a desire to increase utilisation, and also to satisfy other constraints such as those based on event location, and timetabling conflicts. In this paper we explore such trade-off surfaces. The explorations themselves are based on a local search method we introduce that attempts to optimise the space utilisation by means of a \u201cdynamic splitting \u201d strategy. The local moves are designed to improve utilisation and the satisfaction of other constraints, but are also allowed to split, and un-split, courses so as to simultaneously meet the splitting objectives. 1",
            "group": 1737,
            "name": "10.1.1.70.8963",
            "keyword": "",
            "title": "The teaching space allocation problem with splitting"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii The significant amount of data contained in digital images present barriers to methods of learning from the information they hold. Noise and the subjectivity of image evaluation further complicate such automated processes. In this thesis, we examine a particular area in which these difficulties are experienced. We attempt to control the parameters of a multi-step algorithm that processes visual information. A framework for approaching the parameter selection problem using reinforcement learning agents is presented as the main contribution of this research. We focus on the generation of state and action space, as well as task-dependent reward. We first dis-cuss the automatic determination of fuzzy membership functions as a specific case of the above problem. Entropy of a fuzzy event is used as a reinforcement signal. Membership functions representing brightness have been automatically generated for several images. The results show that the reinforcement learning approach is superior to an existing simulated annealing-based approach. The framework has also been evaluated by optimizing ten parameters of the text de-tection for semantic indexing algorithm proposed by Wolf et al. Image features are defined and extracted to construct the state space. Generalization to reduce the state space is performed with the fuzzy ARTMAP neural network, offering much faster learning than in the previous tabular implementation, despite a much larger state and action space. Difficulties in using a continuous action space are overcome by employing the DIRECT method for global optimization without derivatives. The chosen parameters are evaluated using metrics of recall and precision, and are shown to be superior to the parameters previously recommended. We further discuss the inter-play between intermediate and terminal reinforcement. iii",
            "group": 1738,
            "name": "10.1.1.71.337",
            "keyword": "",
            "title": "Reinforcement Learning for Parameter Control of Image-Based Applications"
        },
        {
            "abstract": "This is a preliminary version of a chapter that appeared in the book Local Search in Combinatorial Optimization, E. H. L. Aarts and J. K. Lenstra (eds.), John Wiley and Sons, London, 1997, pp. 215-310. The traveling salesman problem (TSP) has been an early proving ground for many approaches to combinatorial optimization, including classical local optimization techniques as well as many of the more recent variants on local optimization, such as simulated annealing, tabu search, neural networks, and genetic algorithms. This chapter discusses how these various approaches have been adapted to the TSP and evaluates their relative success in this perhaps atypical domain from both a",
            "group": 1739,
            "name": "10.1.1.71.434",
            "keyword": "",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "Abstract-An adaptive learning architecture has been developed for modeling manufacturing processes involving several controlling vari-ables. This paper describes the application of the new architecture to process modeling and recipe synthesis for deposition rate, stress, and film thickness in low pressure chemical vapor deposition (LPCVD) of undoped polysilicon. In this architectqre the model for a process is generated by combining the qualitative knowledge of human experts, tqptured in the form of influence diagrams, and the learning abilities of neural networks for extracting the quantitative knowledge that re-lates parameters of a process. To evaluate the merits of this method-ptogy, we have compared the accuracy of these new models to that of more conventional models generated by the use of first principles and/ or statistical regression analysis. Accuracy of the different models is compared using the same empirical data sets form realistic experi-ments. The models generated by the integrati n of influence diagrams and neural networks are shown to have halfthe error or less, even though given only half as much informatiop in creating the models. Furthermore, it is shown that by employing the generalization ability of neural networks in the synthesis algorithm new recipes can be pro-duced for the process. Two such recipes are generated for the LPCVD process. One is a zero-stress polysilicon film recipe; the second, a uni-form deposition rate recipe which is based on use of a non-uniform temperature distribution during deposition.",
            "group": 1740,
            "name": "10.1.1.71.690",
            "keyword": "",
            "title": "Use of Influence Diagrams and Neural Networks in Modeling Semiconductor Manufacturing Processes"
        },
        {
            "abstract": "",
            "group": 1741,
            "name": "10.1.1.71.1148",
            "keyword": "",
            "title": "Models and Techniques of Dynamic Demand-Responsive Transportation Planning"
        },
        {
            "abstract": "Abstract. We explore a new general-purpose heuristic for nding highquality solutions to hard optimization problems. The method, called extremal optimization, is inspired by \\self-organized criticality, &quot; a concept introduced to describe emergent complexity in many physical systems. In contrast to Genetic Algorithms which operate on an entire \\genepool&quot; of possible solutions, extremal optimization successively replaces extremely undesirable elements of a sub-optimal solution with new, random ones. Large uctuations, called \\avalanches, &quot; ensue that e ciently explore many local optima. Drawing upon models used to simulate farfrom-equilibrium dynamics, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such assimulated annealing. With only one adjustable parameter, its performance has proved competitive with more elaborate methods, especially near phase transitions. Those phase transitions are found in the parameter space of most optimization problems, and have recently been conjectured to be",
            "group": 1742,
            "name": "10.1.1.71.1816",
            "keyword": "",
            "title": "Optimizing through co-evolutionary avalanches"
        },
        {
            "abstract": "Successful software systems cope with complexity by organizing classes into packages. However, a particular organization may be neither straightforward nor obvious for a given developer. As a consequence, classes can be misplaced, leading to duplicated code and ripple effects with minor changes effecting multiple packages. We claim that contextual information is the key to rearchitecture a system. Exploiting contextual information, we propose a technique to detect misplaced classes by analyzing how client packages access the classes of a given provider package. We define locality as a measure of the degree to which classes reused by common clients appear in the same package. We then use locality to guide a simulated annealing algorithm to obtain optimal placements of classes in packages. The result is the identification of classes that are candidates for relocation. We apply the technique to three applications and validate the usefulness of our approach via developer interviews.",
            "group": 1743,
            "name": "10.1.1.71.2045",
            "keyword": "Packagessoftware measurementsimulated annealingprogram understandingreverse engineering",
            "title": "Using Context Information to Re-architect a System \u2217"
        },
        {
            "abstract": "Data mining and knowledge discovery",
            "group": 1744,
            "name": "10.1.1.71.2327",
            "keyword": " statistical mechanics...-2- Lester Ingber",
            "title": "via statistical mechanics in nonlinear stochastic systems"
        },
        {
            "abstract": "stellt einen wesentlichen Eingriff in den menschlichen K\u00f6rper dar und muss genauestens geplant werden. Eine dreidimensionale Visualisierung relevanter Strukturen erlaubt eine verbesserte pr\u00e4operative Einsch\u00e4tzung der Operabilit\u00e4t des Patienten. Hierf\u00fcr sind Segmentierungen der zu visualisierenden Strukturen notwendig. Die Lymphknotensegmentierung ist dabei einer der zeitaufwendigsten Arbeitsschritte, der bisher nur auf manuellem Wege zuverl\u00e4ssig auszuf\u00fchren war. In dieser Arbeit wird eine Methode zur semi-automatischen Segmentierung von Lymphknoten in CT-Daten des Halses in Form eines stabilen Feder-Masse-Modells entwickelt. Hierf\u00fcr werden erstmals die dreidimensionalen Form- und CT-Abbildungseigenschaften sowie Konturinformationen von Lymphknoten in einem Segmentierungsmodell vereint.",
            "group": 1745,
            "name": "10.1.1.71.2375",
            "keyword": "Das vorgestellte Modell eignet sich zur quantitativen Auswertung der segmentierten",
            "title": "in CT-Daten des Halses"
        },
        {
            "abstract": "Abstract: In this paper we present Pepsy,anovelprototyping environment for multi-DSP systems, with the primary goal to support the design and implementation of parallel digital signal processing (DSP) applications subject to various design constraints. Given a specification of the prototyping problem in the form of an application model, a hardware model and mapping constraints, Pepsy automatically maps and schedules the DSP application onto the multi-processor system and synthesizes the complete code for each processor. A detailed performance model of the parallel application is an integral part of Pepsy. Important performance parameters such as computation and communication times as well as memory consumption can be estimated prior to the implementation. Pepsy not only solves the standard mapping and scheduling problem, but it is also able to explore various important design goals for embedded systems and DSP applications such as minimizing memory and power consumption and enforcing the timeliness of tasks. Two complex case studies demonstrate the feasibility of our prototyping environment. Key Words: rapid prototyping; scheduling; performance prediction; multi-DSP; embedded systems; data flow",
            "group": 1746,
            "name": "10.1.1.71.2711",
            "keyword": "CategoryB.8.2D.2.2C.3",
            "title": "A Rapid Prototyping Environment for Multi-DSP Systems based on Accurate Performance Prediction"
        },
        {
            "abstract": "Propositional Satisfiability (SAT) and Constraint Programming (CP) have developed as two relatively independent threads of research, cross-fertilising occasionally. These two approaches to problem solving have a lot in common, as evidenced by similar ideas underlying the branch and prune algorithms that are most successful at solving both kinds of problems. They also exhibit differences in the way they are used to state and solve problems, since SAT\u2019s approach is in general a black-box approach, while CP aims at being tunable and programmable. This survey overviews the two areas in a comparative way, emphasising the similarities and differences between the two and the points where we feel that one technology can benefit from ideas or experience acquired",
            "group": 1747,
            "name": "10.1.1.71.3119",
            "keyword": "Additional Key Words and PhrasesSearchConstraint SatisfactionSAT Contents",
            "title": "Propositional Satisfiability and Constraint Programming: a Comparative Survey"
        },
        {
            "abstract": "",
            "group": 1748,
            "name": "10.1.1.71.3202",
            "keyword": "",
            "title": "Focused Simulated Annealing Search: An Application to Job Shop Scheduling"
        },
        {
            "abstract": "In this paper, we summarize circuit placement techniques and algorithms developed by the BLAC CAD research group; these have been integrated into our recursive bisection based placement tool feng shui. We also briefly describe current research interests. Catagories and Subject Descriptions: J.6 [Computer-Aided Engineering]:",
            "group": 1749,
            "name": "10.1.1.71.3319",
            "keyword": "General termsAlgorithms KeywordsPlacementfloorplanningmixed block design",
            "title": "ABSTRACT Recursive Bisection Placement:"
        },
        {
            "abstract": "Acknowledgements Many people deserve thanks in helping me progress from my mother's womb to finishing my dissertation. Rather than beginning with the doctor who delivered me and filling pages and pages with names, I'll say one big generic thanks to all who deserve to be thanked but aren't mentioned here by name. My advisor, Mitch Marcus, deserves a great deal of thanks for his constant support and encouragement. His enthusiasm helped make my research a lot more fun. I thank my committee (Steve Abney, Lila Gleitman, Aravind Joshi and Mark Liberman) for their feedback, which was significant in shaping my dissertation. I also thank them for allowing me to graduate. I believe the men and women who have sacrificed over the years to defend our Bill of Rights and Constitution deserve special thanks, for without their commitment, freedom of thought and expression might not exist. My parents instilled in me a love for education from an early age, which provided the driving force for me to pursue a degree, as well as the belief that one should always doubt, question, and learn. I met Meiting Lu when I first arrived at the University of Pennsylvania. She managed to keep me sane during my 3.75 years at Penn. It is Meiting and her friendship that deserve the biggest thanks. iii",
            "group": 1750,
            "name": "10.1.1.71.3720",
            "keyword": "",
            "title": "A corpus-based approach to Language learning"
        },
        {
            "abstract": "triangulation of the Poincar'e homology 3-sphere",
            "group": 1751,
            "name": "10.1.1.71.4200",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract \u2014 We describe a novel shape formation algorithm for ensembles of 2-dimensional lattice-arrayed modular robots, based on the manipulation of regularly shaped voids within the lattice (\u201choles\u201d). The algorithm is massively parallel and fully distributed. Constructing a goal shape requires time proportional only to the complexity of the desired target geometry. Construction of the shape by the modules requires no global communication nor broadcast floods after distribution of the target shape. Results in simulation show 97.3 % shape compliance in ensembles of approximately 60,000 modules, and we believe that the algorithm will generalize to 3D and scale to handle millions of modules. This paper is submitted to Invited Session: New Trends in Modular Robotics. I.",
            "group": 1752,
            "name": "10.1.1.71.4554",
            "keyword": "",
            "title": "Scalable shape sculpting via hole motion: Motion planning in lattice-constrained modular robots"
        },
        {
            "abstract": "To be presented in symposium B Abstract. Problems of Combinatorial Optimization distinguish themselves by their well-structured problem description as well as by their huge number of possible action alternatives. Especially in the area of production and operational logistics these problems frequently occur. Their advantage lies in their subjective understanding of action alternatives and their objective functions. The use of classical optimization methods for problems of combinatorial optimization often fails because of the exponentially growing computational effort. Therefore, even if they are not able to ensure a global solution, heuristic methods like Genetic Algorithms (GAs) or Evolution Strategies (ESs) are massively utilized in practice because of their significant lower computational effort. Both, GAs and ESs have a number of drawbacks that reduce their applicability to that kind of problems. During the last decades plenty of work has been investigated in order to introduce new coding standards and operators especially for Genetic Algorithms. All these approaches have one thing in common: They are rather problem specific and often they do not challenge the basic principle of Genetic Algorithms. In the present paper we take a different approach and look upon the concepts of a Standard Genetic Algorithm (SGA) as an artificial self organizing process in order to overcome",
            "group": 1753,
            "name": "10.1.1.71.4743",
            "keyword": "",
            "title": "New variants of genetic algorithms applied to problems of combinatorial optimization"
        },
        {
            "abstract": " ",
            "group": 1754,
            "name": "10.1.1.71.5351",
            "keyword": "Supply chain optimisationvehicle routing problemsales forecastingant colony optimisation",
            "title": "Planning and optimisation of vehicle routes for fuel oil distribution"
        },
        {
            "abstract": "With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with softwareexposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures. In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element. We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.",
            "group": 1755,
            "name": "10.1.1.71.5590",
            "keyword": "",
            "title": "ABSTRACT A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "One of the greatest challenges of biophysics is the prediction of molecular structures. Biomolecules, especially proteins and nucleic acids, are in the focus of numerous efforts, because three dimensional structures of these molecules are essential for their function. The follow up of genome sequencing, often called proteomics, requires simultaneous determination of the structures of thousands of biopolymers. Despite substantial progresses in structural analysis of biopolymers by X-ray crystallography and NMR spectroscopy three dimensional structure determination is often time consuming or expensive at the current state of the art. Therefore determination of three dimensional structures based on theoretical models gained in importance. Although the main research focus is still on proteins, three dimensional investigations on nucleic acids increased due to the discovery of new functions and aspects of RNA. RNA is also challenging from a theoretical point of view, since secondary structures not only can be calculated by efficient algorithms,",
            "group": 1756,
            "name": "10.1.1.71.5674",
            "keyword": "",
            "title": "zur Erlangung des akademischen Grades"
        },
        {
            "abstract": "Hiermit versichere ich, dass ich diese Diplomarbeit selbst\u00e4ndig verfasst habe. Ich habe dazu keine anderen als die angegebenen Quellen und Hilfsmittel verwendet.",
            "group": 1757,
            "name": "10.1.1.71.6180",
            "keyword": "",
            "title": "Prof. Dr. Luc de Raedt (Albert-Ludwigs-Universit\u00e4t Freiburg)"
        },
        {
            "abstract": "It is proposed to apply modern methods of nonlinear nonequilibrium statistical mechanics to develop software algorithms that will optimally respond to targets within short response times with minimal computer resources. This Statistical Mechanics Algorithm for Response to Targets (SMART) can be developed with a view tow ards its future implementation into a hardwired Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed of response to targets (SMART SAM). I. Identification of Sample Problems (A) Target Variables\u2014Recognition Consider the grid in Fig. 1, defined within a given time epoch, where the grid is to be conceived as a generalized \u2018\u2018radar\u2019 \u2019 screen, representing data being accumulated by multiple sensors. Each cell has information pertaining to relocatable targets that may be moving between cells. This information is assumed to be further encoded into target variables, e.g., coordinate position, velocity, acceleration, numbers of targets within these categories, etc.",
            "group": 1758,
            "name": "10.1.1.71.6738",
            "keyword": "",
            "title": "Statistical mechanics algorithm for response to targets (SMART"
        },
        {
            "abstract": "Recent work in statistical mechanics has developed new analytical and numerical techniques to solve coupled stochastic equations. This paper applies the very fast simulated re-annealing and path-integral methodologies to the estimation of the Brennan and Schwartz two-factor term structure model. It is shown that these methodologies can be utilized to estimate more complicated n-factor nonlinear models. 1. CURRENT MODELS OF TERM STRUCTURE The modern theory of term structure of interest rates is based on equilibrium and arbitrage models in which bond prices are determined in terms of a few state variables. The one-factor models of Cox, Ingersoll and Ross (CIR) [1-4], and the two-factor models of Brennan and Schwartz (BS) [5-9] have been instrumental in the development of the valuation of interest dependent securities. The assumptions of these models include: \u2022 Bond prices are functions of a number of state variables, one to several, that follow Markov processes. \u2022 Inv estors are rational and prefer more wealth to less wealth. \u2022 Inv estors have homogeneous expectations.",
            "group": 1759,
            "name": "10.1.1.71.6978",
            "keyword": "",
            "title": "Application of statistical mechanics methodology to term-structure bond-pricing models"
        },
        {
            "abstract": "A series of papers has developed a statistical mechanics of neocortical interactions (SMNI), deriving aggregate behavior of experimentally observed columns of neurons from statistical electrical-chemical properties of synaptic interactions. While not useful to yield insights at the single neuron level, SMNI has demonstrated its capability in describing large-scale properties of short-term memory and electroencephalographic (EEG) systematics. The necessity of including nonlinear and stochastic structures in this development has been stressed. In this paper, amore stringent test is placed on SMNI: The algebraic and numerical algorithms previously developed in this and similar systems are brought to bear to fit large sets of EEG and evoked potential data being collected to investigate genetic predispositions to alcoholism and to extract brain \u201csignatures \u201d of short-term memory. Using the numerical algorithm of Very Fast Simulated Re-Annealing, it is demonstrated that SMNI can indeed fit this data within experimentally observed ranges of its underlying neuronal-synaptic parameters, and use the quantitative modeling results to examine physical neocortical mechanisms to discriminate between high-risk and low-risk populations genetically predisposed to alcoholism. Since this first study is a control to span relatively long time epochs, similar to earlier attempts to establish such correlations, this discrimination is inconclusive because of other neuronal activity which can mask such effects. However, the SMNI model is shown to be consistent",
            "group": 1760,
            "name": "10.1.1.71.7149",
            "keyword": "",
            "title": "Statistical mechanics of neocortical interactions: A scaling paradigm applied to electroencephalography "
        },
        {
            "abstract": "Simulated annealing is a popular method for approaching the solution of a global optimization problem. Existing results on its performance apply to discrete combinatorial optimization where the optimization variables can assume only a finite set of possible values. We introduce a new general formulation of simulated annealing which allows one to guarantee finite-time performance in the optimization of functions of continuous variables. The results hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up-to-date theory of convergence of Markov chain Monte Carlo methods on continuous domains. This work is inspired by the concept of finite-time learning with known accuracy and confidence developed in statistical learning theory. Optimization is the general problem of finding a value of a vector of variables \u03b8 that maximizes (or minimizes) some scalar criterion U(\u03b8). The set of all possible values of the vector \u03b8 is called the optimization domain. The elements of \u03b8 can be discrete or continuous variables. In the first case the optimization domain is usually finite, such as in the well-known traveling salesman problem; in",
            "group": 1761,
            "name": "10.1.1.71.7266",
            "keyword": "",
            "title": "Abstract"
        },
        {
            "abstract": "Abstract \u2014 Broadcasting in wireless networks, unlike wired networks, inherently reaches several nodes with a single transmission. For omnidirectional wireless broadcast to a node, all nodes closer will also be reached. This property can be used to compute routing trees which minimize the sum of the transmitter powers. In this paper we present a mixed integer programming formulation and a simulated annealing algorithm for the problem. Extensive experimental results for the heuristic approach are presented. They show that the algorithm we propose is capable of improving the results of state-of-the-art algorithms for most of the problems considered. The solutions provided by the simulated annealing algorithm can be improved by applying a very fast post-optimization procedure. This leads to the best known mean results for the problems considered. I.",
            "group": 1762,
            "name": "10.1.1.71.7732",
            "keyword": "",
            "title": "The minimum power broadcast problem in wireless networks: a simulated annealing approach"
        },
        {
            "abstract": "Large quantities of structure and biological activity data are quickly accumulated with the development of high-throughput screening (HTS) and combinatorial chemistry. Analysis of structure-activity relationships (SAR) from such large data sets is becoming challenging. Recursive partitioning (RP) is a statistical method that can identify SAR rules for classes of compounds that are acting through different mechanisms in the same data set. We use a newly proposed method called Recursive Partitioning and Simulated Annealing (RP/SA) to produce stochastic regression trees for biological activity. In the new algorithm a set of structural descriptors is extracted at each splitting node by using SA as a stochastic optimization tool. For one data set, results show that RP/SA is advantageous in analyzing the SAR information.",
            "group": 1763,
            "name": "10.1.1.71.7962",
            "keyword": "",
            "title": "Analysis of a Large Structure/Biological Activity Data Set Using Recursive Partitioning and Simulated Annealing"
        },
        {
            "abstract": "%P (to be published) This is an invited paper to a special issue of the Polish Journal Control and Cybernetics on \u201cSimulated",
            "group": 1764,
            "name": "10.1.1.71.8583",
            "keyword": "",
            "title": "Adaptive simulated annealing (ASA): Lessons learned"
        },
        {
            "abstract": "Tagging graphical objects with text labels is a fundamental task in the design of many types of informational graphics. This problem is seen in its most essential form in the field of cartography, where text labels must be placed on maps while avoiding overlaps with cartographic symbols and other labels, though it also arises frequently in the production of other graphics (e.g., scatterplots). Although several techniques have been",
            "group": 1765,
            "name": "10.1.1.72.21",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The powerful techniques of modern nonlinear statistical mechanics are used to compare battalion-scale combat computer models (including simulations and wargames) to exercise data. This is necessary if large-scale combat computer models are to be extrapolated with confidence to develop battle-management, C 3 and procurement decision-aids, and to improve training. This modeling approach to battalion-level missions is amenable to reasonable algebraic and/or heuristic approximations to drive higher-echelon computer models. Each data set is fit to several candidate short-time probability distributions, using methods of \u2018\u2018very fast simulated re-annealing\u2019 \u2019 with a Lagrangian (time-dependent algebraic cost-function) derived from nonlinear stochastic rate equations. These candidate mathematical models are further tested by using path-integral numerical techniques we have dev eloped to calculate long-time probability distributions spanning the combat scenario. We hav e demonstrated proofs of principle, that battalion-level combat exercises can be well represented by the computer simulation JANUS(T), and that modern methods of nonlinear nonequilibrium statistical mechanics can well model these systems. Since only relatively simple drifts and diffusions were required, in larger systems, e.g., at brigade and division levels, it might be possible to \u2018\u2018absorb\u2019 \u2019 other important variables (C 3,human factors, logistics, etc.) into more nonlinear mathematical forms. Otherwise, this battalion-level model should be supplemented with a \u2018\u2018tree\u2019 \u2019 of branches corresponding to estimated values of these variables.",
            "group": 1766,
            "name": "10.1.1.72.228",
            "keyword": "",
            "title": "Mathematical comparison of combat computer models to exercise data"
        },
        {
            "abstract": "A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Pointfeature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We propose two new methods, one based on a discrete form of gradient descent, the other on simulated annealing, and report on a series of empirical tests comparing these and the other known algorithms for the problem. Based on this study, the first to be conducted, we identify the best approaches as a function of available computation time.",
            "group": 1767,
            "name": "10.1.1.72.609",
            "keyword": "ACM Transactions on GraphicsVol",
            "title": "An empirical study of algorithms for point-feature label placement"
        },
        {
            "abstract": "The high bandwidth and low latency of the modern internet has made possible the deployment of distributed computing platforms. The XenoServer platform provides a distributed computing platform open to all and presents three major new challenges for resource discovery: Firstly, network location is key for effectively provisioning services, to mitigate against high-latency, high-load or component failure. Secondly, many services require a presence on several servers, with inter-related requirements. Finally, as the platform is open with respect to users and servers, large numbers of queries and updates are expected. To address these requirements we introduce and evaluate Xeno-Search, a new distributed service for selecting the machines to host components of multi-node distributed systems and which is uniquely able to express and efficiently answer complex queries with inter-related location constraints. We demonstrate that Xeno-Search represents a trade-off between accuracy and query time which avoids exhaustive search and supports multiple resources. In addition the performance of the algorithm and the quality of its server selections is investigated and the performance of the distributed service shown to be invariant as the number of nodes or items indexed increases.",
            "group": 1768,
            "name": "10.1.1.72.1077",
            "keyword": "Microsoft Rese",
            "title": "Location based placement of whole distributed systems"
        },
        {
            "abstract": "It is proposed to incorporate \u2018\u2018intuition\u2019 \u2019 into large complex multivariate nonlinear C 3 I systems requiring stochastic or probabilistic treatment, i.e., to seek regions of variable-space where more local analytic resources can be optimally allocated. These mathematical techniques have been utilized for a variety of other systems, ranging from neuroscience, to nuclear physics, to financial markets. The experiences gained by detailing each of these systems offers specific insights by which to approach C 3 I systems. I.",
            "group": 1769,
            "name": "10.1.1.72.2088",
            "keyword": "",
            "title": "Nonlinear nonequilibrium statistical mechanics approach to C 3 systems"
        },
        {
            "abstract": "",
            "group": 1770,
            "name": "10.1.1.72.2360",
            "keyword": "Key wordsEvolutionary programmingGenetic algorithmsOptimizationEvolution strategies",
            "title": ""
        },
        {
            "abstract": "Abstract\u2014We present techniques for generating addresses for memories containing multiple arrays. Because these techniques rely on the inversion or rearrangement of address bits, they are faster and require less hardware to compute than the traditional technique of addition. Use of these techniques can improve performance and cost of application-specific memory subsystems by decreasing effective access time to arrays and by reducing address generation hardware. The primary drawback to this approach is that extra memory space is occasionally required, but in over a million tested cases, this extra memory space is on average only 2 % and no worse than 17.4 % of the utilized memory space. This amount of wasted address space is significantly less than the amount required by the only known similar technique [7] and rarely necessitates the allocation of additional memory components. These techniques provide a foundation for adderfree address generation for manually and automatically generated application-specific memory designs. Index Terms \u2014 Arrays, compilers (silicon), digital arithmetic, high-level synthesis, memory management, number theoretic transforms. I.",
            "group": 1771,
            "name": "10.1.1.72.2445",
            "keyword": "",
            "title": "Address generation for memories containing multiple arrays"
        },
        {
            "abstract": "Simulated annealing (SA) presents an optimization technique with several striking positive and negative features. Perhaps its most salient feature, statistically promising to deliver anoptimal solution, in current practice is often spurned to use instead modified faster algorithms, \u201csimulated quenching \u201d (SQ). Using the author\u2019s Adaptive Simulated Annealing (ASA) code, some examples are given which demonstrate how SQcan be much faster than SA without sacrificing accuracy. Ke ywords: Simulated annealing, random algorithm, optimization technique SA Practice vs Theory-2- Lester Ingber 1.",
            "group": 1772,
            "name": "10.1.1.72.2612",
            "keyword": "",
            "title": "Simulated annealing: Practice versus theory"
        },
        {
            "abstract": "Locating special points of interest, known as landmarks, on X-rays of human heads is a time consuming manual process in the medical field known as cephalometry. We automate this task using the evolutionary computing approach of particle swarm optimisation (PSO). Particularly, we represent several existing programming solutions produced by genetic programming as linear function optimisation tasks. Seven experiments are performed for landmarks with varying detection difficulties. The detection accuracies, required evaluations and detector sizes were compared. Our results show that PSO is up to 14 % more accurate at testing on the hardest landmarks. It also matched the solutions of genetic programming with 43 % to 74 % less training times, and 33 % to 78 % smaller program sizes. It is observed that PSO finds cephalometric landmark detectors with greater success than the existing genetic programming approaches, in both detection accuracy and computational efficiency.",
            "group": 1773,
            "name": "10.1.1.72.3218",
            "keyword": "Contents",
            "title": "LANDMARK DETECTION ON CEPHALOMETRIC X-RAYS USING PARTICLE SWARM OPTIMISATION GAYAN WIJESINGHE"
        },
        {
            "abstract": "Ant Colony Optimisation is a metaheuristic for combinatorial optimisation problems. In this paper we show its successful application to the Vehicle Routing Problem (VRP). First, we introduce VRP and its many variants, such as VRP with Time Windows, Time Dependent VRP, Dynamic VRP, VRP with Pickup and Delivery. These variants have been formulated in order to bring the VRP as close as possible to the kind of situations encountered in real-world distribution processes. Two case studies are presented: the application of Ant Colony Optimisation to the solution of the Time Dependent VRP, where the travel times depend on the time of the day, and Ant Colony Optimisation for Dynamic VRP, where customers \u2019 orders arrive during the delivery process. Finally, two real-world, industrial-scale applications are presented. The former is an application solving a VRP with Time Windows for a major supermarket chain in Switzerland; the latter is an application solving a VRP with Pickup and Delivery for a leading distribution",
            "group": 1774,
            "name": "10.1.1.72.4393",
            "keyword": "MetaheuristicsAnt Colony OptimisationVehicle Routing Problem",
            "title": "Ant Colony Optimisation for vehicle routing problems: from theory to applications"
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, togenerate prices and derivatives commonly used by option traders. Ke ywords: options; eurodollar; volatility; path integral; optimization; statistical mechanics",
            "group": 1775,
            "name": "10.1.1.72.5660",
            "keyword": "High-resolution path-integral...-2- Lester Ingber",
            "title": "High-resolution path-integral development of financial options"
        },
        {
            "abstract": "Most problems of combinatorial optimization like routing, task allocation, or scheduling belong to the class of NP-complete problems and can be solved efficiently only by heuristics. Both, genetic algorithms and evolutionary strategies have a number of drawbacks that reduce their applicability to that kind of problems. In order to overcome some of these problems, this paper looks upon the standard genetic algorithm as an artificial self organizing process. With the purpose of providing concepts that make the algorithm more open for scalability on the one hand, and that fight premature convergence on the other hand, this paper presents an extension of the standard genetic algorithm that doesn\u2019t introduce any problem specific knowledge. On the basis of an evolutionary strategy like selective pressure some further improvements like the introduction of a concept to handle multiple crossover operators in parallel or the introduction of a concept of segregation and reunification of smaller subpopulations during the evolutionary process are considered. The additional aspects introduced within the scope of that variants of genetic algorithms are inspired from optimization as well as from the views of bionics. In contrast to contributions in the field of genetic algorithms that introduce new coding standards and operators for certain problems, the introduced approach should be considered as a heuristic appliable to multiple problems of combinatorial optimization using exactly the same coding standards and operators for crossover and mutation as done when treating a certain problem with a standard genetic algorithm. In the present paper the new algorithm and some of its variants are discussed for the travelling salesman problem (TSP) as a well documented instance of a multimodal combinatorial optimization problem. 1.",
            "group": 1776,
            "name": "10.1.1.72.5846",
            "keyword": "",
            "title": "Michael Affenzeller * genetic algorithms, evolutionary strategies, selective pressure TRANSFERRING THE CONCEPT OF SELECTIVE PRESSURE FROM EVOLUTIONARY STRATEGIES TO GENETIC ALGORITHMS"
        },
        {
            "abstract": "The compact and harmonious layout of ads and text is a fundamental and costly step in the production of commercial telephone directories (&quot;Yellow Pages&quot;). We formulate a canonical version of Yellow-Pages pagination and layout (YPPL) as an optimization problem in which the task is to position ads and text-stream segments on sequential pages so as to minimize total page length and maximize certain layout aesthetics, subject to cons~aints derived from page-format requirements and positional relations between ads and text. We present a heuristic-search approach to the YPPL problem. Our algorithm has been applied to a sample of real telephone-directory data, and produces solutions that are significantly shorter and better than the published ones. Key Words: directory pagination, page layout, heuristic search, stochastic optimization, simulated annealing 1. I n t r o d u c t i o n The problem of Yellow-Pages (YP) pagination and layout (YPPL) is one of finding a com-pact and harmonious positioning of text and advertisements on the pages of a commercial telephone directory. Figure 1 depicts a sample page from a commercial Yellow Pages directory. We use this figure to define some useful terminology. Regular ads, which can span more than one",
            "group": 1777,
            "name": "10.1.1.72.5901",
            "keyword": "",
            "title": "Harvard University"
        },
        {
            "abstract": "In memory of Mikhail (Misha) Alekhnovich\u2014friend, colleague and brilliant mind Abstract. We analyze the efficiency of the random walk algorithm on random 3-CNF instances and prove linear upper bounds on the running time of this algorithm for small clause density, less than 1.63. This is the first subexponential upper bound on the running time of a local improvement algorithm on random instances. Our proof introduces a simple, yet powerful tool for analyzing such algorithms, which may be of further use. This object, called a terminator, is a weighted satisfying assignment. We show that any CNF having a good (small weight) terminator is assured to be solved quickly by the random walk algorithm. This raises the natural question of the terminator threshold which is the maximal clause density for which such assignments exist (with high probability). We use the analysis of the pure literal heuristic presented by Broder, Frieze, and Upfal [Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, 1993, pp. 322\u2013330] and Luby, Mitzenmacher, and Shokrollahi [Proceedings of the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, 1998, pp. 364\u2013373] and show that for small clause densities good terminators exist. Thus we show that the pure literal threshold (\u22481.63) is a lower bound on the terminator threshold. (We conjecture the terminator threshold to be in fact higher.) One nice property of terminators is that they can be found efficiently via linear programming. This makes tractable the future investigation of the terminator threshold and also provides an efficiently computable certificate for short running time of the simple random walk heuristic.",
            "group": 1778,
            "name": "10.1.1.72.5984",
            "keyword": "Key words. SAT solvingrandom CNFSAT heuristicsrandom walk algorithm",
            "title": "Linear upper bounds for random walk on small density random 3-cnf"
        },
        {
            "abstract": "Abstract This paper presents a simulated annealing search procedure developed to solve job shop scheduling problems simultaneously subject to tardiness and inventory costs. The procedure is shown to significantly increase schedule quality compared to multiple combinations of dispatch rules and release policies, though at the expense of intense computational efforts. A meta-heuristic procedure is developed that aims at increasing the efficiency of simulated annealing by dynamically inflating the costs associated with major inefficiencies in the current solution. Three different variations of this procedure are considered. One of these variations is shown to yield significant reductions in computation time, especially on problems where search is more likely to get trapped in local minima. We analyze why this variation of the meta-heuristic is more effective than the others. 1",
            "group": 1779,
            "name": "10.1.1.72.7717",
            "keyword": "",
            "title": "SCHEDULING"
        },
        {
            "abstract": "A step toward computer-assisted mammography using evolutionary programming and neural networks",
            "group": 1780,
            "name": "10.1.1.72.7952",
            "keyword": "Breast cancerComputer-assisted diagnosisArtificial neural networksEvolutionary computation. t\u2019volutionary",
            "title": ""
        },
        {
            "abstract": "Abstract \u2014 Regular structures are present in many types of circuits. If this structure can be identified and utilized, performance can be improved dramatically. In this paper, we present a novel placement approach that successfully identifies regularity, and obtains placements that are superior to other \u201cgeneral purpose\u201d methods. This method has been integrated into our Feng Shui 2.6 bisection-based placement tool. On experiments with the PEKO benchmarks, our results are within 32 % of optimal for both the large and small suites. The largest example, with 2.1 million cells, can be completed in sixteen hours. The majority of our run time is during detail placement\u2013 global placement takes under three hours. The success of our method shows that it can find structure, even when the structure was not expected or intended. As part of this work, we have made a number of observations related to the nature of suboptimality in placement. These observations have shown that some neglected research areas have great potential, while problems that receive considerable attention are essentially adequately solved. I.",
            "group": 1781,
            "name": "10.1.1.72.8313",
            "keyword": "",
            "title": "On structure and suboptimality in placement"
        },
        {
            "abstract": "",
            "group": 1782,
            "name": "10.1.1.72.8334",
            "keyword": "",
            "title": "1. Introduction Connectionism: Past, Present, and Future"
        },
        {
            "abstract": "In recent years, there has been much interest in phase transitions of combinatorial problems. Phase transitions have been successfully used to analyze combinatorial optimization problems, characterize their typical-case features and locate the hardest problem instances. In this paper, we study phase transitions of the asymmetric Traveling Salesman Problem (ATSP), an NP-hard combinatorial optimization problem that has many real-world applications. Using random instances of up to 1,500 cities in which intercity distances are uniformly distributed, we empirically show that many properties of the problem, including the optimal tour cost and backbone size, experience sharp transitions as the precision of intercity distances increases across a critical value. Our experimental results on the costs of the ATSP tours and assignment problem agree with the theoretical result that the asymptotic cost of assignment problem is \u00a0\u00a2\u00a1\u00a4\u00a3\u00a6 \u00a5 as the number of cities goes to infinity. In addition, we show that the average computational cost of the well-known branch-and-bound subtour elimination algorithm for the problem also exhibits a thrashing behavior, transitioning from easy to difficult as the distance precision increases. These results answer positively an open question regarding the existence of phase transitions in the ATSP, and provide guidance on how difficult ATSP problem instances should be generated.",
            "group": 1783,
            "name": "10.1.1.72.8667",
            "keyword": "",
            "title": "Phase transitions and backbones of the asymmetric traveling salesman problem"
        },
        {
            "abstract": "Abstract. Many studies, mostly empirical, have been devoted to finding an optimal shape parameter for radial basis functions (RBF). When exploring the underlying factors that determine what is a good such choice, we are led to consider the Runge phenomenon (RP; best known in case of high order polynomial interpolation) as a key error mechanism. This observation suggests that it can be advantageous to let the shape parameter vary spatially, rather than assigning a single value to it. Benefits typically include improvements in both accuracy and numerical conditioning. Still another benefit arises if one wishes to improve local accuracy by clustering nodes in select areas. This idea is routinely used when working with splines or finite element methods. However, local refinement with RBFs may cause RP-type errors unless we use a spatially variable shape paremeter. With this enhancement, RBF approximations combine freedom from meshes with spectral accuracy on irregular domains, and furthermore permit local node clustering to improve the resolution wherever this might be needed.",
            "group": 1784,
            "name": "10.1.1.72.8819",
            "keyword": "Key words. Radial basis functions",
            "title": "The Runge phenomenon and spatially variable shape parameters"
        },
        {
            "abstract": "Searching spatial configurations is a particular case of maximal constraint satisfaction prob-lems, where constraints expressed by spatial and non-spatial properties guide the search process. In the spatial domain, binary spatial relations are typically used for specifying constraints while searching spatial configurations. Searching configurations is particularly intractable when con-figurations are derived from a combination of objects, which involves a hard combinatorial problem. This paper presents a genetic algorithm that combines a direct and an indirect ap-proach to treating binary constraints in genetic operators. A new genetic operator combines randomness and heuristics for guiding the reproduction of new individuals in a population. In-dividuals are composed of spatial objects whose relationships are indexed by a content measure. This paper describes the genetic algorithm and presents experimental results that compare the genetic versus a deterministic and a local-search algorithm. These experiments show the con-venience of using a genetic algorithm when the complexity of the queries and databases do no guarantee the tractability of a deterministic strategy. Index Terms: Evolutionary computation, genetic algorithm, geographic information systems, constraint satisfaction problems, information retrieval.",
            "group": 1785,
            "name": "10.1.1.72.8896",
            "keyword": "1",
            "title": "A Genetic Algorithm for Searching Spatial Configurations"
        },
        {
            "abstract": "Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard. We describe deterministic annealing (Rose et al., 1990) as an appealing alternative to the Expectation-Maximization algorithm (Dempster et al., 1977). Seeking to avoid search error, DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1",
            "group": 1786,
            "name": "10.1.1.72.8991",
            "keyword": "",
            "title": "2004. Annealing techniques for unsupervised statistical language learning"
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. We use a previous development of a statistical mechanics of financial markets to model these issues. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, togenerate prices and derivatives commonly used by option traders. The results of our study show that there is only a very small change in at-the money option prices for different probability distributions, both for the one-factor and two-factor models. There still are significant differences in risk parameters, partial derivatives, using more sophisticated models, especially for out-of-the-money options. Ke ywords: options; eurodollar; volatility; path integral; optimization; statistical mechanics Statistical Mechanics...-2- LIngber & JK Wilson",
            "group": 1787,
            "name": "10.1.1.72.9504",
            "keyword": "",
            "title": "and"
        },
        {
            "abstract": "Abstract \u2014 The allocation of scarce spectral resources to support as many user applications as possible while maintaining reasonable quality of service is a fundamental problem in wireless communication. We argue that the problem is best formulated in terms of decision theory. We propose a scheme that takes decision-theoretic concerns (like preferences) into account and discuss the difficulties and subtleties involved in applying standard techniques from the theory of Markov Decision Processes (MDPs) in constructing an algorithm that is decision-theoretically optimal. As an example of the proposed framework, we construct such an algorithm under some simplifying assumptions. Additionally, we present analysis and simulation results that show that our algorithm meets its design goals. Finally, we investigate how far from optimal one well-known heuristic is. The main contribution of our results is in providing insight and guidance for the design of near-optimal admission-control policies. I.",
            "group": 1788,
            "name": "10.1.1.72.9570",
            "keyword": "",
            "title": "A decision-theoretic approach to resource allocation in wireless multimedia networks"
        },
        {
            "abstract": "This paper overviews recent work on ant algorithms, that is, algorithms for discrete optimization which took inspiration from the observation of ant colonies foraging behavior, and introduces the ant colony optimization (ACO) meta-heuristic. In the first part of the paper the basic biological findings on real ants are overviewed, and their artificial counterparts as well as the ACO meta-heuristic are defined. In the second part of the paper a number of applications to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO meta-heuristic. 1",
            "group": 1789,
            "name": "10.1.1.72.9591",
            "keyword": "",
            "title": "Ant algorithms for discrete optimization"
        },
        {
            "abstract": "",
            "group": 1790,
            "name": "10.1.1.72.9717",
            "keyword": "",
            "title": "Triangulated Manifolds with Few Vertices . . . "
        },
        {
            "abstract": "by",
            "group": 1791,
            "name": "10.1.1.72.9751",
            "keyword": "",
            "title": "On-line Data Reconstruction in Redundant Disk Arrays"
        },
        {
            "abstract": "Forest treatment planning and scheduling is an important part of forest resource management. It is a complex task requiring expertise and integration of multi-disciplinary fields. In this paper, we outline a real life problem from the ECOPLAN project called the Long Term Forest Treatment Scheduling Problem (LTFTSP). A review of optimization techniques applicable to forest treatment problems in general is presented, and contrasted with our case. The review suggests that long term scheduling is difficult because of the prohibitive size and complexity inherent to the problem. Based on experience from the successful resolution of a simplified problem, we advocate the use of iterative improvement techniques as a solution strategy. Iterative improvement techniques will in general benefit from high quality initial solutions. We show how a Constraint Satisfaction Problem formulation of the LTFTSP can be used to generate initial solutions. A key element to success is the use of a forest simulator for knowledge based definition of variable domains. The initial solution generator will be used as a module in an integrated forest treatment scheduling system which is under development in the ECOPLAN project.",
            "group": 1792,
            "name": "10.1.1.72.9781",
            "keyword": "",
            "title": "G.: Constraint Technology Applied to Forest Treatment Scheduling, in"
        },
        {
            "abstract": "My overall objectives are threefold: (a) to develop new formalisms and methods for artificial intelligence by combining a sound theoretical approach with a principled experimental component, (b) to provide a practical evaluation of the proposed models and relate them to real-world applications and challenges, and (c) to educate the next generation of CS students about the ambitious and challenging goals of artificial intelligence. The focus of my research is on compute-intensive methods for artificial intelligence (AI). Traditionally, general search and reasoning is largely avoided in AI by explicitly incorporating large amounts of domain-specific knowledge. While such a knowledge-intensive approach has been successful in certain domains, as demonstrated by expert system applications, in other areas, such as planning or general reasoning, progress has been disappointing. However, recent advances in general search and reasoning methods combined with faster hardware and better implementations provide strong evidence that a compute-intensive approach is not only suitable for dealing with the combinatorial nature of many AI formalisms, but may also be required to supplement domainspecific knowledge, especially considering the knowledge-acquisition bottleneck in terms of encoding highly specific domain knowledge.",
            "group": 1793,
            "name": "10.1.1.73.540",
            "keyword": "",
            "title": "Computeintensive methods in artificial intelligence"
        },
        {
            "abstract": "In contrast with the current Web search methods that essentially do document-level ranking and retrieval, we are exploring a new paradigm to enable Web search at the object level. We collect Web information for objects relevant for a specific application domain and rank these objects in terms of their relevance and popularity to answer user queries. Traditional PageRank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationships between objects. This paper introduces PopRank, a domain-independent object-level link analysis model to rank the objects within a specific domain. Specifically we assign a popularity propagation factor to each type of object relationship, study how different popularity propagation factors for these heterogeneous relationships could affect the popularity ranking, and propose efficient approaches to automatically decide these factors. Our experiments are done using 1 million CS papers, and the experimental results show that PopRank can achieve significantly better ranking results than naively applying PageRank on the object graph.",
            "group": 1794,
            "name": "10.1.1.73.596",
            "keyword": "Web objectsPageRankPopRankLink analysis",
            "title": "An Implementation and Experimental"
        },
        {
            "abstract": "Domain-independent planning is a hard combinatorial problem. Taking into account plan quality makes the task even more difficult. This article introduces Planning by Rewriting (PbR), a new paradigm for efficient high-quality domain-independent planning. PbR exploits declarative plan-rewriting rules and efficient local search techniques to transform an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition to addressing the issues of planning efficiency and plan quality, this framework offers a new anytime planning algorithm. We have implemented this planner and applied it to several existing domains. The experimental results show that the PbR approach provides significant savings in planning effort while generating high-quality plans. 1.",
            "group": 1795,
            "name": "10.1.1.73.609",
            "keyword": "",
            "title": "Planning by rewriting"
        },
        {
            "abstract": "This highly interdisciplinary project extends previous work in combat modeling and in control-theoretic descriptions of decision-making human factors in complex activities. A previous paper has established the first theory of the statistical mechanics of combat (SMC), developed using modern methods of statistical mechanics, baselined to empirical data gleaned from the National Training Center (NTC). This previous project has also established aJANUS(T)-NTC computer simulation/wargame of NTC, providing a statistical \u2018\u2018what-if \u2019\u2019 capability for NTC scenarios. This mathematical formulation is ripe for control-theoretic extension to include human factors, a methodology previously developed in the context of teleoperated vehicles. Similar NTC scenarios differing at crucial decision points will be used for data to model the influence of decision making on combat. The results may then be used to improve present human factors and C 2 algorithms in computer simulations/wargames. Our approach is to \u2018\u2018subordinate\u2019 \u2019 the SMC nonlinear stochastic equations, fitted to NTC scenarios, to establish the zeroth order description of that combat. In practice, an equivalent mathematical-physics representation is used, more suitable for numerical and formal work, i.e., a Lagrangian representation. Theoretically, these equations are nested within a larger set of nonlinear stochastic operator-equations which include C 3 human factors, e.g., supervisory decisions. In this study, wepropose to perturb this operator theory about the SMC zeroth order set of equations. Then, subsets of scenarios fit to zeroth order, originally considered to be similarly degenerate, can be further split perturbatively to distinguish C 3 decision-making influences. New methods of Very Fast Simulated Re-Annealing (VFSR), developed in the previous project, will be used for fitting these models to empirical data.",
            "group": 1796,
            "name": "10.1.1.73.1009",
            "keyword": "Human Factors in Combat-2- Ingber &Sworder",
            "title": "and"
        },
        {
            "abstract": "Abstract: Modelling the technological processes used in operator training simulators requires specific approaches. Unlike simulation of control loops where a model representing a part of the controlled process needs to be as exact as possible, training simulators do not require very exact modelling. What is required is to give operators the feeling that they are working with a real process. This can even be achieved by simple approximate models that reflect changes of some measured variables in trends or steady state values that are very close to reality. Results from evolutionary computing methods can be used to establish such a virtual reality. In these methods, real process data provided by a process data acquisition system is used to obtain optimised model parameters, values of which cannot be obtained by other conventional identification methods. The proposed procedure has been developed, while creating a virtual model of a thermal power plant for operator training. Key words: evolutionary computing, genetic algorithm, simulated annealing, optimisation techniques, thermal power plant training simulator 1",
            "group": 1797,
            "name": "10.1.1.73.1300",
            "keyword": "",
            "title": "2002a): Evolutionary Computing Methods for Optimising Virtual Reality Process Models, (submitted to"
        },
        {
            "abstract": "Abstract\u2014A paradigm of statistical mechanics of financial markets (SMFM) using nonlinear nonequilibrium algorithms, first published in L. Ingber, Mathematical Modelling, 5, 343-361 (1984), is fit to multivariate financial markets using Adaptive Simulated Annealing (ASA), a global optimization algorithm, to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta are thereby derived and used as technical indicators in a recursive ASA optimization process to tune trading rules. These trading rules are then used on out-of-sample data, to demonstrate that they can profit from the SMFM model, to illustrate that these markets are likely not efficient.",
            "group": 1798,
            "name": "10.1.1.73.3064",
            "keyword": "EconomicsSimulated AnnealingStatistical Mechanics Nonlinear Nonequilibrium Financial Markets- 2- Lester Ingber",
            "title": "Statistical mechanics of nonlinear nonequilibrium financial markets"
        },
        {
            "abstract": "  Currents flowing in the power and ground (P&G) lines of CMOS digital circuits affect both circuit reliability and performance bycausing excessive voltage drops. Maximum current estimates are therefore needed in the P&G lines to determine the severity of the voltage drop problems and to properly design the supply lines to eliminate these problems. These currents, however, depend on the specific input patterns that are applied to the circuit. Since it is prohibitively expensive to enumerate all possible inputs, this problem has, for a long time, remained largely unsolved. In [1], we proposed a pattern-independent, linear time algorithm (iMax) that estimates an upper bound envelope ofallpossible current waveforms that result from the application of different input patterns to the circuit. While the bound produced by iMax is fairly tight on many circuits, there can be a signi cant loss in accuracy due to correlations between signals internal to the circuit. In this paper, we present a new partial input enumeration (PIE) algorithm to resolve these correlations and significantly improve the upper bound (in one case, reducing the error by 64 % on a circuit with about 1,700 gates). We also show good speedperformance, analyzing circuits with more than 20,000 gates in about 2 hours on a SUN ELC. We demonstrate with extensive experimental results that the algorithm represents a good time-accuracy trade-off and is applicable to large VLSI circuits.  ",
            "group": 1799,
            "name": "10.1.1.73.3418",
            "keyword": "",
            "title": "Resolving Signal Correlations for Estimating Maximum Currents in CMOS Combinational Circuits"
        },
        {
            "abstract": "Abstract. This contribution shows how unsupervised Markovian segmentation techniques can be accelerated when implemented on graphics hardware equipped with a Graphics Processing Unit (GPU). Our strategy exploits the intrinsic properties of local interactions between sites of a Markov Random Field model with the parallel computation ability of a GPU. This paper explains how classical iterative site-wise-update algorithms commonly used to optimize global Markovian cost functions can be efficiently implemented in parallel by fragment shaders driven by a fragment processor. This parallel programming strategy significantly accelerates optimization algorithms such as ICM and simulated annealing. Good acceleration are also achieved for parameter estimation procedures such as K-means and ICE. The experiments reported in this paper have been obtained with a mid-end, affordable graphics card available on the market. 1",
            "group": 1800,
            "name": "10.1.1.73.3767",
            "keyword": "",
            "title": "Unsupervised markovian segmentation on graphics hardware"
        },
        {
            "abstract": "One of the greatest challenges of biophysics is the prediction of molecular structures. Biomolecules, especially proteins and nucleic acids, are in the focus of numerous efforts, because three dimensional structures of these molecules are essential for their function. The follow up of genome sequencing, often called proteomics, requires simultaneous determination of the structures of thousands of biopolymers. Despite substantial progresses in structural analysis of biopolymers by X-ray crystallography and NMR spectroscopy three dimensional structure determination is often time consuming or expensive at the current state of the art. Therefore determination of three dimensional structures based on theoretical models gained in importance. Although the main research focus is still on proteins, three dimensional investigations on nucleic acids increased due to the discovery of new functions and aspects of RNA. RNA is also challenging from a theoretical point of view, since secondary structures not only can be calculated by efficient algorithms,",
            "group": 1801,
            "name": "10.1.1.73.3810",
            "keyword": "",
            "title": "zur Erlangung des akademischen Grades"
        },
        {
            "abstract": "Abstract\u2014This paper provides a review of both Rent\u2019s rule and the placement models derived from it. It is proposed that the power-law form of Rent\u2019s rule, which predicts the number of terminals required by a group of gates for communication with the rest of the circuit, is a consequence of a statistically homogeneous circuit topology and gate placement. The term \u201chomogeneous \u201d is used to imply that quantities such as the average wire length per gate and the average number of terminals per gate are independent of the position within the circuit. Rent\u2019s rule is used to derive a variety of net length distribution models and the approach adopted in this paper is to factor the distribution function into the product of an occupancy probability distribution and a function which represents the number of valid net placement sites. This approach places diverse placement models under a common framework and allows the errors introduced by the modeling process to be isolated and evaluated. Models for both planar and hierarchical gate placement are presented.",
            "group": 1802,
            "name": "10.1.1.73.4223",
            "keyword": "analysisclusteringcritical-pathgate-array",
            "title": "IEEE TRANSACTIONS ON VLSI SYSTEMS 1 The Interpretation and Application of Rent\u2019s Rule"
        },
        {
            "abstract": "First of all I want to mention Peter Schuster for giving me the chance to join his group and his tremendous support. It always has been a pleasure for me to discuss with him. Ivo Hofacker the great hacker: \u2283 assisted with his computational knowledge and debugged with me endless times. Peter Stadler supported me with all his tremendous knowledge and wisdom. Herbert Kratky was of great help during the work and supported me in computational as well in theoretical work. A lot of the data here achieved was a collaboration with him. Christian Haslinger and G\u00fcnther Weberndorfer assisted by the construction of the genetic algorithm. Now to more personal wishes: Thank you Dali for your patience in this last year of my study. I wish to thank my parents for their support and appreciation during my whole study.",
            "group": 1803,
            "name": "10.1.1.73.4475",
            "keyword": "",
            "title": "an der Formal- und Naturwissenschaftlichen"
        },
        {
            "abstract": "Abstract. Many problems of combinatorial optimization belong to the class of NP-complete problems and can be solved efficiently only by heuristics. Both, Genetic Algorithms and Evolution Strategies have a number of drawbacks that reduce their applicability to that kind of problems. During the last decades plenty of work has been investigated in order to introduce new coding standards and operators especially for Genetic Algorithms. All these approaches have one thing in common: They are very problem specific and mostly they do not challenge the basic principle of Genetic Algorithms. In the present paper we take a different approach and look upon the concepts of a Standard Genetic Algorithm (SGA) as an artificial self organizing process in order to overcome some of the fundamental problems Genetic Algorithms are concerned with in almost all areas of application. With the purpose of providing concepts which make the algorithm more open for scalability on the one hand, and which fight premature convergence on the other hand, this paper presents an extension of the Standard Genetic Algorithm that does not introduce any problem specific knowledge. On the basis of an Evolution-Strategy-like selective pressure handling some further improvements like the introduction of a concept of segregation and reunification of smaller subpopulations during the evolutionary process are considered. The additional aspects introduced within the scope of that",
            "group": 1804,
            "name": "10.1.1.73.4727",
            "keyword": "",
            "title": "Received: date Accepted in the final form: date"
        },
        {
            "abstract": "In memory of Mikhail (Misha) Alekhnovich\u2014friend, colleague and brilliant mind Abstract. We analyze the efficiency of the random walk algorithm on random 3-CNF instances and prove linear upper bounds on the running time of this algorithm for small clause density, less than 1.63. This is the first subexponential upper bound on the running time of a local improvement algorithm on random instances. Our proof introduces a simple, yet powerful tool for analyzing such algorithms, which may be of further use. This object, called a terminator, is a weighted satisfying assignment. We show that any CNF having a good (small weight) terminator is assured to be solved quickly by the random walk algorithm. This raises the natural question of the terminator threshold which is the maximal clause density for which such assignments exist (with high probability). We use the analysis of the pure literal heuristic presented by Broder, Frieze, and Upfal [Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, 1993, pp. 322\u2013330] and Luby, Mitzenmacher, and Shokrollahi [Proceedings of the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, 1998, pp. 364\u2013373] and show that for small clause densities good terminators exist. Thus we show that the pure literal threshold (\u22481.63) is a lower bound on the terminator threshold. (We conjecture the terminator threshold to be in fact higher.) One nice property of terminators is that they can be found efficiently via linear programming. This makes tractable the future investigation of the terminator threshold and also provides an efficiently computable certificate for short running time of the simple random walk heuristic.",
            "group": 1805,
            "name": "10.1.1.73.5814",
            "keyword": "Key words. SAT solvingrandom CNFSAT heuristicsrandom walk algorithm",
            "title": "Linear upper bounds for random walk on small density random 3-cnf"
        },
        {
            "abstract": "The Commonality-Based Crossover Framework has been presented as a general model for designing problem specific operators. Following this model, the Common Features/Random Sample Climbing operator has been developed for feature subset selection--a binary string optimization problem. Although this problem should be an ideal application for genetic algorithms with standard crossover operators, experiments show that the new operator can find better feature subsets for classifier training. 1",
            "group": 1806,
            "name": "10.1.1.73.5908",
            "keyword": "",
            "title": "Non-Standard Crossover for a Standard Representation"
        },
        {
            "abstract": "",
            "group": 1807,
            "name": "10.1.1.73.6822",
            "keyword": "",
            "title": "Fuzzy hierarchies"
        },
        {
            "abstract": "The use of antennas for vehicle applications is growing very rapidly due to the development of modern wireless communication technology and service. The need for a computational tool to design and optimize new automobile antennas more simply and easily has been increasing. Currently, an automobile antenna design using the Simple Genetic Algorithm (SGA) has been introduced. In this model, the SGA computation tool attempts to obtain the best design based on a single cost function. The automobile antenna design is a multi-objective problem. The different objectives are combined into a single cost function, each with a weight value. The results of the optimization procedure depend strongly on these weights, and thus, the designer must properly choose each weight value to get the desired optimum. Also, all the weight values must then be changed, and the entire optimization procedure must be repeated whenever the designer wants to change any single objective goal. In addition, a single optimum solution obtained by the SGA can be unrealizable due to various limitations. Present SGA research has focused on antennas",
            "group": 1808,
            "name": "10.1.1.73.6980",
            "keyword": "",
            "title": "Copyright by"
        },
        {
            "abstract": "",
            "group": 1809,
            "name": "10.1.1.73.7181",
            "keyword": "Key wordsBandwidth MinimizationHeuristicsSimulated Annealing",
            "title": "Eduardo Rodriguez-Tello a, \u2217,Jin-KaoHao a"
        },
        {
            "abstract": "Since the beginning of AI, mind games have been studied as relevant application fields. Nowadays, some programs are better than human players in most classical games. Their results highlight the efficiency of AI methods that are now quite standard. Such methods are very useful to Go programs, but they do not enable a strong Go program to be built. The problems related to Computer Go require new AI problem solving methods. Given the great number of problems and the diversity of possible solutions, Computer Go is an attractive research domain for AI. Prospective methods of programming the game of Go will probably be of interest in other domains as well. The goal of this paper is to present Computer Go by showing the links between existing studies on Computer Go and different AI related domains: evaluation function, heuristic search, machine learning, automatic knowledge generation, mathematical morphology and cognitive science. In addition, this paper describes both the practical aspects of Go programming, such as program optimization, and various theoretical aspects such as combinatorial game theory, mathematical morphology, and Monte-Carlo methods. 1.",
            "group": 1810,
            "name": "10.1.1.73.7645",
            "keyword": "",
            "title": "Computer Go: An AI oriented survey"
        },
        {
            "abstract": "Abstract- We explore a new general-purpose heuristic for finding high-quality solutions to hard optimization problems. The method, called extremal optimization, is inspired by \u201cself-organized criticality\u201d, a concept introduced to describe emergent complexity in physical systems. In contrast to genetic algorithms, which operate on an entire \u201cgene-pool \u201d of possible solutions, extremal optimization successively replaces extremely undesirable elements of a single sub-optimal solution with new, random ones. Large fluctuations, or \u201cavalanches\u201d, ensue that efficiently explore many local optima. Drawing upon models used to simulate far-from-equilibrium dynamics, extremal optimization complements heuristics inspired by equilibrium statistical physics, such as simulated annealing. With only one adjustable parameter, its performance",
            "group": 1811,
            "name": "10.1.1.73.8289",
            "keyword": "",
            "title": "Combining local search with co-evolution in a remarkably simple way"
        },
        {
            "abstract": "The world is covered with millions of webcams, many transmit everything in their field of view over the Internet 24 hours a day. A web search finds public webcams in airports, intersections, classrooms, parks, shops, ski resorts, and more. Even more private surveillance cameras cover many private and public facilities. Webcams are an endless resource, but most of the video broadcast will be of little interest due to lack of activity. We propose to generate a short video that will be a synopsis of an endless video streams, generated by webcams or surveillance cameras. We would like to address queries like \u201cI would like to watch in one minute the highlights of this camera broadcast during the past day\u201d. The process includes two major phases: (i) An online conversion of the video stream into a database of objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user\u2019s query. To include maximum information in a short synopsis we simultaneously show activities that may have happened at different times. The synopsis video can also be used as an index into the original video stream.",
            "group": 1812,
            "name": "10.1.1.73.9394",
            "keyword": "",
            "title": "Webcam synopsis: Peeking around the world"
        },
        {
            "abstract": "In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles.",
            "group": 1813,
            "name": "10.1.1.74.671",
            "keyword": "",
            "title": "Tutorial Tutorial on maximum likelihood estimation"
        },
        {
            "abstract": "The demand for high-speed FPGA compilation tools has occurred for three reasons: first, as FPGA device capacity has grown, the computation time devoted to placement and routing has grown more dramatically than the compute power of the available computers. Second, there exists a subset of users who are willing to accept a reduction in the quality 1 of result in exchange for a highspeed compilation. Third, high-speed compile has been a longstanding desire of users of FPGA-based custom computing machines, since their compile time requirements are ideally closer to those of regular computers. This paper focuses on the placement phase of the compile process, and presents an ultra-fast placement algorithm targeted to FPGAs. The algorithm is based on a combination of multiple-level, bottomup clustering and hierarchical simulated annealing. It provides superior area results over a known high-quality placement tool on a set of large benchmark circuits, when both are restricted to a short run time. For example, it can generate a placement for a 100,000gate circuit in 10 seconds on a 300 MHz Sun UltraSPARC workstation that is only 33 % worse than a high-quality placement that takes 524 seconds using a pure simulated annealing implementation. In addition, operating in its fastest mode, this tool can provide an accurate estimate of the wirelength achievable with good quality placement. This can be used, in conjunction with a routing predictor, to very quickly determine the routability of a given circuit on a given FPGA device. 1.",
            "group": 1814,
            "name": "10.1.1.74.865",
            "keyword": "",
            "title": "Abstract Trading Quality for Compile Time: Ultra-Fast Placement for FPGAs"
        },
        {
            "abstract": "As multiobjective optimization problems have many solutions, evolutionary algorithms have been widely used for complex multiobjective problems instead of simulated annealing. However, simulated annealing also has favorable characteristics in the multimodal search. We developed several simulated annealing schemes for the multiobjective optimization based on this fact. Simulated annealing and evolutionary algorithms are compared in multiobjective NK model. The preliminary results of the simulated annealing developed show that simulated annealing method performs well and sometimes better than evolutionary algorithms. More systematical analyses to the various problems are discussed as further researches.",
            "group": 1815,
            "name": "10.1.1.74.2194",
            "keyword": "Multiobjective OptimizationEvolutio- nary AlgorithmsSimulated AnnealingPareto OptimalityNK",
            "title": "Multiobjective Simulated Annealing: A Comparative Study to Evolutionary Algorithms"
        },
        {
            "abstract": "Airspace block organization with metaheurisitics and partitioning packages Abstract\u2014In this paper, different metaheuristics applied on an air traffic control problem. This problem is a graph partitioning problem. It can be solved by classical methods which are spectral and multilevel methods. State-of-the-art public-domain graph partitioning packages, CHACO and METIS are used to resolve it. A comparison between results return by these packages and metaheuristics implementations is made for different objective functions of the literature. Metaheuristics used are simulated annealing, ant colony and a new one called fusion fission developed in the LOG laboratory. Experimental results show that metaheuristics find better results than classical packages. I.",
            "group": 1816,
            "name": "10.1.1.74.2481",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "A detailed study is presented on the combinatorial optimization problem of allocating parallel tasks to a parallel computer. Depending on two application/machine-specific parameters, both a sequential and a parallel optimal allocation phase are shown to exist. A sudden \u201cphase \u201d transition is observed if one of these parameters is varied. Simulated annealing is used to find the optimal allocations, which is justified by the self-similar structure of the task allocation energy landscape. It is shown that the difficulty of finding optimal allocations behaves anomalously near the transition, analogous to critical slowing down of simulated equilibration at second-order phase",
            "group": 1817,
            "name": "10.1.1.74.2572",
            "keyword": "Key Wordscritical behaviorcombinatorial optimizationparallel task allocationlandscapes",
            "title": "On the Complexity of Task Allocation"
        },
        {
            "abstract": "The emergence in the past years of Bayesian analysis in many methodological and applied fields as the solution to the modeling of complex problems cannot be dissociated from major changes in its computational implementation. We show in this review how the advances in both Bayesian analysis and statistical computation are intermingled.",
            "group": 1818,
            "name": "10.1.1.74.2897",
            "keyword": "Monte Carlo methodsimportance samplingMCMC algorithms",
            "title": "Computational advances for and from Bayesian analysis"
        },
        {
            "abstract": "Ab initio protein structure prediction: global optimization versus divide and conquer After almost four decades of intense research, the knights of computational structural biology",
            "group": 1819,
            "name": "10.1.1.74.3307",
            "keyword": "",
            "title": "A novel approach to decoy set generation: Designing a physical energy function having local minima with native structure characteristics"
        },
        {
            "abstract": "Abstract Previous research on cluster-based servers has focused onhomogeneous systems. However, real-life clusters are almost invariably heterogeneous in terms of the performance,capacity, and power consumption of their hardware components. In this paper, we describe a self-configuring Webserver for a heterogeneous cluster. The self-configuration is guided by analytical models of throughput and powerconsumption. Our experimental results for a cluster comprised of traditional and blade nodes show that the model-based server can consume 29 % less energy than an energyoblivious server, with only a negligible loss in throughput.The results also show that our server conserves more than twice as much energy as an energy-conscious server that wepreviously proposed for homogeneous clusters.",
            "group": 1820,
            "name": "10.1.1.74.3759",
            "keyword": "",
            "title": "Self-Configuring Heterogeneous Server Clusters \\Lambda"
        },
        {
            "abstract": "FDA (the Factorized Distribution Algorithm) is an evolutionary algorithm that combines mutation and recombination by using a distribution. The distribution is estimated from a set of selected points. It is then used to generate new points for the next generation. FDA uses a factorization to be able to compute the distribution in polynomial time. Previously, we have shown a convergence theorem for FDA. But it is only valid using Boltzmann selection. Boltzmann selection was not used in practice because a good annealing schedule was lacking. Using a Taylor expansion of the average fitness of the Boltzmann distribution, we have developed an adaptive annealing schedule called SDS. The inverse temperature \u00ac is changed inversely proportional to the standard deviation. In this work, we compare the resulting scheme to truncation selection both theoretically and experimentally with a series of test functions. We find that it behaves similar in terms of complexity, robustness and efficiency.",
            "group": 1821,
            "name": "10.1.1.74.5491",
            "keyword": "genetic algorithmsBoltzmann distributionBoltzmann selectiontruncation selection",
            "title": "Comparing the adaptive Boltzmann selection schedule SDS to truncation selection"
        },
        {
            "abstract": "Recent research has found that operators frequently misconfigure Internet services, causing various availability and performance problems. In this paper, we propose a software infrastructure that eliminates several types of misconfiguration by automating the generation of configuration files in Internet services, even as the services evolve. The infrastructure comprises a custom scripting language, configuration file templates, communicating runtime monitors, and heuristic algorithms to detect dependencies between configuration parameters and select ideal configurations. To demonstrate our infrastructure experimentally, we apply it to a realistic online auction service. Our results show that the infrastructure can simplify operation significantly while eliminating 58 % of the misconfigurations found in a previous study of the same service. Furthermore, our results show that the infrastructure can efficiently determine the configuration parameters that lead to high performance as the service evolves through a hardware upgrade and the scheduled maintenance of a few nodes.  ",
            "group": 1822,
            "name": "10.1.1.74.5926",
            "keyword": "manageabilityoperator mistakesInternet services",
            "title": " Automatic Configuration of Internet Services"
        },
        {
            "abstract": "",
            "group": 1823,
            "name": "10.1.1.74.6168",
            "keyword": "",
            "title": "Content-Based Retrieval using Heuristic Search"
        },
        {
            "abstract": "Automatic graphic object layout methods have long been studied in many application areas in which graphic objects should be laid out to satisfy the constraints specific to each application. In those areas, carefully designed layout algorithms should be used to satisfy each application\u2019s constraints. However, those algorithms tend to be complicated and not reusable for other applications. Moreover, it is difficult to add each user\u2019s preferences to the layout scheme of the algorithm. To overcome these difficulties, we developed a general-purpose interactive graphic layout system GALAPAGOS based on genetic algorithms. GALAPAGOS is general-purpose because graphic objects are laid out not by specifying how to lay them out, but just by specifying the preferences for the layout. GALAPAGOS can not only lay out complicated graphs automatically, but also allow users to modify the constraints at run time so that users can tell the system their own preferences. 1",
            "group": 1824,
            "name": "10.1.1.74.7682",
            "keyword": "",
            "title": "Graphic Object Layout with Interactive Genetic Algorithms"
        },
        {
            "abstract": "Network experimentation of many types require the ability to map virtual resources requested by an experimenter onto available physical resources. These resources include hosts, switches, and the links that connect them. Experimenter requests, such as nodes with special hardware or software, must be satisfied, and bottleneck links and other scarce resources in the physical topology must be conserved. In the face of these constraints, this mapping becomes an NP-hard problem. Yet, in order to prevent mapping time from becoming a serious hinderance to such experimentation, this process cannot consume an excessive amount of time. In this paper, we explore this problem, which we call the network testbed mapping problem. We describe the interesting challenges that characterize this problem, and explore its application to other spaces, such as distributed simulation. We present the design, implementation, and evaluation of a solver for this problem, which is currently in use on the Netbed network testbed. It builds on simulated annealing to find very good solutions in a few seconds for our historical workload, and scales gracefully on large well-connected synthetic topologies. 1",
            "group": 1825,
            "name": "10.1.1.74.8082",
            "keyword": "",
            "title": "A Solver for the Network Testbed Mapping Problem"
        },
        {
            "abstract": "The frontpage shows cross-sections of the landscape of the task allocation problem. Its shape is clearly influenced by of \u00a4 variation.",
            "group": 1826,
            "name": "10.1.1.74.9098",
            "keyword": "1",
            "title": "Contents"
        },
        {
            "abstract": "This paper analyzes the application of large force/torques by robotic systems with limited force/torque actuators. It is shown that such system may be able to apply a force/torque in some configurations only; therefore its useful force workspace is limited. To improve the force capabilities of a system, base mobility and/or redundancy can be employed. A planning algorithm is proposed which results in proper base positioning relative to a large-force quasi-static task. To plan redundant manipulator postures during large force-tasks, a new method based on a min-max optimization scheme is developed. Unlike norm-based methods, this method guarantees that no actuator capabilities are exceeded, and that the force/torque of the most loaded joint is minimized. Examples that demonstrate the validity and usefulness of the proposed methods are included. I.",
            "group": 1827,
            "name": "10.1.1.74.9812",
            "keyword": "",
            "title": "Large Force-task Planning for Mobile and Redundant Robots"
        },
        {
            "abstract": "committee and by majorityvote has been found to be satisfactory.",
            "group": 1828,
            "name": "10.1.1.75.554",
            "keyword": "",
            "title": "Approved for the Major Department"
        },
        {
            "abstract": "Genetic algorithms have been used for neural networks in twomain ways: to optimize the network architecture and to train the weights of a xed architecture. While most previous work focuses on only one of these two options, this paper investigates an alternative evolutionary approach called Breeder Genetic Programming (BGP) in which the architecture and the weights are optimized simultaneously. The genotype of each network is represented as a tree whose depth and width are dynamically adapted to the particular application by speci cally de ned genetic operators. The weights are trained by a next-ascent hillclimbing search. A new tness function is proposed that quanti es the principle of Occam's razor. It makes an optimal trade-o between the error tting ability and the parsimony of the network. Simulation results on two benchmark problems of di ering complexity suggest that the method nds minimal size networks on clean data. The experiments on noisy data show that using Occam's razor not only improves the generalization performance, it also accelerates the convergence speed of evolution.",
            "group": 1829,
            "name": "10.1.1.75.3160",
            "keyword": "",
            "title": "Evolving optimal neural networks using genetic algorithms with Occam's razor"
        },
        {
            "abstract": "Most current AI technology has been based on propositionally represented theoretical knowledge. We argue that if AI is to accomplish its goals, especially in the tasks of sensory interpretation and sensorimotor coordination, then it must solve the problem of representing embodied practical knowledge. Biological evidence shows that animals use this knowledge in a way very different from digital computation. This suggests that if these problems are to be solved, then we will need a new breed of computers, which we call field computers. Examples of field computers are: neurocomputers, optical computers, molecular computers, and any kind of massively parallel analog computer. We claim that the principle characteristic of all these computers is their massive parallelism, but we use this term in a special way. We argue that true massive parallelism comes when the number of processors is so large that it can be considered a continuous quantity. Designing and programming these computers requires a new theory of computation, one version of which is presented in this paper. We describe a universal field computer, that is, a field computer that can emulate any other field computer. It is based on a generalization of Taylor\u2019s theorem to continuous dimensional vector spaces. A number of field computations are illustrated, including several transformations useful in image understanding, and a continuous version of Kosko\u2019s bidirectional associative memory.",
            "group": 1830,
            "name": "10.1.1.75.3882",
            "keyword": "neurocomputersneural networksoptical computersmolecular computersfield computersuniversal field computerassociative memoryparallel processingmassive parallelismparallel distributed processingknowledge representationnonpropositional knowledgethe new AI",
            "title": "Abstract Field Computation and Nonpropositional Knowledge"
        },
        {
            "abstract": "We propose a new evolutionary method of extracting user preferences from examples shown to an automatic graph layout system. Using stochastic methods such as simulated annealing and genetic algorithms, automatic layout systems can find a good layout using an evaluation function which can calculate how good a given layout is. However, the evaluation function is usually not known beforehand, and it might vary from user to user. In our system, users show the system several pairs of good and bad layout examples, and the system infers the evaluation function from the examples using genetic programming technique. After the evaluation function evolves to reflect the preferences of the user, it is used as a general evaluation function for laying out graphs. The same technique can be used for a wide range of adaptive user interface systems.",
            "group": 1831,
            "name": "10.1.1.75.4041",
            "keyword": "Graphic Object LayoutGraph LayoutGenetic AlgorithmsGenetic ProgrammingProgramming by ExampleAdaptive User Interface",
            "title": "Evolutionary Learning of Graph Layout Constraints from Examples"
        },
        {
            "abstract": "Abstract\u2014This paper presents a novel probabilistic approach to hierarchical, exemplar-based shape matching. No feature correspondence is needed among exemplars, just a suitable pairwise similarity measure. The approach uses a template tree to efficiently represent and match the variety of shape exemplars. The tree is generated offline by a bottom-up clustering approach using stochastic optimization. Online matching involves a simultaneous coarse-to-fine approach over the template tree and over the transformation parameters. The main contribution of this paper is a Bayesian model to estimate the a posteriori probability of the object class, after a certain match at a node of the tree. This model takes into account object scale and saliency and allows for a principled setting of the matching thresholds such that unpromising paths in the tree traversal process are eliminated early on. The proposed approach was tested in a variety of application domains. Here, results are presented on one of the more challenging domains: real-time pedestrian detection from a moving vehicle. A significant speed-up is obtained when comparing the proposed probabilistic matching approach with a manually tuned nonprobabilistic variant, both utilizing the same template tree structure. Index Terms\u2014Hierarchical shape matching, chamfer distance, Bayesian models. 1",
            "group": 1832,
            "name": "10.1.1.75.4523",
            "keyword": "",
            "title": "A bayesian, exemplar-based approach to hierarchical shape matching"
        },
        {
            "abstract": "This paper establishes a framework for formal comparisons of several leading optimization algorithms, establishing guidance to practitioners for when to use or not use a particular method. The focus in this paper is five general algorithm forms: random search, simultaneous perturbation stochastic approximation, simulated annealing, evolutionary strategies, and genetic algorithms. We summarize the available theoretical results on rates of convergence for the five algorithm forms and then use the theoretical results to draw some preliminary conclusions on the relative efficiency. Our aim is to sort out some of the competing claims of efficiency and to suggest a structure for comparison that is more general and transferable than the usual problem-specific numerical studies.",
            "group": 1833,
            "name": "10.1.1.75.4614",
            "keyword": "Stochastic optimization",
            "title": "Theoretical Framework for Comparing Several Stochastic Optimization Approaches"
        },
        {
            "abstract": "Abstract \u2014 In this paper, we consider a DVB-H receiver module and investigate effective means to find the most appropriate partition between software and hardware space (FPGA), while taking into account power consumption, hardware area constraints, and the communication between the blocks of the partition. To find the best partition for the considered application we utilize a simplified version of simulated annealing. An ARM processor is used for the software implementations and a Stratix FPGA device for hardware implementations. The software power dissipation is estimated using Sim-Panalyzer, while the hardware power dissipation is estimated using the PowerPlay Power Analyzer. Based on those estimates the annealing methodology provides a DVB-H system partition, which leads to a reduction in power consumption of up to 35% at the cost of up to 1703 FPGA logic elements. Our findings are important as they provide the means for an energy effective practical realization of a DVB-H enabled mobile device.",
            "group": 1834,
            "name": "10.1.1.75.5065",
            "keyword": "HW/SW PartitioningPower ConsumptionSimulated annealing",
            "title": "Power Aware HW/SW Partitioning for a DVB-H Receiver Module"
        },
        {
            "abstract": "We describe the capabilities of and algorithms used in a new FPGA CAD tool, Versatile Place and Route (VPR). In terms of minimizing routing area, VPR outperforms all published FPGA place and route tools to which we can compare. Although the algorithms used are based on previously known approaches, we present several enhancements that improve run-time and quality. We present placement and routing results on a new set of large circuits to allow future benchmark comparisons of FPGA place and route tools on circuit sizes more typical of today\u2019s industrial designs. VPR is capable of targeting a broad range of FPGA architectures, and the source code is publicly available. It and the associated netlist translation / clustering tool VPACK have already been used in a number of research projects worldwide, and should be useful in many areas of FPGA architecture research. 1",
            "group": 1835,
            "name": "10.1.1.75.6153",
            "keyword": "",
            "title": "VPR: A new packing, placement and routing tool for FPGA research"
        },
        {
            "abstract": "Modern embedded systems must increasingly accommodate dynamically changing operating environments, high computational requirements, flexibility (e.g., for the emer-gence of new standards and services), and tight time-to-market windows. Such trends and the ever-increasing design complexity of embedded systems have challenged design-ers to raise the level of abstraction and replace traditional ad-hoc approaches with more efficient synthesis techniques. Additionally, since embedded multiprocessor systems are typically designed as final implementations for dedicated functions, modifications to em-bedded system implementations are rare, and this allows embedded system designers to spend significantly larger amounts of time to optimize the architecture and the employed software. This dissertation presents several system-level synthesis algorithms that employ thorough and hence time-intensive optimization techniques (e.g. evolutionary algorithms) that allow the designer to explore a significantly larger part of the design space. It looks at critical issues that are at the core of the synthesis process \u2014 selecting the architecture, partitioning the functionality over the components of the architecture, and scheduling ac-tivities such that design constraints and optimization objectives are satisfied.",
            "group": 1836,
            "name": "10.1.1.75.7790",
            "keyword": "",
            "title": "ABSTRACT Title of dissertation: System Synthesis for Embedded Multiprocessors"
        },
        {
            "abstract": "deals with applications running on static networks, along with some localisation requirements, but without any motion detection hardware. However, many of these applications require some level of motion detection, if only to notice the cases when a network ceases to be statically located and starts to have moving nodes. As most of the currently used application scenarios rely on the assumption that motion will not happen, if a node does move it will cause significant amounts of damage to any protocols relying on this static assumption e.g. routing, localisation, aggregation, etc. In this paper we look at methods for detecting moving nodes, using only RSSI data, including an anchor-less solution to ensure that we can always detect motion. Our methods are intended to work in co-operation with existing static network localisation algorithms. I.",
            "group": 1837,
            "name": "10.1.1.75.8659",
            "keyword": "",
            "title": "Adumbrate: Motion Detection with Unreliable Range Data"
        },
        {
            "abstract": "Scheduling has been defined as &quot;the art of assigning resources to tasks in order to insure the termination of these tasks in a reasonable amount of time &quot; (1). According to French (2), the general problem is to find a sequence, in which the jobs (e.g., a basic task) pass between the resources (e.g., machines), which is a feasible schedule, and optimal with respect to some performance criterion. Graves (3) introduced a functional classification scheme for scheduling problems. This scheme categorizes problems using the following dimensions: 1. Requirement generation, 2. Processing complexity, 3. Scheduling criteria, 4. Parameter variability, 5. Scheduling environment. Based on requirements generation, a manufacturing shop can be classified as an open shop or a closed shop. An open shop is &quot;build to order&quot;, and no inventory is stocked. In a closed shop the orders are filled from existing inventory. Processing complexity refers to the number of processing steps and workstations associated with the production process. This dimension can be decomposed further as follows:",
            "group": 1838,
            "name": "10.1.1.75.8793",
            "keyword": "",
            "title": "Survey of Job Shop Scheduling Techniques"
        },
        {
            "abstract": "Abstract. Current FPGA placement algorithms estimate the routability of a placement using architecture-specific metrics. The shortcoming of using architecture-specific routability estimates is limited adaptability. A placement algorithm that is targeted to a class of architecturally similar FPGAs may not be easily adapted to other architectures. The subject of this paper is the development of a routability-driven architecture adaptive FPGA placement algorithm called Independence. The core of the Independence algorithm is a simultaneous placeand-route approach that tightly couples a simulated annealing placement algorithm with an architecture adaptive FPGA router (Pathfinder). The results of our experiments demonstrate Independence\u2019s adaptability to island-style FPGAs, a hierarchical FPGA architecture (HSRA), and a coarse-grained reconfigurable architecture (RaPiD). The quality of the placements produced by Independence is within 1.2 % of the quality of VPR\u2019s placements, 17 % better than the placements produced by HSRA\u2019s placer, and within 0.7 % of RaPiD's placer. Further, our results show that Independence produces clearly superior placements on routing-poor island-style FPGA architectures. 1",
            "group": 1839,
            "name": "10.1.1.75.8876",
            "keyword": "",
            "title": "Architecture-Adaptive Routability-Driven Placement for FPGAs"
        },
        {
            "abstract": "Automated design of superscalar processors can provide future system-on-chip (SOC) designers with a key-turn method of generating superscalar processors that are Pareto-optimal in terms of performance, energy consumption, and area for the target application program(s). Unfortunately, current optimization methods are based on time-consuming cycle-accurate simulation, unsuitable for analysis of hundreds of thousands of design options that is required to arrive at Pareto-optimal designs. This dissertation bridges the gap between a large design space of superscalar processors and the inability of cycle-accurate simulation to analyze a large design space, by providing a computationally and conceptually simple analytical method for generating Pareto-optimal superscalar processor designs. The proposed and evaluated analytical method consists of three parts: (1) a method for analytically estimating the performance in terms a cycles-per-instruction (CPI) using the application program statistics and the superscalar processor parameters, (2) a method of analytically estimating various energy consuming activities using the application program statistics and the superscalar processor parameters, and (3) a method of finding the Pareto-",
            "group": 1840,
            "name": "10.1.1.75.9147",
            "keyword": "",
            "title": "AUTOMATED DESIGN OF APPLICATION-SPECIFIC SUPERSCALAR PROCESSORS"
        },
        {
            "abstract": "The FAIM-1 is an ultra-concurrent symbolic multiprocessor which attempts to significantly improve the performance of AI systems. The system includes a language in which concurrent AI application programs can be written, a machine which provides direct hardware support for the language, and a resource allocation mechanism which maps programs onto the machine in order to exploit the program's concurrency in an efficient manner at run-time. The paper provides a brief synopsis of the nature of the language and resource allocation mechanism, but is primarily concerned with the description of the physical architecture of the machine. The architecture is consistent with high performance VLSI implementation and packaging technology, and is easily extended to include arbitrary numbers of processors. I",
            "group": 1841,
            "name": "10.1.1.75.9235",
            "keyword": "",
            "title": "The Architecture of the FAIM-1 Symbolic Multiprocessing System"
        },
        {
            "abstract": "Abstract\u2014Distributed computing systems are increasingly being created as self-organizing collections of many autonomous (human or software) agents cooperating as peers. Peer-to-peer coordination introduces, however, unique and potentially serious challenges. When there is no one \u201cin charge, \u201d dysfunctions can emerge as the collective effect of locally reasonable decisions. In this paper, we consider the dysfunction wherein inefficient resource use oscillations occur due to delayed status information, and describe novel approaches, based on the selective use of misinformation, for dealing with this problem. A model of several servers offering equivalent service to independent clients is presented and studied numerically and analytically; the spreading of misinformation about the queue status is found to dampen oscillations and improve system performance for a wide range of parameters. Index Terms\u2014Emergent dysfunctions, resource oscillations, selective misinformation. I.",
            "group": 1842,
            "name": "10.1.1.75.9846",
            "keyword": "",
            "title": "Handling Emergent Resource Use Oscillations"
        },
        {
            "abstract": "gates on a single integrated circuit, so you have to partition the gates between separate circuits. Let\u2019s assume that you are forced to place exactly n/2 gates each on two integrated circuits. The connections between the gates across the partition are slow, energy consuming, and heat producing, while the cost associated with connections inside an integrated circuit are negligible. So, you want to divide the network of gates such that the cost function C, the number of connections cutting across the partition, is minimized (see Figure 1). Because a million computers will be running almost nonstop for 10 years, removing even one costly connection would be worthwhile. Fortunately, this (simplified) problem can be mapped onto the well-known graph-bipartitioning problem. In this problem, the n gates are the vertices of a graph with edges between two connected gates. Each vertex is a Boolean variable, with state \u201c0 \u201d if placed on the left integrated circuit and state \u201c1 \u201d if placed on the right integrated circuit. Although the graph of connections is fixed, the vertices can be moved so that we may obtain a good partition. Unfortunately, optimizing the equal partition is NP-hard; that is, the computations needed to find the global optimum with certainty for even the cleverest algo-",
            "group": 1843,
            "name": "10.1.1.76.95",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract\u2014In this paper, we define the generalization problem, summarize various approaches in generalization, identify the credit assignment problem, and present the problem and some solutions in measuring generalizability. We discuss anomalies in the ordering of hypotheses in a subdomain when performance is normalized and averaged, and show conditions under which anomalies can be eliminated. To generalize performance across subdomains, we present a measure called probability of win that measures the probability whether one hypothesis is better than another. Finally, we discuss some limitations in using probabilities of win and illustrate their application in finding new parameter values for TimberWolf, a package for VLSI cell placement and routing. Index Terms\u2014Anomalies in generalization, credit assignment problem generalization, machine learning, subdomains, probability of win, VLSI cell placement and routing. 1",
            "group": 1844,
            "name": "10.1.1.76.243",
            "keyword": "",
            "title": "Generalization and generalizability measures"
        },
        {
            "abstract": "Many dynamic-content online services are comprised of multiple interacting components and data partitions distributed across server clusters. Understanding the performance of these services is crucial for efficient system management. This paper presents a profile-driven performance model for cluster-based multi-component online services. Our offline constructed application profiles characterize component resource needs and intercomponent communications. With a given component placement strategy, the application profile can be used to predict system throughput and average response time for the online service. Our model differentiates remote invocations from fast-path calls between co-located components and we measure the network delay caused by blocking inter-component communications. Validation with two J2EE-based online applications show that our model can predict application performance with small errors (less than 13 % for throughput and less than 14% for the average response time). We also explore how this performance model can be used to assist system management functions for multi-component online services, with case examinations on optimized component placement, capacity planning, and cost-effectiveness analysis. 1",
            "group": 1845,
            "name": "10.1.1.76.706",
            "keyword": "",
            "title": "and Implementation (NSDI\u201905). Abstract Performance Modeling and System Management for Multi-component Online Services \u2217"
        },
        {
            "abstract": "RUNWAY SCHEDULE DETERMINATION BY SIMULATION OPTIMIZATION Many airport runway expansion projects are restricted by space limitations imposed by development in the vicinity of the airport. This often causes planners to choose configurations for new runways that limit the use of these runways in time and/or space. Studies that model airports with new runways that are not yet operational need to develop plausible operational models for these new runways since historical data is not available. We look at a runway schedule problem encountered during the configuration and validation step of an earlier study. We develop a method using simulation optimization to approach the runway schedule problem and compare it to a manual approach developed in the earlier study. We use the Total Airspace and Airport Modeler to model the airport and airspace operations and Fast Simulated Annealing for the optimization. 1",
            "group": 1846,
            "name": "10.1.1.76.1133",
            "keyword": "",
            "title": "Proceedings of the 2003 Winter Simulation Conference"
        },
        {
            "abstract": "The main objective of this research is the development of a temporal reasoning system able to manage qualitative and quantitative information also affected by vagueness and uncertainty; this is, in fact, the usual way in which temporal information about the external reality reaches us. A well-known method for modelling temporal reasoning problems is CSP. A limit of this method is its intractability, and there are two directions that can be considered in order to reduce the computational complexity: the first is the development of heuristic techniques that prune the search space, the second is the definition of tractable sub-algebras to work with. Classic CSPs are based on crisp constraints (i. e. constraints that are either completely satisfied or violated), for this reason over-constrained problems do not have solution",
            "group": 1847,
            "name": "10.1.1.76.1370",
            "keyword": "",
            "title": "Towards a scheduling application using fuzzy temporal constraints"
        },
        {
            "abstract": "",
            "group": 1848,
            "name": "10.1.1.76.1509",
            "keyword": "neurocomputersneural networksoptical computersmolecular computersfield computersuniversal field computerassociative memoryparallel processingmassive parallelismparallel distributed processingknowledge representationnonpropositional knowledgethe new AI",
            "title": "Field Computation and Nonpropositional Knowledge"
        },
        {
            "abstract": "University of Rochester in 1999. ii",
            "group": 1849,
            "name": "10.1.1.76.1536",
            "keyword": "",
            "title": "Supervised by"
        },
        {
            "abstract": "We present and compare two stochastic approaches for solving the buffer allocation problem in reliable production lines. The problem entails the determination of near optimal buffer allocation plans in large production lines with the objective of maximizing their throughput. The allocation plan is calculated subject to a given amount of total buffer slots using simulated annealing and genetic algorithms. The throughput is calculated utilizing a decomposition method.",
            "group": 1850,
            "name": "10.1.1.76.2126",
            "keyword": "Simulated annealinggenetic algorithmsproduction linesbuffer allocation",
            "title": "Genetic Algorithms Versus Simulated Annealing"
        },
        {
            "abstract": "In this thesis we study constraint relaxations of various nonlinear programming (NLP) al-gorithms in order to improve their performance. For both stochastic and deterministic algorithms, we study the relationship between the expected time to find a feasible solution and the constraint relaxation level, build an exponential model based on this relationship, and develop a constraint relaxation schedule in such a way that the total time spent to find a feasible solution for all the relaxation levels is of the same order of magnitude as the time spent for finding a solution of similar quality using the last relaxation level alone. When the objective and constraint functions are stochastic, we define new criteria of constraint satisfaction and similar constraint relaxation schedules. Similar to the case when functions are deterministic, we build an exponential model between the expected time to find a feasible solution and the associated constraint relaxation level. We develop an anytime constraint relaxation schedule in such a way that the total time spent to solve a problem for all constraint relaxation levels is of the same order of magnitude as the time spent for finding a feasible solution using the last relaxation level alone. Finally, we study the asymptotic",
            "group": 1851,
            "name": "10.1.1.76.2165",
            "keyword": "",
            "title": "Improving constrained nonlinear search algorithms through constraint relaxation"
        },
        {
            "abstract": "Dedicated to my wife Selda, my daughters Hatice Nur, and Zeynep Hannan for their love and support ii ACKNOWLEDGEMENT First of all, I would like to thank Professor Donald W. Bouldin, my advisor, for his guidance, encouragement, support and valuable criticism during this research. I also thank my other committee members, Dr. Michael A. Langston, Dr. Gregory Peterson and Dr. Chandra Tan for their guidance and valuable feedback. I would also like to",
            "group": 1852,
            "name": "10.1.1.76.2208",
            "keyword": "",
            "title": "AUTOMATED EXPLORATION OF THE ASIC DESIGN SPACE FOR MINIMUM POWER-DELAY-AREA PRODUCT AT THE"
        },
        {
            "abstract": "To date, most of the research in simulation optimization has been focused on single response optimization on the continuous space of input parameters. However, the optimization of more complex systems does not fit this framework. Decision makers often face the problem of optimizing multiple performance measures of systems with both continuous and discrete input parameters. Previously acquired knowledge of the system by experts is seldom incorporated into the simulation optimization engine. Furthermore, when the goals of the system design are stated in natural language or vague terms, current techniques are unable to deal with this situation. For these reasons, we define and study the fuzzy single response simulation optimization (FSO) and fuzzy multiple response simulation optimization (FMSO) problems. The primary objective of this research is to develop an efficient and robust method for simulation optimization of complex systems with multiple vague goals. This method uses a fuzzy controller to incorporate existing knowledge to generate high quality approximate Pareto optimal solutions in a minimum number of simulation runs.",
            "group": 1853,
            "name": "10.1.1.76.2300",
            "keyword": "",
            "title": "Abstract MEDAGLIA, ANDR\u00c9S L. Simulation Optimization Using Soft Computing. (Under"
        },
        {
            "abstract": "Abstract. The emergence in the past years of Bayesian analysis in many methodological and applied fields as the solution to the modeling of complex problems cannot be dissociated from major changes in its computational implementation. We show in this review how the advances in Bayesian analysis and statistical computation are intermingled. Key words and phrases: Monte Carlo methods, importance sampling, Markov chain Monte Carlo (MCMC) algorithms.",
            "group": 1854,
            "name": "10.1.1.76.2523",
            "keyword": "",
            "title": "Computational advances for and from Bayesian analysis"
        },
        {
            "abstract": "There exist Boolean functions on n (odd) variables having nonlinearity> 2 n\u22121 \u2212 2 n\u22121 2 if and only if n> 7",
            "group": 1855,
            "name": "10.1.1.76.2661",
            "keyword": "Boolean FunctionsCombinatorial ProblemsCryptographyHeuristic SearchNonlinearityRotational Symmetry",
            "title": ""
        },
        {
            "abstract": "With the increasing sophistication of circuits and specifically in the presence of IP blocks, new estimation methods are needed in the design flow of large-scale circuits. Up to now, a number of post-placement congestion estimation techniques in the presence of IP blocks have been presented. In this paper we present a unified approach for predicting wirelength, congestion and delay parameters early in the design flow. We also propose a methodology to integrate these prediction methods into the placement framework to handle the large complexity of the designs.",
            "group": 1856,
            "name": "10.1.1.76.3223",
            "keyword": "AlgorithmsMeasurementDesignExperimentationTheory. Keywords PredictionCongestionWirelengthDelayAlgorithm",
            "title": "Tutorial on Congestion Prediction"
        },
        {
            "abstract": "Large production line optimisation using simulated annealing y We present a robust generalised queuing network algorithm as an evaluative procedure for optimising production line configurations using simulated annealing. We compare the results obtained with our algorithm to those of other studies and find some interesting similarities but also striking differences between them in the allocation of buffers, numbers of servers, and their service rates. While context dependent, these patterns of allocation are one of the most important insights which emerge in solving very long production lines. The patterns, however, are often counter-intuitive, which underscores the difficulty of the problem we address. The most interesting feature of our optimisation procedure is its bounded execution time, which makes it viable for optimising very long production line configurations. Based on the bounded execution time property, we have optimised configurations of up to 60 stations with 120 buffers and servers in less than five hours of CPU time.",
            "group": 1857,
            "name": "10.1.1.76.3790",
            "keyword": "Buffer AllocationNon-linearStochasticIntegerNetwork DesignSimulated Annealing",
            "title": ""
        },
        {
            "abstract": "Abstract. The usefulness of the results produced by data mining methods can be critically impaired by several factors such as (1) low quality of data, including errors due to contamination, or incompleteness due to limited bandwidth for data acquisition, and (2) inadequacy of the data model for capturing complex probabilistic relationships in data. Fortunately, a wide spectrum of applications exhibit strong dependencies between data samples. For example, the readings of nearby sensors are generally correlated, and proteins interact with each other when performing crucial functions. Therefore, dependencies among data can be successfully exploited to remedy the problems mentioned above. In this paper, we propose a unified approach to improving mining quality using Markov networks as the data model to exploit local dependencies. Belief propagation is used to efficiently compute the marginal or maximum posterior probabilities, so as to clean the data, to infer missing values, or to improve the mining results from a model that ignores these dependencies. To illustrate the benefits and great generality of the technique, we present its application to three challenging problems: (i) cost-efficient sensor probing, (ii) enhancing protein function predictions, and (iii) sequence data denoising. 1",
            "group": 1858,
            "name": "10.1.1.76.4016",
            "keyword": "",
            "title": "Improving mining quality by exploiting data dependency"
        },
        {
            "abstract": "via Cross-Entropy",
            "group": 1859,
            "name": "10.1.1.76.5024",
            "keyword": "",
            "title": "Combinatorial Optimization"
        },
        {
            "abstract": "An execution environment consisting of virtual machines (VMs) interconnected with a virtual overlay network can use the naturally occurring traffic of an existing, unmodified application running in the VMs to measure the underlying physical network. Based on these characterizations, and characterizations of the application\u2019s own communication topology, the execution environment can optimize the execution of the application using application-independent means such as VM migration and overlay topology changes. In this paper, we demonstrate the feasibility of such free automatic network measurement by fusing the Wren passive monitoring and analysis system with Virtuoso\u2019s virtual networking system. We explain how Wren has been extended to support on-line analysis, and we explain how Virtuoso\u2019s adaptation algorithms have been enhanced to use Wren\u2019s physical network level information to choose VM-to-host mappings, overlay topology, and forwarding rules.",
            "group": 1860,
            "name": "10.1.1.76.6847",
            "keyword": "",
            "title": "Free Network Measurement For Adaptive Virtualized Distributed Computing"
        },
        {
            "abstract": "ABSTRACT An extension of the multi-band model including inter-band control of time asynchrony is described. It is based on the framework of Markov random o/elds. The law of the speech process is given by a parametric Gibbs distribution and a maximum likelihood parameter estimation algorithm is developed. This random o/eld model is applied to isolated word recognition. It is shown that similar performances are obtained with the new model and with standard HMM techniques in the mono-band case. In the multi-band case, it is shown that the recognition rate decreases when the number of band is increased but that modeling inter-band synchrony limits the performance decrease.",
            "group": 1861,
            "name": "10.1.1.76.7277",
            "keyword": "",
            "title": "A markov random field based multi-band model"
        },
        {
            "abstract": "Network experiments of many types, especially emulation, require the ability to map virtual resources requested by an experimenter onto available physical resources. These resources include hosts, routers, switches, and the links that connect them. Experimenter requests, such as nodes with special hardware or software, must be satisfied, and bottleneck links and other scarce resources in the physical topology should be conserved when physical resources are shared. In the face of these constraints, this mapping becomes an NP-hard problem. Yet, in order to prevent mapping time from becoming a serious hindrance to experimentation, this process cannot consume an excessive amount of time. In this paper, we explore this problem, which we call the network testbed mapping problem. We describe the interesting challenges that characterize it, and explore its application to emulation and other spaces, such as distributed simulation. We present the design, implementation, and evaluation of a solver for this problem, which is in production use on the Netbed shared network testbed. Our solver builds on simulated annealing to find very good solutions in a few seconds for our historical workload, and scales gracefully on large well-connected synthetic topologies. 1",
            "group": 1862,
            "name": "10.1.1.76.7523",
            "keyword": "",
            "title": "A Solver for the Network Testbed Mapping Problem"
        },
        {
            "abstract": "The demand for high-speed FPGA compilation tools has occurred for three reasons: first, as FPGA device capacity has grown, the computation time devoted to placement and routing has grown more dramatically than the compute power of the available computers. Second, there exists a subset of users who are willing to accept a reduction in the quality 1 of result in exchange for a highspeed compilation. Third, high-speed compile has been a longstanding desire of users of FPGA-based custom computing machines, since their compile time requirements are ideally closer to those of regular computers. This paper focuses on the placement phase of the compile process, and presents an ultra-fast placement algorithm targeted to FPGAs. The algorithm is based on a combination of multiple-level, bottomup clustering and hierarchical simulated annealing. It provides superior area results over a known high-quality placement tool on a set of large benchmark circuits, when both are restricted to a short run time. For example, it can generate a placement for a 100,000gate circuit in 10 seconds on a 300 MHz Sun UltraSPARC workstation that is only 33 % worse than a high-quality placement that takes 524 seconds using a pure simulated annealing implementation. In addition, operating in its fastest mode, this tool can provide an accurate estimate of the wirelength achievable with good quality placement. This can be used, in conjunction with a routing predictor, to very quickly determine the routability of a given circuit on a given FPGA device. 1.",
            "group": 1863,
            "name": "10.1.1.76.7663",
            "keyword": "",
            "title": "Abstract Trading Quality for Compile Time: Ultra-Fast Placement for FPGAs"
        },
        {
            "abstract": "This paper describes the application of markovian teaming methods to the infer-ence of word transducers. We show how the proposed method dispenses from the difficult design of hand-crafted rules, al-lows the use of weighed non deterministic transducers and is able to translate words by taking into account their whole rather than by making decisions locally. These ar-guments are illustrated on two examples: morphological analysis and grapheme-to-phoneme transcription.",
            "group": 1864,
            "name": "10.1.1.76.7968",
            "keyword": "",
            "title": "Automatic learning of word transducers from examples&quot;. Fifth EACL"
        },
        {
            "abstract": "Existing trajectory clustering algorithms group similar trajectories as a whole, thus discovering common trajectories. Our key observation is that clustering trajectories as a whole could miss common sub-trajectories. Discovering common sub-trajectories is very useful in many applications, especially if we have regions of special interest for analysis. In this paper, we propose a new partition-and-group framework for clustering trajectories, which partitions a trajectory into a set of line segments, and then, groups similar line segments together into a cluster. The primary advantage of this framework is to discover common sub-trajectories from a trajectory database. Based on this partition-and-group framework, we develop a trajectory clustering algorithm TRA-CLUS. Our algorithm consists of two phases: partitioning and grouping. For the first phase, we present a formal trajectory partitioning algorithm using the minimum description length (MDL) principle. For the second phase, we present a density-based line-segment clustering algorithm. Experimental results demonstrate that TRACLUS correctly discovers common sub-trajectories from real trajectory data.",
            "group": 1865,
            "name": "10.1.1.76.8098",
            "keyword": "Categories and Subject DescriptorsH.2.8 [Database ManagementDatabase Applications \u2013 Data Mining General TermsAlgorithms KeywordsPartition-and-group frameworktrajectory clusteringMDL",
            "title": "Trajectory Clustering: A Partition-and-Group Framework"
        },
        {
            "abstract": "Current FPGA placement algorithms estimate the routability of a placement using architecture-specific metrics. The shortcoming of using architecture-specific routability estimates is limited adaptability. A placement algorithm that is targeted to a class of architecturally similar FPGAs may not be easily adapted to other architectures. The subject of this paper is the development of a routability-driven architecture adaptive FPGA placement algorithm called Independence. The core of the Independence algorithm is a simultaneous place-and-route approach that tightly couples a simulated annealing placement algorithm with an architecture adaptive FPGA router (Pathfinder). The results of our experiments demonstrate Independence\u2019s adaptability to island-style and hierarchical FPGA architectures. The quality of the placements produced by Independence is within 5 % of the quality of VPR\u2019s placements and 17 % better than the placements produced by HSRA\u2019s place-and-route tool. Further, our results show that Independence produces clearly superior placements on routing-poor island-style FPGA architectures.",
            "group": 1866,
            "name": "10.1.1.76.8417",
            "keyword": "routingFPGAIndependenceadaptive",
            "title": "Architecture Adaptive Routability-Driven Placement for FPGAs"
        },
        {
            "abstract": null,
            "group": 1867,
            "name": "10.1.1.76.8538",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Volume image registration by template matching",
            "group": 1868,
            "name": "10.1.1.76.8547",
            "keyword": "Image registrationvolumetric imagetemplate matchingsimilarity measuremultimodal",
            "title": ""
        },
        {
            "abstract": "In this paper the first results of the application of the Generalized Extremal Optimization (GEO) algorithm to a discrete structural optimization problem is shown. GEO is an evolutionary brand new algorithm, devised to be easily applicable to a broad class of nonlinear constrained optimization problems, with the presence of any combination of continuos, discrete and integer variables. So far, it has been applied successfully to real optimal design problems with continuos design variables and shown to be competitive to other stochastic methods such as the Genetic Algorithms (GAs) and the Simulated Annealing (SA). Having only one free parameter to adjust, it can be easily set to give its best performance for a given application. This is an a priori advantage over methods such as the SA and GAs since each of them have at least three parameters to be set, making their tuning to a particular application more prone to be computationally expensive and becoming a problem in itself. In this work, the 10-bar truss problem is used as a test case and the performance of GEO, compared to results from other methods available in literature.",
            "group": 1869,
            "name": "10.1.1.76.8740",
            "keyword": "Structural OptimizationTruss OptimizationEvolutionary Algorithms",
            "title": "Discrete Optimal Design of Trusses by Generalized Extremal Optimization"
        },
        {
            "abstract": "In this paper the first results of the application of the Generalized Extremal Optimization (GEO) algorithm to a discrete structural optimization problem is shown. GEO is an evolutionary brand new algorithm, devised to be easily applicable to a broad class of nonlinear constrained optimization problems, with the presence of any combination of continuos, discrete and integer variables. So far, it has been applied successfully to real optimal design problems with continuos design variables and shown to be competitive to other stochastic methods such as the Genetic Algorithms (GAs) and the Simulated Annealing (SA). Having only one free parameter to adjust, it can be easily set to give its best performance for a given application. This is an a priori advantage over methods such as the SA and GAs since each of them have at least three parameters to be set, making their tuning to a particular application more prone to be computationally expensive and becoming a problem in itself. In this work, the 10-bar truss problem is used as a test case and the performance of GEO, compared to results from other methods available in literature.",
            "group": 1870,
            "name": "10.1.1.76.8740",
            "keyword": "Structural OptimizationTruss OptimizationEvolutionary Algorithms",
            "title": "Discrete Optimal Design of Trusses by Generalized Extremal Optimization"
        },
        {
            "abstract": "Analytical modeling is applied to the automated design of application-specific superscalar processors. Using an analytical method bridges the gap between the size of the design space and the time required for detailed cycle-accurate simulations. The proposed design framework takes as inputs the design targets (upper bounds on execution time, area, and energy), design alternatives, and one or more application programs. The output is the set of out-of-order superscalar processors that are Pareto-optimal with respect to performance-energy-area. The core of the new design framework is made up of analytical performance and energy activity models, and an analytical model-based design optimization process. For a set of benchmark programs and a design space of 2000 designs, the design framework arrives at all performanceenergy-area Pareto-optimal design points within 16 minutes on a 2 GHz Pentium-4. In contrast, it is estimated that a na\u00efve cycle-accurate simulation-based exhaustive search would require at least two months to arrive at the Pareto-optimal design points for the same design space.",
            "group": 1871,
            "name": "10.1.1.76.9486",
            "keyword": "model",
            "title": "Automated Design of Application Specific Superscalar Processors: An Analytical Approach"
        },
        {
            "abstract": "Stochastic local search techniques are powerful and flexible methods to optimize difficult functions. While each method is characterized by search trajectories produced through a randomized selection of the next step, a notable difference is caused by the interaction of different searchers, as exemplified by the Particle Swarm methods. In this paper we evaluate two extreme approaches, Particle Swarm Optimization, with interaction between the individual \u201ccognitive \u201d component and the \u201csocial \u201d knowledge, and Repeated Affine Shaker, without any interaction between searchers but with an aggressive capability of scouting out local minima. The results, unexpected to the authors, show that Affine Shaker provides remarkably efficient and effective results when compared with PSO, while the advantage of Particle Swarm is visible only for functions with a very regular structure of the local minima leading to the global optimum and only for specific experimental conditions. 1",
            "group": 1872,
            "name": "10.1.1.77.443",
            "keyword": "",
            "title": "Do not be afraid of local minima: Affine shaker and particle swarm"
        },
        {
            "abstract": "This paper presents a novel approach to assist the user in exploring appropriate transfer functions for the visualization of volumetric datasets. The search for a transfer function is treated as a parameter optimization problem and addressed with stochastic search techniques. Starting from an initial population of (random or pre-defined) transfer functions, the evolution of the stochastic algorithms is controlled by either direct user selection of intermediate images or automatic fitness evaluation using user-specified objective functions. This approach essentially shields the user from the complex and tedious \u201ctrial and error\u201d approach, and demonstrates effective and convenient generation of transfer functions. 1",
            "group": 1873,
            "name": "10.1.1.77.603",
            "keyword": "",
            "title": "Abstract Generation of Transfer Functions with Stochastic Search Techniques"
        },
        {
            "abstract": "Abstract. We define a new method for global optimization, the Homotopy Optimization Method (HOM). This method differs from previous homotopy and continuation methods in that its aim is to find a minimizer for each of a set of values of the homotopy parameter, rather than to follow a path of minimizers. We define a second method, called HOPE, by allowing HOM to follow an ensemble of points obtained by perturbation of previous ones. We relate this new method to standard methods such as simulated annealing and show under what circumstances it is superior. We present results of extensive numerical experiments demonstrating performance of HOM and HOPE.",
            "group": 1874,
            "name": "10.1.1.77.681",
            "keyword": "Key words. homotopy methodstochastic searchmultistartglobal optimization",
            "title": "Homotopy Optimization methods for Global Optimization"
        },
        {
            "abstract": "We present a simulation-based algorithm called \u201cSimulated Annealing Multiplicative Weights\u201d (SAMW) for solving large finite-horizon stochastic dynamic programming problems. At each iteration of the algorithm, a probability distribution over candidate policies is updated by a simple multiplicative weight rule, and with proper annealing of a control parameter, the generated sequence of distributions converges to a distribution concentrated only on the best policies. The algorithm is \u201casymptotically efficient, \u201d in the sense that for the goal of estimating the value of an optimal policy, a provably convergent finite-time upper bound for the sample mean is obtained.",
            "group": 1875,
            "name": "10.1.1.77.752",
            "keyword": "stochastic dynamic programmingMarkov decision processessimulationlearning algorithmssimulated annealing",
            "title": "An asymptotically efficient simulation-based algorithm for finite horizon stochastic dynamic programming"
        },
        {
            "abstract": " ",
            "group": 1876,
            "name": "10.1.1.77.1333",
            "keyword": "NameLast nameY\u0131lmaz Arslano\u011flu",
            "title": "  GENETIC ALGORITHM FOR PERSONNEL ASSIGNMENT PROBLEM WITH MULTIPLE OBJECTIVES"
        },
        {
            "abstract": "The approach proposed in this paper reduces power consumption in single-error correcting, double error-detecting checker circuits that perform memory ECC. Power is minimized with little or no impact on area and delay, using the degrees of freedom in selecting the parity check matrix of the error correcting code. The non-linear power optimization problem is solved using two methods, genetic algorithms and simulated annealing. Both the methods are applied to two SEC-DED codes: standard Hamming codes and odd-column-weight Hsiao codes. Experiments on actual memory traces of Spec and MediaBench benchmarks indicate that considering power along with area and delay when selecting the parity check matrix can result in power reductions of up to 27 % for Hsiao codes and up to 41 % for Hamming codes. Experiments are also performed to motivate the choice of parameters of the non-linear optimization algorithms, using sensitivity analysis of the low-power solutions to the choice of the different parameters of each algorithm.",
            "group": 1877,
            "name": "10.1.1.77.1721",
            "keyword": "Memory error correctionHsiao and Hamming codeslow powernon-linear optimization",
            "title": "N.A.: Selecting error correcting codes to minimize power in memory checker circuits"
        },
        {
            "abstract": "In this paper we describe a number of global optimisation problems connected to spacecraft trajectory design. Each problem is coded in the form of a blackbox objective function accepting, as inputs, the decision vector and returning the objective function and the constraint evaluation. The code is made available on line as a challenge to the community to develop performing algorithms able to solve each of the problems proposed in an efficient manners. All the problems proposed draw inspiration from real trajectory problems, ranging from Cassini to Rossetta to Messenger to possible future missions. As a start we also report the results coming from applying standard global optimisation algorithm to each of the problem. We consider Differential Evolution, Particle Swarm Optimisation, Genetic Algorithm, Adaptive Simulated Annealing and GLOBAL. As all these standard implementations seem to fail to solve more complex problems, we conclude the paper suggesting a cooperative approach between the different algorithm showing performance improvements.",
            "group": 1878,
            "name": "10.1.1.77.2035",
            "keyword": "",
            "title": "with permission and Released to IAF or IAA to publish in all forms. Benchmarking different global optimisation techniques for preliminary space trajectory design"
        },
        {
            "abstract": "We present an implementation of an efficient general simulated annealing schedule and demonstrate experimentally that the new schedule achieves speedups often exceeding an order of magnitude when compared with other general schedules currently available in the literature. To assess the performance of simulated annealing as a general method for solving combinatorial optimization problems, we also compare the method with efficient heuristics on well-studied problems: the traveling salesman problem and the graph partition problem. For high quality solutions and for problems with a small number of close to optimal solutions, our test results indicate that simulated annealing outperforms the heuristics of Lin and Kernighan and of Karp for the traveling salesman problem, and multiple executions of the heuristic of Fiduccia and Mattheyses for the graph partition problem.",
            "group": 1879,
            "name": "10.1.1.77.2077",
            "keyword": "",
            "title": "An efficient simulated annealing schedule: implementation and evaluation"
        },
        {
            "abstract": "Abstract\u2014We present a pipelining-aware router for FPGAs. The problem of routing pipelined signals is different from the conventional FPGA routing problem. The two terminal N D pipelined routing problem is to find the lowest cost route between a source and sink that goes through at least N (N \u2265 1) distinct pipelining resources. In the case of a multi-terminal pipelined signal, the problem is to find a Minimum Spanning Tree that contains sufficient pipelining resources such that pipelining constraints at each sink are satisfied. In this work, we first present an optimal algorithm for finding a lowest cost 1 D route. The optimal 1 D algorithm is then used as a building block for a greedy two terminal N D router. Next, we discuss the development of a multi-terminal routing algorithm (PipeRoute) that effectively leverages both the 1 D and N D routers. Finally, we present a preprocessing heuristic that enables the application of PipeRoute to pipelined FPGA architectures. PipeRoute\u2019s performance is evaluated by routing a set of benchmark netlists on the RaPiD architecture. Our results show that the architecture overhead incurred in routing netlists on RaPiD is less than 20%. Further, the results indicate a possible trend between the architecture overhead and the percentage of pipelined signals in a netlist. Index Terms\u2014Design automation, Field programmable gate",
            "group": 1880,
            "name": "10.1.1.77.3539",
            "keyword": "MemberIEEE",
            "title": ""
        },
        {
            "abstract": "",
            "group": 1881,
            "name": "10.1.1.77.3769",
            "keyword": "",
            "title": "Reactive search: machine learning for memory-based heuristics"
        },
        {
            "abstract": "This paper presents an integrated framework for assembly design. The framework allows the designer to represent knowledge about the design process and constraints, as well as information about the artifact being designed, design history and rationale. Because the complexity of assembly design leads to extremely large design spaces, adequately supporting design space exploration is a key issue that must be addressed. This is achieved in part by allowing the designer to use both top-down and bottom-up approaches to assembly design. Exploration of the design space is further enabled by incorporating a simulated annealing-based optimization tool that allows the designer to rapidly complete partial designs, refine complete designs, and generate multiple design alternatives.",
            "group": 1882,
            "name": "10.1.1.77.4234",
            "keyword": "",
            "title": "COMBINING INTERACTIVE EXPLORATION AND OPTIMIZATION FOR ASSEMBLY DESIGN"
        },
        {
            "abstract": "One way to reduce the delay and area of FPGAs is to employ logiccluster-based architectures, where a logic cluster is a group of logic elements connected with high-speed local interconnections. In this paper we empirically evaluate FPGA architectures with logic clusters ranging in size from 1 to 20, and show that compared to architectures with size 1 clusters, architectures with size 8 clusters have 23 % less delay (30 % faster clock speed), and require 14 % less area. We also show that FPGA architectures with large cluster sizes can significantly reduce design compile time \u2014 an increasingly important concern as the logic capacity of FPGAs rises. For example, an architecture that uses size 20 clusters requires 7 times less compile time than an architecture with size 1 clusters.",
            "group": 1883,
            "name": "10.1.1.77.4277",
            "keyword": "Clusteringdesigngate-arrayhigh-speed-interconnectperformance-trade-offshigh-performance",
            "title": "INDEX TERMS"
        },
        {
            "abstract": "Abstract. Many applications involve matching two graphs in order to identify their common features and compute their similarity. In this paper, we address the problem of computing a graph similarity measure based on a multivalent graph matching and which is generic in the sense that other well known graph similarity measures can be viewed as special cases of it. We propose and compare two different kinds of algorithms: an Ant Colony Optimization based algorithm and a Reactive Search. We compare the efficiency of these two algorithms on two different kinds of difficult graph matching problems and we show that they obtain complementary results. 1",
            "group": 1884,
            "name": "10.1.1.77.4710",
            "keyword": "",
            "title": "A comparative study of ant colony optimization and reactive search for graph matching problems"
        },
        {
            "abstract": "We argue that AI is moving into a new phase characterized by biological rather than psychological metaphors. Full exploitation of this new paradigm will require a new class of computers characterized by massive parallelism: parallelism in which the number of computational units is so large it can be treated as a continuous quantity. We suggest that this leads to a new model of computation based on the transformation of continuous scalar and vector fields. We describe a class of computers, called field computers that conform to this model, and claim that they can be implemented in a variety of technologies (e.g., optical, artificial neural network, molecular). We also describe a universal field computer and show that it can be programmed for the parallel computation of a wide variety of field transformations. 1. The \u2018\u2018New\u2019 \u2019 AI Traditional Artificial Intelligence technology is based on psychological metaphors, that is, idealized models of human cognitive behavior. In particular, models of conscious, goal-directed problem solving have provided the basis for many of AI\u2019s accomplishments to date. As valuable as these metaphors have been, we believe that they are not appropriate for many of the tasks for which we wish to use computers. In particular, symbolic information processing does not seem to be a good model of the way people (or animals) behave skillfully in subcognitive tasks, such as pattern recognition and sensorimotor coordination.",
            "group": 1885,
            "name": "10.1.1.77.4817",
            "keyword": "",
            "title": "Abstract Technology-Independent Design of Neurocomputers: The Universal Field Computer 1"
        },
        {
            "abstract": "",
            "group": 1886,
            "name": "10.1.1.77.4918",
            "keyword": "",
            "title": "Bayesian Methods and Markov Random Fields "
        },
        {
            "abstract": "Abstract\u2014The assemble-to-order (ATO) production strategy considers a tradeoff between the size of a product portfolio and the assembly lead time. The concept of modular design is often used in support of the ATO strategy. Modular design impacts the assembly of products and the supply chain, in particular, the storage, transport, and production are affected by the selected modular structure. The demand for products in a product family impacts the cost of the supply chain. Based on the demand patterns, a mix of modules and their stock are determined by solving an integer programming model. This model cannot be optimally solved due to its high computational complexity and, therefore, two heuristic algorithms are proposed. A simulated annealing algorithm improves on the previously generated solutions. The computational results reported in this paper show that significant savings could be realized by optimizing the composition of modules. The best",
            "group": 1887,
            "name": "10.1.1.77.4949",
            "keyword": "",
            "title": "Design for Cost: Module-Based Mass Customization"
        },
        {
            "abstract": "Spacecraft design optimization is a difficult problem, due to the complexity of optimization cost surfaces and the human expertise in optimization that is necessary in order to achieve good results. In this paper, we propose the use of a set of generic, metaheuristic optimization algorithms (e.g., genetic algorithms, simulated annealing), which is configured for a particular optimization problem by an adaptive problem solver based on artificial intelligence and machine learning techniques. We describe work in progress on OASIS, a selfconfiguring optimization system based on these principles. 1",
            "group": 1888,
            "name": "10.1.1.77.5178",
            "keyword": "",
            "title": "A.Stechert,S.Chien \u201cTowards a self-configuring Optimization System for Spacecraft Design"
        },
        {
            "abstract": "We present a new approach to clustering, based on the physical properties of an inhomogeneous ferromagnet. No assumption is made regarding the underlying distribution of the data. We assign a Potts spin to each data point and introduce an interaction between neighboring points, whose strength is a decreasing function of the distance between the neighbors. This magnetic system exhibits three phases. At very low temperatures, it is completely ordered; all spins are aligned. At very high temperatures, the system does not exhibit any ordering, and in an intermediate regime, clusters of relatively strongly coupled spins become ordered, whereas different clusters remain uncorrelated. This intermediate phase is identified by a jump in the order parameters. The spin-spin correlation function is used to partition the spins and the corresponding data points into clusters. We demonstrate on three synthetic and three real data sets how the method works. Detailed comparison to the performance of other techniques clearly indicates the relative success of our method. 1",
            "group": 1889,
            "name": "10.1.1.77.5569",
            "keyword": "",
            "title": "Data clustering using a model granular magnet"
        },
        {
            "abstract": "Abstract \u2014 Recent large scale experiments have shown that the Normalized Information Distance, an algorithmic information measure, is among the best similarity metrics for melody classification. This paper proposes the use of this distance as a fitness function which may be used by genetic algorithms to automatically generate music in a given pre-defined style. The minimization of this distance of the generated music to a set of musical guides makes it possible to obtain computer-generated music which recalls the style of a certain human author. The recombination operator plays an important role in this problem and thus several variations are tested to fine tune the genetic algorithm for this application. The superiority of the relative pitch envelope over other music parameters, such as the lengths of the notes, brought us to develop a simplified algorithm that nevertheless obtains interesting results. I.",
            "group": 1890,
            "name": "10.1.1.77.6596",
            "keyword": "",
            "title": "A Simple Genetic Algorithm for Music Generation by means of Algorithmic Information Theory"
        },
        {
            "abstract": "Daphna, it was a great pleasure to be your student. You always encouraged me to be creative and to search for new directions and new ideas. Your judgment and critique was always sharp, hence it was especially pleasing to get from you positive feedbacks. I thank you for giving me so many of them, which was a great encouragement along the way. I thank you for being always available for me, always interested in the progress of the work. I do hope that our work will have a continuation, as well as our friendship. I would like to thank all those who collaborated with me along the years, and especially Michael Werman, Ran El-Yaniv, Golan Yona, Noam Shental, Ido Bregman and Assaf Zomet. I thank the members of both the vision lab and the machine learning lab, past and present, for creating a lively and friendly atmosphere. Contents Abstract iii 1",
            "group": 1891,
            "name": "10.1.1.77.6844",
            "keyword": "",
            "title": "This work was carried out under the supervision of"
        },
        {
            "abstract": "The multiple digest mapping problem arising in molecular biology can be stated roughly as follows. A linear or circular segment of DNA is cut at all Occurrences of a specific short pattern by restriction enzymes. By using restriction enzymes singly and in combination it is required to construct a map showing the location of cleavage sites. In this paper we first consider the efficacy of a simulated annealing algorithm towards the solution to the multiple digest problem. Second, the double digest problem, the simplest version of the multiple digest problem with only two restriction enzymes used, is shown to admit an exponentially increasing number of solutions as a function of the length of the segment under a particular probability model. Next, the double digest problem is shown to lie in the class of NP complete problems which are conjectured to have no polynomial time solution. Last, the construction of circular maps is considered and the problem of measurement error is discussed. 0 1987 Academic Press. Inc. 1.",
            "group": 1892,
            "name": "10.1.1.77.7244",
            "keyword": "",
            "title": "ADVANCES IN APPLIED MATHEMATICS 8,194-207 (1987) Mapping DNA by Stochastic Relaxation*#\u2019"
        },
        {
            "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9 % over state-of-the-art systems on two established information extraction tasks. 1",
            "group": 1893,
            "name": "10.1.1.77.7532",
            "keyword": "",
            "title": "Incorporating non-local information into information extraction systems by gibbs sampling"
        },
        {
            "abstract": "Abstract. Memetic algorithms (MAs) constitute a metaheuristic optimization paradigm based on the systematic exploitation of knowledge about the problem being solved, and the synergistic combination of ideas taken from other population-based and trajectory-based metaheuristics. They have been successfully deployed on a plethora of hard combinatorial optimization problems, amongst which scheduling, planning and timetabling are distinguished examples due to their practical interest. This work examines the application of MAs to problems in these domains. We describe the basic architecture of a MA, and present some guidelines to the design of a successful MA for these applications. An overview of the existing literature on the topic is also provided. We conclude with some reflections on the lessons learned, and the future directions that research could take in this area. 1",
            "group": 1894,
            "name": "10.1.1.77.8030",
            "keyword": "",
            "title": "Memetic Algorithms in Planning, Scheduling, and"
        },
        {
            "abstract": "Abstract. Traditional placement algorithms for FPGAs are normally carried out on a fixed clustering solution of a circuit. The impact of clustering on wirelength and delay of the placement solutions is not well quantified. In this paper, we present an algorithm named SCPlace that performs simultaneous clustering and placement to minimize both the total wirelength and longest path delay. We also incorporate a recently proposed path counting-based net weighting scheme [16]. Our algorithm SCPlace consistently outperforms the state-of-the-art FPGA placement flow (T-VPack + VPR) with an average reduction of up to 36 % in total wirelength and 31 % in longest path delay. 1",
            "group": 1895,
            "name": "10.1.1.77.8116",
            "keyword": "",
            "title": "Simultaneous timing driven clustering and placement for FPGAs"
        },
        {
            "abstract": "Parallel genetic algorithms (PGA) use two major modifications compared to the genetic algorithm. Firstly, selection for mating is distributed. Individuals live in a 2-D world. Selection of a mate is done by each individual independently in its neighborhood. Secondly, each individual may improve its tness during its lifetime by e.g. local hill-climbing. The PGA is totally asynchronous, running with maximal efficiency on MIMD parallel computers. The search strategy of the PGA is based on a small number of intelligent and active individuals, whereas a GA uses a large population of passive individuals. We will show the power of the PGA with two combinatorial problems -- the traveling salesman problem and the m graph partitioning problem. In these examples, the PGA has found solutions of very large problems, which are comparable or even better than any other solution found by other heuristics. A comparison between the PGA search strategy and iterated local hill-climbing is made.  ",
            "group": 1896,
            "name": "10.1.1.78.990",
            "keyword": "",
            "title": "Parallel Genetic Algorithms in Combinatorial Optimization"
        },
        {
            "abstract": "Time-difference-of-arrival (TDOA) based source localization has been intensively studied and broadly applied in many fields. In this paper, particle swarm optimization (PSO) is employed for positioning with TDOA measurements in the circumstances of known and unknown propagation speed. The optimization criterion is first developed and the PSO technique is then employed to search the global minimum of the cost function. For sufficiently small noise conditions, simulation results show that the PSO approach provides accurate source location estimation for both known and unknown propagation speed, and also gives an efficient speed estimate in the later case. Index terms: time-difference-of-arrival, source localization, particle swarm optimization 1.",
            "group": 1897,
            "name": "10.1.1.78.1297",
            "keyword": "",
            "title": "PARTICLE SWARM OPTIMIZATION FOR TIME-DIFFERENCE-OF-ARRIVAL BASED LOCALIZATION"
        },
        {
            "abstract": "Abstract \u2014 In mobile computing scenarios, context-aware applications are more effective in relieving from the mobile user the burden of introducing information that can be automatically derived from the environment. In particular, the physical position of the mobile system (and hence of the user) is fundamental for many types of applications. User position estimation methods based on strength of the radio signals received from multiple wireless access points have been recently proposed and implemented by several independent research groups. In this paper a new approach to wireless access point placement is proposed. While previous proposals focus on optimal coverage aimed at connectivity, the proposed method integrates coverage requirements with the reduction of the error of the user position estimate. In particular, this paper proposes a mathematical model of user localization error based on the variability of signal strength measurements. This model has been designed to be independent from the actual localization technique, therefore it is only based on generic assumptions on the behavior of the localization algorithm employed. The proposed error model is used by local search heuristic techniques, such as local search, a prohibition-based variation and simulated annealing. Near-optimal access point placements are computed for various kinds of optimization criteria: localization error minimization, signal coverage maximization, a mixture of the two. The different criteria are not expected to be compatible: maximizing signal coverage alone can lead to degradation of the average positioning error, and vice versa. Some experiments have been dedicated to quantify this phenomenon and to introduce possible trade-offs. I.",
            "group": 1898,
            "name": "10.1.1.78.1862",
            "keyword": "",
            "title": "Optimal Wireless Access Point Placement for Location-Dependent Services"
        },
        {
            "abstract": "is an evolutionary algorithm that combines mutation and recombination by using a distribution. The distribution is estimated from a set of selected points. It is then used to generate new points for the next generation. In general a distribution defined for \u00d2 binary variables has \u00d2 parameters. Therefore it is too expensive to compute. For additively decomposed discrete functions (ADFs) there exists an algorithm that factors the distribution into conditional and marginal distributions, each of which can be computed in polynomial time. Previously, we have shown a convergence theorem for FDA. But it is only valid using Boltzmann selection. Boltzmann selection was not used in practice because a good annealing schedule was lacking. Using a Taylor expansion of the average fitness of the Boltzmann distribution, we have developed an adaptive annealing schedule called SDS (standard deviation schedule) that is introduced in this work. The inverse temperature \u00ac is changed inversely proportional to the standard deviation.",
            "group": 1899,
            "name": "10.1.1.78.1939",
            "keyword": "genetic algorithmssimulated annealingBoltzmann distributionBoltzmann",
            "title": "A New Adaptive Boltzmann Selection Schedule SDS Abstract- FDA (the Factorized Distribution Algorithm)"
        },
        {
            "abstract": "Abstract\u2014Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs sampler [9] provides a general solution but is very slow, while other methods, such as Ncut [24] and graph cuts [4], [22], are computationally effective but only work for specific energy forms [17] and are not generally applicable. In this paper, we present a new inference algorithm that generalizes the Swendsen-Wang method [25] to arbitrary probabilities defined on graph partitions. We begin by computing graph edge weights, based on local image features. Then, the algorithm iterates two steps. 1) Graph clustering: It forms connected components by cutting the edges probabilistically based on their weights. 2) Graph relabeling: It selects one connected component and flips probabilistically, the coloring of all vertices in the component simultaneously. Thus, it realizes the split, merge, and regrouping of a \u201cchunk \u201d of the graph, in contrast to Gibbs sampler that flips a single vertex. We prove that this algorithm simulates ergodic and reversible Markov chain jumps in the space of graph partitions and is applicable to arbitrary posterior probabilities or energy functions defined on graphs. We demonstrate the algorithm on two typical problems in computer vision\u2014image segmentation and stereo vision. Experimentally, we show that it is 100-400 times faster in CPU time than the classical Gibbs sampler and 20-40 times faster then the DDMCMC segmentation algorithm [27]. For stereo, we compare performance with graph cuts and belief propagation. We also show that our algorithm can automatically infer generative models and obtain satisfactory results (better than the graphic cuts or belief propagation) in the same amount of time.",
            "group": 1900,
            "name": "10.1.1.78.1986",
            "keyword": "Index Terms\u2014Swendsen-Wangcluster samplingMarkov chain Monte CarloBayesian inferenceimage segmentationstereo",
            "title": "Generalizing swendsen-wang to sampling arbitrary posterior probabilities"
        },
        {
            "abstract": "Evolutionary Computation (EC) is now a few decades old. The impressive development of the field since its initial conception has made it one of the most vigorous research areas, specifically from an applied viewpoint. This should not hide the existence of some major gaps in our understanding on these techniques. In this essay we propose a number of challenging tasks that \u2013according to our opinion \u2013 should be attacked in order to fill some of these gaps. They mainly refer to the theoretical basis of the paradigm; we believe that an effective cross-fertilization among different areas of Theoretical Computer Science and Artificial Intelligence (such as Parameterized Complexity and Modal Logic) is mandatory for developing a new corpus of knowledge about EC.  ",
            "group": 1901,
            "name": "10.1.1.78.2138",
            "keyword": "",
            "title": " Evolutionary computation: Challenges and duties"
        },
        {
            "abstract": "Abstract\u2014Phase unwrapping is an important problem in many magnetic resonance imaging applications, such as field mapping and flow imaging. The challenge in two-dimensional phase unwrapping lies in distinguishing jumps due to phase wrapping from those due to noise and/or abrupt variations in the actual function. This paper addresses this problem using a Markov random field to model the true phase function, whose parameters are determined by maximizing the a posteriori probability. To reduce the computational complexity of the optimization procedure, an efficient algorithm is also proposed for parameter estimation using a series of dynamic programming connected by the iterated conditional modes. The proposed method has been tested with both simulated and experimental data, yielding better results than some of the state-of-the-art method (e.g., the popular least-squares method) in handling noisy phase images with rapid phase variations. Index Terms\u2014Bayesian estimation, field mapping, magnetic resonance imaging, Markov random field, phase unwrapping. I.",
            "group": 1902,
            "name": "10.1.1.78.2665",
            "keyword": "",
            "title": "Unwrapping of MR phase images using a markov random field model"
        },
        {
            "abstract": "The work reported in this thesis investigates the suitability and effectiveness of combining two different solving technologies and applying them to a set of discrete optimisation problems. The technologies considered are Integer Programming (IP) and Constraint Logic Programming using Finite Domains (CLP(FD)). Both approaches generally address the same set of problems and consist of complete algorithms based on a tree search of the solution space. Yet they are sufficiently different in the techniques each use and hence in the way the tree is constructed. Computational experiments are carried out using both technologies on a number of problems from four different optimisation applications. An in-depth analysis is made with respect to the solver and the problem characteristics that influence performance. From this, it is found that no single technology dominates in terms of performance. In order to find a technology which is more robust, having few instances of bad performance, various",
            "group": 1903,
            "name": "10.1.1.78.2669",
            "keyword": "",
            "title": "INTEGER PROGRAMMING, CONSTRAINT LOGIC PROGRAMMING AND THEIR COLLABORATION IN SOLVING DISCRETE OPTIMISATION PROBLEMS"
        },
        {
            "abstract": "Abstract\u2014This paper presents approaches for effectively computing all global minimizers of an objective function. The approaches include transformations of the objective function through the recently proposed deflection and stretching techniques, as well as a repulsion source at each detected minimizer. The aforementioned techniques are incorporated in the context of the particle swarm optimization (PSO) method, resulting in an efficient algorithm which has the ability to avoid previously detected solutions and, thus, detect all global minimizers of a function. Experimental results on benchmark problems originating from the fields of global optimization, dynamical systems, and game theory, are reported, and conclusions are derived. Index Terms\u2014Deflection technique, detecting all minimizers, dynamical systems, Nash equilibria, particle swarm optimization (PSO), periodic orbits, stretching technique. I.",
            "group": 1904,
            "name": "10.1.1.78.2832",
            "keyword": "",
            "title": "On the Computation of All Global Minimizers Through Particle Swarm Optimization"
        },
        {
            "abstract": "Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs sampler[9] provides general solution but is very slow, while other methods, such as Ncut[24] and graph cuts[4], [22], are computationally effective but only work for specific energy forms[17] and are not generally applicable. In this paper, we present a new inference algorithm that generalizes the Swendsen-Wang method[25] to arbitrary probabilities defined on graph partitions. We begin by computing graph edge weights, based on local image features. Then the algorithm iterates two steps. (i) Graph clustering: it forms connected components by cutting the edges probabilistically based on their weights. (ii) Graph relabeling: it selects one connected component and flips probabilistically, the coloring of all vertices in the component simultaneously. Thus it realizes the split, merge, and re-grouping of a \u201cchunk \u201d of the graph, in contrast to Gibbs sampler that flips a single vertex. We prove that this algorithm simulates ergodic and reversible Markov chain jumps in the space of graph partitions and is applicable to arbitrary posterior probabilities or energy functions defined on graphs. We demonstrate the algorithm on two typical problems in computer vision- image segmentation and stereo vision. Experimentally we show that it is 100-400 times faster in CPU time than the classical Gibbs sampler and 20-40 times faster then the DDM-CMC segmentation algorithm[27]. For stereo, we compare performance with graph cuts and belief propagation. We also show that our algorithm can automatically infer generative models and obtain satisfactory results (better than the graphic cuts or belief propagation) in the same amount of time.",
            "group": 1905,
            "name": "10.1.1.78.3679",
            "keyword": "",
            "title": "Generalizing swendsen-wang to sampling arbitrary posterior probabilities"
        },
        {
            "abstract": "Abstract. Graph matching is often used for image recognition. Different kinds of graph matchings have been proposed such as (sub)graph isomorphism or error-tolerant graph matching, giving rise to different graph similarity measures. A first goal of this paper is to show that these different measures can be viewed as special cases of a generic similarity measure introduced in [8]. This generic similarity measure is based on a non-bijective graph matching (like [4] and [2]) so that it is well suited to image recognition. In particular, over/under-segmentation problems can be handled by linking one vertex to a set of vertices. In a second part, we address the problem of computing this measure and we describe two algorithms: a greedy algorithm, that quickly computes sub-optimal solutions, and a reactive Tabu search algorithm, that may improve these solutions. Some experimental results are given. 1",
            "group": 1906,
            "name": "10.1.1.78.4434",
            "keyword": "",
            "title": "Reactive Tabu Search for Measuring Graph Similarity"
        },
        {
            "abstract": "A mobile wireless sensor network may be deployed to detect and track a large-scale physical phenomenon such as a pollutant spill in a lake. It may be called upon to provide a description of a contour characterized by an isoline of a specific concentration value. In this paper, we examine the problem of tracing a contour of a particular concentration within a bounded region of varying pollutant concentration using a network of mobile sensors. Since controlled movement of sensors within a given region is known to improve the overall quality of measurements by reducing sensing uncertainty, we explore various ways of guiding a set of mobile sensors optimally so as to surround and trace the contour. We formulate the contour estimation problem as a nonlinear multi-extremal optimization problem. We use accuracy and latency as performance metrics and show that in majority of the cases our proposed strategy based on collaboration of sensors delivers the best performance.",
            "group": 1907,
            "name": "10.1.1.78.5445",
            "keyword": "AlgorithmsDesignPerformanceMeasurement Keywords Mobile Wireless Sensor NetworksContour Level Set EstimationPerformance MetricsLatencyAccuracy",
            "title": "Contour estimation using collaborating mobile sensors"
        },
        {
            "abstract": "An execution environment consisting of virtual machines (VMs) interconnected with a virtual overlay network can use the naturally occurring traffic of an existing, unmodified application running in the VMs to measure the underlying physical network. Based on these characterizations, and characterizations of the application's own communication topology, the execution environment can optimize the execution of the application using application-independent means such as VM migration and overlay topology changes. In this paper, we demonstrate the feasibility of such free automatic network measurement by fusing the Wren passive monitoring and analysis system with Virtuoso's virtual networking system. We explain how Wren has been extended to support on-line analysis, and we explain how Virtuoso's adaptation algorithms have been enhanced to use Wren's physical network level information to choose VM-to-host mappings, overlay topology, and forwarding rules.",
            "group": 1908,
            "name": "10.1.1.78.6174",
            "keyword": "virtual machinesadaptive systemsnetwork measurementoverlay networks Free Network Measurement For Adaptive Virtualized Distributed Computing",
            "title": "Free network measurement for adaptive virtualized distributed computing"
        },
        {
            "abstract": "In this thesis, we study optimal anytime stochastic search algorithms (SSAs) for solving general constrained nonlinear programming problems (NLPs) in discrete, continuous and mixed-integer space. The algorithms are general in the sense that they do not assume differentiability or convexity of functions. Based on the search algorithms, we develop the theory of SSAs and propose optimal SSAs with iterative deepening in order to minimize their expected search time. Based on the optimal SSAs, we then develop optimal anytime SSAs that generate improved solutions as more search time is allowed. Our SSAs for solving general constrained NLPs are based on the theory of discrete con-strained optimization using Lagrange multipliers that shows the equivalence between the set of constrained local minima (CLMdn) and the set of discrete-neighborhood saddle points (SPdn). To implement this theory, we propose a general procedural framework for locating an SPdn. By incorporating genetic algorithms in the framework, we evaluate new constrained search algorithms: constrained genetic algorithm (CGA) and combined constrained simulated annealing and genetic algorithm (CSAGA).",
            "group": 1909,
            "name": "10.1.1.78.7216",
            "keyword": "",
            "title": "Optimal Anytime Search for Constrained Nonlinear Programming"
        },
        {
            "abstract": "Summary. Many applications such as information retrieval and classification, involve measuring graph distance or similarity, i.e., matching graphs to identify and quantify their common features. Different kinds of graph matchings have been proposed, giving rise to different graph similarity or distance measures. Graph matchings may be univalent \u2013 when each vertex is associated with at most one vertex of the other graph \u2013 or multivalent \u2013 when each vertex is associated with a set of vertices of the other graph. Also, graph matchings may be exact \u2013 when all vertex and edge features must be preserved by the matching \u2013 or error-tolerant \u2013 when some vertex and edge features may not be preserved by the matching. The first goal of this chapter is to propose a new graph distance measure based on the search of a best matching between the vertices of two graphs, i.e., a matching minimizing vertex and edge distance functions. This distance measure is generic in the sense that it allows both univalent and multivalent matchings and it is parameterized by vertex and edge distance functions defined by the user depending on the considered application. The second goal of this chapter is to show how to use this generic measure to model and to solve classical graph matching problems such as (sub-)graph isomorphism problem, error-tolerant graph matching, and nonbijective graph matching. 1",
            "group": 1910,
            "name": "10.1.1.78.8574",
            "keyword": "",
            "title": "A Generic Graph Distance Measure Based on Multivalent Matchings"
        },
        {
            "abstract": "There are several powerful solvers for satisfiability (SAT), such as wsat, Davis-Putnam, and relsat. However, in practice, the SAT encodings often have so many clauses that we exceed physical memory resources on attempting to solve them. This excessive size often arises because conversion to SAT, from a more natural encoding using quantifications over domains, requires expanding quantifiers. This suggests that we should \u201clift \u201d successful SAT solvers. That is, adapt the solvers to use quantified clauses instead of ground clauses. However, it was generally believed that such lifted solvers would be impractical: Partially, because of the overhead of handling the predicates and quantifiers, and partially because lifting would not allow essential indexing and caching schemes. Here we show that, to the contrary, it is not only practical to handle quantified clauses directly, but that lifting can give exponential savings. We do this by identifying certain tasks that are central to the implementation of a SAT solver. These tasks involve the extraction of information from the set of clauses (such as finding the set of",
            "group": 1911,
            "name": "10.1.1.78.8859",
            "keyword": "",
            "title": "Lifted search engines for satisfiability"
        },
        {
            "abstract": "In this paper a new graph partitioning problem is introduced, the relaxed k-way graph partitioning problem. It is close to the k-way, also called multi-way, graph partitioning problem, but with relaxed imbalance constraints. This problem arises in the air traffic control area. A new graph partitioning method is presented, the Fusion Fission, which can be used to resolve the relaxed k-way graph partitioning problem. The Fusion Fission method is compared to classical Multilevel packages and with a Simulated Annealing algorithm. The Fusion Fission algorithm and the Simulated Annealing algorithm, both require a longer computation time than the Multilevel algorithms, but they also find better partitions. However, the Fusion Fission algorithm partitions the graph with a smaller imbalance and a smaller cut than Simulated Annealing does.",
            "group": 1912,
            "name": "10.1.1.79.934",
            "keyword": "MultilevelMetaheuristicsFusion Fission",
            "title": "A new Method, the Fusion Fission, for the relaxed k-way graph partitioning problem, and comparisons with some Multilevel algorithms. \u2217"
        },
        {
            "abstract": "Abstract \u2013 Current FPGA placement algorithms estimate the routability of a placement using architecture-specific metrics, which limit their adaptability. A placement algorithm that is targeted to a class of architecturally similar FPGAs may not be easily adapted to other architectures. The subject of this paper is the development of routability-driven architecture-adaptive heuristics for FPGA placement. The first heuristic (called Independence) is a simultaneous place-and-route approach that tightly couples a simulated annealing placement algorithm with an architecture adaptive FPGA router. The results of our experiments on three different FPGA architectures show that Independence produces placements that are within-2.5 % to as much as 21% better than targeted placement tools. We also present a heuristic speed-up strategy for Independence that is based on the A * algorithm. The heuristic requires significantly less memory than previously published work, and is able to produce runtimes that are within-7 % to as much as 9 % better than targeted speed-up techniques. Memory improvements range between 30X and 140X. I.",
            "group": 1913,
            "name": "10.1.1.79.1174",
            "keyword": "",
            "title": "Architecture-Adaptive Routability-Driven Placement for FPGAs"
        },
        {
            "abstract": "View management, a relatively new area of research in Augmented Reality (AR) applications, is about the spatial layout of 2D virtual annotations in the view plane. This paper represents the first study in an actual AR application of a specific view management task: evaluating the placement of 2D virtual labels that identify information about real counterparts. Here, we objectively evaluated four different placement algorithms, including a novel algorithm for placement based on identifying existing clusters. The evaluation included both a statistical analysis of traditional metrics (e.g. counting overlaps) and an empirical user study guided by principles from human cognition. The numerical analysis of the three real-time algorithms revealed that our new cluster-based method recorded the best average placement accuracy while requiring only relatively moderate computation time. Measures of objective readability from the user study demonstrated that in practice, human subjects were able to read labels fastest with the algorithms that most quickly prevented overlap, even if placement wasn\u2019t ideal. 1.",
            "group": 1914,
            "name": "10.1.1.79.3306",
            "keyword": "",
            "title": "Evaluating Label Placement for Augmented Reality View Management"
        },
        {
            "abstract": "Microstructure is a feature of crystals with multiple symmetry-related energyminimizing states. Continuum models have been developed explaining microstructure as the mixture of these symmetry-related states on a ne scale to minimize energy. This article is a review of numerical methods and the numerical analysis for the computation of crystalline microstructure.",
            "group": 1915,
            "name": "10.1.1.79.4438",
            "keyword": "",
            "title": "to appear in Acta Numerica (1996) On the Computation of Crystalline"
        },
        {
            "abstract": "We report contrast detection, contrast increment, contrast masking, orientation discrimination, and spatial frequency discrimination thresholds for spatially localized stimuli at 4 \u00b0 of eccentricity. Our stimulus geometry emphasizes interactions among overlapping visual filters and differs from that used in previous threshold measurements, which also admits interactions among distant filters. We quantitatively account for all measurements by simulating a small population of overlapping visual filters interacting through divisive inhibition. We depart from previous models of this kind in the parameters of divisive inhibition and in using a statistically efficient decision stage based on Fisher information. The success of this unified account suggests that, contrary to Bowne [Vision Res. 30, 449 (1990)], spatial vision thresholds reflect a single level of processing, perhaps as early as primary visual cortex. \u00a9 2000 Optical Society of America [S0740-3232(00)02311-5] OCIS codes: 330.0330, 330.1800, 330.4060, 330.5510, 330.6100, 330.7310. 1.",
            "group": 1916,
            "name": "10.1.1.79.4574",
            "keyword": "",
            "title": "Revisiting spatial vision: toward a unifying model"
        },
        {
            "abstract": "The world is covered with millions of webcams, many transmit everything in their field of view over the Internet 24 hours a day. A web search finds public webcams in airports, intersections, classrooms, parks, shops, ski resorts, and more. Even more private surveillance cameras cover many private and public facilities. Webcams are an endless resource, but most of the video broadcast will be of little interest due to lack of activity. We propose to generate a short video that will be a synopsis of an endless video streams, generated by webcams or surveillance cameras. We would like to address queries like \u201cI would like to watch in one minute the highlights of this camera broadcast during the past day\u201d. The process includes two major phases: (i) An online conversion of the video stream into a database of objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user\u2019s query. To include maximum information in a short synopsis we simultaneously show activities that may have happened at different times. The synopsis video can also be used as an index into the original video stream. 1.",
            "group": 1917,
            "name": "10.1.1.79.4744",
            "keyword": "",
            "title": "Webcam synopsis: Peeking around the world"
        },
        {
            "abstract": "In this dissertation, we propose a general approach that can significantly reduce the com-plexity in solving discrete, continuous, and mixed constrained nonlinear optimization (NLP) problems. A key observation we have made is that most application-based NLPs have struc-tured arrangements of constraints. For example, constraints in AI planning are often lo-calized into coherent groups based on their corresponding subgoals. In engineering design problems, such as the design of a power plant, most constraints exhibit a spatial structure based on the layout of the physical components. In optimal control applications, constraints are localized by stages or time. We have developed techniques to exploit these constraint structures by partitioning the constraints into subproblems related by global constraints. Constraint partitioning leads to much relaxed subproblems that are significantly easier to solve. However, there exist global constraints relating multiple subproblems that must be resolved. Previous methods cannot exploit such structures using constraint partitioning because they cannot resolve inconsistent global constraints efficiently.",
            "group": 1918,
            "name": "10.1.1.79.4768",
            "keyword": "",
            "title": "Solving Nonlinear Constrained Optimization Problems Through Constraint Partitioning"
        },
        {
            "abstract": "We propose two efficient heuristics for minimizing the number of oligonucleotide probes needed for analyzing populations of ribosomal RNA gene (rDNA) clones by hybridization experiments on DNA microarrays. Such analyses have applications in the study of microbial communities. Unlike in the classical SBH (sequencing by hybridization) procedure, where multiple probes are on a DNA chip, in our applications we perform a series of experiments, each one consisting of applying a single probe to a DNA microarray containing a large sample of rDNA sequences from the studied population. The overall cost of the analysis is thus roughly proportional to the number of experiments, underscoring the need for minimizing the number of probes. Our algorithms are based on two well-known optimization techniques, i.e. simulated annealing and Lagrangian relaxation, and our preliminary tests demonstrate that both algorithms are able to find satisfactory probe sets for real rDNA data. Contact:",
            "group": 1919,
            "name": "10.1.1.79.5054",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "We introduce a new technique for counting models of Boolean satisfiability problems. Our approach incorporates information obtained from sampling the solution space. Unlike previous approaches, our method does not require uniform or near-uniform samples. It instead converts local search sampling without any guarantees into very good bounds on the model count with guarantees. We give a formal analysis and provide experimental results showing the effectiveness of our approach. 1",
            "group": 1920,
            "name": "10.1.1.79.5360",
            "keyword": "",
            "title": "From sampling to model counting"
        },
        {
            "abstract": "Abstract \u2014 In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment, motivated by real-world graphs such as the Web or Internet topology. Typically these models are designed to mimic particular properties observed in the graphs, such as power-law degree distribution or the small-world phenomenon. The mainstream approach to comparing models for these graphs has been somewhat subjective and very application dependent. Comparisons are often based on specific graph properties, without adequate justification for prioritizing some properties over others. We propose to use the Maximum Likelihood Estimation (MLE) principle to compare graph models: models are scored by the probability with which they generate the real data. Our methodology has several advantages. It is uniform, in that its definition does not presuppose any information about the data or the models. It is unambiguous, in that it yields a clearly defined score for each model, and thus an ordering of models. Moreover, it can be used to determine the best values of the parameters for a given model. We demonstrate the feasibility of the approach by designing and implementing algorithms computing the probability for four natural models: a power-law random graph model, a preferential attachment model, a small-world model, and a uniform random graph model. We tested our algorithms on three different snapshots of the AS-level Internet topology. We found that the preferential attachment model performed the best, closely followed by the power-law model, with the other two models lagging behind. An interesting aspect of the findings is the fact that the optimal parameters for the power-law models have not changed significantly over time, even though the size of the data has grown by an order of magnitude. I.",
            "group": 1921,
            "name": "10.1.1.79.5796",
            "keyword": "",
            "title": "A Uniform Methodology for Ranking Internet Topology Models"
        },
        {
            "abstract": "1. INTRODUCTION 1.1 Background One of the most important open problems in biochemistry is predicting the three dimensional structure of a protein from its amino acid sequence. Proteins are key molecules in all life processes. The function of proteins is directly related to their three dimensional structure. Thus, knowing and understanding the structure of proteins will have a tremendous impact on understanding biological processes, medical discoveries, and biotechnological inventions.",
            "group": 1922,
            "name": "10.1.1.79.6179",
            "keyword": "Key WordsProtein FoldingProtein ThreadingGenetic Algorithms",
            "title": ""
        },
        {
            "abstract": "This PostScript file was produced on host &quot;dimacs.rutgers.edu&quot;. Pass1: Page 1 Abstract. In this working paper I discuss some issues, with the aim of suggesting research top- ics for the DIMACS special year &quot;Combinatorial Optimization&quot;. I plan to focus on the unorthodox, hoping that at least one of the areas I describe below will gain new momen- tum as a result of this special year. A companion preprint, written by Martin Gr&quot;otschel and myself as a chapter of the Handbook of Combinatorics, describes the state of the art, concentrating on the well-established methods and models like polyhedral combinatorics, and should be available soon. Pass1: Page 1 0. Introduction A typical problem in combinatorial optimization is the following: given a combinato- rially defined set of 0-1 vectors (called feasible solutions), find one that maximizes a given linear objective function. A by now standard method (and often the method of choice) is to consider the convex hull of feasible solutions, find a description of it by linear inequalities, and then use linear programming to optimize the given objective function subject to these constraints. However, combinatorial optimization cannot be restricted to polyhedral combina- torics. I would like to give examples of combinatorial optimization problems that are of a different kind and methods that do not work through polyhedral combinatorics.",
            "group": 1923,
            "name": "10.1.1.79.6351",
            "keyword": "",
            "title": "Dvi file name: &quot;92-53.dvi&quot;. This PostScript file was produced on host &quot;dimacs.rutgers.edu&quot;."
        },
        {
            "abstract": "The vividly discussed ACM Computing Curriculum initiatives identify the CS Body of Knowledge. There are many online educational resources. As observed in [Cunningham02]: \u201eHowever, the quality of many if not most of this material is not clearly or immediately discernible, thus diminishing its value by requiring substantial evaluation and modification work to make it usable by educators. \u201c The refereed online resources should hierarchize the quality and establish the set of superior resources. In our opinion, for the quality of courseware we have to discuss the quality of curriculum again. The algorithmic strategies are the fundamental and universal tool for creating graphics and vision solutions. Their central role in the currently proposed CS Body of Knowledge seems to be underestimated. The paper introduces the context and discusses the importance giving the most beautiful examples from the practical classes.",
            "group": 1924,
            "name": "10.1.1.79.6823",
            "keyword": "",
            "title": "On Decreased Importance of Algorithmic Strategies in Current ACM Computing Curricula for Graphics & Visual Computing"
        },
        {
            "abstract": "The core/periphery structure is ubiquitous in network studies. The discrete version of the concept is that individuals in a group belong to either the core, which has a high density of ties, or to the periphery, which has a low density of ties. The density of ties between the core and the periphery may be either high or low. If the core/periphery structure is given a priori, then there is no problem in finding a suitable statistical test. Often, however, the structure is not given, which presents us with two problems, searching for the optimal core/periphery structure, and devising a valid statistical test to replace the one invalidated by the search. UCINET (Borgatti, Everett, and Freeman, 2002), the oldest and most trusted network program, gives incorrect answers in some simple cases for the first problem and does not address the second. This paper solves both problems with an adaptation of the Kernighan-Lin search algorithm, and with a permutation test incorporating this algorithm.",
            "group": 1925,
            "name": "10.1.1.79.6842",
            "keyword": "CorePeripheryPermutation testAlgorithm",
            "title": "Computing core/periphery structures and permutation tests for social relations data. Institute for Mathematical Behavioral Sciences. Paper 16"
        },
        {
            "abstract": "Abstract- In this paper the task of training subsymbolic systems is considered as a combinatorial optimization problem and solved with the heuristic scheme of the reactive tabu search (RTS). An iterative optimization process based on a \u201cmodified local search \u201d component is complemented with a meta-strategy to realize a discrete dynamical system that discourages limit cycles and the confinement of the search trajectory in a limited portion of the search space. The possible cycles are discouraged by prohibiting (i.e., making tabu) the execution of moves that reverse the ones applied in the most recent part of the search. The prohibition period is adapted in an automated way. The confinement is avoided and a proper exploration is obtained by activating a diversification strategy when too many configurations are repeated excessively often. The RTS method is applicable to nondifferentiable functions, is robust with respect to the random initialization, and effective in continuing the search after local minima. Three tests of the technique on feedforward and feedback systems are presented. I.",
            "group": 1926,
            "name": "10.1.1.79.7072",
            "keyword": "",
            "title": "Training Neural Nets with the Reactive Tabu Search"
        },
        {
            "abstract": "In this thesis, we present a new theory of discrete constrained optimization using Lagrange multipliers and an associated first-order search procedure (DLM) to solve general constrained optimization problems in discrete, continuous and mixed-integer space. The constrained problems are general in the sense that they do not assume the differentiability or convexity of functions. Our proposed theory and methods are targeted at discrete problems and can be extended to continuous and mixed-integer problems by coding continuous variables using a floating-point representation (discretization). We have characterized the errors incurred due to such discretization and have proved that there exists upper bounds on the errors. Hence, continuous and mixed-integer constrained problems, as well as discrete ones, can be handled by DLM in a unified way with bounded errors. Starting from new definitions on discrete neighborhoods, constrained local minima in discrete space, and new generalized augmented Lagrangian function, we have developed new discrete-space first-order necessary and sufficient conditions that are able to characterize all constrained local minima in discrete space. Our proposed first-order conditions show a",
            "group": 1927,
            "name": "10.1.1.79.7656",
            "keyword": "",
            "title": "The Theory and Applications of Discrete Constrained Optimization using Lagrange Multipliers"
        },
        {
            "abstract": "This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to nd a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and arti cial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the bene ts of randomization for the construction of oblique decision trees. 1.",
            "group": 1928,
            "name": "10.1.1.79.8022",
            "keyword": "",
            "title": "A system for induction of oblique decision trees"
        },
        {
            "abstract": "In most commercial Field-Programmable Gate Arrays (FPGAs) the number of wiring tracks in each channel is the same across the entire chip. A long-standing open question for both FPGAs and channelled gate arrays is whether or not some uneven distribution of routing tracks across the chip would lead to an area benefit. For example, many circuit designers intuitively believe that most congestion occurs near the center of a chip, and hence expect that having wider routing channels near the chip center would be beneficial. In this paper we determine the relative area-efficiency of several different routing track distributions. We first investigate FPGAs in which horizontal and vertical channels contain different numbers of tracks in order to determine if such a directional bias provides a density advantage. Secondly, we examine routing track distributions in which the track capacities vary from channel to channel. We compare the area-efficiency of these non-uniform routing architectures to that of an FPGA with uniform channel capacities across the entire chip. The main result is that the most area-efficient global routing architecture is one with uniform (or very nearly uniform) channel capacities across the entire chip in both the horizontal and vertical directions. This paper shows why this result, which is contrary to the intuition of many FPGA architects, is true. While a uniform routing architecture is the most area-efficient, several non-uniform and directionally-biased architectures are fairly area-efficient provided that appropriate choices are made for the pin positions on the logic blocks and the logic block array aspect ratio. 1",
            "group": 1929,
            "name": "10.1.1.79.8410",
            "keyword": "",
            "title": "Effect of the Prefabricated Routing Track Distribution on FPGA"
        },
        {
            "abstract": "",
            "group": 1930,
            "name": "10.1.1.79.9171",
            "keyword": "",
            "title": "Markov random field modeling for speech recognition"
        },
        {
            "abstract": "reconstruction",
            "group": 1931,
            "name": "10.1.1.80.1390",
            "keyword": "",
            "title": "Vector quantization and fuzzy ranks for image"
        },
        {
            "abstract": "The amount of captured video is growing with the increased numbers of video cameras, especially the increase of millions of surveillance cameras that operate 24 hours/day. Since video browsing and retrieval is time consuming, most captured video is never watched or examined. Video synopsis is an effective tool for browsing and indexing of such a video. It provides a short video representation, while preserving the essential activities of the original video. The activity in the video is condensed into a shorter period\r\nby simultaneously showing multiple activities, even when they originally occurred at different times. The synopsis video is also an index of the original video by pointing to the original time of each activity. Video synopsis can be applied to create a synopsis of endless video streams, as generated by webcams and by surveillance cameras. It can address queries like \u201cShow in one minute the synopsis of this camera broadcast during the past day.\u201d This process includes two major phases: 1) an online conversion of the endless video stream into a database of objects and activities (rather than frames) and 2) a response phase, generating the video synopsis as a response to the user\u2019s query.",
            "group": 1932,
            "name": "10.1.1.80.1825",
            "keyword": "",
            "title": "Nonchronological Video Synopsis and Indexing"
        },
        {
            "abstract": "Swarm algorithms such as Particle Swarm Optimization (PSO) are non-gradient probabilistic optimization algorithms that have been successfully applied to obtain global optimal solutions for complex problems such as multi-peak problems. However these algorithms have not been applied to complicated structural and mechanical optimization problems since local optimization capability is still inferior to general numerical optimization methods. This paper discusses new swarm metaphors that incorporate design sensitivities concerning objective and constraint functions and are applicable to structural and mechanical design optimization problems. Single-and multiobjective optimization techniques using swarm algorithms are combined with a sequential linear programming (SLP) method. In the proposed techniques, swarm optimization algorithms and SLP are conducted simultaneously. Finally, truss structure design optimization problems are solved by the proposed hybrid method to verify the optimization efficiency.",
            "group": 1933,
            "name": "10.1.1.80.2313",
            "keyword": "Swarm AlgorithmsGlobal OptimumMultiobjective OptimizationDesign SensitivityStructural Optimization",
            "title": "Swarm Optimization Algorithms Incorporating Design Sensitivities"
        },
        {
            "abstract": "Abstract \u2013 Genetic algorithms have been used successfully as a global optimization method when the search space is very large. To characterize and analyze the performance of genetic algorithms on a cluster of workstations, a parallel version of the GENESIS 5.0 was developed using PVM 3.3. This version, called VMGENESIS, was used to study a nonlinear leastsquares problem. Performance results show that linear speedups can be achieved if the basic distributed genetic algorithm is combined with a simple dynamic load-balancing mechanism. Results also show that the quality of search changes significantly with the number of processors involved in the computation and with the frequency of communication. 1",
            "group": 1934,
            "name": "10.1.1.80.2727",
            "keyword": "",
            "title": "A STUDY OF A NON-LINEAR OPTIMIZATION PROBLEM USING A DISTRIBUTED GENETIC ALGORITHM"
        },
        {
            "abstract": "Abstract\u2014One way to reduce the delay and area of field-programmable gate arrays (FPGA\u2019s) is to employ logic-cluster-based architectures, where a logic cluster is a group of logic elements connected with high-speed local interconnections. In this paper, we empirically evaluate FPGA architectures with logic clusters ranging in size from 1 to 20, and show that compared to architectures with size 1 clusters, architectures with size 8 clusters have 23 % less delay (30 % faster clock speed) and require 14 % less area. We also show that FPGA architectures with large cluster sizes can significantly reduce design compile time\u2014an increasingly important concern as the logic capacity of FPGA\u2019s rises. For example, an architecture that uses size 20 clusters requires seven times less compile time than an architecture with size 1 clusters. Index Terms\u2014Clustering, design, gate-array, high-performance, high-speed interconnect, performance tradeoffs.",
            "group": 1935,
            "name": "10.1.1.80.3559",
            "keyword": "",
            "title": "Speed and Area Tradeoffs in Cluster-Based FPGA Architectures"
        },
        {
            "abstract": "Simulated annealing with asymptotic convergence for nonlinear constrained optimization",
            "group": 1936,
            "name": "10.1.1.80.3587",
            "keyword": "",
            "title": "J Glob Optim DOI 10.1007/s10898-006-9107-z ORIGINAL PAPER"
        },
        {
            "abstract": "Abstract\u2014Reconfigurable hardware has been shown to provide an efficient compromise between the flexibility of software and the performance of hardware. However, even coarse-grained reconfigurable architectures target the general case and miss optimization opportunities present if characteristics of the desired application set are known. Restricting the structure to support a class or a specific set of algorithms can increase efficiency while still providing flexibility within that set. By generating a custom array for a given computation domain, we explore the design space between an ASIC and an FPGA. However, the manual creation of these customized reprogrammable architectures would be a labor-intensive process, leading to high design costs. Instead, we propose automatic reconfigurable architecture generation specialized to given application sets. This paper discusses configurable ASIC (cASIC) architecture generation that creates hardware on average up to 12.3x smaller than an FPGA solution with embedded multipliers and 2.2x smaller than a standard cell implementation of individual circuits. Index Terms\u2014Reconfigurable architecture, logic design and synthesis. 1",
            "group": 1937,
            "name": "10.1.1.80.4706",
            "keyword": "",
            "title": "Automatic design of area-efficient configurable ASIC cores"
        },
        {
            "abstract": "In this paper, an improved Two-Stage Simulated Annealing algorithm is presented for the Minimum Linear Arrangement Problem for Graphs. This algorithm integrates several distinguished features including an efficient heuristic to generate good quality initial solutions, a highly discriminating evaluation function, a special neighborhood function and an effective cooling schedule. The algorithm is evaluated on a set of 30 well-known benchmark instances of the literature and compared with several state-of-the-art algorithms, showing improvements of 17 previous best results.",
            "group": 1938,
            "name": "10.1.1.80.5573",
            "keyword": "Key wordsLinear ArrangementEvaluation FunctionHeuristicsSimulated",
            "title": "An Effective Two-Stage Simulated Annealing Algorithm for the Minimum Linear Arrangement Problem Abstract"
        },
        {
            "abstract": "Abstract\u2014Several methods are available for weight and shape optimization of structures, among which Evolutionary Structural Optimization (ESO) is one of the most widely used methods. In ESO, however, the optimization criterion is completely case-dependent. Moreover, only the improving solutions are accepted during the search. In this paper a Simulated Annealing (SA) algorithm is used for structural optimization problem. This algorithm differs from other random search methods by accepting non-improving solutions. The implementation of SA algorithm is done through reducing the number of finite element analyses (function evaluations). Computational results show that SA can efficiently and effectively solve such optimization problems within short search time. Keywords\u2014Simulated annealing, Structural optimization, Compliance, C.V. product.",
            "group": 1939,
            "name": "10.1.1.80.6003",
            "keyword": "",
            "title": "Simulated Annealing Application for Structural Optimization"
        },
        {
            "abstract": "In many complex optimization problems, the so-called stochastic optimization methods as genetic algorithms and simulated annealing have been applied successfully. However, these methods have drawbacks: simulated annealing is too sensitive to the choice of parameters and the canonical genetic algorithm is liable to suboptimal convergence. With the purpose of overcoming these drawbacks, a new stochastic optimization algorithm is introduced in this article. It is loosely inspired by nuclear collision reactions, particularly scattering (where an incident neutron is scattered by a collision with a target nucleus) and absorption (where the incident neutron is absorbed by the target nucleus). This algorithm was named \u201cParticle Collision Algorithm \u201d (PCA). PCA resembles in its structure simulated annealing. A basic difference is that, besides the number of iterations, it does not rely on user-defined parameters. First, the new algorithm is validated using three well known test functions. Then, PCA is applied to an engineering design optimization problem: a nuclear core design optimization. This problem consists in adjusting several reactor cell parameters, such as dimensions, enrichment and materials, in order to minimize the average peak-factor in a 3-enrichment-zone reactor, considering operational restrictions. Minimizing the average peak-factor is important for safety reasons, avoiding the risk of a fuel rod meltdown, and for economical reasons, so that the cycle length is maximized. The results are compared to those obtained in previous efforts using the canonical genetic algorithm and the niching genetic algorithm. PCA performs better than both methods, showing its potential for other applications.",
            "group": 1940,
            "name": "10.1.1.80.6308",
            "keyword": "Stochastic OptimizationMetaheuristics",
            "title": "A New Stochastic Optimization Algorithm based on a Particle Collision Metaheuristic"
        },
        {
            "abstract": "Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers of filled rank positions cause heterogeneity in the data. We propose a mixture approach for clustering of heterogeneous rank data. Rankings of different lengths can be described and compared by means of a single probabilistic model. A maximum entropy approach avoids hidden assumptions about missing rank positions. Parameter estimators and an efficient EM algorithm for unsupervised inference are derived for the ranking mixture model. Experiments on both synthetic data and real-world data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process. 1.",
            "group": 1941,
            "name": "10.1.1.80.7190",
            "keyword": "",
            "title": "Cluster Analysis of Heterogeneous Rank Data"
        },
        {
            "abstract": "This paper explores the association of shallow and selective global tree search with Monte Carlo in 9x9 go. This exploration is based on Olga and Indigo, two experimental Monte Carlo programs. We provide a min-max algorithm that iteratively deepens the tree until one move at the root is proved to be superior to the other ones. At each iteration, random games are started at leaf nodes to compute mean values. The progressive pruning rule and the min-max rule are applied to non terminal nodes. We set up experiments demonstrating the relevance of this approach. Indigo used this algorithm at the 8th Computer Olympiad held in Graz. 1",
            "group": 1942,
            "name": "10.1.1.80.7579",
            "keyword": "",
            "title": "Associating shallow and selective global tree search with monte carlo for 9x9 go"
        },
        {
            "abstract": "Abstract\u2014A hybrid multidimensional image segmentation algorithm is proposed, which combines edge and region-based techniques through the morphological algorithm of watersheds. An edge-preserving statistical noise reduction approach is used as a preprocessing stage in order to compute an accurate estimate of the image gradient. Then, an initial partitioning of the image into primitive regions is produced by applying the watershed transform on the image gradient magnitude. This initial segmentation is the input to a computationally efficient hierarchical (bottomup) region merging process that produces the final segmentation. The latter process uses the region adjacency graph (RAG) representation of the image regions. At each step, the most similar pair of regions is determined (minimum cost RAG edge), the regions are merged and the RAG is updated. Traditionally, the above is implemented by storing all RAG edges in a priority queue. We propose a significantly faster algorithm, which additionally maintains the so-called nearest neighbor graph, due to which the priority queue size and processing time are drastically reduced. The final segmentation provides, due to the RAG, one-pixel wide, closed, and accurately localized contours/surfaces. Experimental results obtained with two-dimensional/three-dimensional (2-D/3-D) magnetic resonance images are presented. Index Terms \u2014 Image segmentation, nearest neighbor region merging, noise reduction, watershed transform. I.",
            "group": 1943,
            "name": "10.1.1.80.8622",
            "keyword": "",
            "title": "Hybrid Image Segmentation Using Watersheds and Fast Region Merging"
        },
        {
            "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9 % over state-of-the-art systems on two established information extraction tasks. 1",
            "group": 1944,
            "name": "10.1.1.80.8813",
            "keyword": "",
            "title": "Incorporating non-local information into information extraction systems by gibbs sampling"
        },
        {
            "abstract": "We present an improved \u201ccooling schedule \u201d for simulated annealing algorithms for combinatorial counting problems. Under our new schedule the rate of cooling accelerates as the temperature decreases. Thus, fewer intermediate temperatures are needed as the simulated annealing algorithm moves from the high temperature (easy region) to the low temperature (difficult region). We present applications of our technique to colorings and the permanent (perfect matchings of bipartite graphs). Moreover, for the permanent, we improve the analysis of the Markov chain underlying the simulated annealing algorithm. This improved analysis, combined with the faster cooling schedule, results in an O(n 7 log 4 n) time algorithm for approximating the permanent of a 0/1 matrix. 1",
            "group": 1945,
            "name": "10.1.1.80.9086",
            "keyword": "",
            "title": "Abstract Accelerating Simulated Annealing for the Permanent and Combinatorial Counting Problems"
        },
        {
            "abstract": "Since its founding, NASA has been dedicated to the advancement of aeronautics and space science. The NASA Scientific and Technical Information (STI) Program Office plays a key part in helping NASA maintain this important role. The NASA STI Program Office is operated by Langley Research Center, the lead center for NASA\u2019s scientific and technical information. The NASA STI Program Office provides access to the NASA STI Database, the largest collection of aeronautical and space science STI in the world. The Program Office is also NASA\u2019s institutional mechanism for disseminating the results of its research and development activities. These results are published by",
            "group": 1946,
            "name": "10.1.1.81.620",
            "keyword": "",
            "title": "NASA/CR-2005-213522 A Comparison of Geographic Information Systems, Complex Networks, and Other Models for Analyzing Transportation Network Topologies"
        },
        {
            "abstract": "Abstract We show how to solve network combinatorial optimization problems using a randomized algorithm based on the cross-entropy method. The proposed algorithm employs an auxiliary random mechanism, like a Markov chain, which converts the original deterministic network into an associated stochastic one, called the associated stochastic network (ASN). Depending on a particular problem, we introduce the randomness in ASN by making either the nodes or the edges of the network random. Each iteration of the randomized algorithm based on the ASN involves the following two phases: 1. Generation of trajectories using the random mechanism and calculation of the associated path (objective functions) and some related quantities, such as rare-event probabilities. 2. Updating the parameters associated with the random mechanism, like the probability matrix P of the Markov chain, on the basis of the data collected at first phase. We show that asymptotically the matrix P converges to a degenerated one P \\Lambda d in the sense that at each row of the MC P \\Lambda d only a single element equals unity, while the remaining elements in each row are zeros. Moreover, the unity elements of each row uniquely define the optimal solution. We also show numericaly that for a finite sample the algorithm converges with very high probability to a very small subset of the optimal values. We finally show that the proposed method can also be used for noisy networks, namely where the deterministic edge distances in the network are replaced by random variables with unknown 0",
            "group": 1947,
            "name": "10.1.1.81.988",
            "keyword": "Combinatorial OptimizationCross entropy",
            "title": "cfl2000 Kluwer Academic Publishers Combinatorial Optimization, Cross-Entropy, Ants and Rare Events"
        },
        {
            "abstract": "These notes provide an introduction to Markov chain Monte Carlo methods that are useful in both Bayesian and frequentist statistical inference. Such methods have revolutionized what can be achieved computationally, primarily but not only in the Bayesian paradigm. The account begins by describing ordinary Monte Carlo methods, which, in principle, have exactly the same goals as the Markov chain versions but can rarely be implemented. Subsequent sections describe basic Markov chain Monte Carlo, founded on the Hastings algorithm and including both the Metropolis method and the Gibbs sampler as special cases, and go on to discuss more recent developments. These include Markov chain Monte Carlo p\u2013values, the Langevin\u2013Hastings algorithm, auxiliary variables techniques, perfect Markov chain Monte Carlo via coupling from the past, and reversible jumps methods for target spaces of varying dimensions. Specimen applications, drawn from several different disciplines, are described throughout the notes. Several of these appear for the first time. All computations use APL as the programming language, though this is not necessarily a recommendation! The author welcomes comments and criticisms.",
            "group": 1948,
            "name": "10.1.1.81.1116",
            "keyword": "Auto\u2013logistic distributionAuxiliary variablesBayesian computationCompeting risksContingency tablesExact p\u2013valuesGibbs samplerHastings algorithmHidden Markov modelsImportance samplingIsing modelLangevin diffusionMarkov chain Monte CarloMarkov random fieldsMaximum likelihood estimationMetropolis methodMixture modelsNoisy binary channelPerfect simulationPoint processesRandom graphsRasch modelReversibilityReversible jumpsSimulated annealingSocial networksSpatial statisticsSwendsen\u2013Wang algorithmWeibull distribution 1 The computational challenge",
            "title": "Markov chain Monte Carlo for statistical inference"
        },
        {
            "abstract": "  Phase contrast magnetic resonance angiography (PC-MRA) is a non-invasive method for 3D vessel delineation, which for each voxel not only provides measurement of speed (conveyed as a speed image), but also gives a three-component estimate of flow direction (in the form of phase images). In this thesis, we present a new approach to reconstructing vessels and aneurysms from PC-MRA, and demonstrate how speed and flow coherence information extracted from a PC-MRA dataset can be combined for detecting and reconstructing normal vessels and aneurysms with relatively low flow rate and low signal-to-noise ratio (SNR). We propose to use a Maxwell-Gaussian mixture density to model the background signal and combine this with a uniform distribution for modelling vascular signal to give a Maxwell-Gaussian-uniform (MGU) mixture model of speed image intensity. The MGU model param-eters are estimated by the Expectation-Maximisation (EM) algorithm. It is shown that the Maxwell-Gaussian mixture distribution models the background signal more accurately than a Maxwell distribution. Although the MGU model works satisfactorily in classifying the back-",
            "group": 1949,
            "name": "10.1.1.81.2573",
            "keyword": "",
            "title": "Vessel and aneurysm reconstruction using speed and flow coherence information in phase contrast magnetic resonance angiograms "
        },
        {
            "abstract": "We consider the following map labelling problem: given distinct points p 1, p 2,..., p n in the plane, and given \u03c3, find a maximum cardinality set of pairwise disjoint axis-parallel \u03c3\u00d7 \u03c3 squares Q1, Q2,..., Qr. This problem reduces to that of finding a maximum cardinality independent set in an associated graph called the conflict graph. We describe several heuristics for the maximum cardinality independent set problem, some of which use an LP solution as input. Also, we describe a branch-and-cut algorithm to solve it to optimality. The standard independent set formulation has an inequality for each edge in the conflict graph which ensures that only one of its endpoints can belong to an independent set. To obtain good starting points for our LP-based heuristics and good upper bounds on the optimal value for our branch-and-cut algorithm we replace this set of inequalities by the set of inequalities describing all maximal cliques in the conflict graph. For this strengthened formulation we also generate lifted odd hole inequalities and mod-k inequalities. We present a comprehensive computational study of solving map labelling instances for sizes up to n = 950 to optimality. Previously, optimal solutions to instances of size n \u2264 300 have been reported on in the literature. By comparing against these optimal solutions we show that our heuristics are capable of producing near-optimal solutions for large-scale instances. ",
            "group": 1950,
            "name": "10.1.1.81.2910",
            "keyword": "",
            "title": "Algorithms for Maximum Independent Set Applied to Map Labelling"
        },
        {
            "abstract": "has been read by each member of the following supervisory committee and by majority vote has been found to be satisfactory.",
            "group": 1951,
            "name": "10.1.1.81.3153",
            "keyword": "",
            "title": "ADAPTIVE, NONPARAMETRIC MARKOV MODELS AND INFORMATION-THEORETIC METHODS FOR IMAGE RESTORATION AND SEGMENTATION"
        },
        {
            "abstract": "In this paper, we present a new technique for statistical modeling of speech segments based on Markov random fields. Classical and multi-stream HMMs are particular cases of this more general family of models. However, the Random Field Model (RFM) proposed here can be seen as an extension of the multiband HMM in which interactions between the frequency bands have been added. In a first experiment, samples are drawn from different models and compared to real observations. This experiment shows that the RFM is able to produce realistic samples but a single HMM still performs better. Isolated word recognition experiments stress the fact that more work must be done on the RFM in order to reach the performances of classical hidden Markov modeling techniques. For the moment, the RFM parameters are estimated using a heuristic. We believe that a real maximum likelihood parameter estimation algorithm should improve the results. The main advantage of this new model is that it can easily be extended since a model is defined by some local interactions and the Gibbs potential functions associated to those interactions. 1",
            "group": 1952,
            "name": "10.1.1.81.3630",
            "keyword": "",
            "title": "TOWARD MARKOV RANDOM FIELD MODELING OF SPEECH"
        },
        {
            "abstract": "Abstract\u2014This paper proposes a novel probabilistic variational method with deterministic annealing for the maximum a posteriori (MAP) estimation of complex stochastic systems. Since the MAP estimation involves global optimization, in general, it is very difficult to achieve. Therefore, most probabilistic inference algorithms are only able to achieve either the exact or the approximate posterior distributions. Our method constrains the mean field variational distribution to be multivariate Gaussian. Then, a deterministic annealing scheme is nicely incorporated into the mean field fix-point iterations to obtain the optimal MAP estimate. This is based on the observation that when the covariance of the variational Gaussian distribution approaches to zero, the infimum point of the Kullback-Leibler (KL) divergence between the variational Gaussian and the real posterior will be the same as the supreme point of the real posterior. Although global optimality may not be guaranteed, our extensive synthetic and real experiments demonstrate the effectiveness and efficiency of the proposed method. Index Terms\u2014Mean field variational analysis, deterministic annealing, maximum a posteriori estimation, graphical model, Markov network. 1",
            "group": 1953,
            "name": "10.1.1.81.4088",
            "keyword": "",
            "title": "Variational Maximum A Posteriori by Annealed Mean Field Analysis"
        },
        {
            "abstract": "The power of video over still images is the ability to represent dynamic activities. But video browsing and retrieval are inconvenient due to inherent spatio-temporal redundancies, where some time intervals may have no activity, or have activities that occur in a small image region. Video synopsis aims to provide a compact video representation, while preserving the essential activities of the original video. We present dynamic video synopsis, where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times. For example, we can create a \u201dstroboscopic movie\u201d, where multiple dynamic instances of a moving object are played simultaneously. This is an extension of the still stroboscopic picture. Previous approaches for video abstraction addressed mostly the temporal redundancy by selecting representative key-frames or time intervals. In dynamic video synopsis the activity is shifted into a significantly shorter period, in which the activity is much denser. Video examples can be found online in",
            "group": 1954,
            "name": "10.1.1.81.4112",
            "keyword": "",
            "title": "Making a long video short: Dynamic video synopsis"
        },
        {
            "abstract": "This paper introduces a novel energy minimization method, namely iterated cross entropy with partition strategy (ICEPS), into the Markov random field theory. The solver, which is based on the theory of cross entropy, is general and stochastic. Unlike some popular optimization methods such as belief propagation (BP) and graph cuts (GC), ICEPS makes no assumption on the form of objective functions and thus can be applied to any type of Markov random field (MRF) models. Furthermore, compared with deterministic MRF solvers, it achieves higher performance of finding lower energies because of its stochastic property. We speed up the original cross entropy algorithm by partitioning the MRF site set and assure the effectiveness by iterating the algorithm. In the experiments, we apply ICEPS to two MRF models for medical image segmentation and show the aforementioned advantages of ICEPS over other popular solvers such as iterated conditional modes (ICM) and GC. Index Terms \u2014 Markov random fields, energy minimization, MRF solvers, cross entropy, image segmentation 1.",
            "group": 1955,
            "name": "10.1.1.81.4921",
            "keyword": "",
            "title": "MARKOV RANDOM FIELD ENERGY MINIMIZATION VIA ITERATED CROSS ENTROPY WITH PARTITION STRATEGY"
        },
        {
            "abstract": "",
            "group": 1956,
            "name": "10.1.1.81.5037",
            "keyword": "1.1 Supervised and Unsupervised Learning............. 3",
            "title": "Contents"
        },
        {
            "abstract": "Abstract We discuss a new paradigm for supervised learning that aims at improving the efficiency of neural network training procedures: active learning. The starting point for active learning is the observation that the traditional approach of randomly selecting training samples leads to large, highly redundant training sets. This redundancy is not always desirable. Especially if the acquisition of training data is expensive, one is rather interested in small, informative training sets. Such training sets can be obtained if the learner is enabled to select those training data that he or she expects to be most informative. In this case, the learner is no longer a passive recipient of information but takes an active role in the selection of the training data. In our contribution, we review recent research on active learning. We discuss the main approaches and give experimental results to demonstrate the power of active learning. 1 Introduction In supervised learning, we are interested in training a student on a set of input-output pairs generated by an unknown target function in such a way that the student does not only remember these samples but is capable of making sensible predictions of the outputs of previously unseen samples as well, i. e. the student should be able to generalize well. The final generalization ability of the student depends on a number of factors among them the architecture of the student, the training procedure and the training data. In recent years, much research effort has been directed towards the optimization of the learning process with regard to both the learning efficiency and generalization performance. This includes algorithms that aim at the adaptive optimization of the learning architecture, e.g. [1, 2, 3], as well as algorithms that improve existing training procedures, e.g. [4]. It was only recently that advanced techniques for the selection of the training data moved into the focus of interest.",
            "group": 1957,
            "name": "10.1.1.81.5081",
            "keyword": "",
            "title": "Active Learning in Neural Networks"
        },
        {
            "abstract": "Ligand-protein interactions are central and ubiquitous phenomena in a variety of biological processes from enzymatic catalysis to signal transduction. A theoretical understanding of",
            "group": 1958,
            "name": "10.1.1.81.5631",
            "keyword": "",
            "title": "Binding mode prediction for a flexible ligand in a flexible pocket using multi-conformation simulated annealing pseudo crystallographic refinement"
        },
        {
            "abstract": "In the last decades significant changes in the manufacturing environment have been noticed: moving from a local economy towards a global economy, with markets asking for products with high quality at lower costs, highly customised and with short life cycle. In this environment, the manufacturing enterprises, to avoid the risk to lose competitiveness, search to answer more closely to the customer demands, by improving their flexibility and agility, while maintaining their productivity and quality. Actually, the dynamic response to emergence is becoming a key issue, due to the weak response of the traditional manufacturing control systems to unexpected disturbances, mainly because of the rigidity of their control architectures. In these circumstances, the challenge is to develop manufacturing control systems with autonomy and intelligence capabilities, fast adaptation to the environment changes, more robustness against the occurrence of disturbances, and easier integration of manufacturing resources and legacy systems. Several architectures using emergent concepts and technologies have been proposed, in particular those based in the holonic manufacturing paradigm. Holonic manufacturing is a paradigm based in the ideas of the philosopher Arthur Koestler,",
            "group": 1959,
            "name": "10.1.1.81.8322",
            "keyword": "",
            "title": "ii An Agile and Adaptive Holonic Architecture for Manufacturing Control"
        },
        {
            "abstract": "In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment, motivated by real-world graphs such as the Web or Internet topology. Typically these models are designed to mimic particular properties observed in the graphs, such as power-law degree distribution or the small-world phenomenon. The mainstream approach to comparing models for these graphs has been somewhat subjective and very application dependent \u2014 comparisons are often based on ad hoc graph properties. We use the Minimum Description Length principle to compare graph models: models are scored based on the degree of compression that they achieve on real data. This principle is popular across fields for various types of model selection because it is objective and not application specific. Unfortunately, computing this metric is usually a daunting algorithmic task, especially for existing models that were not designed with this metric in mind. To illustrate the feasibility of our approach, we design and implement sophisticated algorithms for computing the description length for four natural models: a power-law random graph model, a preferential attachment model, a small-world model, and a uniform random graph model. Based on experiments on three snapshots of the Internet topology graph, we find that the preferential attachment model ranks highest, while the uniform random graph model performs the worst. We hope that this metric will enable a more objective model comparison and the development of improved models. Categories and Subject Descriptors D.2.8 [Software Engineering]: Metrics\u2014complexity measures, performance measures; F.2 [Theory of Computation]:",
            "group": 1960,
            "name": "10.1.1.81.8473",
            "keyword": "graph modelsMDL",
            "title": "Graph Model Selection using the Minimum Description Length Principle"
        },
        {
            "abstract": "Abstract. We introduce a novel algorithm to cluster and order markers on a genetic linkage map, which is based on several theoretical observations. In most cases, the true order of the markers in a linkage group can be efficiently computed from the minimum spanning tree of a graph. Our empirical studies confirm our theoretical observations, and show that our algorithm consistently outperforms the best available tool in the literature, in particular when the genotyping data is noisy or in case of missing observations. 1",
            "group": 1961,
            "name": "10.1.1.81.9403",
            "keyword": "",
            "title": "Efficient and Accurate Construction of Genetic Linkage Maps from Noisy and Missing Genotyping Data"
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN), and propose a general paradigm for multi-way circuit partitioning based on dual net transformation. Given a netlist we first compute a K-way partitioning of nets based on the HDN representation, and then transform the K-way net partition into a K-way module partitioning solution. The main contri-bution of our work is in the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding function between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present an efficient solution to the K-MC problem based on network flow computation. We apply our dual transformation paradigm to the well-known K-way FM partitioning algorithm (K-FM) and show that the new algorithm, named K-DualFM, reduces the net cutsize by 20 % to 31 % compared with the K-FM algorithm. We also apply the same paradigm to the K-MFFC-FM algorithm, a K-FM algorithm based on maximum fanout-free cone (MFFC) clustering reported in [10], and show that the resulting algorithm, K-DualMFFC-FM reduces the net cutsize by 15 % to 26 % compared with K-MFFC-FM. Furthermore, we compare the K-DualFM algorithm with EIG1[18] and Par-aboli [26], two recently proposed spectral-based bipartitioning algorithms. We showed that K-DualFM reduces the net cutsize by 56 % on average when compared with EIG1 and produces comparable results with Paraboli. 1.",
            "group": 1962,
            "name": "10.1.1.81.9477",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "In this paper, westudy methods for developing general heuristics in order to solve problems in knowledge-lean application domains with a large and possibly infinite problem space. Our approach is based on genetic learn-ing that generates heuristics, tests each to a limited extent, and prunes unpromising ones from further consideration. We summarize possible sources of anomalies in performance evaluation of heuristics along with our methods for coping with them. Based on the heuristics learned, we propose and study methods for generalizing heuristics to unlearned problem domains. Our method uses a new statistical measure called probability of win, which assesses the performance of heuristics in a distribution-independent manner. Tovalidate our approach, we show experimen-tal results on generalizing heuristics learned for sequential circuit testing, VLSI cell placement and routing, and branch-and-bound search. We show that generalization can lead to new and robust heuristics that perform well across problem instances of different characteristics in an application domain.",
            "group": 1963,
            "name": "10.1.1.81.9825",
            "keyword": "performance evaluationsequential circuit testingVLSI placement and routing",
            "title": "ABSTRACT GENERALIZATION OF HEURISTICS LEARNED IN GENETICS-BASED LEARNING"
        },
        {
            "abstract": "The relevance of computer science to economic planning is defended. An algorithm for constructing a balanced economic plan is presented and found to be of time order Nlog(N) in the complexity of the economy. The time taken to perform a complete plan optimization in natural units for a whole economy is estimated. Plans and computational costs This is a paper by a computer scientist that disputes an economic hypothesis. The hypothesis rests upon certain premises about computation and is therefore open to criticism from outside economics. The hypothesis is that: the complexities involved with performing the calculations required to optimize an economic plan in natural units are so great that they are beyond the realms of feasibility and that in consequence all rational economic calculation must proceed by means of the intermediary of prices. The hypothesis is an old one. It was originally proposed by Von Mises (1936) between the wars. In recent years it has been ably restated by Nove (1983) and has gained circumstantial support from recent developments in the USSR. It is argued here that computer science has something relevant to say on the topic. It is then argued that the difficulties experienced in socialist economies with planning in natural units are partly a consequence of the particular formalism used to express the problem. Finally, it is argued that techniques developed in artificial intelligence can be applied to solve planning problems with economically acceptable computational costs. In what follows it is assumed that planning authorities start out with a pre-given objective in terms of net output. That is to say they wish to achieve a specified bundle of output flows at the end of the plan period. If possible they would like this bundle of outputs to be exceeded, but excesses of some commodities are not desired if they result in shortfalls in others. In other words these are the classic assumptions proposed by Kantorovich (Ellman 1972). We do not concern ourselves with how these plan requirements are arrived at. It is further assumed that the plan authorities have effective property rights over all means of production and can allocate them between different productive activities in order to achieve plan goals.",
            "group": 1964,
            "name": "10.1.1.83.136",
            "keyword": "",
            "title": "Application of artificial intelligence techniques to economic planning"
        },
        {
            "abstract": "Simulated annealing has been one of the most popular stochastic optimization methods used in the VLSI CAD field in the past two decades for handling NP-hard optimization problems. Recently, a new Monte Carlo and optimization method, named dynamic weighting Monte Carlo [WL97], has been introduced and successfully applied to the traveling salesman problem, neural network training [WL97], and spin-glasses simulation [LW98]. In this paper, we have successfully applied dynamic weighting Monte Carlo algorithm to the constrained floorplan design with consideration of both area and wirelength minimization. Our application scenario is the constrained floorplan design for mixed signal MCMs, where we need to place all the analog modules together in groups so that they can share common power and ground planes, which are separate from those used by the digital modules. Our experiments indicate that the dynamic weighting Monte Carlo algorithm is very effective for constrained floorplan optimization. It outperforms the simulated annealing for a real mixed signal MCM design by \u00a3\u00a5\u00a4\u00a7\u00a6\u00a9\u00a8\ufffd \ufffd in wirelength, while gets slight area improvement. This is the first work adopting the dynamic weighting Monte Carlo optimization method for solving VLSI CAD problems. We believe that this method has applications to many other VLSI CAD optimization problems. 1",
            "group": 1965,
            "name": "10.1.1.83.575",
            "keyword": "",
            "title": "Dynamic Weighting Monte Carlo for Constrained Floorplan Designs in Mixed Signal Application"
        },
        {
            "abstract": "With several commercial tools becoming available, the high-level synthesis of applicationspeci c integrated circuits is nding wide spread acceptance in VLSI industry today. Existing tools for synthesis focus on optimizing cost while meeting performance constraints or vice versa. Yet, veri cation and testing have emerged as major concerns of IC vendors since the repurcussions of chips being recalled are far-reaching. In this paper, we concentrate on the synthesis of testable RTL designs using techniques from Arti cial Intelligence. We present an adaptive version of the well known Simulated Annealing algorithm and describe its application to a combinatorial optimization problem arising in the high-level synthesis of digital systems. The conventional annealing algorithm was conceived with a single perturb operator which applies a small modi cation to the existing solution to derive a new solution. The Metropolis criterion is then used to accept or reject the new solution. In some of the complex optimization problems arising in VLSI design, a set of perturb functions become necessary, leading to the question of how to select a particular function for modifying the current system con guration. The adaptive algorithm described here uses the concept of reward and penalty from the theory of learning automata to \"learn\" to apply the appropriate perturb function. We have applied both the conventional simulated annealing algorithm and the adaptive simulated annealing algorithm to the problem of testability-oriented datapath synthesis for signal processing applications. Our experimental results indicate that the adaptive algorithm can yield better solutions in shorter time.",
            "group": 1966,
            "name": "10.1.1.83.860",
            "keyword": "Key Words Test SynthesisAdaptive AlgorithmsSimulated AnnealingRTL Synthesis This research was conducted when Sumit Gupta and Akshay Jajoowere students of IIT Delh",
            "title": "Jajoo Synthesis of Testable RTL Designs using Adaptive Simulated Annealing Algorithm"
        },
        {
            "abstract": "Abstract. We present a simple model of distributed multi-agent multi-issued contract negotiation for open systems where interactions are competitive and information is private and not shared. We then investigate via simulations two different approximate optimization strategies and quantify the contribution and costs of each towards the quality of the solutions reached. To evaluate the role of knowledge the obtained results are compared to more cooperative strategies where agents share more information. Interesting social dilemmas emerge that suggest the design of incentive mechanisms. 1",
            "group": 1967,
            "name": "10.1.1.83.1139",
            "keyword": "",
            "title": "Simple Negotiating Agents in Complex Games: Emergent Equilibria and Dominance of Strategies"
        },
        {
            "abstract": "This dissertation was presented by",
            "group": 1968,
            "name": "10.1.1.83.1214",
            "keyword": "TABLE OF CONTENTS PREFACE........................................... ix",
            "title": "ii PLANNING IN HYBRID STRUCTURED STOCHASTIC DOMAINS"
        },
        {
            "abstract": "Abstract. An inverse minimum spanning tree problem is to make the least modification on the edge weights such that a predetermined spanning tree is a minimum spanning tree with respect to the new edge weights. In this paper, the inverse minimum spanning tree problem with stochastic edge weights is investigated. The concept of \u03b1-minimum spanning tree is initiated, and subsequently an \u03b1-minimum spanning tree model and a probability maximization model are presented to formulate the problem according to different decision criteria. In order to solve the two stochastic models, hybrid genetic algorithms and hybrid simulated annealing algorithms are designed and illustrated by some computational experiments.",
            "group": 1969,
            "name": "10.1.1.83.1419",
            "keyword": "minimum spanning treeinverse optimizationstochastic programminggenetic algorithmsimulated annealing",
            "title": "Models and hybrid algorithms for inverse minimum spanning tree problem with stochastic edge weights \u2217"
        },
        {
            "abstract": "This paper introduces a maximal allowable workload task allocation problem (MAW) to address a class of distributed real-time systems that have unpredictable execution times due to varying workload. It is known that worst case execution time analysis is not well suited for these systems. This problem seeks to maximize the upper bound of permissible workload in an allocation so that it can sustain major workload fluctuations without the need for reallocation. An answer to the problem is significant in that it increases robustness of the system by reducing expensive reallocation costs such as process migration, which degrades system performance. Previously in [12], we presented and compared several heuristic algorithms experimentally, including simulated annealing, hill-climbing and random search. This paper expands the set of algorithms to include tabu search, genetic algorithm, dynamic programming, and optimal branch-and-bound. The main contribution of this paper is the performance comparisons among these various well known heuristic and optimal algorithms applied to the MAW problem. Through simulation experiments, relative solution quality and time complexity of the algorithms were evaluated and tradeoffs were revealed. 1",
            "group": 1970,
            "name": "10.1.1.83.2031",
            "keyword": "",
            "title": "Experimental Comparison of Heuristic and Optimal Resource Allocation Algorithms for Maximizing Allowable Workload in Dynamic, Distributed Real-Time Systems"
        },
        {
            "abstract": "convex optimization in the bandit setting:",
            "group": 1971,
            "name": "10.1.1.83.2298",
            "keyword": "",
            "title": "gradient descent without a"
        },
        {
            "abstract": "Abstract \u2014 Multihoming has been used by stub networks for several years as a form of redundancy, improving the availability of Internet access. More recently, Intelligent Route Control (IRC) products allow multihomed networks to dynamically switch parts of their egress or ingress traffic between ISPs, also improving cost and performance. IRC products assume that the set of upstream ISPs is given and fixed. Typically, however, a multihomed network has several ISP choices and the actual selection of ISPs can significantly affect cost, availability, and performance. In the first part of this work, we develop a methodology to select the best set of upstream ISPs, optimizing monetary cost and availability. Our results, based on measurements of actual Internet traffic and topology, show that the proposed algorithm selects the best possible set of ISPs in terms of resiliency to inter-AS single-link failures. The algorithm also performs well in the presence of double or triple link failures. In the second part of this work, we focus on the egress path selection problem. Specifically, we propose a stochastic search algorithm, based on simulated annealing, to allocate the network\u2019s egress traffic between upstream ISPs. The objectives are to minimize cost, also ensuring that the selected paths to the major destinations of egress traffic are congestion-free. Simulation results show that the proposed algorithm performs very well in meeting the previous objectives, when congestion-free paths exist.",
            "group": 1972,
            "name": "10.1.1.83.2357",
            "keyword": "MultihomingIntelligent RoutingPath DiversityNetwork MeasurementsSimulated Annealing",
            "title": "ISP and Egress Path Selection for Multihomed Networks"
        },
        {
            "abstract": "",
            "group": 1973,
            "name": "10.1.1.83.2592",
            "keyword": "a Literature Review on Sequence-Dependent Scheduling",
            "title": "THE DYNAMIC, RESOURCE-CONSTRAINED SHORTEST PATH PROBLEM ON AN ACYCLIC GRAPH WITH APPLICATION IN COLUMN GENERATION AND A LITERATURE REVIEW ON SEQUENCE-DEPENDENT SCHEDULING Approved by:"
        },
        {
            "abstract": "fault-tolerant parallel heuristic for assignment problems",
            "group": 1974,
            "name": "10.1.1.83.2650",
            "keyword": "",
            "title": "A"
        },
        {
            "abstract": "Partitioning is aparticularly interesting problem in synthesis of the System-on-a-Chip(SoC) since that heterogeneous hardware/software components exist in the same design. There is not aclear winner among partitioning methods, though research in this area has been conducted for many years, partly due to the intrinsiccomplexity ofthe problem[1]. Themajorityofexistingpartitioningmethodsassumesasingleprocessor(software)andanASIC(hardware) on the same chip. However, future computing systems may have many different types of computing components \u2014ASIC, many different processors, FPGA and other programmable hardware. Thus, novel partitioning methods are needed to determine the computing components where each portion of the application should run. In order to quickly and accurately perform this partitioning, estimation engine is also needed. The estimation engine takes aportion of the application and determines different metrics, such as power,speed,die area,utilization,andreliability,foreach computingelement. In synthesis of embedded systems, both structural and functional partitioning approaches can be used. However, functional partitioning is considered better for SoC since it permits better hardware/software solutions withdifferentmetricsandconstraintsandshorter synthesisruntime[10]. Inthisproject,functionalpartitioning forSoC willbeinvestigated.",
            "group": 1975,
            "name": "10.1.1.83.2845",
            "keyword": "",
            "title": "ECE 594K \u2013 Embedded System Design HW #2: Project Proposal Application Partitioning/Estimation for System-on-a-Chip"
        },
        {
            "abstract": "Clustering is the unsupervised classi cation of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines \ufffd this re ects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a di cult problem combinatorially and di erences in assumptions and contexts in di erent communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.",
            "group": 1976,
            "name": "10.1.1.83.3276",
            "keyword": "I.2.6 [Arti cial IntelligenceLearning|Knowledge Acquisition",
            "title": "Computer Society."
        },
        {
            "abstract": "To my parents Medine & Emin Acikgoz and to my dear husband Sermet iii ACKNOWLEDGEMENTS Completing a PhD is truly a long journey with many ups and downs and if it was not for the great people surrounding me, I wouldn\u2019t have been able to conclude this endeavor. I must first express my gratitude toward my advisor Dr. Carlo Bottasso. His enthusiasm, hard work and brilliant mind have set an example which I wish to match someday. I am so fortunate to have had the opportunity to work with him. He was considerate and very supportive all along, I really appreciate all he has done for me both professionally and personally. I would like to thank to my committee members Dr. Lakshmi Sankar, Dr. Stephen Ruffin, Dr. Luca Dieci and Dr. Zvi Rusak for their time, effort and enlightening suggestions which were important in improving the quality of this research. I am grateful to Dr. Jeff Jagoda for supporting me through my graduate studies and financial problems. My special thanks goto Dr. Sankar whose door was always open when I needed his valuable advises. His courteous personality encouraged me to ask his help during my hard times at Georgia Tech. I would also like to thank to my previous lab mates who are scattered all around the",
            "group": 1977,
            "name": "10.1.1.83.3407",
            "keyword": "",
            "title": "Approved by: ADAPTIVE AND DYNAMIC MESHING METHODS FOR NUMERICAL SIMULATIONS"
        },
        {
            "abstract": "This paper examines the inductive inference of a complex grammar with neural networks\u00bfspecifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through-time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated.",
            "group": 1978,
            "name": "10.1.1.83.3750",
            "keyword": "parameters frameworkautomata extraction",
            "title": "Natural Language Grammatical Inference with Recurrent Neural Networks"
        },
        {
            "abstract": "Bruxelles, Facult\u00e9 de Sciences Appliqu\u00e9es for the",
            "group": 1979,
            "name": "10.1.1.83.4214",
            "keyword": "Metaheuristics Network. The five metaheuristics considered areAnt Colony Optimization",
            "title": "Supervised by"
        },
        {
            "abstract": "Event-driven distributed infrastructures are becoming increasingly important for information dissemination and application integration. We examine the problem of optimal resource allocation for such an infrastructure composed of an overlay of nodes. Resources, like CPU and network bandwidth, are consumed by both message flows and message consumers; therefore, we consider both rate control for flows and admission control for consumers. This makes the optimization problem difficult because the objective function is nonconcave and the constraint set is nonconvex. We present LRGP (Lagrangian Rates, Greedy Populations), a scalable and efficient distributed algorithm to maximize the total system utility. The key insight of our solution involves partitioning the optimization problem into two types of subproblems: a greedy allocation for consumer admission control and a Lagrangian allocation to compute the flow rates, and linking the subproblems in a manner that allows tradeoffs between consumer admission and flow rates while satisfying the nonconvex constraints. LRGP allows an autonomic approach to system management where nodes collaboratively optimize aggregate system performance. We evaluate the quality of results and convergence characteristics under various workloads. 1.",
            "group": 1980,
            "name": "10.1.1.83.4324",
            "keyword": "",
            "title": "Utility optimization for event-driven distributed infrastructures"
        },
        {
            "abstract": "In partial tulllment ot the requirements or the degr",
            "group": 1981,
            "name": "10.1.1.83.4485",
            "keyword": "by",
            "title": "Compressed Feature Correlation and Graph Cut"
        },
        {
            "abstract": "and have found that it is complete and satisfactory in all respects,",
            "group": 1982,
            "name": "10.1.1.83.4687",
            "keyword": "",
            "title": "Annealing Adaptive Search With Hit-and-Run"
        },
        {
            "abstract": "Address",
            "group": 1983,
            "name": "10.1.1.83.5594",
            "keyword": "CityState/ProvincePostal CodeCountry",
            "title": "Parametric Learning for Blackbox Optimization Anonymous Author(s) Affiliation"
        },
        {
            "abstract": "We present anovel technique for datapath allocation, which incorporates interconnection area and delay estimates based on dynamic oorplanning. In this approach, datapath area is minimized by minimizing the number of wires, routing tracks, and multiplexers, while performance is optimized by minimizing wire length. The simultaneous optimization of these physical cost metrics allows the system to explore realistic design solutions. I.",
            "group": 1984,
            "name": "10.1.1.83.6048",
            "keyword": "",
            "title": "Exploration of Area and Performance Optimized Datapath Design Using Realistic Cost Metrics"
        },
        {
            "abstract": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets. 1",
            "group": 1985,
            "name": "10.1.1.83.6387",
            "keyword": "",
            "title": "Visualization of collaborative data"
        },
        {
            "abstract": "Abstract. Many of today\u2019s embedded systems, such as wireless and portable devices rely heavily on the limited power supply. Therefore, energy efficiency becomes one of the major design concerns for embedded systems. The technique of dynamic voltage scaling (DVS) can be exploited to reduce the power consumption of modern processors by slowing down the processor speed. The problem of static DVS scheduling in distributed systems such that the energy consumption of the processors is minimize while guaranteeing the timing constraints of the tasks is an NP hard problem. Previously, we have developed a heuristic search algorithm: Genetic Algorithm (GA) for the DVS scheduling problem. This paper describes a Parallel Genetic Algorithm (PGA) that improves over Genetic Algorithm (GA) for finding better schedules with less time by parallelizing the GA algorithms to run on a cluster. A hybrid parallel algorithm is also developed to further improve the search ability of PGA by combining PGA with the technique of Simulated Annealing (SA). Experiment results show that the energy consumption of the schedules found by the PGA can be significantly reduced comparing to those found by GA. 1",
            "group": 1986,
            "name": "10.1.1.83.6524",
            "keyword": "",
            "title": "Parallel Genetic Algorithms for DVS Scheduling of Distributed Embedded Systems"
        },
        {
            "abstract": "Abstract \u2014 Classifying large datasets without any a-priori information poses a problem especially in the field of bioinformatics. In this work, we explore the task of classifying hundreds of thousands of cell assay images obtained by a high-throughput screening camera. The goal is to label a few selected examples by hand and to automatically label the rest of the images afterwards. Up to now, such images are classified by scripts and classification techniques that are designed to tackle a specific problem. We propose a new adaptive active clustering scheme, based on an initial Fuzzy c-means clustering and Learning Vector Quantization. This scheme can initially cluster large datasets unsupervised and then allows for adjustment of the classification by the user. Motivated by the concept of active learning, the learner tries to query the most \u2019useful \u2019 examples in the learning process and therefore keeps the costs for supervision at a low level. A framework for the classification of cell assay images based on this technique is introduced. We compare our approach to other related techniques in this field based on several datasets. I.",
            "group": 1987,
            "name": "10.1.1.83.7026",
            "keyword": "",
            "title": "Adaptive Fuzzy Clustering"
        },
        {
            "abstract": "This dissertation was presented by",
            "group": 1988,
            "name": "10.1.1.83.7444",
            "keyword": "",
            "title": "NEW CHANGE DETECTION MODELS FOR OBJECT-BASED ENCODING OF PATIENT MONITORING VIDEO"
        },
        {
            "abstract": "Phylogenetic footprints are short pieces of non-coding DNA sequence in the vicinity of a gene that are conserved between evolutionary distant species. A seemingly simple problem is to sort footprints in their order along the genomes. It is complicated by the fact that",
            "group": 1989,
            "name": "10.1.1.83.9215",
            "keyword": "",
            "title": "The footprint sorting problem"
        },
        {
            "abstract": "Abstract. Many resource allocation issues, such as land use- or irrigation planning, require input from extensive spatial databases and involve complex decisionmaking problems. Spatial decision support systems (SDSS) are designed to make these issues more transparent and to support the design and evaluation of resource allocation alternatives. Recent developments in this \ufffd eld focus on the design of allocation plans that utilise mathematical optimisation techniques. These techniques, often referred to as multi-criteria decision-making (MCDM) techniques, run into numerical problems when faced with the high dimensionality encountered in spatial applications. In this paper we demonstrate how simulated annealing, a heuristic algorithm, can be used to solve high-dimensionalnon-linear optimisation problems for multi-site land use allocation (MLUA) problems. The optimisation model both minimises development costs and maximises spatial compactness of the land use. Compactness is achieved by adding a non-linear neighbourhood objective to the objective function. The method is successfully applied to a case study in Galicia, Spain, using an SDSS for supporting the restoration of a former mining area with new land use. 1.",
            "group": 1990,
            "name": "10.1.1.84.821",
            "keyword": "",
            "title": "Research Article Using simulated annealing for resource allocation"
        },
        {
            "abstract": "The random, heuristic search algorithm called simulated annealing is considered for the problem of finding a maximum cardinality matching in a graph. A basic form of the algorithm is shown to produce matchings with nearly maximum cardinality such that the average time required grows as a polynomial in the number of nodes in the graph. In contrast, it is also shown that for a certain family of graphs, neither the basic annealing algorithm, nor any other algorithm in a fairly large related class of algorithms, can find maximum cardinality matchings in polynomial average time.",
            "group": 1991,
            "name": "10.1.1.84.2394",
            "keyword": "",
            "title": "The time complexity of maximum matching by simulated annealing"
        },
        {
            "abstract": "This paper presents and compares three heuristics for the combinatorial auction problem. Besides a simple greedy (SG) mechanism, two metaheuristics, a simulated annealing (SA), and a genetic algorithm (GA) approach are developed which use the combinatorial auction process to find an allocation with maximal revenue for the auctioneer. The performance of these three heuristics is evaluated in the context of a price controlled resource allocation process designed for the control and provision of distributed information services. Comparing the SG and SA method shows that depending on the problem structure the performance of the SA is up to 20 % higher than the performance of the simple greedy allocation method. The proposed GA approach, using a random key encoding, results in a further improvement of the solution quality. Although the metaheuristic approaches result in higher search performance, the computational effort in terms of used CPU time is higher in comparison to the simple greedy mechanism. However, the absolute overall computation time is low enough to enable real-time execution in the considered IS application domain. 1",
            "group": 1992,
            "name": "10.1.1.84.2406",
            "keyword": "",
            "title": "Optimization heuristics for the combinatorial auction problem"
        },
        {
            "abstract": "",
            "group": 1993,
            "name": "10.1.1.84.2481",
            "keyword": "",
            "title": "Reinforcement Learning for Autonomous Vehicles"
        },
        {
            "abstract": "Abstract. This paper shows how parallelism has been integrated into SCOOP, a C++ class library for solving optimisation problems. After a description of the modeling and the optimisation parts of SCOOP, two new classes that permit parallel optimisation are presented: a class whose only purpose is to handle messages and a class for managing optimiser and message handler objects. Two of the most interesting aspects of SCOOP, modularity and generality, are preserved by clearly separating problem representation, solution techniques and parallelisation scheme. This allows the user to easily model a problem and construct a parallel optimiser for solving it by combining existing SCOOP classes. 1",
            "group": 1994,
            "name": "10.1.1.84.2518",
            "keyword": "",
            "title": "Parallel Optimisation in the SCOOP Library"
        },
        {
            "abstract": "In the area of computer simulation Latin hypercube designs play an important role. In this paper the class of maximin Latin hypercube designs is considered. Up to now only several two-dimensional designs and designs for some small number of points are known for this class. Using periodic designs and simulated annealing we extend the known results and construct approximate maximin Latin hypercube designs for up to ten dimensions and for up to 100 design points. All these designs can be downloaded from the website",
            "group": 1995,
            "name": "10.1.1.84.2744",
            "keyword": "Computer experimentLatin hypercube designnon-collapsingpacking problemsimulated annealing",
            "title": "Space-filling Latin hypercube designs for computer experiments"
        },
        {
            "abstract": "as representing the official policies of the U.S. Government.",
            "group": 1996,
            "name": "10.1.1.84.3101",
            "keyword": "online algorithmssubmodular functionsalgorithm portfoliosrestart schedules",
            "title": "Using Online Algorithms to Solve NP-Hard Problems More Efficiently in Practice"
        },
        {
            "abstract": "An important class of methodologies for the parallel processing of computational models defined on some discrete geometric data structures (i.e., meshes, grids) is the so called geometry decomposition or splitting approach. Compared to the sequential processing of such models, the geometry splitting parallel methodology requires an additional computational phase. It consists of the decomposition of the associated geometric data structure into a number of balanced subdomains that satisfy a number of conditions that ensure the load balancing and minimum communication requirement of the underlying computations on a parallel hardware platform. It is well known that the implementation of the mesh decomposition phase requires the solution of a computationally intensive problem. For this reason several fast heuristics have been proposed. In this paper we explore a decomposition approach which is part of a parallel adaptive finite element mesh procedure. The proposed integrated approach consists of five steps. It starts with a coarse background mesh that is optimally decomposed by applying well known heuristics. Then, the initial mesh is refined in each subdomain after linking the new boundaries introduced by its decomposition. Finally, the decomposition of the new refined mesh is improved so that it satisfies the objectives and conditions of the mesh decomposition problem. Extensive experimentation indicates the effectiveness and efficiency of the proposed parallel mesh and decomposition approach.- 1-1.",
            "group": 1997,
            "name": "10.1.1.84.3364",
            "keyword": "",
            "title": "Parallel Adaptive Mesh Generation and Decomposition"
        },
        {
            "abstract": "clarify the relationships between heuristic search andcontrol. We thank Rich Sutton, Chris Watkins, Paul Werbos, and Ron Williams for sharing their fundamental insights into this subject through numerous discussions, and we further thank Rich Sutton for rst making us aware of Korf's research and for his very thoughtful comments on the manuscript. We arevery grateful to Dimitri Bertsekas and Steven Sullivan for independently pointing out an error in an earlier version of this article. Finally, we thank Harry Klopf, whose insight and persistence encouraged our interest in this class of learning problems. This research was supported by grants to A.G. Barto from the National Science Foundation (ECS-8912623 and ECS-9214866) and the Air Force O ce of Scienti c Research, Bolling AFB (AFOSR-89-0526).",
            "group": 1998,
            "name": "10.1.1.84.3495",
            "keyword": "",
            "title": "Learning to act using real-time dynamic programming"
        },
        {
            "abstract": "System administration has often been considered to be a \u201cpractice \u201d with no theoretical underpin-nings. In this thesis, we begin to define a theory of system administration, based upon two activities of system administrators: configuration management and dependency analysis. We formalize and explore the complexity of these activities, and demonstrate that they are intractable in the general case. We define the concepts of system behavior, kinds of configuration operations, a model of configuration management, a model of reproducibility, and proofs that several parts of the process are NP-complete or NP-hard. We also explore how system administrators keep these tasks tractable in practice. This is a first step toward a theory of system administration and a common language for discussing the theoretical underpinnings of the practice. ii Acknowledgements This thesis is the result of four years of work whereby I have been accompanied and supported by many people. It is a pleasant aspect that I have now the opportunity to express my gratitude to all of them. The first person I would like to thank is my advisor Alva L. Couch. I have been working with him since 2001 when I started my Master\u2019s project. His enthusiasm and integral view of research and his humor when things get tough have made a deep impact upon me. He patiently guided me and supported me throughout the course of my study. I would like to thank Professor Kofi Laing and Professor Lenore Cowen for their help on com-putation theory and for being committee members. I also thank Professor Ricardo Pucella for reviewing my work.",
            "group": 1999,
            "name": "10.1.1.84.3739",
            "keyword": "",
            "title": "Complexity of System Configuration Management"
        },
        {
            "abstract": "We present a method that exploits an information theoretic framework to extract optimized audio features using video information. A simple measure of mutual information (MI) between the resulting audio features and the video ones allows to detect the active speaker among different candidates. Our method involves the optimization of an MI-based objective function. No approximation is introduced to solve this optimization problem, neither for the estimation of the probability density functions (pdf) of the features, nor for the cost function itself. The pdf are estimated from the samples using a non-parametric approach. As far as the optimization process itself is concerned, three different optimization methods (one local and two global) are compared in this paper. The Differential Evolution algorithm is eventually retained as it outperforms the other methods. Two information theoretic optimization criteria are compared and their ability to extract audio features specific to speech is discussed. As a result, our method achieves a speaker detection rate of 100 % on our test sequences, and of 95 % on a most commonly used one. I.",
            "group": 2000,
            "name": "10.1.1.84.4126",
            "keyword": "",
            "title": "M.: Extraction of audio features specific to speech production for multimodal speaker detection"
        },
        {
            "abstract": "Our general objective in this research was to design, develop, implement, and evaluate advanced classification approaches for more accurate land-use/land-cover mapping using remotely sensed data. The overall research consists of three interrelated studies. In the first study, we developed Simulated Annealing (SA) based classification systems for land cover mapping. SA has been shown to be able to overcome the local minimum problem that is typical with many unsupervised classification approaches. Our hypothesis in this study was that SA-based classification systems could help overcome the local minimum problem in one of such approaches, K-means, and thus improve the classification performance. Two SA based classification systems have been developed. The Single SA-based (S-SA) system was developed based on the standard SA algorithm. The Integrated SA-based (I-SA) system was developed by combining the standard SA algorithm and K-means into a two-level classification system. We have used Landsat Thematic Mapper (TM) images to test the suitability of the SA-based systems. Experimental results have demonstrated that the SA-based systems significantly",
            "group": 2001,
            "name": "10.1.1.84.4373",
            "keyword": "",
            "title": "ABSTRACT YUAN, HUI. Development and Evaluation of Advanced Classification Systems using Remotely Sensed Data for Accurate Land-Use/Land-Cover Mapping. (Under"
        },
        {
            "abstract": "For my parents. iii ACKNOWLEDGEMENTS I would like to thank my advisors Dana Randall and Eric Vigoda for their guidance and support, which they gave generously. I was fortunate to have the opportunity to work with two of the leading researchers in my field. Dana never ceased to surprise me with her quick insights and ability to get to crux of a problem. I thoroughly enjoyed working with Eric and am always inspired by his persistent approach to solving hard problems. During my stay at Georgia Tech, I had the opportunity to interact with other faculty in theory group and the ACO program and I would like to thank them as well. I would like to thank Prasad Tetali especially, who is the nicest of people, and from whose methodical approach to research I learned a lot. I\u2019d like to thank Vijay Vazirani for his timely advice as well as for some enjoyable discussions on problems. I\u2019d like to thank Santosh Vempala for some fun discussions as well. Thanks to Ivona Bez\u00e1kov\u00e1, Juan Vera and Sam Greenberg for enjoyable discussions in the course of our collaborations. I\u2019d like to thank all the friends I made at Georgia Tech;",
            "group": 2002,
            "name": "10.1.1.84.5334",
            "keyword": "DEDICATION...................................... iii",
            "title": "Annealing and Tempering for Sampling and Counting Approved by:"
        },
        {
            "abstract": "Dedicated to my mother, Mary Ignatia Minz and my late father, Anthony Minz iii ACKNOWLEDGEMENTS I would like to express my deepest gratitude to Prof. Sung Kyu Lim for his support and guidance during my PhD at Georgia Tech. His support has been crucial for the successful completion of this research and thesis. I also thank the members of my reading committee Prof. Madhavan Swaminathan, and Prof. Abhijit Chatterjee, and other members of my dis-sertation committee, Prof. Gabriel Rincon-Mora, and Prof. Gabriel Loh for their time and helpful feedbacks. Prof. Partha Pratim Chakrabarti, and Prof. Dipwanita Raychaudhuri from Indian Institute of Technology, Kharagpur have been very instrumental and influential in my decision to pursue PhD. They will always have my respect and appreciation for being excellent teachers. I would also like to remember late Prof. John Uyemura who encouraged me to pursue PhD at Georgia Tech. I would like to extend my heartfelt thanks to the members (past and present) of the",
            "group": 2003,
            "name": "10.1.1.84.5404",
            "keyword": "",
            "title": "Physical Design Automation for System-on-Packages and Approved by:"
        },
        {
            "abstract": "Men are often defined by the relationships they keep, and I am no exception. I have been blessed with wonderful family, friends, and colleagues who have sustained me throughout graduate school and life. I have grown intellectually, emotionally, and spiritually through the companions who have walked with me in this journey, and I wish I had space to thank them all. I wish to especially thank my mother Charlotte and my late father Peter for their tireless encour-agement and love. I extend my greatest love and appreciation to them and the rest of my family. My friends, both past and present, have also sustained me tremendously. I extend my heartfelt appreciation to Erich, Chris, Karen, Amanda, Heather, Teresa, Lisa, Megan, and the many others for their support and friendship over the years. Professionally I have been surrounded by some of the most amazing professors and colleagues. I thank my advisor, Prof. D. Scott Wills, for the flexibility to find my own research direction while keeping me grounded in the big picture of graduate study. I thank Prof. Gabriel Loh for challenging me to elevate the quality of my work and tackle harder and harder problems. Prof. Hsien-Hsin Lee has also been invaluable in my studies here, unselfishly providing me with advice and favors without delay. I also wish to thank the other members of my thesis committee, Prof. David Schimmel and Prof. Yorai Wardi, for the advice they brought and the laughs they provided during my defense process. Additionally I would like to recognize the rest of my research group, both alumni and current members:",
            "group": 2004,
            "name": "10.1.1.84.5816",
            "keyword": "",
            "title": "Characterization and Avoidance of Critical Pipeline Structures in Approved by:"
        },
        {
            "abstract": "The problem addressed in this paper is scheduling jobs on unrelated parallel machines with sequence-dependent setup times to minimize the maximum completion time (i.e., the makespan). This problem is NP-hard even without including setup times. Adding sequence-dependent setup times adds another dimension of complexity to the problem and obtaining optimal solutions becomes very difficult especially for large problems. In this paper a Simulated Annealing (SA) algorithm is applied to the problem at hand to reach near-optimum solution. The effectiveness of the Simulated Annealing algorithm is measured by comparing the quality of its solutions to optimal solutions for small problems. The results show that the SA efficiently obtained optimal solutions for all test problems.",
            "group": 2005,
            "name": "10.1.1.84.6247",
            "keyword": "schedulingunrelated parallel machinessetup timessimulated annealinginteger",
            "title": "A SIMULATED ANNEALING ALGORITHM FOR THE UNRELATED PARALLEL MACHINE SCHEDULING PROBLEM"
        },
        {
            "abstract": "The crossover operator is traditionally viewed as the distinguishing feature and primary strength of a genetic algorithm. This multi-parent operator can recombine the useful features of two parent solutions into a single \u201csuper\u201d offspring. However, a new analysis suggests that the primary benefit of crossover operators is the preservation of common components. In creating an offspring solution, crossover can focus its changes on the uncommon components of its two parents. This focus is a surprisingly important feature of genetic algorithms. To demonstrate the contribution of preserving common components to the search process of genetic algorithms, this feature has been isolated and transferred to simulated annealing. Results on the Travelling Salesman Problem indicate that the preservation of common components can lead to significant improvements in the performance of simulated annealing.",
            "group": 2006,
            "name": "10.1.1.84.6624",
            "keyword": "",
            "title": "SAGA: Demonstrating the Benefits of Commonality-Based Crossover Operators in Simulated Annealing"
        },
        {
            "abstract": "Research on planning for robots is in such a state of flux that there is disagreement about what planning is and whether it is necessary. We can take planning to be the optimization and debugging of a robot\u2019s program by reasoning about possible courses of execution. It is necessary to the extent that fragments of robot programs are combined at run time. There are several strands of research in the field; I survey six: (1) attempts to avoid planning; (2) the design of flexible plan notations; (3) theories of time-constrained planning; (4) planning by projecting and repairing faulty plans; (5) motion planning; and (6) the learning of optimal behaviors from reinforcements. More research is needed on formal semantics for robot plans. However, we are already beginning to see how to mesh plan execution with plan generation and learning. Copyright \u00a91992, AAAI/$2.00",
            "group": 2007,
            "name": "10.1.1.84.6686",
            "keyword": "",
            "title": "Robot Planning"
        },
        {
            "abstract": "Data Mining (DM) has been identified as one of the ten main challenges of the 21st century (MIT Technological Review, fev. 2001). The goal is to exploit the massive amounts of data produced in scientific labs, industrial plants, banks, hospitals or supermarkets, in order to extract valid, new and useful regularities. In other words, DM resumes the Machine Learning (ML) goal, finding (partial) models for the complex system underlying the data. DM and ML problems can be set as optimization problems, thus leading to two possible approaches 1. The first approach is to simplify the learning problem to make it tractable by standard statistical or optimization methods. The alternative approach is to preserve as much as possible the genuine complexity of the goals (yielding \u201cinteresting \u201d models, accounting for prior knowledge): more flexible optimization approaches are therefore required, such as those offered by Evolutionary Computation. Symmetrically, optimization techniques are increasingly used in all scientific and technological fields, from optimum design to risk assessment. Evolutionary Computation (EC) techniques, mimicking the Darwinian paradigm of natural evolution, are stochastic population-based dynamical",
            "group": 2008,
            "name": "10.1.1.84.6741",
            "keyword": "\u2022 Marc SchoenauerDirecteur de Recherche INRIA",
            "title": "TAO- Th\u00e8mes Apprentissage et Optimisation Machine Learning, Data Mining and Evolutionary Optimization 1 Preamble"
        },
        {
            "abstract": "of simulated annealing and mean field annealing as applied to the generation of block designs",
            "group": 2009,
            "name": "10.1.1.84.8147",
            "keyword": "",
            "title": "Comparison"
        },
        {
            "abstract": "Engineering optimization problems are known to be difficult to solve using mathematical programming techniques because of large search spaces, complex objective and constraint functions, and, in many cases, their combinatorial nature. Simulated annealing is a well known heuristic optimization technique that has been used to solve a number of problems in discrete, non-differential, and combinatorial optimization and hence is suitable for solving such engineering optimization problems. However, computationally intensive problems are frequently encountered in the field of engineering optimization, in which case the use of simulated annealing can be prohibitively time consuming. The objective of this thesis is to develop an object oriented framework that imple-ments a distributed simulated annealing algorithm, which can be easily extended to solve computationally intensive engineering optimization problems. A distributed simulated an-nealing algorithm (DSA Algorithm) was developed and incorporated into a distributed simulated annealing framework called the DSA Framework. The framework defines in-terfaces, through which optimization problems can be modeled, utilizing a distributed com-puting framework, Vitri, to engage multiple desktop computers in a collective effort to solve problems. The framework was used to solve a 40 variable knapsack problem as a benchmark prob-lem to analyze the performance of the algorithm. The framework was also used to optimize support locations in a piping system subject to seismic loads. The DSA framework proves to be an efficient, fairly scalable tool that shows consistent reduction in execution time with increasing number of servers, thus proving to be a valuable tool in solving computationally intensive engineering optimization problems.",
            "group": 2010,
            "name": "10.1.1.84.9659",
            "keyword": "",
            "title": "A DISTRIBUTED SIMULATED ANNEALING FRAMEWORK FOR ENGINEERING OPTIMIZATION"
        },
        {
            "abstract": "It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.",
            "group": 2011,
            "name": "10.1.1.84.9691",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract. We present a method to speed up the computation of a high-quality data-dependent triangulation approximating an image using simulated annealing by probability distributions guided by local approximation error and its variance. The triangulation encodes the image, yielding compression rates comparable to or even superior to JPEG and JPEG2000 compression. The specific contributions of our paper are a speed-up of the simulated annealing optimization and a comparison of our approach to other image approximation and compression methods. Furthermore, we propose an adaptive vertex insertion/removal strategy and termination criteria for the simulated annealing to achieve specified approximation error bounds. 1",
            "group": 2012,
            "name": "10.1.1.84.9756",
            "keyword": "",
            "title": "Image Compression Using"
        },
        {
            "abstract": "Abstract. Automatic placement of text corresponding to graphical objects is an important issue in several applications such as Geographical Data Systems (GIS), Cartography, and Graph Drawing. While usually only a finite number of possible placements is available, in this paper we allow for an infinite number of placements and only require the label to be as close as possible to its corresponding feature. We focus on realistic data and present a hybrid algorithm for labeling both line and point features. In the method\u2019s first step that works on the discretized map image processing tools are used to obtain an initial placement of all labels in allowed (i.e., non overlapping) position. The second step works on the continuous map and uses a force-directed iterative algorithm to improve this initial placement. In a comprehensive study on realistic data sets we investigate the performance of our method. 1.",
            "group": 2013,
            "name": "10.1.1.84.9827",
            "keyword": "",
            "title": "A PRACTICAL MAP LABELING ALGORITHM UTILIZING IMAGE PROCESSING AND FORCE-DIRECTED METHODS"
        },
        {
            "abstract": "A statistical model to segment clinical magnetic resonance (MR) images in the presence of noise and intensity inhomogeneities is proposed. Inhomogeneities are considered to be multiplicative low-frequency variations of intensities that are due to the anomalies of the magnetic fields of the scanners. The measurements are modeled as a Gaussian mixture where inhomogeneities present a bias field in the distributions. The piecewise contiguous nature of the segmentation is modeled by a Markov random field (MRF). A greedy algorithm based on the iterative conditional modes (ICM) algorithm is used to find an optimal segmentation while estimating the model parameters. Results with simulated and hand-segmented images are presented to compare performance of the algorithm with other statistical methods. Seg-mentation results with MR head scans acquired from four different clinical scanners are presented. 0 1998 Elsevier Science B.V.",
            "group": 2014,
            "name": "10.1.1.85.1480",
            "keyword": "Bias fieldBrain imagingMagnetic resonance imagesImage segmentationIntensity inhomogeneitiesStatistical modeling",
            "title": "Abstract Segmentation of MR images with intensity inhomogeneities"
        },
        {
            "abstract": "We introduce a novel methodology for determining the difficulty of modeling a given data set. The method utilizes formulation of modeling as an optimization problem instance that consists of an objective function and a set of constraints. The properties of the data set that could affect the quality of optimization are categorized. In large optimization problems with multiple properties that contribute to the solution quality, it is practically impossible to analytically study the effect of each property. A number of metrics for evaluating the effectiveness of the optimization on each data set are proposed. Using the well known Plackett and Burmann fast simulation methodology, for each metric, the impact of the categorized properties of the data are determined for the specified optimization method. A new approach for combining the impacts resulting from different properties on various metrics is described. The method is illustrated on distance measurement data used for estimating the locations of wireless nodes in ad-hoc networks. I.",
            "group": 2015,
            "name": "10.1.1.85.1964",
            "keyword": "",
            "title": "How challenging is modeling of a data set?"
        },
        {
            "abstract": "Droplet-based microfluidic biochips have recently gained much attention and are expected to revolutionize the biological laboratory procedure. As biochips are adopted for the complex procedures in molecular biology, its complexity is expected to increase due to the need of multiple and concurrent assays on a chip. In this paper, we formulate the placement problem of digital microfluidic biochips with a tree-based topological representation, called T-tree. To the best knowledge of the authors, this is the first work that adopts a topological representation to solve the placement problem of digital microfluidic biochips. Experimental results demonstrate that our approach is much more efficient and effective, compared with the previous unified synthesis and placement framework.",
            "group": 2016,
            "name": "10.1.1.85.1975",
            "keyword": "B.7.2 [Integrated CircuitsDesign Aids General Terms AlgorithmPerformanceDesign Keywords Microfluidicsbiochipplacementfloorplanning",
            "title": "Placement of digital microfluidic biochips using the T-tree formulation"
        },
        {
            "abstract": "Abstract: Locomotion and navigation of a surface walking/climbing robot \u2013 Planar Walker, based on a novel planar 8-bar mechanism are studied. The robot moves on a surface through decoupled transverse gaits and turning gaits with finite lengths and finite rotation angles. Motions of the gaits are modeled using planar rigid motion group SE(2). Three point-topoint navigation methods are developed for various situations:",
            "group": 2017,
            "name": "10.1.1.85.3164",
            "keyword": "",
            "title": "Simple Line of Sight (SLS), Simulated Annealing Accurate"
        },
        {
            "abstract": "Citations (this article cites 2 articles hosted on the",
            "group": 2018,
            "name": "10.1.1.85.4184",
            "keyword": "",
            "title": "Downloaded from"
        },
        {
            "abstract": "quantization for energy-efficient information transfer",
            "group": 2019,
            "name": "10.1.1.85.4670",
            "keyword": "stochastic resonancesuprathreshold stochastic resonancenoiseneural codingpopulation codesenergy efficient coding",
            "title": "in"
        },
        {
            "abstract": "I hereby declare that I am the sole author of this thesis. I authorize the University of Waterloo to lend this thesis to other institutions or individuals for the purpose of scholarly research.",
            "group": 2020,
            "name": "10.1.1.85.8031",
            "keyword": "",
            "title": "A Continuous Gibbs Annealer for Contour Estimation"
        },
        {
            "abstract": "Abstract. Classifying large datasets without any a-priori information poses a problem in many tasks. Especially in the field of bioinformatics, often huge unlabeled datasets have to be explored mostly manually by a biology expert. In this work we consider an application that is motivated by the development of high-throughput microscope screening cameras. These devices are able to produce hundreds of thousands of images per day. We propose a new adaptive active classification scheme which establishes ties between the two opposing concepts of unsupervised clustering of the underlying data and the supervised task of classification. Based on Fuzzy c-means clustering and Learning Vector Quantization, the scheme allows for an initial clustering of large datasets and subsequently for the adjustment of the classification based on a small number of carefully chosen examples. Motivated by the concept of active learning, the learner tries to query the most informative examples in the learning process and therefore keeps the costs for supervision at a low level. We compare our approach to Learning Vector Quantization with random selection and Support Vector Machines with Active Learning on several datasets. 1",
            "group": 2021,
            "name": "10.1.1.85.8380",
            "keyword": "",
            "title": "Adaptive active classification of cell assay images"
        },
        {
            "abstract": "Abstract\u2014This paper proposes a novel probabilistic variational method with deterministic annealing for the maximum a posteriori (MAP) estimation of complex stochastic systems. Since the MAP estimation involves global optimization, in general, it is very difficult to achieve. Therefore, most probabilistic inference algorithms are only able to achieve either the exact or the approximate posterior distributions. Our method constrains the mean field variational distribution to be multivariate Gaussian. Then, a deterministic annealing scheme is nicely incorporated into the mean field fix-point iterations to obtain the optimal MAP estimate. This is based on the observation that when the covariance of the variational Gaussian distribution approaches to zero, the infimum point of the Kullback-Leibler (KL) divergence between the variational Gaussian and the real posterior will be the same as the supreme point of the real posterior. Although global optimality may not be guaranteed, our extensive synthetic and real experiments demonstrate the effectiveness and efficiency of the proposed method. Index Terms\u2014Mean field variational analysis, deterministic annealing, maximum a posteriori estimation, graphical model, Markov network. 1",
            "group": 2022,
            "name": "10.1.1.85.8830",
            "keyword": "",
            "title": "Variational Maximum A Posteriori by Annealed Mean Field Analysis"
        },
        {
            "abstract": "We compare convergence rates of Metropolis\u2013Hastings chains to multi-modal target distributions when the proposal distributions can be of \u201clocal \u201d and \u201csmall world \u201d type. In particular, we show that by adding occasional long-range jumps to a given local proposal distri-bution, one can turn a chain that is \u201cslowly mixing \u201d (in the com-plexity of the problem) into a chain that is \u201crapidly mixing. \u201d To do this, we obtain spectral gap estimates via a new state decomposition theorem and apply an isoperimetric inequality for log-concave prob-ability measures. We discuss potential applicability of our result to Metropolis-coupled Markov chain Monte Carlo schemes.",
            "group": 2023,
            "name": "10.1.1.85.9615",
            "keyword": "",
            "title": "Submitted to the Annals of Applied Probability SMALL-WORLD MCMC AND CONVERGENCE TO MULTI-MODAL DISTRIBUTIONS: FROM SLOW MIXING TO FAST MIXING"
        },
        {
            "abstract": "Power delivery is a growing reliability concern in microprocessors as the industry moves toward feature-rich, powerhungrier designs. To battle the ever-aggravating power consumption, modern microprocessor designers or researchers propose and apply aggressive power-saving techniques in the form of clock-gating and/or power-gating in order to operate the processor within a given power envelope. However, these techniques often lead to high-frequency current variations, which can stress the power delivery system and jeopardize reliability due to inductive noise (L di) in the power sup-dt ply network. In addition, with the advent of 3D stacked IC technology that facilitates the design of processors with much higher module density, the design of a low impedance powerdelivery network can be a daunting challenge. To counteract these issues, modern microprocessors are designed to operate under the worst-case current assumption by deploying adequate decoupling capacitance. With the lowering of supply voltages and increased leakage power and current consumption, designing a processor for the worst case is becoming less appealing. In this paper, we propose a new dynamic inductive-noise controlling mechanism at the microarchitectural level that will limit the on-die current demand within predefined bounds, regardless of the native power and current characteristics of running applications. By dynamically monitoring the access patterns of microarchitectural modules, our mechanism can effectively limit simultaneous switching activity of closeby modules, thereby leveling voltage ringing at local powerpins. Compared to prior art, our di/dt controller is the first that takes the processor\u2019s floorplan as well as its power-pin distribution into account to provide a finer-grained control with minimal performance degradation. Based on the evaluation results using 2D and 3D floorplans, we show that our techniques can significantly improve inductive noise induced by current demand variation and reduce the average current variability by up to 7 times with an average performance overhead of 4.0 % (2D floorplan) and 3.8 % (3D floorplan). 1.",
            "group": 2024,
            "name": "10.1.1.85.9676",
            "keyword": "",
            "title": "A Floorplan-Aware Dynamic Inductive Noise Controller for Reliable Processor Design"
        },
        {
            "abstract": "QSAR studies can be broadly divided into two types- regression and classification. The development of QSAR models essentially consists of the application of statistical methods to chemical datasets. As such, the statistical and machine learning literature provides a number of useful techniques. Some techniques are specifically designed to build classification models whereas others can carry out both classification as well as regression. In addition to these techniques, a number of methods are available for the optimization of various parameters and selection of variables required in the model building process. These can be deterministic methods such as the BFGS algorithm 1\u20134 and the Nelder-Mead simplex algorithm 5 or stochastic methods such as genetic algorithms 6\u20138 and simulated annealing. 9 This chapter discusses the underlying details of the various modeling and optimization techniques used in this work. 2.1 Linear Methods As the title of this section indicates linear methods employ a linear relationship between the predictor variables and the observed response to develop a predictive model. In many QSAR problems, structure property trends can be modeled reasonably well by linear approximations. In general it is observed that physical properties are well modeled by these types of methods. In the case of biological properties linear models do not always exhibit good predictive performance. The poorer behavior of linear models when faced with biological structure property trends is understandable when we consider the fact that biological properties in general are the result of a number of interactions that might include absorption, metabolic degradation, excretion and so on. Clearly the relationship between molecular structure and these factors is complex and in general nonlinear. However, linear methods are useful as a first step in the modeling process and, though not always very accurate, the simple interpretation methods that can be applied to linear models makes up, to some extent, for the lack of predictive ability for these methods. Though linear methods can be applied to both classification and regression we focus on the latter application in this section. 15",
            "group": 2025,
            "name": "10.1.1.86.559",
            "keyword": "",
            "title": "Chapter 2 Statistical & Optimization Techniques"
        },
        {
            "abstract": "Local search for shift design",
            "group": 2026,
            "name": "10.1.1.86.1226",
            "keyword": "",
            "title": "ABTEILUNG DATENBANKEN UND ARTIFICIAL INTELLIGENCE Local search for shift design"
        },
        {
            "abstract": "In this research, a novel population-based global optimization method has been studied. The method is called Electromagnetism-like Mechanism or in short EM. The proposed method mimicks the behavior of electrically charged particles. In other words, a set of points is sampled from the feasible region and these points imitate the role of the charged particles in basic electromagnetism. The underlying idea of the method is directing sample points toward local optimizers, which point out attractive regions of the feasible space. The proposed method has been applied to different test problems from the literature. Moreover, the viability of the method has been tested by comparing its results with other reported results from the literature. Without using the higher order information, EM has converged rapidly (in terms of the number of function evaluations) to the global optimum and produced highly efficient results for problems of varying degree of difficulty. After a systematic study of the underlying stochastic process, the proof of convergence to the global optimum has been given for the proposed method. The thrust of the proof has been to show that in the limit, at least one of the points in the population moves to the neighborhood of the global optimum with probability one. The structure of the proposed method is very flexible permitting the easy development of variations. Capitalizing on this, several variants of the proposed method has been devel-oped and compared with the other methods from the literature. These variants of EM have been able to provide accurate answers to selected problems and in many cases have been able to outperform other well-known methods.",
            "group": 2027,
            "name": "10.1.1.86.1334",
            "keyword": "",
            "title": "SHU-CHERNG FANG CHAIR OF ADVISORY COMMITTEE"
        },
        {
            "abstract": "Iterative improvement partitioning algorithms such as the FM algorithm of Fiduccia and Mattheyses [8], the algorithm of Krishnamurthy [13], and Sanchis's extensions of these algorithms to multi-way partitioning [16], all rely on e cient data structures to select the modules to be moved from one partition to the other. The implementation choices for one of these data structures, the gain bucket, isinvestigated. Surprisingly, selection from gain buckets maintained as LIFO (Last-In-First-Out) stacks leads to signi cantly better results than gain buckets maintained randomly (as in previous studies of the FM algorithm [13] [16]) or as FIFO (First-In-First-Out) queues. In particular, LIFO buckets result in a 36 % improvement over random buckets and a 43 % improvementover FIFO buckets for minimum-cut bisection. Eliminating randomization from the bucket selection not only improves the solution quality, but has a greater impact on FM performance than adding the Krishnamurthy gainvector. The LIFO selection scheme also results in improvement over random schemes for multi-way partitioning [16] and for more sophisticated partitioning strategies such asthetwo-phase FM methodology [2]. Finally, by combining insights from the LIFO gain buckets with the Krishnamurthy higher-level gain formulation, a new higherlevel gain formulation is proposed. This alternative formulation results in a further 22 % reduction in the average cut cost when compared directly to the Krishnamurthy formulation for higher-level gains, assuming LIFO organization for the gain buckets. 1",
            "group": 2028,
            "name": "10.1.1.86.1360",
            "keyword": "",
            "title": "On Implementation Choices for Iterative Improvement Partitioning Algorithms"
        },
        {
            "abstract": "annealing for multi-robot hierarchical",
            "group": 2029,
            "name": "10.1.1.86.2096",
            "keyword": "",
            "title": "task allocation with MinMax objective"
        },
        {
            "abstract": "",
            "group": 2030,
            "name": "10.1.1.86.2716",
            "keyword": "",
            "title": " Force-Directed Drawing Algorithms"
        },
        {
            "abstract": "This work was supported by the Semiconductor Research Corporation. With increasing speed requirement for high-performance digital systems, digital cells have become more and more &quot;analog &quot; in nature. These analog behaviors determine the overall performance of a digital cell, like rise time, fall time and delay time. To address these analog concerns, we use a set of existing cell-level analog synthesis tools, ASTRX/OBLX, and apply it on digital cell synthesis. In this research we focus on the synthesis of single input, single output case of a buffer cell for a better understanding of the synthesis approach. Multiple input case can be solved base upon this approach. We use a piecewise linear drive curve table to efficiently estimate the transient behavior for a digital cell switching. A drive curve table can estimate a transient response within +/-3 % of simulated response from HSPICE. Results show that this method can optimize a digital cell to perform with specified rise, fall, delay time but with minimum area and dynamic power consumption. Moreover, the switching edge slopes can be controlled by the user to minimize switching current, which usually contributes to",
            "group": 2031,
            "name": "10.1.1.86.2994",
            "keyword": "Table Of Contents",
            "title": "Advisor: Prof. Carley Synthesis of High-Performance"
        },
        {
            "abstract": "On behalf of:",
            "group": 2032,
            "name": "10.1.1.86.4347",
            "keyword": "Discrete parameter optimizationstochastic approximation algorithmstwo-t",
            "title": "A discrete parameter stochastic approximation algorithm for simulation optimization\u201d, Simulation"
        },
        {
            "abstract": "Nowadays, design of embedded systems is confronted with complex signal processing algorithms and a multitude of computational intensive multimedia applications, while time to product launch has been extremely reduced. Especially in the wireless domain, those challenges are stacked with tough requirements on power consumption and chip size. Unfortunately, design productivity did not undergo a similar progression, and therefore fails to cope with the heterogeneity of modern architectures. Electronic design automation tools exhibit deep gaps in the design flow like high-level characterization of algorithms, floating-point to fixed-point conversion, hardware/software partitioning, and virtual prototyping. This tutorial paper surveys several promising approaches to solve the widespread design problems in this field. An overview over consistent design methodologies that establish a framework for connecting the different design tasks is given. This is followed by a discussion of solutions for the integrated automation of specific design tasks. Copyright \u00a9 2006 M. Holzer et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1.",
            "group": 2033,
            "name": "10.1.1.86.4809",
            "keyword": "",
            "title": "Efficient Design Methods for Embedded Communication Systems"
        },
        {
            "abstract": "von",
            "group": 2034,
            "name": "10.1.1.86.4893",
            "keyword": "",
            "title": "1.3 Theoretical Foundations and Practical Design of EAs............. 5"
        },
        {
            "abstract": "Clustering is the unsupervised classi cation of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines \ufffd this re ects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a di cult problem combinatorially and di erences in assumptions and contexts in di erent communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.",
            "group": 2035,
            "name": "10.1.1.86.5197",
            "keyword": "I.2.6 [Arti cial IntelligenceLearning|Knowledge Acquisition",
            "title": "Computer Society."
        },
        {
            "abstract": "Abstract-Partitioning is an important step in the top-down design of large complicated integrated circuits. In this paper, a cut and reported 38 % average reduction in nets cut when compared with those of the F&M [l] method. However, simple yet effective partitioning technique is described. It is based on the clustering of \u201cclosely \u201d connected cells and the gradual enforcement of size-constraints. At the beginning, clusters are formed in the bottom-up fashion to reduce the problem size. Then the clusters are partitioned using several different parameters to experimental results show that our method works better with the exact cost function as explained in Section V. Another method [6] used \u201clevel gain \u201d to predict the cost changes when moving each cell and Sanchis [7] adapted this find a good starting point. The best result achieved during the cluster partitioning is used as the initial solution for the lower level partitioning. The gradual constraint enforcement technique is used to cope with the local minimum problems. It allows cells or clusters to move with more freedom among the subsets during earlier iterations and thus may effectively find a near model to multiple-way partitioning. In [8], [9], an evolutionbased approach was reported which outperformed the F&M method by 27 % and a version of annealing-based algorithm with an efficient annealing schedule [lo] by 54 % on the average. These methods move or exchange nodes so that the optimum solution. Several experimental results show that the new partitioning technique produces favorable results. In particular, our method outperforms the F&M method [l] by more than 60% in the number of crossing nets on the average. size constraints are always satisfied. Recently, several authors reported a ratio-cut [ 1 11 approach which does not impose hard limits on the subset sizes. In these methods, subset sizes may be significantly different when the I.",
            "group": 2036,
            "name": "10.1.1.86.5713",
            "keyword": "",
            "title": "A Simple Yet Effective Technique for Partitioning"
        },
        {
            "abstract": "Many vision tasks, such as segmentation, grouping, and recognition can be formulated as graph partition problems. The recent literature has witnessed two popular graph cut algorithms: one is the Ncut using spectral graph analysis and the other is minimum-cut using maximum flow algorithm. This paper presents a third major approach by generalizing the Swendsen-Wang method \u2013 a well celebrated algorithm in statistical mechanics. Our algorithm simulates ergodic and reversible Markov chain jumps in the space of all possible graph partitions to search for global optima of a Bayesian posterior probability. At each step, the algorithm can split, merge, or re-group a sizable subgraph, and thus achieve fast mixing at low temperature eliminating the slow simulated annealing procedure. Our experiments show that it converges in 15-60 seconds in a PC for image segmentation and curve grouping. This is about 400 times faster than the classical Gibbs sampler which flips a single vertex each time, and is 20-40 times faster than the DDMCMC algorithm. The algorithm can optimize over the number of models and works for general forms of posterior probabilities. Therefore it is more general than the existing graph cut approaches. 1.",
            "group": 2037,
            "name": "10.1.1.86.7089",
            "keyword": "",
            "title": "Graph Partition by Swendsen-Wang Cuts"
        },
        {
            "abstract": "Abstract-Transport triggered architecture (TTA) has been shown to provide an efficient way to design application specific instruction set processors. However, the interconnection network of TTA is based on simple-bus, which consumes much extra power for specific data transport. In this paper, we employ segmented-bus to solve this problem. How to partition the buses lies on the placement of macro blocks, and the manual creation of an optimal placement would be a labor-intensive process, leading to high design costs. Instead, we propose automatic approach to place the macro blocks specialized to given applications. For several real life cryptographic applications, a factor of 1.14 in bus power reduction can be achieved while maintaining the same performance.",
            "group": 2038,
            "name": "10.1.1.86.8560",
            "keyword": "Application specific instruction-set processorTTASegmented-busLow power design",
            "title": "Power optimization of interconnection networks for transport triggered architecture"
        },
        {
            "abstract": "Abstract: This paper presents a hybrid scheduling technique for generating the predictive schedules of passenger trains. The algorithm, which represents a combination of simulated annealing and a constraint-based heuristic, has been designed using an object-oriented methodology and it is suitable for a primarily single-track railway with some double-track sections. The search process gets started from a good initial solution created by the scheduling heuristic and continues according to the simulated annealing search control strategy. The heuristic is also used in the neighbourhood exploration process. This hybrid approach, which solves the problem in a short span of time, increases the efficiency of the simulated annealing which is known as a slow process. Simulation experiments with the real data of manual timetables and two corridors of Iran\u2019s railway shows the superiority of the hybrid method to the heuristic designed and the manual system in terms of the three performance measures used. Keywords: Train Timetabling, Constraint-based Scheduling, Railways, Simulated Annealing, Metaheuristics",
            "group": 2039,
            "name": "10.1.1.86.9671",
            "keyword": "",
            "title": "AN INTELLIGENT SEARCH TECHNIQUE FOR SOLVING TRAIN SCHEDULING PROBLEMS: SIMULATED ANNEALING & CONSTRAINT SATISFACTION"
        },
        {
            "abstract": "Abstract. Recent advances in the theory of deterministic global optimization have resulted in the development of very efficient algorithmic procedures for identifying the global minimum of certain classes of nonconvex optimization problems. The advent of powerful multiprocessor machines combined with such developments make it possible to tackle with substantial efficiency otherwise intractable global optimization problems. In this paper, we will discuss implementation issues and computational results associated with the distributed implementation of the decomposition\u2013based global optimization algorithm, GOP, [5], [6]. The NP-complete character of the global optimization problem, translated into extremely high computational requirements, had made it difficult to address problems of large size.The parallel implementation made it possible to successfully tackle the increased computational requirements in in order to identify the global minimum in computationally realistic times. The key computational bottlnecks are identified and properly addressed. Finaly, results on an Intel-Paragon machine are presented for large scale Indefinite Quadratic Programming problems, with up to 350 quadratic variables, and Blending\u2013Pooling Problems, with up to 12 components and 30 qualities.",
            "group": 2040,
            "name": "10.1.1.86.9778",
            "keyword": "and probabilistic. Deterministic methods includeLipschitzian methods9Branch and Bound methods1Cutting Plane methods7Difference of Convex Function methods15Outer Approximation methods [10Reformulation-Linearization methods [14Interval",
            "title": "Distributed Decomposition\u2013based Approaches in Global Optimization"
        },
        {
            "abstract": "In this paper, on the one hand, we aim to give a review on literature dealing with the problem of supervised learning aided by additional unlabeled data. On the other hand, being a part of the author\u2019s first year PhD report, the paper serves as a frame to bundle related work by the author as well as numerous suggestions for potential future work. Therefore, this work contains more speculative and partly subjective material than the reader might expect from a literature review. We give a rigorous definition of the problem and relate it to supervised and unsupervised learning. The crucial role of prior knowledge is put forward, and we discuss the important notion of input-dependent regularization. We postulate a number of baseline methods, being algorithms or algorithmic schemes which can more or less straightforwardly be applied to the problem, without the need for genuinely new concepts. However, some of them might serve as basis for a genuine method. In the literature review, we try to cover the wide variety of (recent) work and to classify this work into meaningful categories. We also mention work done on related problems and suggest some ideas towards synthesis. Finally, we discuss some caveats and tradeoffs of central importance to the problem.",
            "group": 2041,
            "name": "10.1.1.87.10",
            "keyword": "",
            "title": "Learning with labeled and unlabeled data"
        },
        {
            "abstract": "The routing and wavelength assignment (RWA) problem is essential for achieving efficient performance in wavelength-routed optical networks. For a network without wavelength conversion capabilities, the RWA problem consists of selecting an appropriate path and wave-length for each connection request while ensuring that paths that share common links are not assigned the same wavelength. The purpose of this research is to develop efficient adaptive methods for routing and wavelength assignment in wavelength-routed optical networks with dynamic traffic. The proposed methods utilize soft computing techniques including genetic algorithms, fuzzy control theory, simulated annealing, and tabu search. All four algorithms consider the current availability of network resources before making a routing decision. Simulations for each algorithm show that each method outperforms fixed and alternate routing strategies. The fuzzy-controlled algorithm achieved the lowest blocking rates and the shortest running times in most cases.",
            "group": 2042,
            "name": "10.1.1.87.266",
            "keyword": "Biography",
            "title": "Soft Computing Approaches to . . . "
        },
        {
            "abstract": "The paper presents two heuristics for hardwarelsoftware partitioning of system level",
            "group": 2043,
            "name": "10.1.1.87.556",
            "keyword": "",
            "title": "HardwareBoftware Partitioning with Iterative Improvement Heuristics"
        },
        {
            "abstract": "informs \u00ae doi 10.1287/moor.1040.0095",
            "group": 2044,
            "name": "10.1.1.87.873",
            "keyword": "",
            "title": "\u00a9 2004 INFORMS Convergence in Probability of Compressed Annealing"
        },
        {
            "abstract": "In this paper two different evolutionary strategies are tested by means of harmonic landscapes. Both strategies are based on ensembles of searchers, spreading over the search space according to laws inspired by nature. The main difference between the two prototypes is given by the underlying selection mechanism, governing the increase or decrease of the local population of searchers in certain regions of the search space. More precisely, we compare a thermodynamic strategy, which is based on a physically motivated local selection criterion, with a biologically motivated strategy, which features a global selection scheme (i.e., global coupling of the searchers). Confining ourselves to a special class of initial conditions, we show that, in the simple case of harmonic test potentials, both strategies possess particular analytical solutions of the same type. By means of these special solutions, the velocities of the two strategies can be compared exactly. In the last part of the paper, we extend the scope of our discussion to a mixed strategy, combining local and global selection.",
            "group": 2045,
            "name": "10.1.1.87.1153",
            "keyword": "generalized",
            "title": "Exact Solutions for Evolutionary Strategies on Harmonic Landscapes"
        },
        {
            "abstract": "approaches",
            "group": 2046,
            "name": "10.1.1.87.1583",
            "keyword": "SchedulingCommon due dateMeta-heuristic",
            "title": "Single-machine scheduling for minimizing earliness and tardiness penalties by meta-heuristic approaches"
        },
        {
            "abstract": "Abstract \u2014 We introduce a new approach for QoS provisioning in packet networks based on the notion of differentiated traffic engineering (DTE). We consider a single AS network capable of source based multi-path routing. We do not require sophisticated queuing or per-class scheduling at individual routers; instead, if a link is used to forward QoS sensitive packets, we maintain its utilization below a threshold. As a consequence, DTE eliminates the need for per-flow (IntServ) or per-class (DiffServ) packet processing tasks such as traffic classification, queueing, shaping, policing and scheduling in the core, and hence poses a lower burden on the network management unit. Conversely, DTE utilizes network bandwidth much more efficiently than simple over-provisioning. In this paper, we propose a complete architecture and an algorithmic structure for DTE. We show that our scheme can be formulated as a non-convex optimization problem, and we present an optimal solution framework based on simulated annealing. We present a simulationbased performance evaluation of DTE, and compare our scheme to existing (Gradient Projection) methods. Index Terms \u2014 Mathematical programming / optimization. A. DTE overview",
            "group": 2047,
            "name": "10.1.1.87.2022",
            "keyword": "",
            "title": "Differentiated Traffic Engineering for QoS Provisioning"
        },
        {
            "abstract": "Heterogeneous computing (HC) environments are well suited tomeet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (de ned as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a di cult problem, because comparisons are often clouded by di erent underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined",
            "group": 2048,
            "name": "10.1.1.87.2064",
            "keyword": "are Opportunistic Load BalancingUser-Directed AssignmentFast GreedyMin-minMax-minGreedyGenetic AlgorithmSimulated AnnealingGenetic Simulated AnnealingTabuand A*. This study provides",
            "title": "A Comparison Study of Static Mapping Heuristics for a Class of Meta-tasks on Heterogeneous Computing Systems"
        },
        {
            "abstract": "In many recent applications, data are continuously being disseminated from a source to a set of servers. In this paper, we propose a cost-based approach to construct dissemination trees to minimize the average loss of fidelity of the system. Our cost model takes into account both the processing cost and the communication cost. To adapt to inaccurate statistics, runtime fluctuations of data characteristics, server workloads, and network conditions etc., we propose a runtime adaptive scheme to incrementally transform a dissemination tree to a more cost-effective one. Given apriori statistics of the system, we propose two static algorithms to construct a dissemination tree for relatively static environments. The performance study shows that the adaptive mechanisms are effective in a dynamic context and the proposed static tree construction algorithms perform close to optimal in a static environment. 1.",
            "group": 2049,
            "name": "10.1.1.87.2071",
            "keyword": "",
            "title": "Adaptive reorganization of coherency-preserving dissemination tree for streaming data"
        },
        {
            "abstract": "This work deals with the animation and control of flexible and active characters. These are characters whose rigidity and shape can vary in accordance with the desired aesthetic result and goal of the motion. In our approach characters change shape and learn to move using a set of user-defined deformation models, implemented using free-form deforma-tions. Restricting the possible deformations to those which can be constructed by the set of predefined deformation models allows for both efficient simulation and predictable results. The interaction with the environment is physics-based and it is implemented using Lagrangian dynamics. Lagrangian dynamics and the use of parameterized defor-mations lead to a compact formulation of the equations of motion. Using this physical framework, the control problem can be addressed using methods that have been devel-oped for controlling the motion of simulated articulated figures. In general, our work combines key-framing, physics-based animation techniques, control and motion synthesis for flexible characters. ii",
            "group": 2050,
            "name": "10.1.1.87.2366",
            "keyword": "MichalisPiotrekJoannaDankaDavid and Richard \u201cthe bear\u201d. Special thanks to",
            "title": "PHYSICS-BASED ANIMATION AND CONTROL OF FLEXIBLE CHARACTERS"
        },
        {
            "abstract": "There has been significant progress in the area of short-term scheduling of batch processes, including the solution of industrial-sized problems, in the last 20 years. The main goal of this paper is to provide an up-to-date review of the state-of-the-art in this challenging area. Main features, strengths and limitations of existing modeling and optimization techniques as well as other available major solution methods are examined through this paper. We first present a general classification for scheduling problems of batch processes as well as for the corresponding optimization models. Subsequently, the modeling of representative optimization approaches for the different problem types are introduced in detail, focusing on both discrete and continuous time models. A comparison of effectiveness and efficiency of these models is given for two benchmarking examples from the literature. We also discuss two real-world applications of scheduling problems that cannot be readily accommodated using existing methods. For the sake of completeness, other alternative solution methods applied in the field of scheduling are also reviewed, followed by a discussion related to solving large-scale problems through rigorous optimization approaches. Finally, we list available academic and commercial software and briefly address the issue of rescheduling capabilities of the various optimization approaches.",
            "group": 2051,
            "name": "10.1.1.87.3002",
            "keyword": "short-term schedulingoptimization modelsbatch processes 1",
            "title": "STATE-OF-THE-ART REVIEW OF OPTIMIZATION METHODS FOR SHORT-TERM SCHEDULING OF BATCH PROCESSES"
        },
        {
            "abstract": "Rotating workforce scheduling appears in different forms in a broad range of workplaces, such as industrial plants, call centers, public transportation, and airline companies. It is of a high practical relevance to find workforce schedules that fulfill the ergonomic criteria of the employees, and reduce costs for the organization. In this paper we propose new heuristic methods for automatic generation of rotating workforce schedules. To improve the quality of each heuristic method alone, we further propose the hybridization of these methods. The following methods are proposed: (1)A Tabu Search (TS) based algorithm, (2) A heuristic method based on min-conflicts heuristic (MC), (3) A method that includes in the tabu search algorithm the min-conflicts heuristic (TS-MC) and random walk (TS-RW), (4) A method that includes in the min-conflicts heuristic the tabu mechanism (MC-T), random walk (MC-RW), and both the tabu mechanism and the random walk (MC-T-RW). The appropriate neighborhood structure, tabu mechanism, and fitness function, based on the specifics of the problem are proposed. The proposed methods are implemented and experimentally evaluated on the benchmark examples given in the literature and on the real life test problems, which we collected from a broad range of organizations. Empirical results show that the combination of the min-conflicts heuristic with tabu search can be used to solve this problem very effectively. The hybrid methods improve the performance of the commercial system for generation of rotating workforce schedules and are currently in the procees of being included in a commercial package for automatic generation of rotating workforce schedules.",
            "group": 2052,
            "name": "10.1.1.87.3568",
            "keyword": "rotating workforce schedulingheuristic searchminconflicts heuristic",
            "title": "Heuristic Methods for Automatic Rotating Workforce Scheduling"
        },
        {
            "abstract": "Scheduling and load-balancing techniques play an integral role in reducing the overall execution time of scientific applications on clustered multi-node systems. The increasing computational complexity and higher modeling resolutions of scientific applications makes balanced resource allocation a high priority. While many existing load-balancing approaches attempt to reduce cluster communication overhead by statically estimating job task allocation to the system configuration, there exist significant opportunities to integrate domain-specific information into adaptive scheduling strategies. One such domain is that of atmospheric geophysical data. The evolution of Atmospheric General Circulation Models (AGCMs) and the atmospheric geophysical data on which they rely deem it necessary to revisit prior work in the area of load-balancing. We present a technique that employs Very Fast Simulated Annealing (VFSA) to reduce the load-imbalances associated with radiative-transfer processes found in AGCMs. The technique is evaluated within the Load Balancing and Scheduling Framework (LBSF), a simplification of a climate model that isolates the computations of climate models that relate to load-balancing. Results of the VFSA technique are encouraging when compared to existing algorithms in a distributed AGCM. 1",
            "group": 2053,
            "name": "10.1.1.87.4175",
            "keyword": "",
            "title": "A VFSA Scheduler for Radiative Transfer data in Climate Models"
        },
        {
            "abstract": "Modeling music involves capturing long-term dependencies in time series, which has proved very difficult to achieve with traditional statistical methods. The same problem occurs when only considering rhythms. In this paper, we introduce a generative model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases. 1",
            "group": 2054,
            "name": "10.1.1.87.4495",
            "keyword": "",
            "title": "Abstract"
        },
        {
            "abstract": "Media processing applications, such as image-processing, signal processing, and graphics, motivate new processor architectures that place new burdens on the compiler. These appli-cations demand very high arithmetic rates on the order of 10-100 billion operations per",
            "group": 2055,
            "name": "10.1.1.87.4994",
            "keyword": "",
            "title": "1.1 The Problem"
        },
        {
            "abstract": null,
            "group": 2056,
            "name": "10.1.1.87.5800",
            "keyword": "",
            "title": "MIC2005: The Sixth Metaheuristics International Conference??-1 An Analysis of the Hardness of TSP Instances for Two High-performance Algorithms"
        },
        {
            "abstract": "Approved for public release; distribution unlimited",
            "group": 2057,
            "name": "10.1.1.87.6710",
            "keyword": "",
            "title": "b6. NAME OF PERFORMING ORGANIZATION /6b. OFFICE SYMBOL 7a. NAME OF MONITORING ORGANIZATION"
        },
        {
            "abstract": "Abstract \u2014 Partial transmit sequences (PTS) is a popular technique to reduce the peak-to-average power ratio (PAR) in orthogonal frequency division multiplexing (OFDM) systems. PTS is highly successful in PAR reduction and efficient redundancy utilization, but the considerable computational complexity for the required search through a high-dimensional vector space and the necessary transmission of side information (SI) to the receiver are potential problems for a practical implementation. In this paper, we revisit PTS for PAR reduction and tackle these two problems. To address the complexity issue, we formulate the search problem of PTS as a combinatorial optimization (CO) problem. This enables us to (i) unify various search strategies proposed earlier in the PTS literature and (ii) adapt efficient search algorithms known from the CO literature to PTS. We also propose a modified PTS objective function, which reduces the number of multiplications required for PTS. Numerical results show that, perhaps surprisingly, simple random search yields the best performance-complexity tradeoff for moderate PAR reduction, whereas two novel CO-based methods excel if close-to-optimum PAR reduction is desired. The SI transmission problem is solved by a simple preprocessing of the data stream before PAR reduction. This preprocessing introduces the minimal possible redundancy and allows SI embedding without affecting the PAR reduction capability of PTS or causing peak regrowth. Index terms: Orthogonal frequency division multiplexing (OFDM), peak-to-average power ratio (PAR) reduction, partial transmit sequences (PTS), combinatorial optimization, simulated annealing, tabu search, trellis shaping, side information.",
            "group": 2058,
            "name": "10.1.1.87.8974",
            "keyword": "",
            "title": "On Partial Transmit Sequences for PAR Reduction in OFDM Systems \u2217"
        },
        {
            "abstract": "Abstract: The goal of this work is to provide attendees with a survey of topics related to Heritage Preservation and Digital Archaeology, which are challenging and motivating subjects to both computer vision and graphics community. These issues have been gaining increasing attention and priority within the scientific scenario and among funding agencies and development organizations over the last years. Motivations to this work are the recent efforts in the digital preservation of cultural heritage objects and sites before degradation or damage caused by environmental factors or human development. One of the main focuses of these researches is the development of new techniques for realistic 3D model building from images, preserving as much information as possible. We intend to introduce and discuss several emerging topics in computer vision and graphics related to the proposed theme while highlighting the major contributions and advances in these fields.",
            "group": 2059,
            "name": "10.1.1.88.2090",
            "keyword": "digital archaeologyheritage preservationrange imagesmultiview registration",
            "title": "Computer Vision and Graphics for Heritage Preservation and Digital Archaeology"
        },
        {
            "abstract": "M\u00e9moire de fin d\u2019\u00e9tudes pr\u00e9sent\u00e9 par Leonora Bianchi en vue de l\u2019obtention du titre de Docteur en Sciences Appliqu\u00e9es",
            "group": 2060,
            "name": "10.1.1.88.3237",
            "keyword": "",
            "title": "Probabilistic Traveling Salesman Problem: A Case Study in Stochastic Combinatorial Optimization"
        },
        {
            "abstract": "A common approach to parallelizing simulated annealing to generate several perturbations to the current solution simultaneously, requiring synchronization to guarantee correct evaluation of the cost function. The cost of this synchronization may be reduced by allowing inaccuracies in the cost calculations. We provide a framework for understanding the theoretical implications of this approach based on a model of processor interaction under reduced synchronization that demonstrates how errors in cost calculations occur and how to estimate them. We show how bounds on error in the cost calculations in a simulated annealing algorithm can be translated into worst-case bounds on perturbations in the parameters which describe the behavior of the algorithm.",
            "group": 2061,
            "name": "10.1.1.88.3833",
            "keyword": "Parallel simulated annealingMetropolis algorithm",
            "title": "Trading Accuracy for Speed in Parallel Simulated Annealing with Simultaneous Moves \u2217"
        },
        {
            "abstract": "More powerful personal computers and higher network bandwidth has meant that graphics has become increasingly important on the web. Graph-based diagrams are one of the most important types of structured graphical information. Here we demonstrate how XML can be used as basis for contents-based delivery of graph-based diagrams. The main distinguishing feature of our approach is that it separates style and content of diagrams in the same way as (XML-based) markup languages for textual information do: The diagram itself is marked-up according to its logical structure and its visual appearance is defined via attached style-sheets. Such an approach poses interesting challenges for the browser component, because it requires automatic layout of complex diagrammatic information that takes stylistic constraints into account. We present a prototype system for our approach comprised of three main components: A contents-based markup language, GXML, for specifying graph-based diagrams, a style sheet language, GXSL, for such diagrams and a browser that can display styled graphs from this information. keywords: graph layout, constraints, style sheets, World Wide Web. 1",
            "group": 2062,
            "name": "10.1.1.88.3861",
            "keyword": "",
            "title": "Flexible Graph Layout for the Web"
        },
        {
            "abstract": "The Traveling Salesperson Problem (TSP), is an NP-complete combinatorial optimization problem of substantial importance in many scheduling applications. Here we show the viability of SPAN, a hybrid approach to solving the TSP that incorporates a perturbation method applied to a classic heuristic in the overall context of a probabilistic search control strategy. In particular, the heuristic for the TSP is based on the minimal spanning tree of the city locations, the perturbation method is a simple modification of the city locations, and the control strategy is a genetic algorithm (GA). The crucial concept here is that the perturbation of the problem (since the city locations specify the problem instance) allows variant solutions (to the perturbed problem) to be generated by the heuristic and applied to the original problem, thus providing the GA with capabilities for both exploration and exploitation in its search process. We demonstrate that SPAN outperforms, with regard to solution quality, one of the best GA systems reported in the literature.",
            "group": 2063,
            "name": "10.1.1.88.4971",
            "keyword": "Genetic algorithmsTraveling salesperson problemprobabilistic searchminimum spanning tree",
            "title": "Perturbation Method for Probabilistic Search for the Traveling"
        },
        {
            "abstract": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1",
            "group": 2064,
            "name": "10.1.1.88.5058",
            "keyword": "",
            "title": "Forest rescoring: Faster decoding with integrated language models"
        },
        {
            "abstract": "Many of the today\u2019s problems, such as those involved in weather prediction, aerodynamics, and genetic mapping, require tremendous computational resources to be solved accurately. These applications are computationally very intensive and require vast amounts of processing power and memory requirements. Therefore, to give accurate results, powerful computers are needed to reduce the run time, for example, finding genes in DNA sequences, predicting the structure and functions of new proteins, clustering proteins into families, aligning similar proteins, and generating phylogenetic trees to examine evolutionary relationships all need complex computations. To develop parallel computing programs for such kinds of computational biology problems, the role of a computer architect is important; his or her role is to design and engineer the various levels of a computer system to maximize performance and programmability within limits of technology and cost. Thus, parallel computing is an effective way to tackle problems in biology; multiple processors being used to solve the same problem. The scaling of memory with processors enables the solution of larger problems than would be otherwise possible, while modeling a",
            "group": 2065,
            "name": "10.1.1.88.5112",
            "keyword": "",
            "title": "Parallel and Evolutionary Approaches to Computational Biology"
        },
        {
            "abstract": "A snake-in-the box code, first described in [Kautz, 1958], is an achordal open path in a hypercube. Finding bounds for the longest snake at each dimension has proven to be a difficult problem in mathematics and computer science. Evolutionary techniques have succeeded in tightening the bounds of longest snakes in several dimensions [Potter, 1994] [Casella, 2005]. This thesis utilizes an Iterated Local Search heuristic with adaptive memory on the snake-in-the-box problem. The results match the best published results for problem instances up to dimension 8. The lack of implicit parallelism segregates this experiment from previous heuristics applied to this problem. As a result, this thesis provides insight into those problems in which evolutionary methods dominate.",
            "group": 2066,
            "name": "10.1.1.88.5128",
            "keyword": "ITERATED LOCAL SEARCH WITH ADAPTIVE MEMORY APPLIED TO THE SNAKE",
            "title": "IN THE BOX PROBLEM"
        },
        {
            "abstract": "iii First, I would like to thank my advisor, Professor K. Mani Chandy, for giving me tremendous support and guidance throughout the years of my graduate study. I feel extremely fortunate and honored to be his student and have the opportunity to work with him. He is not only an advisor and a friend, but also my role model. I especially want to thank him for the great effort he puts into helping his students find and achieve their personal goals in life. I would also like to thank the members of my group\u2014Mr. Agostino Capponi, Mr. Andrey Khorlin, and Dr. Daniel M. Zimmerman\u2014for their help and collaboration, and their effort in creating an excellent working environment. I must also acknowledge my good friends Patrick Hung, Kevin Tang, and Daniel M. Zim-merman for their great support, help, and friendship throughout the years, especially their excellent advice and inspiration regarding my thesis work. Last but not least, I want to thank my family, my mother, my father, and my twin sister for always being there for me with remarkable support and understanding.",
            "group": 2067,
            "name": "10.1.1.88.5457",
            "keyword": "",
            "title": "(Lu Tian)"
        },
        {
            "abstract": "We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance. 1",
            "group": 2068,
            "name": "10.1.1.88.6223",
            "keyword": "",
            "title": "J.M.: The noisy euclidean traveling salesman problem and learning"
        },
        {
            "abstract": "Abstract. A method is given to construct tight frames that minimize an error term, which in quantum physics has the interpretation of the probability of a detection error. The method converts the frame problem into a set of ordinary differential equations using concepts from classical mechanics and orthogonal group techniques. The minimum energy solutions of the differential equations are proven to correspond to the tight frames that minimize the error term. Because of this perspective, several numerical methods become available to compute the tight frames. Beyond the applications of quantum detection in quantum mechanics, solutions to this frame optimization problem can be viewed as a generalization of classical matched filtering solutions. As such, the methods we develop are a generalization of fundamental detection techniques in radar. 1.",
            "group": 2069,
            "name": "10.1.1.88.6277",
            "keyword": "",
            "title": "THE ROLE OF FRAME FORCE IN QUANTUM DETECTION"
        },
        {
            "abstract": "Simulated annealing and and single trial versions of evolution strategies possess a close relationship when they are designed for optimization over continuous variables. Analytical investigations of their di erences and similarities lead to a cross-fertilization of both approaches, resulting in new theoretical results, new parallel population based algorithms, and a better understanding of the interrelationships.",
            "group": 2070,
            "name": "10.1.1.88.6558",
            "keyword": "global optimizationparallel simulated annealingparallel evolutionary algo- rithmsneighborhood",
            "title": "Massively Parallel Simulated Annealing and its Relation to Evolutionary Algorithms"
        },
        {
            "abstract": "This survey describes research directions in netlist partitioning during the past two decades, in terms of both problem formulations and solution approaches. We discuss the traditional min-cut and ratio cut bipartitioning formulations along with multi-way extensions and newer problem formulations, e.g., constraint-driven partitioning (for FPGAs) and partitioning with module replication. Our discussion of solution approaches is divided into four major categories: move-based approaches, geometric representations, combinatorial formulations, and clustering approaches. Move-based algorithms iteratively explore the space of feasible solutions according to a neighborhood operator; such methods include greed, iterative exchange, simulated annealing, and evolutionary algorithms. Algorithms based on geometric representations embed the circuit netlist in some type of \\geometry&quot;, e.g, a 1-dimensional linear ordering or a multi-dimensional vector space; the embeddings are commonly constructed using spectral methods. Combinatorial methods transform the partitioning problem into another type of optimization, e.g., based on network ows or mathematical programming. Finally, clustering algorithms merge the netlist modules into many small clusters; we discuss methods which combine clustering with existing algorithms (e.g., two-phase partitioning). The paper concludes with a discussion of benchmarking in the VLSI CAD partitioning literature and some perspectives on more promising directions for future work. 1",
            "group": 2071,
            "name": "10.1.1.88.6975",
            "keyword": "",
            "title": "Recent directions in netlist partitioning: A survey"
        },
        {
            "abstract": "This paper reviews the various studies that have introduced adaptive and selfadaptive parameters into Evolutionary Computations. A formal definition of an adaptive evolutionary computation is provided with an analysis of the types of adaptive and self-adaptive parameter update rules currently in use. Previous studies are reviewed and placed into a categorization that helps to illustrate their similarities and differences.",
            "group": 2072,
            "name": "10.1.1.88.7230",
            "keyword": "",
            "title": "Adaptive and Self-Adaptive Evolutionary Computations"
        },
        {
            "abstract": "A perspective of biomolecular simulations today is given, with illustrative applications and an emphasis on algorithmic challenges, as reflected by the work of a multidisciplinary team of investigators from five institutions. Included are overviews and recent descriptions of algorithmic work in long-time integration for molecular dynamics; fast electrostatic evaluation; crystallographic refinement approaches; and implementation of large, computation-intensive programs on modern architectures. Expected future developments of the field are also discussed. c \u25cb 1999 Academic Press Key Words: biomolecular simulations; molecular dynamics; long-time integration; fast electrostatics; crystallographic refinement; high-performance platforms.",
            "group": 2073,
            "name": "10.1.1.88.7611",
            "keyword": "",
            "title": "Algorithmic challenges in computational molecular biophysics"
        },
        {
            "abstract": "This thesis develops an algorithm that combines color and motion information to produce superior optical flow and segmentation output. The algorithm\u2019s main contribution is its ability to exploit the duality of segmentation and flow information. In areas where color/texture segmentation is well-defined, optical flow is under-constrained, and vice-versa. The thesis offers a review of relevant literature, a detailed description of the algorithm, and information about its performance on a variety of natural images.",
            "group": 2074,
            "name": "10.1.1.88.7789",
            "keyword": "Acknowledgments",
            "title": "Certified by.........................................................."
        },
        {
            "abstract": "Abstract\u2014Computer-aided design (CAD) tools are now making it possible to automate many aspects of the design process. This has mainly been made possible by the use of effective and efficient algorithms and corresponding software structures. The very large scale integration (VLSI) design process is extremely complex, and even after breaking the entire process into several conceptually easier steps, it has been shown that each step is still computationally hard. To researchers, the goal of understanding the fundamental structure of the problem is often as important as producing a solution of immediate applicability. Despite this emphasis, it turns out that results that might first appear to be only of theoretical value are sometimes of profound relevance to practical problems. VLSI CAD is a dynamic area where problem definitions are continually changing due to complexity, technology and design methodology. In this paper, we focus on several of the fundamental CAD abstractions, models, concepts and algorithms that have had a significant impact on this field. This material should be of great value to researchers interested in entering these areas of research, since it will allow them to quickly focus on much of the key material in our field. We emphasize algorithms in the area of test, physical design, logic synthesis, and formal verification. These algorithms are responsible for the effectiveness and efficiency of a variety of CAD tools. Furthermore, a number of these algorithms have found applications in many other domains. Index Terms\u2014Algorithms, computer-aided design, computational complexity, formal verification, logic synthesis, physical design, test. I.",
            "group": 2075,
            "name": "10.1.1.88.8487",
            "keyword": "",
            "title": "Fundamental CAD algorithms"
        },
        {
            "abstract": "Abstract This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies\u2019 foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the \ufffdrst part of the article the basic biological \ufffdndings on real ants are reviewed and their arti\ufffdcial counterparts as well as the ACO metaheuristic are de\ufffdned. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of",
            "group": 2076,
            "name": "10.1.1.88.9811",
            "keyword": "",
            "title": "Ant algorithms for discrete optimization"
        },
        {
            "abstract": "We introduce a technique for analyzing the behavior of sophisticated A.I. search programs working on realistic, large-scale problems. This approach allows us to predict where, in a space of problem instances, the hardest problems are to be found and where the fluctuations in difficulty are greatest. Our key insight is to shift emphasis from modelling sophisticated algorithms directly to modelling a search space that captures their principal effects. We compare our model\u2019s predictions with actual data on real problems obtained independently and show that the agreement is quite good. By systematically relaxing our underlying modelling assumptions we identify their relative contribution to the remaining error and then remedy it. We also discuss further applications of our model and suggest how this type of analysis can be generalized to other kinds of A.I. problems. Chapter 1",
            "group": 2077,
            "name": "10.1.1.89.140",
            "keyword": "",
            "title": "Exploiting the deep structure of constraint problems"
        },
        {
            "abstract": "University of Illinois at Urbana\u2212Champaign, College of Business International business research has long acknowledged the importance of regional factors for foreign direct investment (\u201cFDI\u201d) by multinational corporations (\u201cMNCs\u201d). However, significant differences when defining these regions obscure the analysis about how and why regions matter. In response, we develop and empirically document support for a framework to evaluate alternative regional grouping schemes. We demonstrate application of this evaluative framework using data on the global location decisions by US\u2212based MNCs from 1980\u22122000 and two alternative regional grouping schemes. We conclude with discussion of implications for future academic research related to understanding the impact of country groupings on MNC FDI decisions.",
            "group": 2078,
            "name": "10.1.1.89.205",
            "keyword": "",
            "title": "NEW METHODS FOR EX POST EVALUATION OF REGIONAL GROUPING SCHEMES IN INTERNATIONAL BUSINESS RESEARCH: A SIMULATED ANNEALING APPROACH*"
        },
        {
            "abstract": "",
            "group": 2079,
            "name": "10.1.1.89.718",
            "keyword": "",
            "title": "S.: A survey of computational approaches to threedimensional layout problems. Computer-Aided Design 34"
        },
        {
            "abstract": "We present a fully automatic method for the alignment SAR images, which is capable of precise and robust alignment. A multiresolution SAR image matching metric is first used to automatically determine tie-points, which are then used to perform coarse-to-fine resolution image alignment. A formalism is developed for the automatic determination of tie-point regions that contain sufficiently distinctive structure to provide strong constraints on alignment. The coarse-to-fine procedure for the refinement of the alignment estimate both improves computational efficiency and yields robust and consistent image alignment.",
            "group": 2080,
            "name": "10.1.1.89.1454",
            "keyword": "RegistrationAlignmentCoarse-to-fineMultiresolutionTie-PointTexture",
            "title": "Structure-driven SAR image registration a Learning & Vision Group"
        },
        {
            "abstract": "In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures. 1",
            "group": 2081,
            "name": "10.1.1.89.4429",
            "keyword": "",
            "title": "MIMIC: Finding Optima by Estimating Probability Densities"
        },
        {
            "abstract": "The close links that now exist between biology and computation raise the prospect of setting species evolution in a broader, computational context. This context would comprise theories about the origins of pattern and order in complex systems. We argue that several processes play crucial roles in both natural and artificial evolution. One mechanism consists of critical changes between different system phases that promote order and variation respectively. Processes that lead to modular structures promote both robustness and increasing complexity in evolution. Examples of such processes include genetic convergence, gene shuffling and ordered asynchronous processing. We demonstrate that gene shuffling causes colocation of genes on chromosomes, thus promoting genetic modularity.",
            "group": 2082,
            "name": "10.1.1.89.4560",
            "keyword": "natural computationphase changesmodularityordered asynchronous processinggene",
            "title": "On evolutionary processes in natural and artificial systems"
        },
        {
            "abstract": "Augmented reality requires real and virtual objects to be registered in three dimensions from any viewing direction. Therefore, accurate, large field of regard head tracking is needed. As a part of a research effort to design probes to track the position and orientation of the head of a user in a virtual environment, an algorithm is provided for the uniform distribution of an arbitrary number of beacons on a spherical probe using simulated annealing. The validity of the algorithm is tested by comparison to the tetrahedron, octahedron, and icosahedron, which are spherical equivalents. Additionally, variations upon the cooling schedule implemented in the algorithm and the effects upon the resulting point distributions are examined. Finally, a successfully constructed headtracking probe is presented and the generalization of the algorithm to probes of other shapes is discussed.",
            "group": 2083,
            "name": "10.1.1.89.6019",
            "keyword": "Augmented RealityVirtual EnvironmentsHead TrackingProbe DesignSimulated Annealing",
            "title": "Methods for designing head-tracking probes"
        },
        {
            "abstract": "A simulated annealing approach for manufacturing",
            "group": 2084,
            "name": "10.1.1.89.6297",
            "keyword": "ManufacturingCellular manufacturing systemsSimulated annealing",
            "title": "cell"
        },
        {
            "abstract": "The Imprecise Computation technique has been proposed as an approach to the construction of realtime systems that are able to provide both guarantee and flexibility. This paper analyzes the use of Imprecise Computation in the scheduling of distributed real-time applications. Initially it is presented an approach to the scheduling of distributed imprecise tasks. Then we discuss the main problems associated with that goal and some possible solutions. Keywords: Real-time systems, scheduling, imprecise computation, distributed systems.",
            "group": 2085,
            "name": "10.1.1.89.6330",
            "keyword": "",
            "title": "On Scheduling Imprecise Tasks in Real-Time Distributed Systems"
        },
        {
            "abstract": "We consider combinatorial optimization problems for which the formation of a neighborhood structure of feasible solutions is impeded by a set of constraints. Neighborhoods are recovered by relaxing the complicating constraints into the objective function within a penalty term. We examine a heuristic called compressed annealing that integrates a variable penalty multiplier approach within the framework of simulated annealing. We refer to the value of the penalty multiplier as \u201cpressure. \u201d We analyze the behavior of compressed annealing by exploring the interaction between temperature (which controls the ability of compressed annealing to climb hills) and pressure (which controls the height of the hills). We develop a necessary and sufficient condition on the joint cooling and compression schedules for compressed annealing to converge in probability to the set of global minima. Our work generalizes the results of Hajek (1988) in the sense that when there are no relaxed constraints, our results reduce to his.",
            "group": 2086,
            "name": "10.1.1.89.6421",
            "keyword": "simulated annealingpenalty methodsconstrained optimization",
            "title": "Convergence in Probability of Compressed Annealing"
        },
        {
            "abstract": "Home health care, i.e. visiting and nursing patients in their homes, is a growing sector in the medical service business. From a staff rostering point of view, the problem is to find a feasible working plan for all nurses that has to respect a variety of hard and soft constraints, and preferences. Additionally, home health care problems contain a routing component: A nurse must be able to visit her patients in a given roster using a car or public transport. It is desired to design rosters that consider both, the staff rostering and vehicle routing components while minimizing transportation costs and maximizing satisfaction of patients and nurses. In this paper we present the core optimization components of the PARPAP software. In the optimization kernel, a combination of linear programming, constraint programming, and (meta-)heuristics for the home health care problem is used, and we show how to apply these different heuristics efficiently to solve home health care problems. The overall concept is able to adapt to various changes in the constraint structure, thus providing the flexibility needed in a generic tool for real-world settings. 1",
            "group": 2087,
            "name": "10.1.1.89.6747",
            "keyword": "",
            "title": "Proceedings CPAIOR\u201903 A Hybrid Setup for a Hybrid Scenario: Combining Heuristics for the Home Health Care Problem"
        },
        {
            "abstract": "An ambitious goal in the area of physics-based computer animation is the creation of virtual actors that autonomously synthesize realistic human motions and possess a broad repertoire of lifelike motor skills. To this end, the control of dynamic, anthropomorphic figures subject to gravity and contact forces remains a difficult open problem. We propose a framework for composing controllers in order to enhance the motor abilities of such figures. A key contribution of our composition framework is an explicit model of the \u201cpre-conditions \u201d under which motor controllers are expected to function properly. We demonstrate controller composition with pre-conditions determined not only manually, but also automatically based on Support Vector Machine (SVM) learning theory. We evaluate our composition framework using a family of controllers capable of synthesizing basic actions such as balance, protective stepping when balance is disturbed, protective arm reactions when falling, and multiple ways of standing up after a fall. We furthermore demonstrate these basic controllers working in conjunction with more dynamic motor skills within a two-dimensional and a three-dimensional prototype virtual stuntperson. Our composition framework promises to enable the community of physics-based animation practitioners to more easily exchange motor controllers and integrate them into dynamic characters. ii",
            "group": 2088,
            "name": "10.1.1.89.6785",
            "keyword": "",
            "title": "Abstract Composable Controllers for Physics-Based Character Animation"
        },
        {
            "abstract": "If we know that the objective function has only one maximum then all up-hill trails lead to that maximum andwecan nditslocation (and therefore the best- tting model) by simply stepping uphill again and again from any convenient starting point. There are manyways of going upthe surface of the objectivefunction to amaximum\ufffdthey generally rely on estimates of the derivatives of the objective function in the neighborhood of the starting point. These techniques are collectively called \\hill-climbing &quot; and are usually quite straightforward: follow the gradient vector uphill until the gradient is zero or as close to zero are you are willing totolerate. If there is only one local maximumthen the resulting model is the best solution to the inverse problem. Unfortunately, many geophysically interesting inverse problems have objective functions which are highly complicated and possess many local maxima. As a consequence, the result ofaniterative, gradient-following, or hill-climbing calculation may depend strongly on the starting model: the algorithm may very well converge to alocalmaximumwhich is not the globally highest peak. An example of this complexity isshown in Figure 11.1. The computation from which",
            "group": 2089,
            "name": "10.1.1.89.7015",
            "keyword": "",
            "title": "Chapter 11 Genetic Algoritms"
        },
        {
            "abstract": "Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two largescale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient \u201ctime machine \u201d scalable to large versioned",
            "group": 2090,
            "name": "10.1.1.89.7329",
            "keyword": "Time-Travel Text SearchWeb Archives Contents",
            "title": "Authors \u2019 Addresses"
        },
        {
            "abstract": "On the discrete Bak-Sneppen model of self-organized criticality We propose a discrete variant of the Bak-Sneppen model for self-organized criticality. In this process, a configuration is an n-bit word, and at each step one chooses a random bit of minimum value (usually a zero) and replaces it and its two neighbors by independent Bernoulli variables with parameter p. We prove bounds on the average number of ones in the stationary distribution and present experimental results. 1",
            "group": 2091,
            "name": "10.1.1.89.7785",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. This is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. We give a survey of the nowadays most important metaheuristics from a conceptual point of view. We outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. Two very important concepts in metaheuristics are intensification and diversification. These are the two forces that largely determine the behavior of a metaheuristic. They are in some way contrary but also complementary to each other. We introduce a framework, that we call the I&D frame, in order to put different intensification and diversification components into relation with each other. Outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.",
            "group": 2092,
            "name": "10.1.1.89.7958",
            "keyword": "Categories and Subject DescriptorsG.2.1 [Discrete MathematicsCombinatorics\u2014 combinatorial algorithmsI.2.8 [Artificial IntelligenceProblem SolvingControl Methodsand Search\u2014heuristic methods General TermsAlgorithms Additional Key Words and PhrasesMetaheuristicscombinatorial optimizationintensificationdiversification. C. Blum acknowledges support by the \u201cMetaheuristics Network\u201d a Research Training Network funded by",
            "title": "AND"
        },
        {
            "abstract": "Abstract. This paper explores the use of the stochastic optimization technique of simulated annealing for map generalization. An algorithm is presented that performs operations of displacement, size exaggeration, deletion and size reduction of multiple map objects in order to resolve graphic conflict resulting from map scale reduction. It adopts a trial position approach in which each of n discrete polygonal objects is assigned k candidate trial positions that represent the original, displaced, size exaggerated, deleted and size reduced states of the object. This gives rise to a possible k n distinct map configurations; the expectation is that some of these configurations will contain reduced levels of graphic conflict. Finding the configuration with least conflict by means of an exhaustive search is, however, not practical for realistic values of n and k. We show that evaluation of a subset of the configurations, using simulated annealing, can result in effective resolution of graphic conflict. 1.",
            "group": 2093,
            "name": "10.1.1.89.9119",
            "keyword": "",
            "title": "Research Article Automated map generalization with multiple operators: a simulated annealing approach"
        },
        {
            "abstract": "algorithms",
            "group": 2094,
            "name": "10.1.1.89.9786",
            "keyword": "",
            "title": "An optimization system for container loading based on metaheuristic"
        },
        {
            "abstract": "(Dr. sc. nat.) vorgelegt der",
            "group": 2095,
            "name": "10.1.1.89.9806",
            "keyword": "",
            "title": "Mathematisch-naturwissenschaftlichen Fakult\u00e4t"
        },
        {
            "abstract": "(Dr. sc. nat.) vorgelegt der",
            "group": 2096,
            "name": "10.1.1.89.9806",
            "keyword": "",
            "title": "Mathematisch-naturwissenschaftlichen Fakult\u00e4t"
        },
        {
            "abstract": "Abstract. It is well known that domain specific heuristics can produce good quality solutions for timetabling problems in a short amount of time. However they often lack the ability to do any thorough optimisation. In this paper we will study the effects of applying local search",
            "group": 2097,
            "name": "10.1.1.90.442",
            "keyword": "",
            "title": "Enhancing timetable solutions with local search methods"
        },
        {
            "abstract": "We describe features of inverse problems and illustrate them with simple examples. The focus is on the main concepts and caveats rather than mathematical detail. Properties of linear and nonlinear inverse problems are discussed, and the effect of non-uniqueness highlighted. Iterative algorithms for nonlinear problems are briefly introduced and comments on their performance included. Fully nonlinear direct search algorithms are also mentioned. An example is given in which the performance of a direct search and an iterative optimization algorithm is compared.",
            "group": 2098,
            "name": "10.1.1.90.488",
            "keyword": "Inverse theoryParameter estimationInference techniques",
            "title": "Inverse problems in a nutshell"
        },
        {
            "abstract": "Parallel ELLPACK [35, 61] is a machine independent problem solving environment (PSE) that supports PDE (partial di erential equations) computing across many hardware platforms. In this paper we review parallel methodologies based on the \\divide and conquer&quot; computational paradigm and their infrastructure for solving general elliptic PDEs. Particularly, we describe those that have been implemented and tested in the parallel ELLPACK PSE. Moreover, we describe two parallel frameworks that allow the reuse of the discretization part of the sequential elliptic PDE solvers. Numerical results indicate the e ectiveness of the reuse frameworks implemented. 1",
            "group": 2099,
            "name": "10.1.1.90.1994",
            "keyword": "",
            "title": "Parallel ELLPACK Elliptic PDE Solvers"
        },
        {
            "abstract": "Partial support of the EC Centre of Excellence programme (No. ICA1-CT-2000-70025) and",
            "group": 2100,
            "name": "10.1.1.90.2554",
            "keyword": "",
            "title": "Placement of Nodes in an Adaptive Distributed Multimedia Server"
        },
        {
            "abstract": "Abstract. In this paper we present the results of an investigation of the possibilities offered by three wellknown metaheuristic algorithms to solve the timetable problem, a multi-constrained, NP-hard, combinatorial optimization problem with real-world applications. First, we present our model of the problem, including the definition of a hierarchical structure for the objective function, and of the neighborhood search operators which we apply to matrices representing timetables. Then we report about the outcomes of the utilization of the implemented systems to the specific case of the generation of a school timetable. We compare the results obtained by simulated annealing, tabu search and two versions, with and without local search, of the genetic algorithm. Our results show that GA with local search and tabu search based on temporary problem relaxations both outperform simulated annealing and handmade timetables.",
            "group": 2101,
            "name": "10.1.1.90.3861",
            "keyword": "timetable problemtabu searchsimulated annealinggenetic algorithms",
            "title": "Computational Optimization and Applications 9, 275\u2013298 (1998) c \u25cb 1998 Kluwer Academic Publishers. Manufactured in The Netherlands."
        },
        {
            "abstract": "The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a prob-lem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connec-tivity structure. 1.",
            "group": 2102,
            "name": "10.1.1.90.7282",
            "keyword": "",
            "title": "A learning algorithm for Boltzmann machines"
        },
        {
            "abstract": "Scope and Purpose-A summary is provided of some of the recent (and a few not-so-recent) developments that otTer promise for enhancing our ability to solve combinatorial optimization problems. These developments may be usefully viewed as a synthesis of the perspectives of operations research and artificial intelligence. Although compatible with the use of algorithmic subroutines, the frameworks examined are primarily heuristic, based on the supposition that etTective solution of complex combinatorial structures in some cases may require a level of flexibility beyond that attainable by methods with formally demonstrable convergence properties. Abstract-Integer programming has benefited from many innovations in models and methods. Some of the promising directions for elaborating these innovations in the future may be viewed from a framework that links the perspectives of artificial intelligence and operations research. To demonstrate this, four key areas are examined: (1) controlled randomization, (2) learning strategies, (3) induced decomposition and (4) tabu search. Each of these is shown to have characteristics that appear usefully relevant to developments on the horizon. 1.",
            "group": 2103,
            "name": "10.1.1.90.7851",
            "keyword": "",
            "title": "Copyright @ 1986 Pergamon Journals Lid FUTURE PATHS FOR INTEGER PROGRAMMING AND LINKS TO"
        },
        {
            "abstract": "I am deeply appreciative of the committee members who evaluated this disserta-tion. They provided essential and timely feedback on this thesis as well as other collaborative efforts. I sincerely thank Karsten, Ramesh, Gregory and Mark for their cooperation. During my long career at Georgia Tech, I have encountered a long list of dedicated, knowledgeable and compassionate faculty and staff. Gus Baird, Jim Greenlee, Mark Guzdial, Ellen Zegura, H. Venkateswaran and Yannis Smaragdakis deserve special mention for having motivated me to pursue excellence in research and academics. Among the many staff members I have relied on, Barbara Binder, Cathy Dunnahoo, and Dani Denton also deserve special mention for helping me negotiate the graduate student path. I will always remember fondly the dedication and friendliness of the support staff, especially Neil Bright, and research scientists Phil Hutto and Matthew Wolf. I am highly indebted to them for their guidance and friendship. I wish to thank Brian Cooper for his advice on some of the evaluations in this thesis. A huge part of my success at Georgia Tech has been the undying support of fellow",
            "group": 2104,
            "name": "10.1.1.90.7883",
            "keyword": "",
            "title": "ACKNOWLEDGEMENTS"
        },
        {
            "abstract": "Abstract. The performance of Support Vector Machines, as many other machine learning algorithms, is very sensitive to parameter tuning, mainly in real world problems. In this paper, two well known and widely used SVM implementations, Weka SMO and LIBSVM, were compared using Simulated Annealing as a parameter tuner. This approach increased significantly the classification accuracy over the Weka SMO and LIBSVM standard configuration. The paper also presents an empirical evaluation of SVM against AdaBoost and MLP, for solving the leather defect classification problem. The results obtained are very promising in successfully discriminating leather defects, with the highest overall accuracy, of 99.59%, being achieved by LIBSVM tuned with Simulated Annealing.",
            "group": 2105,
            "name": "10.1.1.90.8062",
            "keyword": "Key wordsSupport Vector MachinesPattern RecognitionParameter Tuning",
            "title": "SVM with Stochastic Parameter Selection for Bovine Leather Defect Classification"
        },
        {
            "abstract": "To my parents, For all the sacrifices you have made as well as the love and the support you have always provided me. iii ACKNOWLEDGEMENTS I would first like to thank my dissertation committee members, Jim Xu, Constantinos Dovrolis, Ellen Zegura and Henry Owen, for all of their time and suggestions. I greatly appreciate their insight and comments that have influenced and improved my research. I am so fortunate to be a part of the Networking and Telecommunication research group, from which I have received not only inspiration and technical support but also encourage-ment and friendship during the years. Paul Judge, Li Zou, Zongming Fei, and Shashidhar Merugu provided me with very useful feedback on my research on many occasions. Qi He, Taehyun Kim, Pradnya Karbhari and Donghua Xu offered a lot of help in job searching. All the members of the NTG group have played an important role in making my years at the GCATT building pleasant and unforgettable.",
            "group": 2106,
            "name": "10.1.1.90.8855",
            "keyword": "",
            "title": "Approved by: Enabling Performance Tradeoffs Through Dynamic Configuration of Advanced Network Services"
        },
        {
            "abstract": "Abstract. We describe a linear-time algorithm that recovers absolute camera orientations and positions, along with uncertainty estimates, for networks of terrestrial image nodes spanning hundreds of meters in outdoor urban scenes. The algorithm produces pose estimates globally consistent to roughly 0.1 \u25e6 (2 milliradians) and 5 centimeters on average, or about four pixels of epipolar alignment. We assume that adjacent nodes observe overlapping portions of the scene, and that at least two distinct vanishing points are observed by each node. The algorithm decouples registration into pure rotation and translation stages. The rotation stage aligns nodes to commonly observed scene line directions; the translation stage assigns node positions consistent with locally estimated motion directions, then registers the resulting network to absolute (Earth) coordinates. The paper\u2019s principal contributions include: extension of classic registration methods to large scale and dimensional extent; a consistent probabilistic framework for modeling projective uncertainty; and a new hybrid of Hough transform and expectation maximization algorithms. We assess the algorithm\u2019s performance on synthetic and real data, and draw several conclusions. First, by fusing thousands of observations the algorithm achieves accurate registration even in the face of significant lighting variations, low-level feature noise, and error in initial pose estimates. Second, the algorithm\u2019s robustness and accuracy increase with image field of view. Third, the algorithm surmounts the usual tradeoff between speed and accuracy; it is both faster and more accurate than manual bundle adjustment.",
            "group": 2107,
            "name": "10.1.1.90.9269",
            "keyword": "Exterior orientationegomotionstructure from motionpanoramas",
            "title": "Scalable extrinsic calibration of omnidirectional image networks"
        },
        {
            "abstract": "As Field Programmable Gate Array (FPGA) power consumption continues to increase, lower power FPGA circuitry, architectures, and Computer-Aided Design (CAD) tools need to be developed. Before designing low-power FPGA circuitry, architectures, or CAD tools, we must first determine where the biggest gains (in terms of energy reduction) are to be made and whether these gains are cumulative. In this thesis, we focus on FPGA CAD tools. Specifically, we describe a new power-aware CAD flow for FPGAs that was developed to answer the above questions. Estimating energy using very detailed post-route power and delay models, we determine the gains obtained by our power-aware technology mapping, clustering, placement, and routing algorithms and investigate how each gain behaves when the algorithms are applied concurrently. The individual energy reductions of the power-aware technology-mapping, clustering, placement, and routing algorithms were 7.6%, 12.6%, 3.0%, and 2.6 % respectively. The majority of the overall energy reduction was achieved during the technology mapping and clustering stages of the power-aware FPGA CAD flow. In addition, the gains were mostly cumulative when the individual power-aware CAD algorithms were applied concurrently with",
            "group": 2108,
            "name": "10.1.1.91.726",
            "keyword": "TABLE OF CONTENTS...................................................................................",
            "title": "ABSTRACT ON THE INTERACTION BETWEEN POWER-AWARE COMPUTER-AIDED DESIGN ALGORITHMS FOR FIELD-PROGRAMMABLE GATE ARRAYS"
        },
        {
            "abstract": "Optical character recognition (OCR) systems for machine-printed documents typically require large numbers of font styles and character models to work well. When given a document printed in an unseen font, the performance of those systems degrade even in the absence of noise. In this paper, we perform OCR in an unsupervised fashion without using any character models by using a cryptogram decoding algorithm. We present results on real and artificial OCR data. 1",
            "group": 2109,
            "name": "10.1.1.91.898",
            "keyword": "",
            "title": "Cryptogram decoding for optical character recognition"
        },
        {
            "abstract": "Abstract-Our deterministic annealing approach to clustering is derived on the basis of the principle of maximum entropy, is independent of the initial state, and produces natural hier-archical clustering solutions by going through a sequence of phase transitions. This approach is modified here for a larger class of optimization problems by adding constraints to the free energy. The concept of constrained clustering is explained, and then, three examples are given in which it is used as means to introduce deterministic annealing. First, the previous clustering method is improved by adding cluster mass variables and a total mass constraint. Second, the traveling salesman problem (TSP) is reformulated as constrained clustering, yielding the elastic net (EN) approach to the problem. More insight is gained by identifying a second Lagrange multiplier that is related to the tour length add can also be used to control the annealing process. Finally, the \u201copen path \u201d constraint formulation is shown to relate to dimensionality reduction by self-organization in unsupervised learning. A similar annealing procedure is applicable in this case as well. Index Terms-Annealing, clustering, maximum entropy, neural networks, nonconvex optimization, self-organization.",
            "group": 2110,
            "name": "10.1.1.91.1171",
            "keyword": "",
            "title": "Constrained clustering as an optimization method"
        },
        {
            "abstract": "The AGDISP aerial spray simulation model is used to predict the deposition of spray material released from an aircraft. Determining the optimal input values to AGDISP in order to produce a desired spray material deposition is extremely difficult (NP hard). SAGA, an intelligent optimization method based on the simple genetic algorithm, was developed to solve this problem. Our project is the subsequent work of SAGA. We apply several nature inspired heuristics, mainly based on genetic algorithms, to this problem. The first method still uses the genetic algorithm, but changes genetic algorithm type, selection method, crossover and mutation operators. The second method applies a neural network to improve the initial population, crossover and mutation. The third method uses GADO, a general-purpose approach to solving the parametric design problem. The fourth method uses simulated annealing. Finally, we compare their performance in the aerial spray deposition problem.",
            "group": 2111,
            "name": "10.1.1.91.1864",
            "keyword": "INDEX WORDSGenetic AlgorithmsNeural NetworksSimulated Annealing",
            "title": "A COMPARISON OF NATURE INSPIRED INTELLIGENT OPTIMIZATION METHODS IN AERIAL SPRAY DEPOSITION MANAGEMENT Electronic Version Approved:"
        },
        {
            "abstract": "payment of fee is granted for non-profit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of the Paris Research Laboratory of Digital Equipment Centre Technique Europe, in Rueil-Malmaison, France; an acknowledgement of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Paris Research Laboratory. All rights reserved. ii In this article, we consider words over f0 \ufffd 1g. Theautodistance of such a word is the lowest among the Hamming distances between the word and its images by circular permutations other than identity; the word\u2019s reverse autodistance is the highest among these distances. For each l \ufffd 2, we study the words of length l whose autodistance and reverse autodistance are close to l\ufffd2 (we call such words synchronizing sequences). We establish, for every l \ufffd 3, an upper bound on the autodistance of words of length l. This upper bound, called up (l), is very close to l\ufffd2.",
            "group": 2112,
            "name": "10.1.1.91.2477",
            "keyword": "",
            "title": "6 Binary Periodic Synchronizing Sequences"
        },
        {
            "abstract": "I would like to express my sincere gratitude and appreciation to my advisor, Dr. A. Emin Aktan, for his guidance and support throughout the course of my research. His excellent mentorship and encouragement will continue to influence me in my career. I would like to thank Dr. Franlin L. Moon for his comments and advise during my research. I am also thankful to all the other members of my dissertation committee, Drs. Patrick Gurian, Franco Motalto and Tein-Men Tan for their interest, participation and advice. I would also like to thank the New York City Department of Transportation and Maria G. Bruschi and Serafim G. Arzoumanidis from Parsons Transportation Group for their guidance and support to the Henry Hudson Bridge project in the thesis. I am also grateful to my dear friends and colleagues Korhan Ciloglu, Kirk Grimmelsman, John Prader and Hesham Sayed for their discussions, input and wonderful friendship. Last but certainly not least, I would thank my parents for their countless love, sacrifice and support through all of the challenges I have faced.",
            "group": 2113,
            "name": "10.1.1.91.3013",
            "keyword": "",
            "title": "ACKNOWLEDGEMENT"
        },
        {
            "abstract": "Abstract. The three mechanisms of crossover are transmission, assortment, and respect. Of these three mechanisms, assortment (i.e. recombination) is traditionally viewed as the primary feature and key advantage of crossover. However, respect (the preservation of common components) is also a feature that is unique to multi-parent operators like crossover \u2013 it takes two (or more) parents to have/identify common components. The effects of respect are isolated from all other aspects of genetic algorithms by using a parallel implementation of simulated annealing. In this implementation, the preservation of common components is used to focus the search process and this focus has improved the performance of simulated annealing on the TSP. Since only the mechanism of respect is transferred from genetic algorithms to simulated annealing, these experiments isolate and demonstrate the benefits of respect. 1",
            "group": 2114,
            "name": "10.1.1.91.4010",
            "keyword": "",
            "title": "Isolating the Benefits of Respect"
        },
        {
            "abstract": "Abstract \u2014 An automated minimum description length (MDL) based approach is presented for selecting optimal parameter settings for segmentation software without detailed knowledge of the internal (often arcane) mechanisms. The method trades off the coverage of image content provided by the segmentation against its conciseness. The coverage is measured by a multi-scale probabilistic segmentation evaluation measure, made robust by an \u03b5-insensitive loss function. The conciseness, as measured by the description length, is modulated by a coefficient w to account for the unavoidable inefficiency of real-world segmentation description formats. A recursive random search algorithm is used for efficient global optimization. The proposed method is modular and extensible, allowing the segmentation and optimization software to be modified/substituted independently. When applied to vessel/neurite segmentation problems in neurobiology and ophthalmology, the proposed method resulted in parameter settings that were always superior to fixed default values. Even with 1000 iterations, estimated settings were within 5% of the globally optimal result for a tube segmentation algorithm with 9 settings. It enables non-expert users across application areas to select parameters effectively, and objectively, minimizing the need for technical support. It is also an effective approach",
            "group": 2115,
            "name": "10.1.1.91.4102",
            "keyword": "",
            "title": "Abdul-Karim et al. IEEE Trans. Image Processing MS Word XP 1 Automatic Selection of Parameters for Vessel/Neurite Segmentation Algorithms"
        },
        {
            "abstract": "Another year, another thesis, another subject. Cryptography has always drawn my in-terest, though I have never known what it basically comes down to. Genetic algorithms on the other hand seemed a pretty nice idea to me, but I wanted to check its merits on a real problem. So this two reasons led me to choosing the subject of this thesis. I learned new things, and I discovered unexpected problems. Call it a thesis, call it life. There are always some people to thank. I would like to start with Ozgul Ku\u00e7uk and Jan Cappaert, my daily supervisors. Thanks for the ideas, the corrections and all the lot. Thanks to Ben Delronge, my thesis partner with whom I worked the first weeks until he gave up the program. Thanks to my family and friends, for never giving up. A special thank you to my housemates, for the infrastructure and stuff. i",
            "group": 2116,
            "name": "10.1.1.91.4792",
            "keyword": "Contents",
            "title": "AUTOMATED CREATION AND SELECTION OF CRYPTOGRAPHIC PRIMITIVES"
        },
        {
            "abstract": "Abstract-An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call Ant System. We propose it as a viable new approach to sto-chastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical Traveling Salesman Problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the Ant System (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS. I.",
            "group": 2117,
            "name": "10.1.1.91.5587",
            "keyword": "",
            "title": "The Ant System: Optimization by a Colony of Cooperating Agents"
        },
        {
            "abstract": "This work has attempted to exploit information sharing to improve the results of Adaptive Simulated Annealing [1] as an optimization algorithm of the high-level synthesis of testable data paths. We have used Messengers [3] as a coordination tool to run several parallel instances of the annealing algorithm on the same design with di erent probability arrays for the perturbations. When all these instances complete annealing, they exchange information about the best design among them which is given by a cost function [2] based on area, speed and testability costs of the digital design. This best design is then used as a starting point and several instances of annealing are run again in an attempt to further improve the design.",
            "group": 2118,
            "name": "10.1.1.91.5891",
            "keyword": "",
            "title": "Distributed adaptive simulated annealing for synthesis design space exploration"
        },
        {
            "abstract": "Monte Carlo methods have become popular for obtaining solutions to global optimization problems. One such Monte Carlo optimization technique is simulated annealing (SA). Typically in SA the parameters of the search are determined a priori. Using an aggregated, or lumped, version of SA\u2019s associated Markov chain and the concept of expected hitting time, we adjust the search parameters dynamically using information gained from the SA search process. We present an algorithm that varies the SA search parameters dynamically, and show that, on average, dynamic adjustment of the parameters attains better solutions on a set of test problems than those attained with a logarithmic cooling schedule. c \u25cb 1998 Academic Press 1.",
            "group": 2119,
            "name": "10.1.1.91.6366",
            "keyword": "",
            "title": "JOURNAL OF COMPUTATIONAL PHYSICS 146, 263\u2013281 (1998) ARTICLE NO. CP986065 A Feedback Algorithm for Determining Search Parameters for Monte Carlo Optimization"
        },
        {
            "abstract": "Abstract. Many optimization techniques have been adopted for efficient job scheduling in grid computing, such as: genetic algorithms, simulated annealing and stochastic methods. Such techniques present common problems related to the use of inaccurate and out-of-date information, which degrade the global system performance. Besides that, they also do not properly model a grid environment. In order to adequately model a real grid environments and approach the scheduling using updated information, this paper uses complex network models and the simulated annealing optimization technique. The complex network concepts are used to better model the grid and extract environment characteristics, such as the degree distribution, the geodesic path, latency. The complex network vertices represent grid process elements, which are generalized as computers. The random and scale free models were implemented in a simulator. These models, associated with Dijkstra algorithm, helps the simulated annealing technique to find out efficient allocation solutions, which minimize the application response time. 1",
            "group": 2120,
            "name": "10.1.1.91.6744",
            "keyword": "",
            "title": "A Complex Network-Based Approach for Job Scheduling in Grid Environments"
        },
        {
            "abstract": "We present a robust datapath allocation method that is flexible enough to handle constraints imposed by a variety of target architectures. Key features of this method are its ability to handle accurate modeling of datapath units and the simultaneous optimization of direct objective functions. The proposed method consists of a new binding model construction scheme and an optimization technique based on simulated annealing. To illustrate the flexibility of this method, two datapath allocation procedures have been developed for two problem environments: (1) a procedure that incorporates interconnection area and delay estimates, where floor-planning is tightly integrated into datapath allocation; and (2) a procedure that handles registers, register files, and multiport memories for data storage, as well as random and linear topologies for interconnection architectures. Results from these two applications show our method produces competitive designs for benchmark circuits, as well as being flexible enough to be used for a variety of different domains.",
            "group": 2121,
            "name": "10.1.1.91.8065",
            "keyword": "Categories and Subject DescriptorsB.5.2 [Register-Transfer-Level ImplementationDesign Aids\u2014Automatic synthesisOptimizationB.7.2 [Integrated CircuitsDesign Aids\u2014 Placement and routingG.1.6 [Numerical AnalysisOptimizationJ.6 [Computer ApplicationsComputer-Aided Engineering\u2014Computer-aided design (CAD) Additional Key Words and PhrasesAllocation and bindinghigh-level synthesis",
            "title": "A flexible datapath allocation method for architectural synthesis"
        },
        {
            "abstract": "This paper describes a variant of simulated annealing incorporating a variable penalty method to solve the traveling salesman problem with time windows (TSPTW). Augmenting temperature from traditional simulated annealing with the concept of pressure (analogous to the value of the penalty multiplier), compressed annealing integrates a penalty method with heuristic search to address the TSPTW. Computational results validate the value of a variable penalty method versus a static penalty approach. Compressed annealing compares favorably with benchmark results in the literature, obtaining best-known results in numerous instances.",
            "group": 2122,
            "name": "10.1.1.91.9311",
            "keyword": "simulated annealingpenalty methodstraveling salesman problemtime windows",
            "title": "A Compressed Annealing Approach to the Traveling Salesman Problem with Time Windows"
        },
        {
            "abstract": "Search for the global minimum in a molecular energy landscape populated with numerous local minima is a difficult task. Search techniques relevant to such complex spaces can be classified as either global or local. Global search explores the entire space, guaranteeing the global extremum will be found. To accomplish this, the number of samples required grows exponentially with the number of dimensions. Since this is clearly not computationally tractable, global search is impractical in highdimensional spaces. Local search, on the other hand, employs gradient descent to avoid searching the entire exponential space. Gradient descent methods are susceptible to getting stalled in local minima and consequently, no guarantees can be made about finding the global minimum. We propose a middle ground that minimizes the effects of exponential space and local minima by integrating domain knowledge and information generated during search into a model, and then using this model to focus computation on regions of increasing relevance. Directing resources to multiple relevant regions prevents oversampling local minima. At the same time the exploration of only significant regions avoids the intractable computational requirements of high-dimensional spaces. The proposed method, called Model-Based Search (MBS), is compared to the local search method Monte Carlo as implemented in Rosetta- currently considered the best computational protein structure prediction method. The results indicate that MBS is significantly better at finding lower energy minima than the Monte Carlo technique implemented as part of Rosetta. This effect is amplified as the dimensionality of the search space increases. 1",
            "group": 2123,
            "name": "10.1.1.91.9791",
            "keyword": "",
            "title": "Model-based search to determine minima in molecular energy landscapes"
        },
        {
            "abstract": "This project deals with the development of a computer vision system for handling an instance of the bin picking problem. The system performs pose estimation of randomly organised stator housings containing circular features with a known CAD model. The implemented system obtains 3-D information through a binocular stereo setup. The individual stereo images are processed by a Canny edge detector followed by an edge segmentation and a least squares ellipse detection. The detected ellipses in each image are subject to a monocular pose estimation technique estimating two possible circular pose candidates for each ellipse. The correct pose candidates are determined by performing a matching between the circle pose candidates in each stereo image. The matching is done via an association graph search locating the maximally weighted clique through the use of simulated annealing. The final pose candidates are verified by back projection in a distance transformed edge image. System tests justified the choice of the circular feature due to the presence of detectable stator housings per bin configuration. Tests on simulated data showed acceptable error measures both for position and orientation estimation. The final conclusion was that the implemented method could form a useful part, handling certain pose situations, in a final bin picking system.",
            "group": 2124,
            "name": "10.1.1.92.497",
            "keyword": "",
            "title": "Project Data Project Group: Group 925"
        },
        {
            "abstract": "",
            "group": 2125,
            "name": "10.1.1.92.778",
            "keyword": "",
            "title": "The Pickup and Delivery Problem with Split Loads"
        },
        {
            "abstract": "Abstract: This paper presents a contextual classifier based on quadtree structures and Markov random fields theory. The initial classification is realized by a clustering algorithm, then for each level of the tree, boundary regions are found. Pixels of boundary regions are classified by using a combination of nearest class mean criterion, Mahalanobis distance criterion and finally a Markov model. Our scheme is simple to implement and performs well, giving satisfactory results for SAR images.",
            "group": 2126,
            "name": "10.1.1.92.800",
            "keyword": "",
            "title": "Segmentation Of SAR Images Using Quadtree And Potts Model"
        },
        {
            "abstract": "Abstract A growing body of work demonstrates that syntactic structure can evolve in populations of genetically identical agents. Traditional explanations for the emergence of syntactic structure employ an argument based on genetic evolution: Syntactic structure is speci\ufffded by an innate language acquisition device (LAD). Knowledge of language is complex, yet the data available to the language learner are sparse. This incongruous situation, termed the \u201cpoverty of the stimulus, \u201d is accounted for by placing much of the speci\ufffdcation of language in the LAD. The assumption is that the characteristic structure of language is somehow coded genetically. The effect of language evolution on the cultural substrate, in the absence of genetic change, is not addressed by this explanation. We show that the poverty of the stimulus introduces a pressure for compositional language structure",
            "group": 2127,
            "name": "10.1.1.92.944",
            "keyword": "",
            "title": "Compositional Syntax From Cultural Transmission"
        },
        {
            "abstract": "Many dynamic-content online services are comprised of multiple interacting components and data partitions distributed across server clusters. Understanding the performance of these services is crucial for efficient system management. This paper presents a profile-driven performance model for cluster-based multi-component online services. Our offline constructed application profiles characterize component resource needs and intercomponent communications. With a given component placement strategy, the application profile can be used to predict system throughput and average response time for the online service. Our model differentiates remote invocations from fast-path calls between co-located components and we measure the network delay caused by blocking inter-component communications. Validation with two J2EE-based online applications show that our model can predict application performance with small errors (less than 13 % for throughput and less than 14% for the average response time). We also explore how this performance model can be used to assist system management functions for multi-component online services, with case examinations on optimized component placement, capacity planning, and cost-effectiveness analysis. 1",
            "group": 2128,
            "name": "10.1.1.92.1270",
            "keyword": "",
            "title": "Abstract Performance Modeling and System Management for Multi-component Online Services \u2217"
        },
        {
            "abstract": "This research was supported by the Deutsche",
            "group": 2129,
            "name": "10.1.1.92.1317",
            "keyword": "Risk",
            "title": "Projection Pursuit for"
        },
        {
            "abstract": "This is a preliminary version of a chapter that appeared in the book Local Search in Combinatorial Optimization, E. H. L. Aarts and J. K. Lenstra (eds.), John Wiley and Sons, London, 1997, pp. 215-310. The traveling salesman problem (TSP) has been an early proving ground for many approaches to combinatorial optimization, including classical local optimization techniques as well as many of the more recent variants on local optimization, such as simulated annealing, tabu search, neural networks, and genetic algorithms. This chapter discusses how these various approaches have been adapted to the TSP and evaluates their relative success in this perhaps atypical domain from both a",
            "group": 2130,
            "name": "10.1.1.92.1635",
            "keyword": "",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "Innovations in high performance computing and high bandwidth networks have led to the onset of data explosion. Along with large size, the datasets are typically multivariate. The need for effective exploration of this data has led to the area of multidimensional visualization. Research in low level human visual system has resulted in the construction of perceptual guidelines that can produce effective visualizations. However, application of these guidelines to a dataset requires users to be experts in the visualization domain. ViA is a semi-automated visualization assistant that uses perceptual guidelines along with a heuristic search algorithm to generate perceptually salient visualizations. This thesis aims to study the behavior of the current hint-based search strategy and de-termine its efficiency. We compare hint-based search with two generic heuristic search algorithms, simulated annealing and reactive tabu search, by adapting them to ViA\u2019s search domain. We use time efficiency, space efficiency, ability to find multiple optimal solutions and optimality as performance metrics. Further, in order to \u201csee \u201d the areas of the search space explored by each search algorithm, we have developed a focus + context visualization system using hyperbolic geometry.",
            "group": 2131,
            "name": "10.1.1.92.2082",
            "keyword": "",
            "title": "VISUALIZATION SEARCH STRATEGIES"
        },
        {
            "abstract": "Abstract: This paper presents a meta-heuristic optimization algorithm, Tabu Search (TS), and describes how it can be used to solve a wide variety of chemical engineering problems. Modifications to the original algorithm and constraint handling techniques are described and integrated to extend its applicability. All components of TS are described in detail. Initial values for each key parameter of TS are provided. In addition, guidelines for adjusting these parameters are provided to relieve a significant amount of time-consuming trial-and-error experiments that are typically required with stochastic optimization. Several small NLP and MINLP test cases and three small- to middle- scale chemical process synthesis problems demonstrate the feasibility and effectiveness of the techniques with recommended parameters. 1.",
            "group": 2132,
            "name": "10.1.1.92.2107",
            "keyword": "",
            "title": "Tabu search algorithm for chemical process optimization"
        },
        {
            "abstract": "A hierarchical procedure is developed to determine maximum overall yield of a process and optimize process changes to achieve such a yield. First, a targeting procedure is developed to identify an upper bound of the overall yield ahead of detailed design. Several mass integration strategies are proposed to attain maximum yield. These strategies include rerouting of raw materials, optimization of reaction yield, rerouting of product from undesirable outlets to desirable outlets, and recycling of unreacted raw materials. Path equations are tailored to provide the appropriate level of detail for modeling process performance as a function of the optimization variables pertaining to design and operating variables. Interval analysis is used as an inclusion technique that provides rigorous bounds regardless of the process nonlinearities and without enumeration. Then, a new approach for identification of cost-effective implementation of maximum attainable targets for yield is presented. In this approach, a mathematical program was developed to identify the maximum feasible yield using a combination of iterative additions of constraints and problem reformulation. Next, cost objectives were employed to identify a cost-effective solution with the details of design and operating",
            "group": 2133,
            "name": "10.1.1.92.3129",
            "keyword": "Chair of Advisory CommitteeDr. Mahmoud M. El-Halwagi",
            "title": "Approved by: PRODUCTIVITY ENHANCEMENT THROUGH PROCESS INTEGRATION"
        },
        {
            "abstract": "(Dr. sc. nat.) vorgelegt der",
            "group": 2134,
            "name": "10.1.1.92.3541",
            "keyword": "",
            "title": "Mathematisch-naturwissenschaftlichen Fakult\u00e4t"
        },
        {
            "abstract": "(Dr. sc. nat.) vorgelegt der",
            "group": 2135,
            "name": "10.1.1.92.3541",
            "keyword": "",
            "title": "Mathematisch-naturwissenschaftlichen Fakult\u00e4t"
        },
        {
            "abstract": "The relevance of computer science to economic planning is defended. An algorithm for constructing a balanced economic plan is presented and found to be of time order Nlog(N) in the complexity of the economy. The time taken to perform a complete plan optimization in natural units for a whole economy is estimated. Plans and computational costs This is a paper by a computer scientist that disputes an economic hypothesis. The hypothesis rests upon certain premises about computation and is therefore open to criticism from outside economics. The hypothesis is that: the complexities involved with performing the calculations required to optimize an economic plan in natural units are so great that they are beyond the realms of feasibility and that in consequence all rational economic calculation must proceed by means of the intermediary of prices. The hypothesis is an old one. It was originally proposed by Von Mises (1936) between the wars. In recent years it has been ably restated by Nove (1983) and has gained circumstantial support from recent developments in the USSR. It is argued here that",
            "group": 2136,
            "name": "10.1.1.92.4508",
            "keyword": "",
            "title": "Economic Planning"
        },
        {
            "abstract": "Abstract. In single-particle reconstruction, a 3D structure is reconstructed from a large number of randomly oriented 2D projections, using techniques related to computed tomography. Unlike in computed tomography, however, the orientations of the projections must be estimated at the same time as the 3D structure, and hence the reconstruction process can be error-prone, converging to an incorrect local optimum rather than the true 3D structure. In this paper, we discuss and further develop a maximum-likelihood approach to reconstruction, and demonstrate that this approach can help avoid incorrect local optima for both 2D and 3D reconstructions. 1",
            "group": 2137,
            "name": "10.1.1.92.5113",
            "keyword": "",
            "title": "Avoiding Local Optima in Single Particle Reconstruction"
        },
        {
            "abstract": "Image segmentation is a fundamental task in image analysis responsible for partitioning an image into multiple sub-regions based on a desired feature. Active contours have been widely used as attractive image segmentation methods because they always produce sub-regions with continuous boundaries, while the kernel-based edge detection methods, e.g. Sobel edge detectors, often produce discontinuous boundaries. The use of level set theory has provided more flexibility and convenience in the implementation of active contours. However, traditional edge-based active contour models have been applicable to only relatively simple images whose sub-regions are uniform without internal edges. A partial solution to the problem of internal edges is to partition an image based on the statistical information of image intensity measured within sub-regions instead of looking for edges. Although representing an image as a piecewise-constant or unimodal probability density functions produces better results than traditional edge-based methods, the performances of such methods is still poor on images with sub-regions consisting of multiple components, e.g. a zebra on the field. The segmentation of this kind of multispectral images is even a more difficult problem. The object of this work is to develop advanced segmentation methods which provide",
            "group": 2138,
            "name": "10.1.1.92.5682",
            "keyword": "",
            "title": "ABSTRACT Lee, Cheolha Pedro. Robust Image Segmentation using Active Contours: Level Set Approaches."
        },
        {
            "abstract": "Location determination of mobile users within a building has attracted much attention lately due to its many applications in mobile networking including network intrusion detection problems. However, it is challenging due to the complexities of the indoor radio propagation characteristics exacerbated by the mobility of the user. A common practice is to mechanically generate a table showing the radio signal strength at different known locations in the building. A mobile user\u2019s location at an arbitrary point in the building is determined by measuring the signal strength at the location in question and determining the location by referring to the above table using a LMSE (least mean square error) criterion. Obviously, this is a very tedious and time consuming task. This paper proposes a novel and automated location determination method called ARIADNE. Using a two dimensional construction floor plan and only a single actual signal strength measurement, ARIADNE generates an estimated signal strength map comparable to those generated manually by actual measurements. Given the signal measurements for a mobile, a proposed clustering algorithm searches that signal strength map to determine the current mobile\u2019s location. The results from ARIADNE are comparable and may even be superior to those from existing localization schemes.",
            "group": 2139,
            "name": "10.1.1.92.6725",
            "keyword": "General Terms DesignAlgorithms Keywords Localization",
            "title": "Ariadne: a dynamic indoor signal map construction and localization system"
        },
        {
            "abstract": "Integer and combinatorial optimization problems constitute a major challenge for algorithmics. They arise when a large number of discrete organizational decisions have to be made, subject to constraints and optimization criteria. This thesis describes and investigates new domain-independent local search strategies for linear integer optimization. We introduce WSAT(OIP), an integer local search method which operates on an algebraic problem representation. WSAT(OIP) generalizes Walksat, a successful local search procedure for propositional satisfiability (SAT), to more expressive constraint systems. For this purpose, we introduce over-constrained integer programs (OIPs), a constraint class which is closely related to integer programs. OIP allows for a natural generalization of the principles of SAT local search to integer optimization. Further, it will be shown that OIPs are a special case of integer linear programs and permit combinations with linear programming for bound computation, initialization by rounding, search space reduction, and feasibility testing. The representation",
            "group": 2140,
            "name": "10.1.1.92.8008",
            "keyword": "",
            "title": "This document was prepared with LaTeX."
        },
        {
            "abstract": "methodology\u2014 A recipe for first-time success This paper describes the methodology employed by the IBM Microelectronics Division for the design of its Blue Logic \u00ae applicationspecific integrated circuits (ASICs) and system-on-a-chip (SoC) designs. This methodology is used by both IBM ASIC and SoC designers, as well as OEM customers. A key focus of the IBM ASIC/SoC methodology, outlined in the first section of this paper, is the first-time-right methods of design and verification that maximize correct operation of the chip upon product integration. The second section of this paper describes advances in methodology that deal with the physical effects of shrinking device geometries and enable design using the performance and density capabilities available in the new technologies, and methodology advances that have improved design turnaround time (TAT) for large, complex designs. Upcoming nanometerlevel technologies present new opportunities to integrate systems on a single chip, including functional components of mixed libraries and mixed analog and digital design. The final section of this paper outlines strategies that are enabling SoC design at these levels. by G. W. Doerre D. E. Lackey",
            "group": 2141,
            "name": "10.1.1.92.8110",
            "keyword": "",
            "title": "The IBM ASIC/SoC"
        },
        {
            "abstract": "Abstract. Machine vision is today a well-established technology in industry where especially conveyer belt applications are successful. A related application area is the situation where a number of objects are located in a bin and each has to be picked from the bin. This problem is known as the automatic bin-picking problem and has a huge market potential due to the countless situations where bin-picking is done manually. In this paper we address a general bin-picking problem present at a large pump manufacturer, Grundfos, where many objects with circular openings are handled each day. We pose estimate the objects by finding the 3D opening based on the elliptic projections into two cameras. The ellipses from the two cameras are handled in a unifying manner using graph theory together with an approach that links a pose and an ellipse via the equation for a general cone. Tests show that the presented algorithm can estimate the poses for a large variety of orientations and handle both noise and occlusions. 1",
            "group": 2142,
            "name": "10.1.1.92.9103",
            "keyword": "",
            "title": "Pose Estimation of Randomly Organized"
        },
        {
            "abstract": null,
            "group": 2143,
            "name": "10.1.1.92.9277",
            "keyword": "",
            "title": "Table of Contents Table of Contents......................................................................................................................... 1"
        },
        {
            "abstract": "\u2018Separator \u2019 based divide and conquer algorithms have been used to solve many computational problems successfully ([10],[13], etc.) These algorithms perform very well when the \u2018separator size \u2019 is small. An interesting question is: \u2018How well do these algorithms function on arbitrary instances of the problem (for which we may have no knowledge about the separability of the underlying graphs)? \u2019 This paper provides answers to this question. In particular, we give tight bounds for the \u2018separability \u2019 of a random graph. We then use these bounds to analyze the expected behavior of separator based algorithms for three different problems namely 1) Combinatorial Optimization, 2) Matrix Inversion and related problems, and 3) Satisfiability. There are two popular models of a random graph and the one we employ in this paper assumes that each possible edge in the graph occurs independently with a specified probability, say p. A member in this model is denoted as Gp. We derive optimal bounds for the separability of Gp. In particular we prove that almost all Gps with n nodes are n \u2212 \u03b8(1/p) separable. We apply these separability results to analyze the",
            "group": 2144,
            "name": "10.1.1.92.9328",
            "keyword": "",
            "title": "Separability of a Random Graph and Applications"
        },
        {
            "abstract": "Maximum a Posteriori assignment (MAP) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a Bayesian network. MAP has been shown to be a NP-hard problem [22], even for constrained networks, such as polytrees [18]. Hence, previous approaches often fail to yield any results for MAP problems in large complex Bayesian networks. To address this problem, we propose AnnealedMAP algorithm, a simulated annealing-based MAP algorithm. The AnnealedMAP algorithm simulates a non-homogeneous Markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density. We tested this algorithm on several real Bayesian networks. The results show that, while maintaining good quality of the MAP solutions, the AnnealedMAP algorithm is also able to solve many problems that are beyond the reach of previous approaches. 1",
            "group": 2145,
            "name": "10.1.1.93.143",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract \u2014 This paper presents a decentralized negotiation protocol for cooperative economic scheduling in a supply chain environment. For this purpose we designed autonomous agents that maximize their profits by optimizing their local schedule and offer side payments to compensate other agents for lost profit or extra expense if cumulative profit is achievable. To further increase their income the agents have to apply a randomized local search heuristic to prevent the negotiation from stopping in locally optimal contracts. We show that the welfare could be increased by using a search strategy similar to Simulated Annealing. Unfortunately, a na\u00efve application of this strategy makes the agents vulnerable to exploitation by untruthful partners. We develop and test a straightforward mechanism based on trust accounts to protect the agents against systematic exploitation. This \u201cTrusted \u201d Simulated Annealing mechanism assures truthful revelation of the individual opportunity cost situation as the basis for the calculation of side payments. I.",
            "group": 2146,
            "name": "10.1.1.93.316",
            "keyword": "",
            "title": "A Trust-based Negotiation Mechanism for Decentralized Economic Scheduling"
        },
        {
            "abstract": "",
            "group": 2147,
            "name": "10.1.1.93.778",
            "keyword": "RepresentationInference and Learning",
            "title": "by"
        },
        {
            "abstract": "Simulated annealing is a probabilistic algorithm for minimizing a general cost function which may have multiple local minima. The amount of randomness in this algorithm is controlled by the &quot;temperature&quot;, a scalar parameter which is decreased to zero as the algorithm progresses. We consider the case where the minimization is carried out over a finite domain and we present a survey of several results and analytical tools for studying the asymptotic behavior of the simulated annealing algorithm, as time goes to infinity and temperature approaches zero. I. Introduction. Simulated annealing is a probabilistic algorithm for minimizing a general cost function which may have multiple local minima. It has been introduced in [1] and [21 and was motivated by the Metropolis algorithm [3] in statistical mechanics. Since then, it has been applied to a variety of problems, the main ones arising in the context of combinatorial optimization [1,4,5,6] and in the",
            "group": 2148,
            "name": "10.1.1.93.945",
            "keyword": "",
            "title": "Abstract A SURVEY OF LARGE TIME ASYMPTOTICS OF SIMULATED ANNEALING ALGORITHMSt"
        },
        {
            "abstract": "I would like to thank to my supervisor, Hana Rudov\u00e1, for help and encouragement throughout the work. I am very grateful for her advice and valuable discussions. This work was partially supported by the Grant Agency of the Czech Republic under the contract 201/01/0942 and by Purdue University. I would also like to thank to the Supercomputer Center Brno where the experiments with random problems were accomplished. Declaration I declare that this thesis was composed by myself, and all presented results are my own, unless otherwise stated. All sources and literature that I have used during the elaboration of this thesis are cited with complete reference to the corresponding source. ii Kamil Ve\u02c7rmi\u02c7rovsk\u00b4y The first part of this thesis contains a classification of the algorithms for constraint satisfaction problems. A formal description of some algorithms of each approach is given, including uniform pseudo-codes. An iterative repair search algorithm is introduced. It is a revised and formalized version of the algorithm first introduced for the needs of a real-life large-scale university timetabling problem. It iteratively tries to find a partial solution with the maximal number of assigned variables. It is devoted to tightly constrained problems, where constraint propagation is not strong enough, variable and value ordering heuristics are not good enough, or the problem is over-constrained. Its importance lies in its natural implementation in a constraint logic programming language. One step is a special incomplete version of chronological backtracking with constraint propagation. Subsequent searches try to improve the last computed partial assignment. This is done by developing variable and value heuristics based on the results of previous iterations. Different parameters of the algorithm are compared. Some possible extensions are proposed and their contribution is assessed. The comparison with other algorithms is given. Last but not least, the results for the real-life problem are presented, both original and after the revision, and compared.",
            "group": 2149,
            "name": "10.1.1.93.1009",
            "keyword": "iii Contents",
            "title": "Acknowledgments"
        },
        {
            "abstract": "Abstract. We present a modified version of the Particle swarm Optimization algorithm in which we adjust the virtual swarm search by incorporating inter-agent dynamics native to multi-robot search scenarios. The neighborhood structure of PSO is modified to accurately represent feasible neighborhoods in multiple robot systems with limited communication in several different ways. The new algorithms are tested on several standard benchmark problems with a varying number of dimensions and are shown to offer superior performances to the standard algorithm in some cases. Further potential modifications and uses of the new algorithms are discussed. 1",
            "group": 2150,
            "name": "10.1.1.93.1521",
            "keyword": "",
            "title": "Applying Aspects of Multi-Robot Search to Particle Swarm Optimization"
        },
        {
            "abstract": "Placement is an essential step in the physical design flow since it assigns exact locations for various circuit components within the chip\u2019s core area. An inferior placement assignment will not only affect the chip\u2019s performance but might also yield it non-manufacturable by producing excessive wirelength beyond available routing resources. Consequently, a placer must perform the",
            "group": 2151,
            "name": "10.1.1.93.1834",
            "keyword": "",
            "title": "Digital Layout- Placement"
        },
        {
            "abstract": "ter verkrijging van de graad van doctor aan de Universiteit van Amsterdam op gezag van de Rector Magnificus prof.mr. P.F. van der Heijden ten overstaan van een door het college voor promoties ingestelde commissie, in het openbaar te verdedigen in de Aula der Universiteit op vrijdag 23 februari 2007, te 10.00 uur door",
            "group": 2152,
            "name": "10.1.1.93.1890",
            "keyword": "",
            "title": "Promotiecommissie:"
        },
        {
            "abstract": "Abstract. Recently Nickle et al. introduced a new model of genetic diversity that summarizes a large input dataset into a short sequence containing overlapping subsequences from the dataset. This model has direct applications to rational vaccine design. In this paper we formally investigate the combinatorics of the vaccine optimization problem. Here the vaccine is constructed as a sequence S of amino-acids such that as many of the most frequently occurring epitopes found in mutated viruses are subsequences to S. We rigorously present the related design optimization problem, establish its complexity, and present a simple probabilistic algorithm to find an efficient solution. Our vaccine designs show improvement of over 20 % in the coverage score over the previously best designs and produce over 15 % shorter vaccines that achieve equivalent epitope coverage. 1",
            "group": 2153,
            "name": "10.1.1.93.2255",
            "keyword": "",
            "title": "Combinatorics of The Vaccine Design Problem: Definition and An Algorithm"
        },
        {
            "abstract": "This article reviews the available methods forautomated identification ofobjectsin digital images. The techniques are classified into groups according to the nature of the computational strategy used. Four classes are proposed: (1) the s~mplest strategies, which work on data appropriate for feature vector classification, (2) methods that",
            "group": 2154,
            "name": "10.1.1.93.2832",
            "keyword": "",
            "title": "Cornputatima! Strategies for Object Recognition"
        },
        {
            "abstract": "One of the primary and most important uses of simulations is for optimization. Optimization consists of selecting simulation configuration parameters that can minimize (or maximize) output attributes of the scenario, based on a set of input attributes. For example, a product manufacturer may wish to find the number of factories, workers, and pieces of equipment that will yield the highest return per investment. This is often a difficult problem, because complex simulations can rarely be expressed as a continuous function in which standard calculus optimization techniques can be employed. There are, however, several popular techniques that have been developed over the years, and successfully used to optimize complex simulations. This paper provides a survey of three recent, practical techniques, and a brief discussion of major software simulation packages that employ them.",
            "group": 2155,
            "name": "10.1.1.93.3241",
            "keyword": "Simulation OptimizationStochastic OptimizationSimulated AnnealingGenetic algorithms",
            "title": "ABSTRACT A Review of Simulation Optimization Techniques"
        },
        {
            "abstract": "Abstract \u2014 In this paper we present a method for FPGA datapath precision optimization subject to user-defined area and error constraints. This work builds upon our previous research [1] which presented a methodology for optimizing for dynamic range\u2014the most significant bit position. In this work, we present an automated optimization technique for the least-significant bit position of circuit datapaths. We present results describing the effectiveness of our methods on typical signal and image processing kernels. I.",
            "group": 2156,
            "name": "10.1.1.93.3439",
            "keyword": "",
            "title": "Automated Least-Significant Bit Datapath Optimization for FPGAs"
        },
        {
            "abstract": "Power consumption is a crucial concern in nanometer chip design. Researchers have shown that multiple supply voltage (MSV) is an effective method for power consumption reduction. The underlying idea behind MSV is the trade-off between power saving and performance. In this paper, we present an effective voltage assignment technique based on dynamic programming. Given a netlist without reconvergent fanouts, the dynamic programming can guarantee an optimal solution for the voltage assignment. We then generate a level shifter for each net that connects two blocks in different voltage domains, and perform power-network aware floorplanning for the MSV design. Experimental results show that our floorplanner is very effective in optimizing power consumption under timing constraints. 1.",
            "group": 2157,
            "name": "10.1.1.93.3953",
            "keyword": "",
            "title": "Voltage island aware floorplanning for power and timing optimization"
        },
        {
            "abstract": "The genetic code is well-known to be fault tolerant, in the sense that transcription errors in the third codon position frequently do not influence the amino acid expressed, while errors in other codon positions often",
            "group": 2158,
            "name": "10.1.1.93.4077",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "This research has been supported by the Ministry of Education, Youth and Sports under the research program #J04/98:212300014. Tom\u00e1\u02c7s Zahradnick\u00b4y",
            "group": 2159,
            "name": "10.1.1.93.4305",
            "keyword": "",
            "title": "2.2 Error Analysis.................................... 4"
        },
        {
            "abstract": "Hidden Markov models, the expectation\u2013maximization algorithm, and the Gibbs sampler were introduced for biological sequence analysis in early 1990s. Since then the use of formal statistical models and inference procedures has revolutionized the field of computational biology. This chapter reviews the hidden Markov and related models, as well as their Bayesian inference procedures and algorithms, for sequence alignments and gene regulatory binding motif discoveries. We emphasize that the combination of Markov chain Monte Carlo and dynamic-programming techniques often results in effective algorithms for NP-hard problems in sequence analysis. In the past decade, we have witnessed the development of the likelihood approach to pairwise sequence alignments (Bishop and Thompson, 1986; Thorne et al., 1991); probabilistic models for RNA secondary structure (Zuker, 1989; Lowe and Eddy, 1997);",
            "group": 2160,
            "name": "10.1.1.93.4631",
            "keyword": "",
            "title": "3 Bayesian Methods in Biological Sequence Analysis"
        },
        {
            "abstract": "To my parents, Stella and Eugene Gomes. iii ACKNOWLEDGEMENTS I would like to express my gratitude to Professor Abhijit Chatterjee for his advice, guid-ance, and for creating an atmosphere conducive for learning and growth. His deep insight and endless stream of ideas provided a fertile ground for conducting challenging research work. I will always be grateful for giving me the opportunity to conduct this research. His unwavering support during the ups and downs helped me to complete my thesis.",
            "group": 2161,
            "name": "10.1.1.93.5088",
            "keyword": "",
            "title": "ALTERNATE TEST GENERATION FOR DETECTION OF PARAMETRIC FAULTS"
        },
        {
            "abstract": "The development of spatial optimization and simulation techniques in GIS can greatly improve the quality and efficiency of maps generated to pursue spatial pattern objectives in landscape design and planning. This chapter describes how pattern optimization and simulation can be useful in visualizing alternative possible landscapes and for evaluating landscape planning and design scenarios, and describes various approaches that can be used to generate prescribed landscape patterns. Analyses are presented of performance benchmarks and external factors that affect optimal landscape patterns, using simulated-annealing-based algorithms on hypothetical two-dimensional gridded landscapes. The use and value of landscape simulation in environmental GIS applications is discussed",
            "group": 2162,
            "name": "10.1.1.93.5142",
            "keyword": "",
            "title": "GENERATING PRESCRIBED PATTERNS IN LANDSCAPE MODELS"
        },
        {
            "abstract": "Product design and development is recognized by many firms as a crucial activity. The ability to quickly introduce new products into the market is a key factor for determining corporate health and profitability \ufffd1\ufffd. As a result, design management researchers",
            "group": 2163,
            "name": "10.1.1.93.5152",
            "keyword": "",
            "title": "Institute of Astronautics,"
        },
        {
            "abstract": "Abstract \u2014 An automated method is presented for selecting optimal parameter settings for vessel/neurite segmentation algorithms using the minimum description length principle and a recursive random search algorithm. It trades off a probabilistic measure of image-content coverage, against its conciseness. It enables non-expert users to select parameter settings objectively, without knowledge of underlying algorithms, broadening the applicability of the segmentation algorithm, and delivering higher morphometric accuracy. It enables adaptation of parameters across batches of images. It simplifies the user interface to just one optional parameter, and reduces the cost of technical support. Finally, the method is modular, extensible, and amenable to parallel computation. The method is applied to 223 images of human retinas and cultured neurons, from four different sources, using a single segmentation algorithm with 8 parameters. Improvements in segmentation quality compared to default settings using 1000 iterations ranged from 4.7 \u2013 21%. Paired t-tests showed that improvements are statistically significant (p < 0.0005). Most of the improvement occurred in the first 44 iterations. Improvements in description lengths and agreement with the ground truth were strongly correlated ( \u03c1 = 0.78).",
            "group": 2164,
            "name": "10.1.1.93.5540",
            "keyword": "Muhammad-Amri Abdul-KarimBadrinath RoysamMemberIEEENatalie Dowell-Mesfin",
            "title": "Automatic Selection of Parameters for"
        },
        {
            "abstract": "This is the sixth edition of a quarterly column the purpose of which is to provide continuing coverage of new developments in the theory of NP-completeness. The presentation is modeled on that used by M. R. Garey and myself in our book \u2018\u2018Computers and Intractability: A Guide to the Theory of NP-Completeness,\u2019\u2019 W. H. Freeman & Co., San Francisco, 1979 (hereinafter referred to as \u2018\u2018[G&J]\u2019\u2019; previous columns will be referred to by their dates). A background equivalent to that provided by [G&J] is assumed, and, when appropriate, cross-references will be given to that book and the list of problems (NP-complete and harder) presented there. Readers who have results they would like mentioned (NP-hardness, PSPACE-hardness, polynomial-time-solvability, etc.), or open problems they would like publicized, should send them to David S. Johnson, Room 2C-355, Bell Laboratories, Murray Hill, NJ 07974",
            "group": 2165,
            "name": "10.1.1.93.5914",
            "keyword": "",
            "title": "The NP-Completeness Column: An Ongoing Guide"
        },
        {
            "abstract": "In software testing, it is often desirable to find test inputs that exercise specific program features. To find these inputs by hand is extremely time-consuming, especially when the software is complex. Therefore, many attempts have been made to automate the process. Random test data generation consists of generating test inputs at random, in the hope that they will exercise the desired software features. Often, the desired inputs must satisfy complex constraints, and this makes a random approach seem unlikely to succeed. In contrast, combinatorial optimization techniques, such as those using genetic algorithms, are meant to solve difficult problems involving the simultaneous satisfaction of many constraints. In this paper, we discuss experiments with test generation problems that are harder than the ones discussed in earlier literature \u2014 we use larger programs and more complex test adequacy criteria. We find a widening gap between a technique based on genetic algorithms and those based on random test generation. 1.",
            "group": 2166,
            "name": "10.1.1.93.6051",
            "keyword": "",
            "title": "Genetic algorithms for dynamic test data generation"
        },
        {
            "abstract": "",
            "group": 2167,
            "name": "10.1.1.93.6125",
            "keyword": "",
            "title": "Distributed Simulated Annealing"
        },
        {
            "abstract": "The automated synthesis of mask geometry for VLSI leaf cells, referred to as the cell syn-thesis problem, is an important component of any structured custom integrated circuit design envi-ronment. Traditional approaches based on the classic functional cell style of Uehara & VanCleemput pose this problem as a straightforward one-dimensional graph optimization problem for which optimal solution methods are known. However, these approaches are only directly appli-cable to static CMOS circuits and they break down when faced with more exotic logic styles. There is an increasing need in modern VLSI designs for circuits implemented in high-per-formance logic families such as Cascode Voltage Switch Logic (CVSL), Pass Transistor Logic (PTL), and domino CMOS. Circuits implemented in these non-dual ratioed logic families can be highly irregular with complex geometry sharing and non-trivial routing. Such cells require a rela-tively unconstrained two-dimensional full-custom layout style which current methods are unable to synthesize. In this work we define the synthesis of complex two-dimensional digital cells as a new problem which we call transistor-level micro-placement and routing. To address this problem we develop a complete end-to-end methodology which is implemented in a prototype tool named TEMPO. A series of experiments on a new set of benchmark circuits verifies the effectiveness of",
            "group": 2168,
            "name": "10.1.1.93.6230",
            "keyword": "",
            "title": "ABSTRACT TRANSISTOR LEVEL MICRO PLACEMENT AND ROUTING FOR TWO-DIMENSIONAL DIGITAL VLSI CELL"
        },
        {
            "abstract": "Abstract. Bayesian inference provides a powerful framework to optimally integrate statistically learned prior knowledge into numerous computer vision algorithms. While the Bayesian approach has been successfully applied in the Markov random field literature, the resulting combinatorial optimization problems have been commonly treated with rather inefficient and inexact general purpose optimization methods such as Simulated Annealing. An efficient method to compute the global optima of certain classes of cost functions defined on binary-valued variables is given by graph min-cuts. In this paper, we propose to reconsider the problem of statistical learning for Bayesian inference in the context of efficient optimization schemes. Specifically, we address the question: Which prior information may be learned while retaining the ability to apply Graph Cut optimization? We provide a framework to learn and impose prior knowledge on the distribution of pairs and triplets of labels. As an illustration, we demonstrate that one can optimally restore binary textures from very noisy images with runtimes on the order of a second while imposing hundreds of statistically learned constraints per pixel. 1",
            "group": 2169,
            "name": "10.1.1.93.6283",
            "keyword": "",
            "title": "Learning statistical priors for efficient combinatorial optimization via graph cuts"
        },
        {
            "abstract": "This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued \u2018black box \u2019 function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the \u2018vanilla \u2019 gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on several tasks, while outperforming it on one task that is rich in deceptive local optima, the Rastrigin benchmark. 1",
            "group": 2170,
            "name": "10.1.1.93.7737",
            "keyword": "",
            "title": "Natural Evolution Strategies"
        },
        {
            "abstract": "This paper presents a novel broadband multimedia service called TV-Anytime. The basic idea of this service is to store broadcast media assets onto media server systems and allow clients to access these streams at any time. We propose a hierarchical structure of a distributed server network to support a high quality TV-anytime service. A key issue, how to map the media assets onto such a hierarchical server network is addressed and formalized as a combinatorial optimization problem. In order to solve this optimization problem, a set of heuristic solutions by use of a parallel simulated annealing library is proposed and verified by a set of benchmark instances. Finally, the TV Cache is presented as a prototype of a scalable TV-Anytime system. 1",
            "group": 2171,
            "name": "10.1.1.93.8326",
            "keyword": "",
            "title": "Heuristic Solutions for a Mapping Problem in a TV-Anytime Server Network \u00a3"
        },
        {
            "abstract": "For design problems involving computation-intensive analysis or simulation processes, approximation models are usually introduced to reduce computation time. Most approximation-based optimization methods make step-by-step improvements to the approximation model by adjusting the limits of the design variables. In this work, a new approximation-based optimization method for computation-intensive design problems   \u2014 the adaptive response surface method (ARSM), is presented. The ARSM creates quadratic approximation models for the computation-intensive design objective function in a gradually reduced design space. The ARSM was designed to avoid being trapped by local optimum and to identify the global design optimum with a modest number of objective function evaluations. Extensive tests on the ARSM as a global optimization scheme using benchmark problems, as well as an industrial design application of the method, are presented. Advantages and limitations of the approach are also discussed.",
            "group": 2172,
            "name": "10.1.1.93.8622",
            "keyword": "Response Surface MethodApproximate OptimizationGlobal OptimizationDesign Automation",
            "title": "Adaptive Response Surface Method -- A Global Optimization Scheme for Computation-intensive Design Problems"
        },
        {
            "abstract": "Abstract. We present a framework for distributed combinatorial optimization. The framework is implemented in Java, and simulates a multiagent environment in a single Java virtual machine. Each agent in the environment is executed asynchronously in a separate execution thread, and communicates with its peers through message exchange. The framework is highly customizable, allowing the user to implement and experiment with any distributed optimization algorithm. Support for synchronous/asynchronous message passing, monitoring and statistics, as well as problem visualization tools are provided. A number of distributed algorithms are already implemented in this framework, like the Distributed Breakout Algorithm [17] and the DPOP Algorithm [13]. A number of random evaluation problems are also provided, from two distinct domains: meeting scheduling and resource allocation in a sensor network. 1",
            "group": 2173,
            "name": "10.1.1.93.9677",
            "keyword": "",
            "title": "FRODO: A FRamework for Open/Distributed constraint Optimization"
        },
        {
            "abstract": "Testing. (Under the direction of Professor Michael Devetsikiotis). Modern technologies have provided us with highly available services. Systems such as optical backbone networks, robust web servers, and reliable software can provide a service with unavailability probability lower than 10 \u22126. Although rare, service unavailability can cause serious problems such as significant performance drop, or violation of Service Level Agreements (SLA). Moreover, providers of these services need to know the value of service unavailability probability so they can provide reasonable SLAs and corresponding Quality of Service (QoS). However, due to the extremely low values of the service unavailability probabilities, estimating them using traditional simulation or testing methods can require a vast amount of time to obtain a satisfactory confidence interval. As a result, efficient evaluation techniques are necessary. In this dissertation, we propose efficient evaluation methods based on importance sampling (IS). For fast simulation, we introduce several types of IS tuning methods: Our static IS method, which is based on asymptotically efficient IS biasing methods for a single queue, is proven to have bounded relative error. Our adaptive IS method, which is based on guidelines of \u201coptimal biasing\u201d, is efficient and can be widely employed. Moreover, IS methods that are stochastically optimized by simulated annealing can be used when the system is complicated, or when the knowledge of the system is limited. Finally, for performance evaluation and optimization of a system under various parameter settings, we propose a framework based on IS and metamodeling methodologies. All of these methods provided in this dissertation are verified by either proof or simulation to be both accurate and efficient.",
            "group": 2174,
            "name": "10.1.1.94.162",
            "keyword": "",
            "title": "Efficient Evaluation of Highly Available Services: Fast Simulation and Testing"
        },
        {
            "abstract": "Communicated by (Name) In an industry project with a German car manufacturer we are faced with the challenge of placing a maximum number of uniform rigid rectangular boxes in the interior of a car trunk. The problem is of practical importance due to a European industry norm which requires car manufacturers to state the trunk volume according to this measure. No really satisfactory automated solution for this problem has been known in the past. In spite of its NP hardness, combinatorial optimization techniques, which consider only grid-aligned placements, produce solutions which are very close to the one achievable by a human expert in several hours of tedious work. The remaining gap is mostly due to the constraints imposed by the chosen grid. In this paper we present a new approach which combines the grid-based combinatorial method with Simulated Annealing on a continuous model. This allows us to explore arbitrary orientations and placements of boxes, hence closing the gap even further, and \u2013 in some cases \u2013 even surpass the manual expert solution. 1",
            "group": 2175,
            "name": "10.1.1.94.449",
            "keyword": "putational Geometry",
            "title": "c \u25cb World Scientific Publishing Company Packing a Trunk \u2013 now with a Twist!"
        },
        {
            "abstract": null,
            "group": 2176,
            "name": "10.1.1.94.626",
            "keyword": "",
            "title": "Calculation of totally optimized button configurations using Fitts' law"
        },
        {
            "abstract": "Peer-to-peer (P2P) file sharing systems such as Gnutella have been widely acknowledged as the fastest growing Internet applications ever. The P2P model has many potential advantages including high flexibility and server-less management. However, these systems suffer from the well-known performance mismatch between the randomly constructed overlay network topology and the underlying IP-layer topology. This paper proposes to structure the P2P overlay topology using a heterogeneity-aware multi-tier topology to better balance the load at peers with heterogeneous capacities and to prevent low capability nodes from throttling the performance of the system. An analytical model is developed to enable the construction and maintenance of heterogeneity-aware overlay topologies with good node connectivity and better load balance. We also develop an efficient routing scheme, called probabilistic selective routing, that further utilizes heterogeneity-awareness to enhance the routing performance. We evaluate our design through simulations. The results show that our multi-tier topologies alone can provide eight to ten times improvements in the messaging cost, two to three orders of magnitude improvement in terms of load balancing, and seven to eight times lower topology construction and maintenance costs when compared to Gnutella\u2019s random power-law topology. Moreover, our heterogeneity-aware routing scheme provides further improvements on all evaluation metrics, when used with our heterogeneity-aware overlay topologies.",
            "group": 2177,
            "name": "10.1.1.94.1137",
            "keyword": "Peer-to-Peer SystemsOverlay TopologyOverlay RoutingNode HeterogeneityLoad Balancing",
            "title": "Scaling unstructured peer-to-peer networks with multi-tier capability aware topologies"
        },
        {
            "abstract": "Abstract--K-Nearest Neighbor is used broadly in text classification, but it has one deficiency\u2014computational efficiency. In this paper, we propose a heuristic search way to find out the k nearest neighbors quickly. Simulated annealing algorithm and inverted array are used to help find out the expected neighbors. Our experimental results demonstrate a significant improvement in classification computational efficiency in comparison with the conventional KNN. 1.",
            "group": 2178,
            "name": "10.1.1.94.1655",
            "keyword": "",
            "title": "A Fast KNN Algorithm Based on Simulated Annealing"
        },
        {
            "abstract": "Abstract. Large-scale scientific investigation often includes collaborative data exploration among geographically distributed researchers. The tools used for this exploration typically include some communicative component, and this component often forms the basis for insight and idea sharing among collaborators. Minimizing the tool interaction required to locate \u201cinteresting \u201d communications is therefore of paramount importance. We present the design of a novel visualization interface for representing the communications among multiple collaborating authors, and detail the benefits of our approach versus traditional methods. Our visualization integrates directly with the existing data exploration interface. We present our system in the context of an international research effort conducting collaborative analysis of accelerator simulations.",
            "group": 2179,
            "name": "10.1.1.94.2607",
            "keyword": "multiple location collaborative design applicationsinformation visualization",
            "title": "VICA: A Voronoi Interface for Visualizing Collaborative Annotations"
        },
        {
            "abstract": "The Multidimensional Assignment Problem (MAP) is an NP-hard combinatorial optimization problem occurring in many applications, such as data association, target tracking, and resource planning. As many solution approaches to this problem rely, at least partly, on local neighborhood search algorithms, the number of local minima affects solution difficulty for these algorithms. This paper investigates the expected number of local minima in randomly generated instances of the MAP. Lower and upper bounds are developed for the expected number of local minima, E[M], in an MAP with iid standard normal coefficients. In a special case of the MAP, a closed-form expression for E[M] is obtained when costs are iid continuous random variables. These results imply that the expected number of local minima is exponential in the number of dimensions of the MAP. Our numerical experiments indicate that larger numbers of local minima have a statistically significant negative effect on the quality of solutions produced by several heuristic algorithms that involve local neighborhood search.",
            "group": 2180,
            "name": "10.1.1.94.3017",
            "keyword": "Multidimensional Assignment ProblemRandom CostsCombinatorial OptimizationLocal MinimaNeighborhood Search",
            "title": "On the Number of Local Minima for the Multidimensional Assignment Problem"
        },
        {
            "abstract": "A local-search heuristic for finding high-quality solutions for many hard optimization problems is explored. The method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of selforganized criticality, a concept introduced to describe emergent complexity in physical systems. This method, called extremal optimization, successively replaces the value of extremely undesirable variables in a sub-optimal solution with new, random ones. Large, avalanche-like fluctuations in the cost function emerge dynamically. These enable the search to effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters. Drawing upon models used to simulate the dynamics of granular media, evolution, or geology, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such as simulated annealing. This method is very general and so far has proved competitive with\u2014and even superior to\u2014more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to 10 5 variables, such as bipartitioning, coloring, and spin glasses. Analysis of a model problem predicts the only free parameter of the method in accordance with all experimental results. \u00a9 2003 Wiley Periodicals, Inc.* Key Words: extremal optimization; criticality; simulated annealing; punctuated equilibrium Many natural systems have, without any centralized",
            "group": 2181,
            "name": "10.1.1.94.3835",
            "keyword": "",
            "title": "Optimization with extremal dynamics"
        },
        {
            "abstract": "Abstract. Recent work (Yedidia, Freeman, Weiss [22]) has shown that stable points of belief propagation (BP) algorithms [12] for graphs with loops correspond to extrema of the Bethe free energy [3]. These BP algorithms have been used to obtain good solutions to problems for which alternative algorithms fail to work [4], [5], [10] [11]. In this paper we introduce a discrete iterative algorithm which we prove is guaranteed to converge to a minimum of the Bethe free energy. We call this the double-loop algorithm because it contains an inner and an outer loop. The algorithm is developed by decomposing the free energy into a convex part and a concave part, see [25], and extends a class of mean field theory algorithms developed by [7],[8] and, in particular, [13]. Moreover, the double-loop algorithm is formally very similar to BP which may help understand when BP converges. In related work [24] we extend this work to the more general Kikuchi approximation [3] which includes the Bethe free energy as a special case. It is anticipated that these double-loop algorithms will be useful for solving optimization problems in computer vision and other applications. 1",
            "group": 2182,
            "name": "10.1.1.94.4043",
            "keyword": "",
            "title": "A Double-Loop Algorithm to Minimize the Bethe Free"
        },
        {
            "abstract": "We report the results of testing the performance of a new, efficient, and highly general-purpose parallel optimization method, based upon simulated annealing. This optimization algorithm was applied to analyze the network of interacting genes that control embryonic development and other fundamental biological processes. We found several sets of algorithmic parameters that lead to optimal parallel efficiency for up to 100 processors on distributed-memory MIMD architectures. Our strategy contains two major elements. First, we monitor and pool performance statistics obtained simultaneously on all processors. Second, we mix states at intervals to ensure a Boltzmann distribution of energies. The central scientific issue is the inverse problem, the determination of the parameters of a set of nonlinear ordinary differential equations by minimizing the total error between the model behavior and experimental",
            "group": 2183,
            "name": "10.1.1.94.4404",
            "keyword": "Key Wordssimulated annealingparallel processinginverse problems",
            "title": "Parallel simulated annealing by mixing of states"
        },
        {
            "abstract": "keyword1, keyword2, keyword3, keyword4 In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment and small-world models, motivated by real-world graphs such as the Internet topology. To address the natural question of which model is best for a particular data set, we propose a model selection criterion for graph models. Since each model is in fact a probability distribution over graphs, we suggest using Maximum Likelihood to compare graph models and select their parameters. Interestingly, for the case of graph models, computing likelihoods is a difficult algorithmic task. However, we design and implement MCMC algorithms for computing the maximum likelihood for four popular models: a power-law random graph model, a preferential attachment model, a small-world model, and a uniform random graph model. We hope that this novel use of ML will objectify comparisons between graph models. 1.",
            "group": 2184,
            "name": "10.1.1.94.5509",
            "keyword": "keyword2keyword3keyword4",
            "title": "Graph Model Selection using Maximum Likelihood keyword1, keyword2, keyword3, keyword4"
        },
        {
            "abstract": "Abstract. The problem of semi-supervised image segmentation is frequently posed e.g. in remote sensing applications. In this setting, one aims at finding a decomposition of a given image into its constituent regions, which are typically assumed to have homogeneously distributed pixel values. In addition, it is requested that these regions can be equipped with some semantics, i.e. that they can be matched to particular land cover classes. For this purpose, class labels are provided for a small subset of the image data. The demand that the image segmentation respects those class labels implies that the segmentation algorithm should be posed as a constrained optimization problem. We extend the Parametric Distributional Clustering (PDC) algorithm to fit into this learning framework. The resulting optimization problem is solved by constrained Deterministic Annealing. The approach is illustrated for both artificial data and real-world synthetic aperture radar (SAR) imagery.",
            "group": 2185,
            "name": "10.1.1.94.5530",
            "keyword": "image segmentationdistributional clusteringsemi-supervised learning",
            "title": "c\u25cbSpringer-Verlag Semi-Supervised Image Segmentation by Parametric Distributional Clustering"
        },
        {
            "abstract": "Towards a theory of everything?  \u2013 Grand challenges in complexity and informatics",
            "group": 2186,
            "name": "10.1.1.94.5531",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The development of next-generation CAD tools and FPGA architectures require benchmark circuits to experiment with new algorithms and architectures. There has always been a shortage of good public benchmarks for these purposes, and even companies that have access to proprietary customer designs could benefit from designs that meet size and other particular specifications. In this thesis, we present a new method of generating realistic synthetic benchmark circuits to help alleviate this shortage. The method significantly improves the quality of previous work by imposing the natural hierarchy of circuits through clustering and by using a simpler method of characterizing the nature of sequential circuits. Also, in contrast to current constructive generation methods, we employ new iterative techniques in the generation that provide better control over the generated circuit\u2019s characteristics. As in previous work, we assess the realism of the generated circuits by comparing properties of real circuits and generated &quot;clones &quot; of the real circuit after placement and routing. On average, the real and clone circuits ' total detailed wirelength differed by only 14%, a major improvement over previous results. In addition, the minimum track count was within 14 % and the critical path delay was within 10%.",
            "group": 2187,
            "name": "10.1.1.94.6126",
            "keyword": "",
            "title": "Synthetic Circuit Generation Using Clustering and Iteration"
        },
        {
            "abstract": "",
            "group": 2188,
            "name": "10.1.1.94.7006",
            "keyword": "",
            "title": "Classification and Computational Analysis"
        },
        {
            "abstract": "by",
            "group": 2189,
            "name": "10.1.1.94.7284",
            "keyword": "",
            "title": "Interval Methods for Global Optimization"
        },
        {
            "abstract": "Efficient partitioning of large data sets into homogenous clusters is a fundamental problem in data mining. The standard hierarchical clustering methods provide no solution for this problem due to their computational inefficiency. The k-means based methods are promising for their efficiency in processing large data sets. However, their use is often limited to numeric data. In this paper we present a k-prototypes algorithm which is based on the k-means paradigm but removes the numeric data limitation whilst preserving its efficiency. In the algorithm, objects are clustered against k prototypes. A method is developed to dynamically update the k prototypes in order to maximise the intra cluster similarity of objects. When applied to numeric data the algorithm is identical to the kmeans. To assist interpretation of clusters we use decision tree induction algorithms to create rules for clusters. These rules, together with other statistics about clusters, can assist data miners to understand and identify interesting clusters. 1",
            "group": 2190,
            "name": "10.1.1.94.9984",
            "keyword": "",
            "title": "Clustering large data sets with mixed numeric and categorical values"
        },
        {
            "abstract": "Address for correspondence",
            "group": 2191,
            "name": "10.1.1.95.347",
            "keyword": "JEL codesR22R12C61",
            "title": "METHODOLOGICAL PROPOSAL *"
        },
        {
            "abstract": "Abstract \u2014 Distributed computing systems are increasingly being created as self-organizing collections of many autonomous (human or software) agents cooperating as peers. Peer-to-peer coordination introduces, however, unique and potentially serious challenges. When there is no one \u2018in charge\u2019, dysfunctions can emerge as the collective effect of locally reasonable decisions. In this paper, we consider the dysfunction wherein inefficient resource use oscillations occur due to delayed status information, and describe novel approaches, based on the selective use of misinformation, for dealing with this problem. A model of several servers offering equivalent service to independent clients is presented and studied numerically and analytically; the spreading of misinformation about the queue status is found to dampen oscillations and improve system performance for a wide range of parameters. 1 Index \u2014emergent terms dysfunctions, resource oscillations, selective misinformation I.",
            "group": 2192,
            "name": "10.1.1.95.1136",
            "keyword": "",
            "title": "Handling Emergent Resource Use Oscillations"
        },
        {
            "abstract": "Abstract. In this paper, we demonstrate the ease in which an adaptive simulated annealing algorithm can be designed. Specifically, we use the adaptive annealing schedule known as the modified Lam schedule to apply simulated annealing to the weighted tardiness scheduling problem with sequence-dependent setups. The modified Lam annealing schedule adjusts the temperature to track the theoretical optimal rate of accepted moves. Employing the modified Lam schedule allows us to avoid the often tedious tuning of the annealing schedule; as the algorithm tunes itself for each instance during problem solving. Our results show that an adaptive simulated annealer can be competitive when compared to highly tuned, hand crafted algorithms. Specifically, we compare our results to a state-of-theart genetic algorithm for weighted tardiness scheduling with sequence-dependent setups. Our study serves as an illustration of the ease with which a parameter-free simulated annealer can be designed and implemented. 1",
            "group": 2193,
            "name": "10.1.1.95.2024",
            "keyword": "",
            "title": "On the Design of an Adaptive Simulated Annealing Algorithm"
        },
        {
            "abstract": "Software and hardware applications for human face image processing are be-coming more ubiquitous. One form of biometrics uses human face recognition [WPB + 98, GL02, LW02, CR02, Tit02, LY98] to identify or verify a person\u2019s iden-tity. Such a system automatically extracts and recognises a person\u2019s face from any",
            "group": 2194,
            "name": "10.1.1.95.2900",
            "keyword": "",
            "title": "Chapter 2 Eye Detection Using Support Vector Machines"
        },
        {
            "abstract": "Randomization is a standard technique for improving the per-formance of local search algorithms for constraint satisfac-tion. However, it is well-known that local search algorithms are sessitive to the noise values selected. We investigate the use of an adaptive noise mechanism in an iterative repair-based planner/scheduler for spacecraft operations. Prelimi-nary results indicate that adaptive noise makes the use of ran-domized repair moves safe and robust; that is, using adaptive noise makes it possible to consistently achieve performance comparable with the best tuned noise setting without the need for manually tuning the noise parameter., 1",
            "group": 2195,
            "name": "10.1.1.95.4728",
            "keyword": "",
            "title": "Robust local search for spacecraft operations using adaptive noise"
        },
        {
            "abstract": "Abstract\u2014Themean field theory approach to knapsackproblems is extended to multiple knapsacks and generalized assignmentproblems withPotts meanfield equationsgoverning the dynamics.Numerical tests against \u201cstate of the art\u2019 conventional algorithms shows good performance for the meanfield approach. The inherentlyparallelism of the mean field equations makes them suitable for direct implementations in microchips. It is demonstrated numerically that the performance is essentially not affected when only a limited number of bits is used in the meanjield equations. Also, a hybrid algorithm with linearprogramming andmeanfield componentsis showed tofirther improve theperjormance for",
            "group": 2196,
            "name": "10.1.1.95.4817",
            "keyword": "GeneralizedassignmentproblemsMean field theoryNeural networksFinite precision",
            "title": "A Study of the Mean Field Approach to Knapsack Problems"
        },
        {
            "abstract": "In a gene expression data matrix a bicluster is a grouping of a subset of genes and a subset of conditions which show correlating levels of expression activity. The difficulty of finding significant biclusters in gene expression data grows exponentially with the size of the dataset and heuristic approaches such as Cheng and Church\u2019s greedy node deletion algorithm are required. It is to be expected that stochastic search techniques such as Genetic Algorithms or Simulated Annealing might produce better solutions than greedy search. In this paper we show that a Simulated Annealing approach is well suited to this problem and we present a comparative evaluation of Simulated Annealing and node deletion on a variety of datasets. We show that Simulated Annealing discovers more significant biclusters in many cases. 1.",
            "group": 2197,
            "name": "10.1.1.95.5071",
            "keyword": "",
            "title": "Biclustering of expression data using simulated annealing"
        },
        {
            "abstract": "We present biorthogonal and orthonormal wavelets for embedded zerotree wavelet compression of fingerprint images. By simulated annealing over the wavelet filter coefficients, using a composite cost function dependent upon a parameter k 2 which weights the relative importance of the wavelet\u2019s bandwidth and time dispersion, a series of wavelets is obtained. Each of these is optimal in terms of a particular Heisenberg uncertainty \u2018footprint\u2019, i.e. a particular trade-off between bandwidth and time dispersion. The psychovisually optimal wavelet for fingerprint image compression is determined by fingerprint experts by examination of images compressed and recovered using the series of wavelets. Psychovisually tuned wavelets were found to yield superior visual fidelity to standard wavelets and also to wavelets optimized to produce minimum rms error on either fingerprint or general test images. 1. BACKGROUND Most large police forces use automated fingerprint identification systems (AFIS) to match fingerprints when seeking to identify individuals during criminal investigations. Fingerprint image compression is an essential component of AFIS systems because of the large sizes of data bases, which may contain several million fingerprint images. The U.S. Federal Bureau of Investigation has specified a wavelet method for use in its fingerprint data base [1]. Fingerprints have special local ridge properties [2, 3] which are well suited to processing via the discrete wavelet transform (DWT). In particular, the compact support of the basis functions implies an ability to adapt to local image structures. The well-known Heisenberg uncertainty relationship places a lower limit upon the time-frequency uncertainty of any signal. The aim of the present work was to determine the optimum trade-off between frequency and time resolution for wavelets when applied to the compression of fingerprint images for criminological identification purposes.",
            "group": 2198,
            "name": "10.1.1.95.5458",
            "keyword": "",
            "title": "Psychovisually tuned wavelet fingerprint compression"
        },
        {
            "abstract": "Superposition of sigmoid function over a finite time interval is shown to be equivalent to the linear combination of the solutions of a linearly parameterized system of logistic differential equations. Due to the linearity with respect to the parameters of the system, it is possible to design an effective procedure for parameter adjustment. Stability properties of this procedure are analyzed.",
            "group": 2199,
            "name": "10.1.1.95.5924",
            "keyword": "Neural NetworksControl TheoryAdaptive ControlLearning Algorithms",
            "title": "Parameter Estimation of Sigmoid Superpositions: Dynamical System Approach"
        },
        {
            "abstract": " As the size and complexity of very large scale integrated (VLSI) circuits increase, the need for faster floorplanning algorithms also grows. This paper introduces trapezoidal floorplanning for integrated circuits (Traffic), a new method for creating wire- and area-optimized floorplans. Through the use of connectivity grouping, simple geometry, and a constrained bruteforce approach, Traffic achieves an average of 18 % lower wire estimate than simulated annealing (SA) in orders of magnitude less time. This speed allows designers to rapidly explore a large circuit design space, to evaluate small changes to big circuits, to fit bounding boxes, and to produce initial solutions for other floorplanning algorithms. ",
            "group": 2200,
            "name": "10.1.1.95.6631",
            "keyword": "",
            "title": "Traffic: A Novel Geometric Algorithm for Fast Wire-optimized Floorplanning"
        },
        {
            "abstract": "Developing Grid applications is a challenging endeavor, which at the moment requires both extensive labor and expertise. The Grid Application Development Software Project (GrADS) provides a system to simplify Grid application development. This system incorporates tools at all stages of the application development and execution cycle. In this chapter we focus on application scheduling, and present the three scheduling approaches developed in GrADS: development of an initial application schedule (launch-time scheduling), modification of the execution platform during execution (rescheduling), and negotiation between multiple applications in the system (metascheduling). These approaches have been developed and evaluated for platforms that consist of distributed networks of shared workstations, and applied to real-world parallel applications.",
            "group": 2201,
            "name": "10.1.1.95.6938",
            "keyword": "scheduling rescheduling metascheduling Grid data-parallel",
            "title": "Chapter 1 SCHEDULING IN THE GRID APPLICATION DEVELOPMENT SOFTWARE PROJECT"
        },
        {
            "abstract": "Abstract: A new genetic algorithm coding is proposed in this paper to solve flowshop scheduling problems. To show the efficiency of the considered approach, two examples, in pharmaceutical and agro-food industries are considered with minimization of different costs related to each problem as a scope. Multi-objective optimization is thus, used and its performances proved.",
            "group": 2202,
            "name": "10.1.1.95.7313",
            "keyword": "genetic algorithmoperations codingflow-shop problemsmultiobjective optimization",
            "title": "A Proposed Genetic Algorithm Coding for Flow-Shop Scheduling Problems"
        },
        {
            "abstract": "A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving. A number of \\no free lunch &quot; (NFL) theorems are presented that establish that for any algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed are time-varying optimization problems and a priori \\head-to-head &quot; minimax distinctions between optimization algorithms, distinctions that can obtain despite the NFL theorems ' enforcing of a type of uniformity over all algorithms. 1",
            "group": 2203,
            "name": "10.1.1.95.7770",
            "keyword": "",
            "title": "No free lunch theorems for optimization"
        },
        {
            "abstract": "Let the outcome of some arbitrary process be a set S of orders of the elements of a finite set \u03a3. These orders may encompass all items of \u03a3 or only a subset of it. Assume that all orders in S have been generated by an unknown set of partial orders. An order is said to be generated by a partial order P if the order is a linear extension of P or if it can be obtained from one by omitting some elements of \u03a3. The topic of this work is to design algorithms for discovering the underlying partial orders. The methods have no a priori information on the partial orders and must operate only on the set S. Discovering one or several partial orders are considered separately. For the problem of finding one partial order the algorithm greedy-orderis devised. This algorithm maximizes a score function by adding such ordered pairs (i, j) to the partial order under construction that are more frequent than the opposite ordering (j, i). The problem of discovering several partial orders is solved by first finding such a partition of S that orders generated by the same partial order reside in the same member of the partition. Finally greedy-orderis applied to every member of the partition.",
            "group": 2204,
            "name": "10.1.1.95.8521",
            "keyword": "data miningpartial orderrankingclusteringfrequent itemsetsimulated annealing 2",
            "title": ""
        },
        {
            "abstract": "iii",
            "group": 2205,
            "name": "10.1.1.95.8572",
            "keyword": "List of Figures xi",
            "title": "Acknowledgements"
        },
        {
            "abstract": "iii",
            "group": 2206,
            "name": "10.1.1.95.8572",
            "keyword": "List of Figures xi",
            "title": "Acknowledgements"
        },
        {
            "abstract": "When the computing environment becomes heterogeneous and applications become modular with reusable components, automatic performance tuning is needed for these applications to run well in different environments. We present the Active Harmony automated runtime tuning system and describe the interface used by programs to make applications tunable. We present the optimization algorithm used to adjust application parameters and the Library Specification Layer which helps program library developers expose multiple variations of the same API using different algorithms. By comparing the experience stored in a database, the tuning server is able to find appropriate configurations more rapidly. Utilizing historical data together with a mechanism that estimates performance speeds up the tuning process. To avoid performance oscillations during the initial phase of the tuning process, we use improved search refinement techniques that use configurations equally spaced throughout the performance search space to make the tuning process smoother. We also introduce a parameter prioritizing tool to focus on those performance critical parameters. We demonstrate how to reduce the time when tuning a large system with",
            "group": 2207,
            "name": "10.1.1.95.9546",
            "keyword": "",
            "title": "ABSTRACT Title of Dissertation: TOWARDS AUTOMATIC PERFORMANCE TUNING"
        },
        {
            "abstract": "This paper presents a new Bayesian framework for motion segmentation\u2014dividing a frame from an image sequence into layers representing different moving objects\u2014by tracking edges between frames. Edges are found using the Canny edge detector, and the Expectation-Maximisation algorithm is then used to fit motion models to these edges and also to calculate the probabilities of the edges obeying each motion model. The edges are also used to segment the image into regions of similar colour. The most likely labelling for these regions is then calculated by using the edge probabilities, in association with a Markov Random Field-style prior. The identification of the relative depth ordering of the different motion layers is also determined, as an integral part of the process. An efficient implementation of this framework is presented for segmenting two motions (fore-ground and background) using two frames. It is then demonstrated how, by tracking the edges into further frames, the probabilities may be accumulated to provide an even more accurate and robust estimate, and segment an entire sequence. Further extensions are then presented to address the segmentation of more than two motions. Here, a hierarchical method of initialising the Expectation-Maximisation algorithm is described, and it is demonstrated that the Minimum Description Length principle may be used to automatically select the best number of motion layers. The results from over 30 sequences (demonstrating both two and three motions) are presented and discussed.",
            "group": 2208,
            "name": "10.1.1.96.449",
            "keyword": "Index Terms Video analysisMotionSegmentationDepth cues",
            "title": "Layered motion segmentation and depth ordering by tracking edges"
        },
        {
            "abstract": "Abstract. The quadratic assignment problem (QAP) is one of the well-known combinatorial optimization problems and is known for its various applications. In this paper, we propose a modified simulated annealing algorithm for the QAP \u2013 M-SA-QAP. The novelty of the proposed algorithm is an advanced formula of calculation of the initial and final temperatures, as well as an original cooling schedule with oscillation, i.e., periodical decreasing and increasing of the temperature. In addition, in order to improve the results obtained, the simulated annealing algorithm is combined with a tabu search approach based algorithm. We tested our algorithm on a number of instances from the library of the QAP instances \u2013 QAPLIB. The results obtained from the experiments show that the proposed algorithm appears to be superior to earlier versions of the simulated annealing for the QAP. The power of M-SA-QAP is also corroborated by the fact that the new best known solution was found for the one of the largest QAP instances \u2013 THO150. Key words: heuristics, local search, simulated annealing, quadratic assignment problem. 1.",
            "group": 2209,
            "name": "10.1.1.96.885",
            "keyword": "",
            "title": "A Modified Simulated Annealing Algorithm for the Quadratic Assignment Problem"
        },
        {
            "abstract": "In this paper, we propose a robust adaptive region segmentation algorithm for noisy images, within a Bayesian framework. A multiresolution implementation of the algorithm is performed using a wavelets basis and can be used to process both 2D and 3D data. In this work we focus on the adaptive character of the algorithm and we discuss how global and local statistics can be utilised in the segmentation process. We propose an improvement on the adaptivity by introducing an enhancement to control the adaptive properties of the segmentation process. This takes the form of a weighting function accounting for both local and global statistics, and is introduced in the minimisation. A new formulation of the segmentation problem allows us to control the effective contribution of each statistical component. The segmentation algorithm is demonstrated on synthetic data, 2D breast ultrasound data and on echocardiographic sequences \u00f02D \u00fe T\u00de. An evaluation of the performance of the proposed algorithm is also presented.",
            "group": 2210,
            "name": "10.1.1.96.1194",
            "keyword": "UltrasoundBayesian segmentationAdaptive algorithmMultiresolution",
            "title": "Segmentation of ultrasound images\u2013\u2013multiresolution 2D and 3D algorithm based on global and local statistics"
        },
        {
            "abstract": "An important areaof research in computational biochemistry is the design of molecules for speci c applications. The design of these molecules, which depends on the accurate determination of their three-dimensional structure, can be formulated as a global optimization problem. In this study, we present results from the application of a new conformation searching method based on direct search methods. We compare these results to some earlier results using genetic algorithms and simulated annealing.",
            "group": 2211,
            "name": "10.1.1.96.2476",
            "keyword": "global optimizationmolecular conformationnonlinear programming",
            "title": "On the use of direct search methods for the molecular conformation problem"
        },
        {
            "abstract": "Bibliographic data and classifications of all the ERIM reports are also available on the ERIM website: www.erim.eur.nl",
            "group": 2212,
            "name": "10.1.1.96.2538",
            "keyword": "Internetwww.erim.eur.nl",
            "title": "Journal of Economic"
        },
        {
            "abstract": "UC-411 An important area of research in computational biochemistry is the design of molecules for speci c applications. The design of these molecules depends on the accurate determination of their three-dimensional structure or conformation. Under the assumption that molecules will settle into a con guration for which their energy is at a minimum, this design problem can be formulated as a global optimization problem. The solution of the molecular conformation problem can then be obtained, at least in principle, through any number of optimization algorithms. Unfortunately, it can easily be shown that there exist a large number of local minima for most molecules which makes this an extremely di cult problem for any standard optimization method. In this study, we present results for a direct search method applied to a molecular conformation problem. We compare the new method against genetic algorithms and simulated annealing. The major result of this study is that the direct search method when used in combination with standard enery minimization algorithms can nd a large number of low",
            "group": 2213,
            "name": "10.1.1.96.2676",
            "keyword": "Contents",
            "title": "Direct search methods for the molecular conformation problem"
        },
        {
            "abstract": "The clonal selection is a mechanism used by the natural immune system to select cells that recognize the antigens to proliferate. The proliferated cells are subject to an affinity maturation process, which improves their affinity to the selective antigens. The concept of clonal selection is a vitally important one to the success of the human immune system, and it provides an excellent example of the principles of selection at work. The Positive and negative selection is another interesting mechanism in the immune system that work together to both retain cells that recognize the self peptides, while also removing cells that recognize any self peptides. In this paper, a cloning-based algorithm inspired by the clonal and the positive/negative selection mechanism of the natural immune system is presented. This algorithm is inherently parallel and the cloning strategy employs greedy criteria which lends to an adaptive approach. The well known TSP is used to illustrate the approach with experimental comparison with Ant approach. Simulations demonstrate that this approach generates good solutions to traveling salesman problem and greatly improve the convergence speed compared to the Ant-based optimization approach.",
            "group": 2214,
            "name": "10.1.1.96.2833",
            "keyword": "OptimizationImmune systemClonal and negative/positive selectionAnt coloniesTraveling salesman problem",
            "title": "An Immune Inspired-based Optimization Algorithm: Application to the Traveling Salesman Problem"
        },
        {
            "abstract": "In this paper a novel approach for improving automatic image annotation methods is proposed. The approach is based on the fact that accuracy of current image annotation methods is low if we look at the most confident label only. Instead, accuracy is improved if we look for the correct label within the set of the top\u2212k candidate labels. We take advantage of this fact and propose a Markov random field (MRF) based on word co-occurrence information for the improvement of annotation systems. Through the MRF structure we take into account spatial dependencies between connected regions. As a result, we are considering semantic relationships between labels. We performed experiments with iterated conditional modes and simulated annealing as optimization strategies in a subset of the Corel benchmark collection. Experimental results of the proposed method together with a k\u2212nearest neighbors classifier as our annotation method show important error reductions. 1",
            "group": 2215,
            "name": "10.1.1.96.3117",
            "keyword": "",
            "title": "Word Co-occurrence and Markov Random Fields for Improving Automatic Image Annotation"
        },
        {
            "abstract": "Automatic detection of the optic disc, fovea and vascular arch in digital color photographs of the retina",
            "group": 2216,
            "name": "10.1.1.96.3655",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "This thesis was set in Computer Modern 10",
            "group": 2217,
            "name": "10.1.1.96.3723",
            "keyword": "Acknowledgments xi",
            "title": "Annealing Based Optimization Methods for Signal Processing Applications Per Persson"
        },
        {
            "abstract": "Different algorithms are presented and evaluated for designing Virtual Private/Overlay Networks (VPNs/VONs) over any network that supports resource partitioning e.g. ATM, MPLS or SDH/SONET. All algorithms incorporate protection as well. The VPNs/VONs are formed by full mesh demand sets be-tween VPN/VON endpoints. The service demands of VPNs/VONs are characterised by the bandwidth requirements of node-pairs (pipe-model). We investigated four design modes with three pro-active path based shared protection path algorithms and four heuristics to calculate the pairs of paths. The design mode determines the means of traffic concentration. The protection path algorithms use Dijkstra\u2019s shortest path calculation with different edge weights. The demands are routed one-by-one, therefore the order in which they are processed matters. To eliminate this factor we used three heuristics (simulated allocation, simulated annealing, threshold accepting). We present numerical results obtained by simulation regarding the required total amount of capacity, the number of reserved edges, and the average length of paths. Keywords: VPN, VON, shared protection, heuristic algorithm, Dijkstra\u2019s algorithm, simulated alloca-tion, design, configuration",
            "group": 2218,
            "name": "10.1.1.96.5206",
            "keyword": "",
            "title": "Virtual Private/Overlay Network Design with Traffic Concentration and Shared Protection 1"
        },
        {
            "abstract": "Application authorized by",
            "group": 2219,
            "name": "10.1.1.96.5819",
            "keyword": "",
            "title": "AUTONOMY REQUIREMENTS"
        },
        {
            "abstract": "This research explores the application of a simulationbased scheduling algorithm to generate unload schedules for processing feeder trailers in a parcel consolidation terminal. The study compares the performance of iterative improvement and simulated annealing to produce quality schedules. The paper reports the results from a number of experimental test problems.",
            "group": 2220,
            "name": "10.1.1.96.5961",
            "keyword": "",
            "title": "SIMULATION-BASED SCHEDULING FOR PARCEL CONSOLIDATION TERMINALS: A COMPARISON OF ITERATIVE IMPROVEMENT AND SIMULATED ANNEALING"
        },
        {
            "abstract": "The RNA secondary structure prediction problem (2\u02daRNA) is a critical one in molecular biology. Secondary structure can be determined directly by x-ray diffraction, but this is difficult, slow, and expensive. Moreover, it is currently impossible to crystallize most RNAs. Mathematical models for prediction",
            "group": 2221,
            "name": "10.1.1.96.7205",
            "keyword": "",
            "title": "Neural networks, adaptive optimization, and RNA secondary structure prediction"
        },
        {
            "abstract": "Abstract A recently introduced general-purpose heuristic for finding high-quality solutions for many hard optimization problems is reviewed. The method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of self-organized criticality, a concept introduced to describe emergent complexity in physical systems. This method, called extremal optimization, successively replaces the value of extremely undesirable variables in a sub-optimal solution with new, random ones. Large, avalanche-like fluctuations in the cost function self-organize from this dynamics, effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters. Drawing upon models used to simulate the dynamics of granular media, evolution, or geology, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such as simulated annealing. It may be but one example of applying new insights into non-equilibrium phenomena systematically to hard optimization problems. This method is widely applicable and so far has proved competitive with \u2013 and even superior to \u2013 more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to 10 5 variables, such as bipartitioning, coloring, and satisfiability. Analysis of a suitable model predicts the only free parameter of the method in accordance with all experimental results.",
            "group": 2222,
            "name": "10.1.1.96.7670",
            "keyword": "Combinatorial OptimizationHeuristic MethodsEvolutionary AlgorithmsSelf- Organized Criticality. \u2217 www.physics.emory.edu/faculty/boettcher",
            "title": "Extremal optimization: An evolutionary local-search algorithm"
        },
        {
            "abstract": "convex optimization in the bandit setting:",
            "group": 2223,
            "name": "10.1.1.96.8506",
            "keyword": "",
            "title": "gradient descent without a"
        },
        {
            "abstract": "PRECON S.A is a manufacturing company dedicated to produce prefabricated concrete parts to several industries as rail transportation and agricultural industries. Recently, PRECON signed a contract with RENFE, the Spanish Nnational Rail Transportation Company to manufacture pre-stressed concrete sleepers for siding of the new railways of the high speed train AVE. The scheduling problem associated with the manufacturing process of the sleepers is very complex since it involves several constraints and objectives. The constraints are related with production capacity, the quantity of available moulds, satisfying demand and other operational constraints. The two main objectives are related with maximizing the usage of the manufacturing resources and minimizing the moulds movements. We developed a deterministic crowding genetic algorithm for this multiobjective problem. The algorithm has proved to be a powerful and flexible tool to solve the large-scale instance of this complex real scheduling problem.",
            "group": 2224,
            "name": "10.1.1.96.8905",
            "keyword": "production schedulinggenetic algorithms. JELL61L23D83 1",
            "title": "Solving a Concrete Sleepers Production Scheduling by Genetic Algorithms"
        },
        {
            "abstract": "Abstract \u2014 Controlled movement of sensors within a given region is known to improve the overall quality of measurements by reducing sensing uncertainty. A mobile wireless sensor network may be deployed to detect and track a large-scale physical phenomenon in a geographical region such as an oil spill in the ocean. It may be called upon to provide a description of a contour characterized by an isoline of a specific concentration value. In this paper, we examine the problem of tracing a contour of a particular concentration within a bounded geographical region of varying pollutant concentration using a network of mobile sensors. We explore various ways of guiding a set of mobile sensors optimally so as to surround and trace the contour. We formulate the contour estimation problem as a nonlinear multi-extremal optimization problem and use gradient free finite difference based technique to estimate the target contour in the given region. We use accuracy and latency as performance metrics and show that in the majority of the cases our proposed strategy based on collaboration of sensors delivers the best performance. I.",
            "group": 2225,
            "name": "10.1.1.96.9535",
            "keyword": "",
            "title": "Contour estimation using collaborating mobile sensors"
        },
        {
            "abstract": "",
            "group": 2226,
            "name": "10.1.1.96.9898",
            "keyword": "",
            "title": "The dynamics of the computational modeling of analogy-making"
        },
        {
            "abstract": "The Grid vision is to allow heterogeneous computational resources to be shared and utilised globally. Grid users are able to submit tasks to remote resources for execution. However, these resources may be unreliable and there is a risk that submitted tasks may fail or cost more than expected. The notion of trust is often used in agent-based systems to manage such risk, and in this paper we apply trust to the problem of resource selection in Grid computing. We propose a number of resource selection algorithms based upon trust, and evaluate their effectiveness in a simulated Grid. 1",
            "group": 2227,
            "name": "10.1.1.97.882",
            "keyword": "",
            "title": "Experience-based trust: Enabling effective resource selection in a grid environment"
        },
        {
            "abstract": "simulated annealing band selection approach for",
            "group": 2228,
            "name": "10.1.1.97.1014",
            "keyword": "PCA",
            "title": "Jyh Perng Fang a, Yang-Lang Chang a, Hsuan Ren b,"
        },
        {
            "abstract": "Abstract. In this paper we consider architectural layout problem that seeks to determine the layout of Units based on lighting, heating, available sizes and other objectives and constraints. For a conceptual design of architectural layout we present an approach based on evolutionary search method known as the genetic algorithms (GAs). However, the rate of convergence of GAs is often not good enough at their current stage. For this reason, the improved genetic algorithm is proposed. We have analysed and compared the performance of standard and improved genetic algorithm for architectural layout problem solutions and presented the results of performance.",
            "group": 2229,
            "name": "10.1.1.97.3551",
            "keyword": "architectural designfloorplanninglayoutconceptual designoptimizationgenetic algorithms",
            "title": "OPTIMIZATION OF ARCHITECTURAL LAYOUT BY THE IMPROVED GENETIC ALGORITHM"
        },
        {
            "abstract": "Abstract\u2014The design of large libraries of oligonucleotides having constant-content and satisfying Hamming distance constraints between oligonucleotides and their Watson-Crick complements is important in reducing hybridization errors in DNA computing, DNA microarray technologies, and molecular bar coding. Various techniques have been studied for the construction of such oligonucleotide libraries, ranging from algorithmic constructions via stochastic local search to theoretical constructions via coding theory. A new stochastic local search method is introduced, which yields improvements for more than one third of the benchmark lower bounds of Gaborit and King (2005) for \ufffd-mer oligonucleotide libraries when \ufffd IR. Several optimal libraries are also found by computing maximum cliques on certain graphs. Index Terms\u2014DNA codes, exhaustive search, Hamming distance model, oligonucleotide libraries, stochastic local search. I.",
            "group": 2230,
            "name": "10.1.1.97.3740",
            "keyword": "",
            "title": "Improved Lower Bounds for Constant GC-Content DNA Codes"
        },
        {
            "abstract": "Abstract\u2014Computer-aided design (CAD) tools are now making it possible to automate many aspects of the design process. This has mainly been made possible by the use of effective and efficient algorithms and corresponding software structures. The very large scale integration (VLSI) design process is extremely complex, and even after breaking the entire process into several conceptually easier steps, it has been shown that each step is still computationally hard. To researchers, the goal of understanding the fundamental structure of the problem is often as important as producing a solution of immediate applicability. Despite this emphasis, it turns out that results that might first appear to be only of theoretical value are sometimes of profound relevance to practical problems. VLSI CAD is a dynamic area where problem definitions are continually changing due to complexity, technology and design methodology. In this paper, we focus on several of the fundamental CAD abstractions, models, concepts and algorithms that have had a significant impact on this field. This material should be of great value to researchers interested in entering these areas of research, since it will allow them to quickly focus on much of the key material in our field. We emphasize algorithms in the area of test, physical design, logic synthesis, and formal verification. These algorithms are responsible for the effectiveness and efficiency of a variety of CAD tools. Furthermore, a number of these algorithms have found applications in many other domains. Index Terms\u2014Algorithms, computer-aided design, computational complexity, formal verification, logic synthesis, physical design, test. I.",
            "group": 2231,
            "name": "10.1.1.97.3965",
            "keyword": "",
            "title": "Fundamental CAD algorithms"
        },
        {
            "abstract": "Abstract A new stochastic learning algorithm using Gaussian white noise sequence, referred to as Subconscious Noise Reaction (SNR), is proposed for a class of discrete-time neural networks with time-dependent connection weights. Unlike the back-propagation-through-time (BTT) algorithm, SNR does not require the synchronous transmission of information backward along connection weights, while it uses only ubiquitous noise and local signals, which are correlated against a single performance functional, to achieve simple sequential (chronologically ordered) updating of connection weights. The algorithm is derived and analyzed on the basis of a functional derivative formulation of the gradient descent method in conjunction with stochastic ~ensit~ivity analysis techniques using the ~ariationa~l approach. 1.",
            "group": 2232,
            "name": "10.1.1.97.4472",
            "keyword": "",
            "title": "A NEW STOCHASTIC LEARNING ALGORITHM FOR NEURAL NETWORKS"
        },
        {
            "abstract": "Current technology provides a means to obtain sampled data that digitally describes three-dimensional surfaces and objects. Three-dimensional digitizing cameras can be used to obtain sampled data that maps the surface of three dimensional figures and models. Data obtained from such sources enable accurate renderings of the original surface. However, the digitizing process often provides much more data than is needed to accurately recreate the surface or object. In order to use such data in real-time visual simulators, a significant reduction in the data needed to accurately render the sampled surfaces is required. The techniques presented were developed to drastically reduce the number of data points required to depict an object without sacrificing the detail and accuracy inherent in the digitizing process. * Contact author. 1.",
            "group": 2233,
            "name": "10.1.1.97.5130",
            "keyword": "",
            "title": "Simplification of objects rendered by Polygonal Approximations"
        },
        {
            "abstract": "Scheduling problems, e.g., a job-shop scheduling, are classical NP-hard problems. In the paper a two-level adaptation method is proposed to solve the scheduling problem in a dynamically changing and uncertain environment. It is applied to the heterarchical multi-agent architecture developed by Valckenaers et al. Their work is improved by applying machine learning techniques, such as: neurodynamic programming (reinforcement learning + neural networks) and simulated annealing. The paper focuses on manufacturing control, however, a lot of these ideas can be applied to other kinds of decision-making, as well. ",
            "group": 2234,
            "name": "10.1.1.97.6573",
            "keyword": "",
            "title": " Improving Multi-Agent Based Scheduling by Neurodynamic Programming"
        },
        {
            "abstract": "The insecticide imidacloprid and structurally related neonicotinoids act selectively on insect nicotinic acetylcholine receptors (nAChRs). To investigate the mechanism of neonicotinoid selectivity, we have examined the effects of mutations to basic amino acid residues in loop D of the nAChR acetylcholine (ACh) binding site on the interactions with imidacloprid. The receptors investigated are the recombinant chicken \ufffd4\ufffd2 nAChR and Drosophila melanogaster D\ufffd2/chicken \ufffd2 hybrid nAChR expressed in Xenopus laevis oocytes. Although mutations of Thr77 in loop D of the \ufffd2 subunit resulted in a barely detectable effect on the imidacloprid concentration-response curve for the \ufffd4\ufffd2 nAChR, T77R;E79V double mutations shifted the curve dramatically to higher affinity binding of imidacloprid. Likewise, T77K;E79R and T77N;E79R double mutations in the D\ufffd2\ufffd2 nAChR also resulted in a shift to a higher affinity for imidaclo-Nicotinic acetylcholine receptors (nAChRs) play a central role in rapid cholinergic synaptic transmission (Sattelle, 1980; Sattelle and Breer, 1990) and are important targets of insecticides (Gepner et al., 1978; Matsuda et al., 2001, 2005). Of the insecticides acting on insect nAChRs, imidacloprid and its analogs (Fig. 1), referred to as neonicotinoids, are used worldwide as agrochemicals (Matsuda et al., 2001, 2005; Tomizawa and Casida, 2005). In addition, neonicotinoids are employed in animal health as flea repellants",
            "group": 2235,
            "name": "10.1.1.97.6578",
            "keyword": "",
            "title": "Role in the Selectivity of Neonicotinoids of Insect-Specific Basic Residues in Loop D of the Nicotinic Acetylcholine Receptor Agonist Binding Site"
        },
        {
            "abstract": "We extend in this paper the concept of the P-admissible floorplan representation to that of the P*-admissible one. A P*-admissible representation can model the most general floorplans. Each of the currently existing P*-admissible representations, SP, BSG, and TCG, has its strengths as well as weaknesses. We show the equivalence of the two most promising P*-admissible representations, TCG and SP, and integrate TCG with a packing sequence (part of SP) into a new representation, called TCG-S. TCG-S combines the advantages of SP and TCG and at the same time eliminates their disadvantages. With the property of SP, faster packing and perturbation schemes are possible. Inherited nice properties from TCG, the geometric relations among modules are transparent to TCG-S (implying faster convergence to a desired solution), placement with position constraints becomes much easier, and incremental update for cost evaluation can be realized. These nice properties make TCG-S a superior representation which exhibits an elegant solution structure to facilitate the search for a desired floorplan/placement. Extensive experiments show that TCG-S results in the best area utilization, wirelength optimization, convergence speed, and stability among existing works and is very flexible in handling placement with special constraints. 1",
            "group": 2236,
            "name": "10.1.1.97.6801",
            "keyword": "",
            "title": "TCG-S: Orthogonal coupling of P*-admissible representations for general floorplans"
        },
        {
            "abstract": "Software Multiclass discovery in array data",
            "group": 2237,
            "name": "10.1.1.97.6805",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "d esign for accurately estimating spectral pectral reflectance eflectance of of art rt paintings aintings",
            "group": 2238,
            "name": "10.1.1.97.7707",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The landscapes currently studied in ecology are either \u201cdiscontinuous \u201d (category-based or patchbased), as in the case of mosaics of agricultural units, or of more \u201ccontinuous \u201d type (raster lattices), as used for representing elevation or other ecological gradients. The main landscape models either involve explicit processes or are neutral, recreating spatial patterns in the absence of studied processes (using statistical rules). This article presents neutral models suitable for the creation and handling of patchy landscapes. These models (Patchy Landscape Neutral Models) adapt the Gibbs process already used successfully in forestry and biology to describe the local interactions between landscape units. These interactions can be either ecological, if justified for example by natural mechanisms of dispersal (plant species dynamics) or crop successions in anthropized landscapes, or statistical (geometrical). We define a global &quot;cost function &quot; representative of the landscape to be simulated by summing the &quot;pair function &quot; that expresses the interactions between units for the whole landscape. This generic approach makes it possible to reconstruct different kinds of patchy landscape compositions (land cover) and opens the way to studying changes in landscape configuration (unit arrangements) as well as an analytical description of landscapes. Key words: forestry; landscape ecology; Gibbs process; ecosystem modelling; categorical map",
            "group": 2239,
            "name": "10.1.1.97.8908",
            "keyword": "",
            "title": "Correspondence to:"
        },
        {
            "abstract": "The game of Go is one of the games that still withstand classical Artificial Intelligence approaches. Hence, it is a good testbed for new AI methods. Amongst them, Monte-Carlo led to promising results. This method consists of building an evaluation function by averaging the outcome of several randomized games. The paper introduces a new strategy, which we call Objective Monte-Carlo, to improve this evaluation. Objective Monte-Carlo is composed of two parts. The first one is a move-selection strategy that adjusts the amount of exploration and exploitation automatically. We show experimentally that it outperforms the two classical strategies previously proposed for Monte-Carlo Go: Simulated Annealing and Progressive Pruning. The second part of our algorithm is a new backpropagation strategy. We show that it gives better results than Minimax in this context. Finally we discuss the extension of this method to other problems. 1",
            "group": 2240,
            "name": "10.1.1.97.8924",
            "keyword": "",
            "title": "Monte-Carlo Strategies for Computer Go"
        },
        {
            "abstract": "Computers that \u201cprogram themselves\u201d; science fact or fiction? Genetic Programming uses novel optimisation techniques to \u201cevolve \u201d simple programs; mimicking the way humans construct programs by progressively re-writing them. Trial programs are repeatedly modified in the search for \u201cbetter/fitter \u201d solutions. The underlying basis is Genetic Algorithms (GAs). Genetic Algorithms, pioneered by Holland [Hol92], Goldberg [Gol89] and others, are evolutionary search techniques inspired by natural selection (i.e survival of the fittest). GAs work with a \u201cpopulation \u201d of trial solutions to a problem, frequently encoded as strings, and repeatedly select the \u201cfitter \u201d solutions, attempting to evolve better ones. The power of GAs is being demonstrated for an increasing range of applications; financial, imaging, VLSI circuit layout, gas pipeline control and production scheduling [Dav91]. But one of the most intriguing uses of GAs- driven by Koza [Koz92]- is automatic program generation. Genetic Programming applies GAs to a \u201cpopulation \u201d of programs- typically encoded as tree-structures. Trial programs are evaluated against a \u201cfitness function \u201d and the best solutions selected for modification and re-evaluation. This modification-evaluation cycle is repeated",
            "group": 2241,
            "name": "10.1.1.97.9405",
            "keyword": "Machine LearningGenetic AlgorithmsGenetic Programming. 1 Contents",
            "title": "Genetic programming { computers using \\natural selection&quot; to generate programs. Research Note RN/95/76"
        },
        {
            "abstract": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.",
            "group": 2242,
            "name": "10.1.1.98.349",
            "keyword": "General TermsAlgorithms Additional Key Words and PhrasesCluster analysisclustering applicationsexploratory data analysis",
            "title": "AND"
        },
        {
            "abstract": "A formalism for modelling the dynamics of genetic algorithms using methods from statistical physics, originally due to Pr\u00fcgel-Bennett and Shapiro, is extended to ranking selection, a form of selection commonly used in the genetic algorithm com-munity. The extension allows a reduction in the number of macroscopic variables required to model the mean behaviour of the genetic algorithm. This reduction allows a more qualitative understanding of the dynamics to be developed without sacrificing quantitative accuracy. The work is extended beyond modelling the dynamics of the genetic algorithm. A caricature of an optimisation problem with many local minima is considered \u2014 the basin with a barrier problem. The first passage time \u2014 the time required to escape the local minima to the global minimum \u2014 is calculated and insights gained as to how the genetic algorithm is searching the landscape. The interaction of the various genetic algorithm operators and how these interactions give rise to optimal parameters values is studied. i",
            "group": 2243,
            "name": "10.1.1.98.2080",
            "keyword": "1.1 The Genetic Algorithm......................... 1",
            "title": "Modelling Genetic Algorithms and Evolving Populations"
        },
        {
            "abstract": "Abstract. Several activities related to semantically annotated resources can be enabled by a notion of similarity, spanning from clustering to retrieval, matchmaking and other forms of inductive reasoning. We propose the definition of a family of semi-distances over the set of objects in a knowledge base which can be used in these activities. In the line of works on distance-induction on clausal spaces, the family is parameterized on a committee of concepts expressed with clauses. Hence, we also present a method based on the idea of simulated annealing to be used to optimize the choice of the best concept committee. 1",
            "group": 2244,
            "name": "10.1.1.98.2327",
            "keyword": "",
            "title": "Induction of Optimal Semantic Semi-distances for Clausal Knowledge Bases"
        },
        {
            "abstract": "Abstract: The MAXimum propositional SATisfiability problem (MAXSAT) is a well known NP-hard optimization problem with many theoretical and practical applications in artificial intelligence and mathematical logic. Heuristic local search algorithms are widely recognized as the most effective approaches used to solve them. However, their performance depends both on their complexity and their tuning parameters which are controlled experimentally and remain a difficult task. Extremal Optimization (EO) is one of the simplest heuristic methods with only one free parameter, which has proved competitive with the more elaborate general-purpose method on graph partitioning and coloring. It is inspired by the dynamics of physical systems with emergent complexity and their ability to self-organize to reach an optimal adaptation state. In this paper, we propose an extremal optimization procedure for MAXSAT and consider its effectiveness by computational experiments on a benchmark of random instances. Comparative tests showed that this procedure improves significantly previous results obtained on the same benchmark with other modern local search methods like WSAT, simulated annealing and Tabu Search (TS).",
            "group": 2245,
            "name": "10.1.1.98.2342",
            "keyword": "Constraint satisfactionMAXSATheuristic local searchextremal optimization",
            "title": "Solving the Maximum Satisfiability Problem Using an Evolutionary Local Search Algorithm"
        },
        {
            "abstract": "In memory of my father",
            "group": 2246,
            "name": "10.1.1.98.2542",
            "keyword": "VLSI Design",
            "title": "vi"
        },
        {
            "abstract": "Preface: I would like to thank my tutors Fredrik Manne and Noureddine Bhoumala for support and help. I would also like to thank my girlfriend Line Carlsen for her support and patience, and Per Otto Hjertenes for his assistance in proofreading my texts. This text is about the Travelling Salesman Problem. An observant reader would already now have discovered that I am using travelling instead of traveling. This spelling difference comes from the subtle differences between U.K English and U.S English. If you are interested in more facts about the name travelling and traveling confusion I would like to refer to the following web site [29], or to [30] where it is also mentioned. The picture on the front page is by Applegate, Bixby, Chv\u00e1tal, and Cook, and is collected from [31]. The picture shows the optimal tour of the tsp instance d15112 from",
            "group": 2247,
            "name": "10.1.1.98.3355",
            "keyword": "",
            "title": "A Multilevel Scheme for the Travelling Salesman Problem"
        },
        {
            "abstract": "Presented here is a fast method that combines curve matching techniques with a surface matching algorithm to estimate the positioning and respective matching error for the joining of three-dimensional fragmented objects. Furthermore, this paper describes how multiple joints are evaluated and how the broken artefacts are clustered and transformed to form potential solutions of the assemblage problem.",
            "group": 2248,
            "name": "10.1.1.98.3856",
            "keyword": "curve matchingdepth bufferrange mapsoptimisation methodssurface matching",
            "title": "A.: On the automatic assemblage of arbitrary broken solid artefacts"
        },
        {
            "abstract": "the plausibility of using skin texture as virtual markers in the human motion analysis context, a",
            "group": 2249,
            "name": "10.1.1.98.3873",
            "keyword": "Image RegistrationSkin TextureMutual InformationSimulated Annealing",
            "title": "2D study"
        },
        {
            "abstract": "This paper is a complete survey of flowshop-scheduling problems and contributions from early works of Johnson of 1954 to recent approaches of metaheuristics of 2004. It mainly considers a flowshop problem with a makespan criterion and it surveys some exact methods (for small size problems), constructive heuristics and developed improving metaheuristic and evolutionary approaches as well as some well-known properties and rules for this problem. Each part has a brief literature review of the contributions and a glimpse of that approach before discussing the implementation for a flowshop problem. Moreover, in the first section, a complete literature review of flowshop-related scheduling problems with different assumptions as well as contributions in solving these other aspects is considered. This paper can be seen as a reference to past contributions (particularly in n/m/ p/c max or equivalently F/prmu/c max) for future research needs of improving and developing better approaches to flowshop-related scheduling problems.",
            "group": 2250,
            "name": "10.1.1.98.3992",
            "keyword": "FlowshopMakespanDynamic programmingHeuristicsMetaheuristicsSimulated annealingGenetic algorithmTabu searchAnt colony",
            "title": "Flowshop-scheduling problems with makespan criterion: a review"
        },
        {
            "abstract": "Abstract This paper describes a semi-automatic system for the reconstruction of archaeological finds from their fragments. Virtual Archaeologist is a system that uses computer graphics to calculate a measure of complementary matching between scanned data and employs optimization algorithms in order to estimate the correct relative pose between fragments and cluster those fragments that belong to the same entity.",
            "group": 2251,
            "name": "10.1.1.98.4520",
            "keyword": "",
            "title": "Virtual Archaeologist: Assembling the past Georgios Papaioannou IEEE Member, Evaggelia-Aggeliki Karabassi IEEE Member,"
        },
        {
            "abstract": "E-commerce has transformed the way firms develop their pricing strategies, producing shift away from fixed pricing to dynamic pricing. In this paper, we use two different Estimation of distribution algorithms (EDAs), a Genetic Algorithm (GA) and a Simulated Annealing (SA) algorithm for solving two different dynamic pricing models. Promising results were obtained for an EDA confirming its suitability for resource management in the proposed model. Our analysis gives interesting insights into the application of population based optimization techniques for dynamic pricing. Categories and Subject Descriptors",
            "group": 2252,
            "name": "10.1.1.98.4665",
            "keyword": "General Terms AlgorithmsManagementPerformanceDesignEconomics Keywords Estimation of Distribution AlgorithmsDynamic PricingEvolutionary ComputationResource Management",
            "title": "ABSTRACT An Application of EDA and GA to Dynamic Pricing"
        },
        {
            "abstract": "In this paper, a novel algorithm of genetic ant colony optimization (GACO) is proposed for traveling salesman problem (TSP). TSP is to minimize the cost of travel of a salesman in visiting all the cities in a given set, and return to the starting city. Basically, the proposed algorithm combines ant colony optimization (ACO) with genetic algorithm (GA) and can explore and exploit search spaces. It has both the advantage of ACO, the ability to find feasible solutions and to avoid premature convergence, and that of GA, the ability to avoid being trapped in local optima. In this paper, variant test problems are drawn from randomly generated data and the most well known TSP problems chosen from TSPLIB. Simulation results are reported, and the proposed algorithm seems to have admirable performance.",
            "group": 2253,
            "name": "10.1.1.98.5249",
            "keyword": "Genetic AlgorithmAnt colony optimizationTraveling Salesman Problem",
            "title": "A Novel algorithm of Genetic Ant Colony Optimization (GACO) for Traveling Salesman Problem"
        },
        {
            "abstract": "As a typical combinatorial optimization problem, the Traveling Salesman Problem (TSP) has attracted extensive research interest. In this paper, we develop a Self-Organizing Map (SOM) with a novel learning rule. It is called the Integrated SOM (ISOM) since its learning rule integrates the three learning mechanisms in the SOM literature. Within a single learning step, the excited neuron is first dragged towards the input city, then pushed to the convex hull of the TSP, and finally drawn towards the middle point of its two neighboring neurons. A genetic algorithm is successfully specified to determine the elaborate coordination among the three learning mechanisms as well as the suitable parameter setting. The evolved ISOM (eISOM) is examined on three sets of TSPs to demonstrate its power and efficiency. The computation complexity of the eISOM is quadratic, which is comparable to other SOM-like neural networks. Moreover, the eISOM can generate more accurate solutions than several typical approaches for TSPs including the SOM developed by Budinich, the expanding SOM, the convex elastic net, and the FLEXMAP algorithm. Though its solution accuracy is not yet comparable to some sophisticated heuristics, the eISOM is one of the most accurate neural networks for the TSP.",
            "group": 2254,
            "name": "10.1.1.98.6426",
            "keyword": "Traveling salesman problemneural networksself-organizing mapneural-evolutionary sys- temgenetic algorithmsconvex hull 1",
            "title": "An efficient self-organizing map designed by genetic algorithms for the traveling salesman problem"
        },
        {
            "abstract": "This paper proposes a gradient ascent learning algorithm of the Hopfield neural networks for solving fixed linear crossing number problem. The fixed linear crossing number problem is an important problem in printed circuit board layout, VLSI circuit routing, and automated graph drawing. The objective of this problem which is shown to be NP-hard is to embed the edges so that the total number of crossings is minimized. The proposed algorithm uses the Hopfield neural network to get a near-minimal edge crossings, and increases the energy by modifying weights in a gradient ascent direction to help the network escape from the state of the near-minimal edge crossings to the state of the minimal edge crossings or better one. The proposed algorithm is tested on complete graph. We compare the proposed learning algorithm with some other existing algorithms. The experimental results indicate that the proposed algorithm could yield optimal or near-optimal solutions and outperforms the other algorithms. Key words:",
            "group": 2255,
            "name": "10.1.1.98.6669",
            "keyword": "Fixed linear crossing number problemGraph layoutNPcomplete problemHopfield neural networkGradient ascent",
            "title": "Manuscript revised November 25, 2006."
        },
        {
            "abstract": "Abstract\u2014Bayesian neural networks were used to model the relationship between input parameters, Democracy, Allies, Contingency, Distance, Capability, Dependency and Major Power, and the output parameter which is either peace or conflict. The automatic relevance determination was used to rank the importance of input variables. Control theory approach was used to identify input variables that would give a peaceful outcome. It was found that using all four controllable variables Democracy, Allies, Capability and Dependency; or using only Dependency or only Capabilities avoids all the predicted conflicts. I.",
            "group": 2256,
            "name": "10.1.1.98.7399",
            "keyword": "",
            "title": "Modeling and Controlling Interstate Conflict"
        },
        {
            "abstract": "Abstract: This paper proposes two ant colony system (ACS) based approaches, called stepwise-ACS (SACS) and cheapest-insertion-stepwise-ACS (CISACS), to solve the airexpress courier\u2019s routing problem. The courier visits N predetermined delivery points and M pickup requests during the en route delivery. The SACS performs an ACS to obtain the initial tour for the N delivery points and then to find the shortest Hamiltonian path, in a stepwise manner, when the M requests arrive. The CISACS, following the SACS, incorporates the cheapest insertion into the SACS as new requests become known. Experiments with various pickup emergence patterns that characterize the real-world circumstances are tested for the proposed algorithms. The computational results, in terms of total traveled distance, are compared with the baseline results by a cheapest insertion (CI) heuristic. The results show that both SACS and CISACS perform better than the CI heuristic and that CISACS yields the lowest traveled distance. Key Words: air-express courier, pickup and delivery, ant colony system, cheapest insertion 1.",
            "group": 2257,
            "name": "10.1.1.98.7478",
            "keyword": "",
            "title": "Traffic Management"
        },
        {
            "abstract": "Abstract\u2014The paper attempts to solve the generalized \u201cAssignment problem \u201d through genetic algorithm and simulated annealing. The generalized assignment problem is basically the \u201cN men- N jobs \u201d problem where a single job can be assigned to only one person in such a way that the overall cost of assignment is minimized. While solving this problem through genetic algorithm (GA), a unique encoding scheme is used together with Partially Matched Crossover (PMX). The population size can also be varied in each iteration. In simulated annealing (SA) method, an exponential cooling schedule based on Newtonian cooling process is employed and experimentation is done on choosing the number of iterations (m) at each step. The source codes for the above have been developed in C language and compiled in GCC. Several test cases have been taken and the results obtained from both the methods have been tabulated and compared against the results obtained by coding in AMPL. Index Terms\u2014Assignment problem, Genetic Algorithm, Newtonian cooling schedule, Partially Matched Crossover (PMX),",
            "group": 2258,
            "name": "10.1.1.98.8085",
            "keyword": "Simulated Annealing",
            "title": "Solving the Assignment problem using Genetic Algorithm and Simulated Annealing"
        },
        {
            "abstract": "Abstract. RND (Radio Network Design) is an important problem in mobile telecommunications (for example in mobile/cellular telephony), being also relevant in the rising area of sensor networks. This problem consists in covering a certain geographical area by using the smallest number of radio antennas achieving the biggest cover rate. To date, several radio antenna models have been used: square coverage antennas, omnidirectional antennas that cover a circular area, etc. In this work we use omnidirectional antennas. On the other hand, RND is an NP-hard problem; therefore its solution by means of evolutionary algorithms is appropriate. In this work we study different evolutionary approaches to tackle this problem. PBIL (Population-Based Incremental Learning) is based on genetic algorithms and competitive learning (typical in neural networks). DE (Differential Evolution) is a very simple population-based stochastic function minimizer used in a wide range of optimization problems, including multi-objective optimization. SA (Simulated Annealing) is a classic trajectory descent optimization technique. Finally, CHC is a particular class of evolutionary algorithm which does not use mutation and relies instead on incest prevention and disruptive crossover. Due to the complexity of such a large analysis including so many techniques, we have used not only sequential algorithms, but also grid computing with BOINC in order to execute thousands of experiments in only several days using around 100 computers.",
            "group": 2259,
            "name": "10.1.1.98.8520",
            "keyword": "Omnidirectional BTSRNDPBILDESACHC",
            "title": "Using Omnidirectional BTS and Different Evolutionary Approaches to Solve the RND Problem"
        },
        {
            "abstract": "We present an algorithm for use in an interactive music system that automatically generates music playlists that fit the music preferences given by a user. To this end, we introduce a formal model, define the problem of automatic playlist generation (APG) and indicate its NP-hardness. We use a local search (LS) procedure based on simulated annealing (SA) to solve the APG problem. In order to employ this LS procedure, we introduce an optimization variant of the APG problem, which includes the definition of penalty functions and a neighborhood structure. To improve upon the performance of the standard SA algorithm, we incorporated three heuristics referred to as song domain reduction, partial constraint voting, and two-level neighborhood structure. In tests, LS performed better than a constraint satisfaction (CS) solution in terms of run time, scalability and playlist quality.",
            "group": 2260,
            "name": "10.1.1.98.8790",
            "keyword": "local searchsimulated annealingmusic",
            "title": "Fast Generation of Optimal Music Playlists using Local Search"
        },
        {
            "abstract": "Summary: SARGE is a tool for creating, visualizing and manipulating a putative genetic network from time series microarray data. The tool assigns potential edges through time-lagged correlation, incorporates a clustering mechanism, an interactive visual graph representation and employs simulated annealing for network optimization. Availability: The application is available as a.jar file from",
            "group": 2261,
            "name": "10.1.1.99.654",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract. In computer-aided logic design, partitioning is the task of clustering circuit elements into groups so that a given objective function is optimized with respect to a set of design constraints. In this paper we present a genetic algorithm for the circuit partitioning problem. The objective of the proposed algorithm is not only to balance the size of the two portions, but also to evenly distribute the connections among them. The implemented algorithm has been compared to a simulated annealing partitioning algorithm. Experiments show that the execution time of the genetic algorithm is lower, the results being comparable to that obtained by simulated annealing. 1.",
            "group": 2262,
            "name": "10.1.1.99.1017",
            "keyword": "",
            "title": "Genetic Algorithm for Circuit Partitioning"
        },
        {
            "abstract": "VLSI standard cell placement is the process of arranging circuit components (modules) on a silicon layout. The cell placement problem is a proven NP hard combinatorial optimization problem. The complexity of this problem increases when multiple optimization objectives are considered simultaneously. In this paper, a novel technique is presented to address this hard problem, while optimizing multiple objectives. A major difficulty with such multi-objective combinatorial optimization problems is the existence of a very large solution search space, one of which is the desired optimal solution. Simulated Evolution (SE) a general iterative heuristic is used to traverse the large search space, while fuzzy logic is resorted to assist in multi-criteria decision making and overcome the imprecise nature of design information at placement stage. New fuzzy aggregation functions are proposed. SE is hybridized with force directed algorithm to speed-up the search. The proposed schemes are compared with previously presented SE based heuristics. The implementations exhibit considerable improvement in terms of both solution quality and runtime. 1.",
            "group": 2263,
            "name": "10.1.1.99.2214",
            "keyword": "",
            "title": "FAST FUZZY FORCE-DIRECTED/SIMULATED EVOLUTION METAHEURISTIC FOR MULTIOBJECTIVE VLSI CELL PLACEMENT"
        },
        {
            "abstract": "Today, with digitally stored information available in abundance, even for many minor languages, this information must by some means be filtered and extracted in order to avoid drowning in it. Automatic summarization is one such technique, where a computer summarizes a longer text to a shorter non-rendundant form. Apart from the major languages of the world there are a lot of languages for which large bodies of data aimed at language technology research to a high degree are lacking. There might also not be resources available to develop such bodies of data, since it is usually time consuming and requires substantial manual labor, hence being expensive. Nevertheless, there will still be a need for automatic text summarization for these languages in order to subdue this constantly increasing amount of electronically produced text. This thesis thus sets the focus on automatic summarization of text and the evaluation of summaries using as few human resources as possible. The resources that are used should to as high extent as possible be already existing, not specifically aimed at summarization or evaluation of summaries and, preferably, created as part of natural literary processes.",
            "group": 2264,
            "name": "10.1.1.99.2618",
            "keyword": "",
            "title": "Resource Lean and Portable Automatic Text Summarization"
        },
        {
            "abstract": "This paper presents a systematic model, two-phase optimization algorithms (TPOA), for Mastermind. TPOA is not only able to efficiently obtain approximate results but also effectively discover results that are getting closer to the optima. This systematic approach could be regarded as a general improver for heuristics. That is, given a constructive heuristic, TPOA has a higher chance to obtain results better than those obtained by the heuristic. Moreover, it sometimes can achieve optimal results that are difficult to find by the given heuristic. Experimental results show that (i) TPOA with parameter setting (k, d) 5 (1, 1) is able to obtain the optimal result for the game in the worst case, where k is the branching factor and d is the exploration depth of the search space. (ii) Using a simple heuristic, TPOA achieves the optimal result for the game in the expected case with (k, d) 5 (180, 2). This is the first approximate approach to achieve the optimal result in the expected case. 1.",
            "group": 2265,
            "name": "10.1.1.99.4112",
            "keyword": "",
            "title": "A Two-Phase Optimization Algorithm For Mastermind"
        },
        {
            "abstract": "Generalized hill climbing (GHC) algorithms provide a framework for using local search algorithms to address intractable discrete optimization problems. Many well-known local search algorithms can be formulated as GHC algorithms, including simulated annealing, threshold accepting, Monte Carlo search, and pure local search (among others). This dissertation develops a mathematical framework for simultaneously addressing a set of related discrete optimization problems using GHC algorithms. The resulting algorithms, termed simultaneous generalized hill climbing (SGHC) algorithms, can be applied to a wide variety of sets of related discrete optimization problems. The SGHC algorithm probabilistically moves between these discrete optimization problems according to a problem generation probability function. This dissertation establishes that the problem generation probability function is a stochastic process that satisfies the Markov property. Therefore, given a SGHC algorithm, movement between these discrete optimization problems can be",
            "group": 2266,
            "name": "10.1.1.99.4397",
            "keyword": "Local SearchGeneralized Hill Climbing AlgorithmsSimulated AnnealingMarkov ChainsErgodicityTraveling Salesman ProblemManufacturing Applications",
            "title": "Simultaneous Generalized Hill Climbing Algorithms for Addressing Sets of Discrete Optimization Problems"
        },
        {
            "abstract": "Generalized hill climbing (GHC) algorithms provide a framework for using local search algorithms to address intractable discrete optimization problems. Many well-known local search algorithms can be formulated as GHC algorithms, including simulated annealing, threshold accepting, Monte Carlo search, and pure local search (among others). This dissertation develops a mathematical framework for simultaneously addressing a set of related discrete optimization problems using GHC algorithms. The resulting algorithms, termed simultaneous generalized hill climbing (SGHC) algorithms, can be applied to a wide variety of sets of related discrete optimization problems. The SGHC algorithm probabilistically moves between these discrete optimization problems according to a problem generation probability function. This dissertation establishes that the problem generation probability function is a stochastic process that satisfies the Markov property. Therefore, given a SGHC algorithm, movement between these discrete optimization problems can be",
            "group": 2267,
            "name": "10.1.1.99.4397",
            "keyword": "Local SearchGeneralized Hill Climbing AlgorithmsSimulated AnnealingMarkov ChainsErgodicityTraveling Salesman ProblemManufacturing Applications",
            "title": "Simultaneous Generalized Hill Climbing Algorithms for Addressing Sets of Discrete Optimization Problems"
        },
        {
            "abstract": "A number of quite successful SFM solutions proposed focus on producing a sparse or dense set of features of the scene such as 3D points or line segments. While these representations are sometimes adequate, for many applications a more compact model is required usually in the form of solid objects or surfaces. Triangular meshes are an appealing representation because of their simplicity and the fact that they can be readily used by graphics applications. This work attempts to shed light on the problem of fitting a triangular mesh to visual data. We briefly review some existing work on the problem and propose our own mesh optimisation scheme. The method proposed, starts from a set of 3D points that are thought to lie on the scene surface and a set of calibrated images of the scene, and produces a piecewise planar model in the form of a general triangular mesh that is smooth and visually consistent with the images. The algorithm is robust to a significant amount of outliers in the initial point set. On some occasions where the scene is well represented by a piecewise planar model, the method is able to simplify and optimise the model considerably.",
            "group": 2268,
            "name": "10.1.1.99.5418",
            "keyword": "Contents",
            "title": "Mesh Optimisation for Image-Based Surface Reconstruction"
        },
        {
            "abstract": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local opti-mal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization lit-erature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the param-eters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and ma-chine learning problems. TRUST-TECH explores the dynamic and geometric char-acteristics of stability boundaries of a nonlinear dynamical system corresponding to the nonlinear function of interest. Basically, our method coalesces the advantages of the traditional local optimizers with that of the dynamic and geometric charac-teristics of the stability regions of the corresponding nonlinear dynamical system.",
            "group": 2269,
            "name": "10.1.1.99.5430",
            "keyword": "",
            "title": "TRUST-TECH BASED METHODS FOR OPTIMIZATION AND LEARNING"
        },
        {
            "abstract": "Abstract \u2014 A central issue in distributed systems engineering is enabling agents with only a local view of their environment to take actions that advance global system objectives. One example of this tension is that individual agents may take actions that consume system resources even when they are not advancing the overall system objectives. Thus, paradoxically, system performance can sometimes improve if individual agents reduce their activity. Agents in such systems need a way to modulate their individual behavior in the light of the system\u2019s state, preferably in a way that does not require centralized control. We illustrate the problem of hyperactive agents in three application domains. We describe a simple, decentralized scheme, inspired by insect pheromones, that enables individual agents to adjust their level of activity as the system operates, and extend this mechanism to provide a general approach for dealing with approaching deadlines. Then we demonstrate the effectiveness of these mechanisms in the example domains.",
            "group": 2270,
            "name": "10.1.1.99.6609",
            "keyword": "Index Terms\u2014Cooperative SystemsLearningComplexity TheoryResource ManagementClustering Methods A Pheromone Learning for Self-Organizing",
            "title": "Agents"
        },
        {
            "abstract": "Abstract\u2014The main goal of this work is to propose a way for combined use of two nontraditional algorithms by solving topological problems on telecommunications concentrator networks. The algorithms suggested are the Simulated Annealing algorithm and the Genetic Algorithm. The Algorithm of Simulated Annealing unifies the well known local search algorithms. In addition- Simulated Annealing allows acceptation of moves in the search space witch lead to decisions with higher cost in order to attempt to overcome any local minima obtained. The Genetic Algorithm is a heuristic approach witch is being used in wide areas of optimization works. In the last years this approach is also widely implemented in Telecommunications Networks Planning. In order to solve less or more complex planning problem it is important to find the most appropriate parameters for initializing the function of the algorithm. Keywords\u2014Concentrator network, genetic algorithm, simulated annealing, UCPL. T",
            "group": 2271,
            "name": "10.1.1.99.6727",
            "keyword": "",
            "title": "Simulated Annealing and Genetic Algorithm in Telecommunications Network Planning"
        },
        {
            "abstract": "ABSTRACT Protein environments substantially influence the balance of molecular interactions that generate structural stability. Transmembrane helices exist in the relatively uniform low dielectric interstices of the lipid bilayer, largely devoid of water and with a very hydrophobic distribution of amino acid residues. Here, through an analysis of bacteriorhodopsin crystal structures and the transmembrane helix structure from M2 protein of influenza A, some helices are shown to be exceptionally uniform in hydrogen bond geometry, peptide plane tilt angle, and backbone torsion angles. Evidence from both the x-ray crystal structures and solid-state NMR structure suggests that the intramolecular backbone hydrogen bonds are shorter than their counterparts in water-soluble proteins. Moreover, the geometry is consistent with a dominance of electrostatic versus covalent contributions to these bonds. A comparison of structure as a function of resolution shows that as the structures become better characterized the helices become much more uniform, suggesting that there is a possibility that many more uniform helices will be observed, even among the moderate resolution membrane protein structures that are currently in the Protein Data Bank that do not show such features.",
            "group": 2272,
            "name": "10.1.1.99.6777",
            "keyword": "",
            "title": "Uniformity, Ideality, and Hydrogen Bonds in Transmembrane \ufffd-Helices"
        },
        {
            "abstract": "[14] I. Guyon et al., \u201cUnipen project of on-line data exchange and recognizer",
            "group": 2273,
            "name": "10.1.1.99.6923",
            "keyword": "",
            "title": "A Mean Field Annealing Approach to Robust Corner Detection"
        },
        {
            "abstract": "Some statistical methods which have been shown to have direct neural network analogs are surveyed here; we discuss sampling, optimization, and representation methods which make them feasible when applied in conjunction with, or in place of, neural networks. We present the foremost of these, the Gibbs sampler, both in its successful role as a convergence heuristic derived from statistical physics and under its probabilistic learning interpretation. We then review various manifestations of Gibbs sampling in Bayesian learning; its relation to \u201ctraditional \u201d simulated annealing; specializations and instances such as EM; and its application as a model construction technique for the Bayesian network formalism. Next, we examine the ramifications of recent advances in Markov chain Monte Carlo methods for learning by backpropagation. Finally, we consider how the Bayesian network formalism informs the causal reasoning interpretation of some neural networks, and how it prescribes optimizations for efficient random sampling in Bayesian learning applications.",
            "group": 2274,
            "name": "10.1.1.99.8365",
            "keyword": "",
            "title": "A Position Paper on Statistical Inference Techniques which Integrate Neural Network and Bayesian Network Models"
        },
        {
            "abstract": "Distributed problem solving by a multiagent system represents a promising approach to solving complex computational problems. However, many multiagent systems require certain degree of planning, coordination and negotiation to achieve the given goal. This paper presents a multiagent framework for tackling global optimization tasks inspired by diffusion in nature. The framework is designed for situations where agent communication must be kept to a minimal. Hence, complicated coordination and negotiation is not possible. Distributed agents in this framework share the common goal of finding the global optimal solution. They cooperate to achieve this common goal by sharing and updating a common belief that captures their estimation of the whereabouts of the optimal solution. To facilitate this, agents are naturally organized in families with a parent and its offsprings as members. This paper also presents an algorithm called Evolutionary Diffusion Optimization, which is implemented base on the proposed agent framework. Experimental results on some benchmark problems are presented together with performance comparison with a simulated annealing algorithm.",
            "group": 2275,
            "name": "10.1.1.99.8467",
            "keyword": "General Terms AlgorithmsExperimentationTheory Keywords Diffusion model",
            "title": "ABSTRACT Multiagent Diffusion and Distributed Optimization"
        },
        {
            "abstract": "Abstract. A swarm algorithm framework (SWAF), realized by agent-based modeling, is presented to solve numerical optimization problems. Each agent is a bare bones cognitive architecture, which learns knowledge by appropriately deploying a set of simple rules in fast and frugal heuristics. Two essential categories of rules, the generate-and-test and the problem-formulation rules, are implemented, and both of the macro rules by simple combination and subsymbolic deploying of multiple rules among them are also studied. Experimental results on benchmark problems are presented, and performance comparison between SWAF and other existing algorithms indicates that it is efficiently. 1",
            "group": 2276,
            "name": "10.1.1.99.8940",
            "keyword": "",
            "title": "SWAF: Swarm Algorithm Framework for Numerical Optimization. In"
        },
        {
            "abstract": "Abstract. This paper explores the use of the stochastic optimization technique of simulated annealing for map generalization. An algorithm is presented that performs operations of displacement, size exaggeration, deletion and size reduction of multiple map objects in order to resolve graphic conflict resulting from map scale reduction. It adopts a trial position approach in which each of n discrete polygonal objects is assigned k candidate trial positions that represent the original, displaced, size exaggerated, deleted and size reduced states of the object. This gives rise to a possible k n distinct map configurations; the expectation is that some of these configurations will contain reduced levels of graphic conflict. Finding the configuration with least conflict by means of an exhaustive search is, however, not practical for realistic values of n and k. We show that evaluation of a subset of the configurations, using simulated annealing, can result in effective resolution of graphic conflict. 1.",
            "group": 2277,
            "name": "10.1.1.99.9156",
            "keyword": "",
            "title": "Research Article Automated map generalization with multiple operators: a simulated annealing approach"
        },
        {
            "abstract": "Computational inference of the molecular logic for",
            "group": 2278,
            "name": "10.1.1.100.162",
            "keyword": "",
            "title": "synaptic connectivity in C. elegans"
        },
        {
            "abstract": "Abstract. The problem of reassembling an object from its parts or fragments has never been addressed with a unified computational approach, which depends on the pure geometric form of the parts and not application-specific features. We propose a method for the automatic reconstruction of a model based on the geometry of its parts, which may be computergenerated models or range-scanned models. The matching process can benefit from any other external constraint imposed by the specific application. Index Terms \u2013 Object reconstruction, complementary matching, depth buffer, virtual assemblage 1",
            "group": 2279,
            "name": "10.1.1.100.369",
            "keyword": "",
            "title": "Reconstruction of three-dimensional objects through matching of their parts"
        },
        {
            "abstract": "Traditionally, application software developers carry out their tests on their own local development databases. However, such local databases usually have only a small number of sample data and hence cannot simulate satisfactorily a live environment, especially in terms of performance and scalability testing. On the other hand, the idea of testing applications over live production databases is increasingly problematic in most situations primarily due to the fact that such use of live production databases has the potential to expose sensitive data to an unauthorized tester and to incorrectly update information in the underlying database. In this paper, we investigate techniques to generate mock databases for application software testing without revealing any confidential information from the live production databases. Specifically, we will design mechanisms to create the deterministic rule set R, non-deterministic rule set N R, and statistic data set S for a live production database. We will then build a security Analyzer which will process the triplet \u2329R, N R, S \u232a together with security requirements (security policy) and output a new triplet \u2329R \u2032 , N R \u2032 , S \u2032 \u232a. The security Analyzer will guarantee that no confidential information could be inferred from the new triplet \u2329R \u2032 , N R \u2032 , S \u2032 \u232a. The mock database generated from this new triplet can simulate the live environment for testing purpose, while maintaining the privacy of data in the original database.",
            "group": 2280,
            "name": "10.1.1.100.1434",
            "keyword": "Categories and Subject Descriptors D.2.5 [Software EngineeringTesting and Debugging\u2014 testing toolsH.1.1 [Models and PrinciplesSystems and Information Theory\u2014information theoryH.2.8 [Database ManagementDatabase Applications\u2014statistical databases General Terms Algorithmsperformancesecuritytheory",
            "title": "Privacy preserving database application testing"
        },
        {
            "abstract": "The development of new mission concepts requires efficient methodologies to analyze, design, and simulate the concepts before implementation. New mission concepts are increasingly considering the use of ion thrusters for fuel-efficient navigation in deep space. This paper presents parallel, evolutionary computing methods to design trajectories of spacecraft propelled by ion thrusters and assesses the trade-off between delivered payload mass and required flight time. The developed methods utilize a distributed computing environment in order to speed up computation, and use evolutionary algorithms to find globally Paretooptimal solutions. The methods are coupled with two main traditional trajectory design approaches, which are called direct and indirect. In the direct approach, thrust control is discretized in either arc time or arc length, and the resulting discrete thrust vectors are optimized. In the indirect approach, the thrust control problem is transformed into a co-state control problem and the initial values of the co-state vector are optimized. The developed methods are applied to two problems: 1) an orbit transfer around the Earth and 2) a transfer between two distance retrograde orbits around Europa, the icy Galilean moon closest to Jupiter. The optimal solutions found with the present methods are comparable to other",
            "group": 2281,
            "name": "10.1.1.100.1912",
            "keyword": "",
            "title": "Evolutionary Computing for Low-Thrust Navigation"
        },
        {
            "abstract": null,
            "group": 2282,
            "name": "10.1.1.100.2145",
            "keyword": "",
            "title": "Machine Learning in Design Using Genetic Engineering-Based Genetic Algorithms 1"
        },
        {
            "abstract": "Wide varieties of register file architectures \u2014 developed for embedded processors \u2014 have turned to aim at reducing the power dissipation and die size these years, by contrast with the traditional unified register file structures. This article presents a novel register allocation scheme for a clustered VLIW DSP, which is designed with distinctively banked register files in which port access is highly restricted. Whilst the organization of the register files is designed to decrease the power consumption by using fewer port connections, the cluster-based design makes register access across clusters an additional issue, and the switched-access nature of the register file demands further investigations into optimizing register assignment for increasing the instruction-level parallelism. We propose a heuristic algorithm, named ping-pong aware local favorable (PALF) register allocation, to obtain advantageous register allocation that is expected to better utilize irregular register file architectures. The results of experiments performed using a compiler based on the Open Research Compiler (ORC), showed significant performance improvement over the original ORC\u2019s approach, which is considered to be an optimized approach for common register file architectures. key words: register allocation; ping-pong register file; DSP; VLIW",
            "group": 2283,
            "name": "10.1.1.100.2929",
            "keyword": "PALFCOMPILER SUPPORTS FOR IRREGULAR REGISTER FILES 1",
            "title": "PALF: Compiler Supports for Irregular Register Files in Clustered VLIW DSP Processors"
        },
        {
            "abstract": "Abstract. Information processing operations in support of intelligence analysis are of two kinds. They may sift relevant data from a larger body, thus reducing its quantity, or sort that data, thus reducing its entropy. These two classes of operation typically alternate with one another, successively shrinking and organizing the available data to make it more accessible and understandable. We term the resulting construct, the \u201csemantic pyramid. \u201d We sketch the general structure of this construct, and illustrate two adjacent layers of it that we have implemented in the Ant CAF\u00c9. 1.",
            "group": 2284,
            "name": "10.1.1.100.3307",
            "keyword": "",
            "title": "Sift and Sort: Climbing the Semantic Pyramid"
        },
        {
            "abstract": "service",
            "group": 2285,
            "name": "10.1.1.100.3853",
            "keyword": "www.genome.org on February 72008- Published by Cold Spring Harbor Laboratory Press",
            "title": "Multilocus Genotypic Partitions That Predict"
        },
        {
            "abstract": "The module placement problem is to determine the coordinates of logic modules in a chip such that no two modules overlap and some cost (e.g., silicon area, interconnection length, etc) is optimized. To shorten connections between inputs and outputs and/or make related modules adjacent, it is desired to place some modules along the specific boundaries of a chip. To deal with such boundary constraints, we explore the feasibility conditions of a B*-tree with boundary constraints and develop a simulated an-nealing based algorithm using B*-trees. Unlike most previous works, our algorithm guarantees a feasible B*-tree with boundary constraints for each perturbation. Experimental results show that our algorithm can obtain smaller silicon area than the most recent work based on sequence pair. 1",
            "group": 2286,
            "name": "10.1.1.100.4041",
            "keyword": "",
            "title": "Module placement with boundary constraints using the sequence-pair representation"
        },
        {
            "abstract": "The field of reconfigurable hardware has seen quite a lot of research interest in the past few years. The intent of this field is to bridge the gap between fast but rigid pure hardware solutions and flexible but slow pure software solutions",
            "group": 2287,
            "name": "10.1.1.100.4186",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "In modern embedded systems including communication and multimedia applications, large frac-tion of power is consumed during memory access and data transfer. Thus, buses should be de-signed and optimized to consume reasonable power while delivering sufficient performance. In this paper, we address bus ordering problems for low-power application-specific systems. A heuristic algorithm is proposed to determine the order in a way that effective lateral component of capacitance is reduced, thereby reducing the power consumed by buses. Experimental results for various examples indicate that the average power saving from 30 % to 46.7 % depending on capacitance components can be obtained without any circuit overhead. 1",
            "group": 2288,
            "name": "10.1.1.100.4735",
            "keyword": "",
            "title": "Coupling-Driven Bus Design for Low-Power Application-Specific Systems"
        },
        {
            "abstract": "Abstract \u2014 Chen and Aihara recently proposed a chaotic simulated annealing approach to solving optimization problems. By adding a negative self-coupling to a network model proposed earlier by Aihara et al. and gradually removing this negative self-coupling, they used the transient chaos for searching and self-organizing, thereby achieving remarkable improvement over other neural-network approaches to optimization problems with or without simulated annealing. In this paper we suggest a new approach to chaotic simulated annealing with guaranteed convergence and minimization of the energy function by gradually reducing the time step in the Euler approximation of the differential equations that describe the continuous Hopfield neural network. This approach eliminates the need to carefully select other system parameters. We also generalize the convergence theorems of Chen and Aihara to arbitrarily increasing neuronal input\u2013output functions and to less restrictive and yet more compact forms.",
            "group": 2289,
            "name": "10.1.1.100.7402",
            "keyword": "Index Terms \u2014 Annealingchaosenergy functionHopfieldneural",
            "title": "On chaotic simulated annealing"
        },
        {
            "abstract": "Even the recognition of an individual whom we see every day is only possible as the result of an abstract idea of him formed by generalisation from his appearances in the past- James G. Frazer (in Malinowski & Bronislaw: Argonauts of the Western Pacific, 1922) The fundamental question studied in this thesis is how to evaluate and analyse supervised learning algorithms and classifiers. As a first step, we analyse current evaluation methods. Each method is described and categorised according to a number of properties. One conclusion of the analysis is that performance is often only measured in terms of accuracy, e.g., through cross-validation tests. However, some researchers have questioned the validity of using accuracy as the only performance metric. Also, the number of instances available for evaluation is usually very limited. In order to deal with these issues, measure functions have been suggested as a promising approach. However, a limitation of current measure functions is that they can only handle two-dimensional instance spaces. We present the design and implementation of a generalised multi-dimensional",
            "group": 2290,
            "name": "10.1.1.100.8681",
            "keyword": "",
            "title": "Evaluation and Analysis of Supervised Learning Algorithms and Classifiers"
        },
        {
            "abstract": "We consider computations associated with data parallel iterative solvers used for the numerical solution of Partial Differential Equations (PDEs). The mapping of such computations into load balanced tasks requiring minimum synchronization and communication is a difficult combinatorial optimization problem. Its optimal solution is essential for the efficient parallel processing of PDE computations. Determining data mappings that optimize a number of criteria, likeworkload balance, synchronization and local communication, often involves the solution of an NP-Complete problem. Although data mapping algorithms have been known for a few years there is lack of qualitative and quantitative comparisons based on the actual performance of the parallel computation. In this paper we present two new data mapping algorithms and evaluate them together with a large number of existing ones using the actual performance of data parallel iterative PDE solvers on the nCUBE II. Comparisons on the performance of data parallel iterative PDE solvers on medium and large scale problems demonstrate that some computationally inexpensive data block partitioning algorithms are as effective as the computationally expensive deterministic optimization algorithms. Also, these comparisons demonstrate that the existing approach in solving the data partitioning problem is inefficient for large scale problems. Finally, a software environment for the solution of the partitioning problem of data parallel iterative solvers is presented.",
            "group": 2291,
            "name": "10.1.1.100.9203",
            "keyword": "",
            "title": "Mapping Algorithms and Software Environment for Data Parallel PDE . . . "
        },
        {
            "abstract": "Optimal node placement in an optical packet switching",
            "group": 2292,
            "name": "10.1.1.100.9445",
            "keyword": "Manhattan street networkOptical packet switchingCombinatorial optimisationMulti-processor interconnection",
            "title": "Manhattan street network"
        },
        {
            "abstract": "In this paper, we present a new version of our content-based image retrieval system RETIN. It is based on adaptive quantization of the color space, together with new features aiming at representing the spatial relationship between colors. Color analysis is also extended to texture. Using these powerful indexes, an original interactive retrieval strategy is introduced. The process is based on two steps for handling the retrieval of very large image categories. First, a controlled exploration method of the database is presented. Second, a relevance feedback method based on statistical learning is proposed. All the steps are evaluated by experiments on a generalist database. 1",
            "group": 2293,
            "name": "10.1.1.100.9826",
            "keyword": "",
            "title": "Interactive exploration to image retrieval"
        },
        {
            "abstract": "A novel method is presented and explored within the framework of Potts neural networks for solving optimization problems with a non-trivial topology, with the airline crew scheduling problem as a target application. The key ingredient to handle the topological complications is a propagator defined in terms of Potts neurons. The approach is tested on artificial problems generated with two real-world problems as templates. The results are compared against the properties of the corresponding unrestricted problems. The latter are subject to a detailed analysis in a companion paper [1]. Very good results are obtained for a variety of problem sizes. The computer time demand for the approach only grows like (number of flights) 3. A realistic problem typically is solved within minutes, partly due to a prior reduction of the problem size, based on an analysis of the local arrival/departure structure at the single airports. To facilitate the reading for audiences not familiar with Potts neurons and mean field techniques, a brief review is given of recent advances in their application to resource allocation problems.",
            "group": 2294,
            "name": "10.1.1.101.1125",
            "keyword": "",
            "title": "Abstract: Airline Crew Scheduling Using Potts Mean Field Techniques"
        },
        {
            "abstract": "Abstract- A generalized evolutionary algorithm based on Tsallis statistics is proposed. The algorithm uses Tsallis generalized canonical distribution, which is one parameter generalization of Boltzmann distribution, to weigh the configurations in the selection mechanism. This generalization is motivated by the recently proposed generalized simulated annealing algorithm based on Tsallis statistics. We also present an information theoretic justification to use Boltzmann distribution in the selection mechanism, since these \u2018canonical \u2019 distributions have deep roots in information theory. Our simulation results show that for an appropriate choice of nonextensive index that is offered by Tsallis statistics, evolutionary algorithms based on this generalization outperform algorithms based on Boltzmann distribution. 1",
            "group": 2295,
            "name": "10.1.1.101.1748",
            "keyword": "",
            "title": "Information theoretic justification of Boltzmann selection and its generalization to Tsallis case"
        },
        {
            "abstract": "Frequently, a set of conserved (common) motifs in a group of functionally related biological sequences (DNA/RNA or proteins) has a specific biological function. Determination of compact groups of these common patterns is a prerequisite for their efficient modeling. We discuss some of the most crucial aspects of computer implementation of three heuristic algorithms which can be used in computational extraction of such conserved motifs from a set of unaligned DNA/RNA sequences. The algorithms included are tabu search, simulated annealing and a population-based genetic algorithm. A server with these algorithms implemented is available as a public web application free for academic and non-profit users at",
            "group": 2296,
            "name": "10.1.1.101.3162",
            "keyword": "ab initio DNA motif discoveryheuristic methodsgenetic algorithmssimulated annealing",
            "title": "Some implementation issues of heuristic methods for motif extraction from DNA sequences "
        },
        {
            "abstract": "Abstract-- Location area (LA) planning plays an important role in cellular networks because of the trade-off caused by paging and registration signaling. The upper bound on the size of an LA is the service area of a mobile switching center (MSC). In that extreme case, the cost of paging is at its maximum, but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal, but the registration cost is the largest. In general, the most important component of these costs is the load on the signaling resources. Between the extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we try to find an optimal method for determining the location areas. For that purpose, we use the available network information to formulate a realistic optimization problem. We propose an algorithm based on simulated annealing (SA) for the solution of the resulting problem. Then, we investigate the quality of the SA technique by comparing its results to greedy search and random generation methods.",
            "group": 2297,
            "name": "10.1.1.101.3461",
            "keyword": "Index terms\u2014Location AreaCellular NetworksSimulated",
            "title": "Location Area Planning in Cellular Networks Using Simulated Annealing *"
        },
        {
            "abstract": "In this papec we propose a domain decomposition scheme that seeks to minimize totalparallel execution time by considering the relative importance of two competing concerns- balancing the load and minimizing communi-cation-for a particular application and architecture. A simulated annealing approach is used to optimize an ob-jective function with components that measure both load balance and communication requirements. We develop an analytical model of execution time based upon a finite el-ement code executed on the Intel Paragon. This model is used to compare partitions with varying degrees of load imbalance. Most literature in the area of decomposition methods heavily emphasizes load balancing over the min-imization of communication. Our results indicate that this restrictive approach to load balancing can be relaxed with-outper$ormance degradation. Further, our results indicate that the degree of relaxation possible is dependent upon the target machine and the application; neither one can be neglected. 1",
            "group": 2298,
            "name": "10.1.1.101.3638",
            "keyword": "",
            "title": "Balancing Load versus Decreasing Communication: Exploring the Tradeoffs"
        },
        {
            "abstract": "Page iii Page iv All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. This book was set in Palatino by Achorn Graphic Services and was printed and bound in the United States of America. Library of Congress Cataloging-in-Publication Data",
            "group": 2299,
            "name": "10.1.1.101.3764",
            "keyword": "",
            "title": "Essays on Designing Minds"
        },
        {
            "abstract": "Shot boundary detection The shot boundary detection system in 2007 is basically the same as that of last year. We make three major modifications in the system of this year. First, CUT detector and GT detector use block based RGB color histogram with the different parameters instead of the same ones. Secondly, we add a motion detection module to the GT detector so that we can remove the false alarms caused by camera motion or large object movements. Finally, we add a post-processing module based on SIFT feature after both CUT and GT detector. The evaluation results show that all these modifications bring performance improvements to the system. The brief introduction to each run is shown in the following table: Run_id Description Thu01 Baseline system: RGB4_48 for CUT and GT detector, no motion detector, no sift post-processing, only using development set of 2005 as training set Thu02 Same algorithm as thu01, but with RGB16_48 for CUT detector, RGB4_48 for GT detector Thu03 Same algorithm as thu02, but with SIFT post-processing for CUT Thu04 Same algorithm as thu03, but with Motion detector for GT",
            "group": 2300,
            "name": "10.1.1.101.7826",
            "keyword": "",
            "title": "THU and ICRC at TRECVID 2007"
        },
        {
            "abstract": "Shot boundary detection The shot boundary detection system in 2007 is basically the same as that of last year. We make three major modifications in the system of this year. First, CUT detector and GT detector use block based RGB color histogram with the different parameters instead of the same ones. Secondly, we add a motion detection module to the GT detector so that we can remove the false alarms caused by camera motion or large object movements. Finally, we add a post-processing module based on SIFT feature after both CUT and GT detector. The evaluation results show that all these modifications bring performance improvements to the system. The brief introduction to each run is shown in the following table: Run_id Description Thu01 Baseline system: RGB4_48 for CUT and GT detector, no motion detector, no sift post-processing, only using development set of 2005 as training set Thu02 Same algorithm as thu01, but with RGB16_48 for CUT detector, RGB4_48 for GT detector Thu03 Same algorithm as thu02, but with SIFT post-processing for CUT Thu04 Same algorithm as thu03, but with Motion detector for GT",
            "group": 2301,
            "name": "10.1.1.101.7826",
            "keyword": "",
            "title": "THU and ICRC at TRECVID 2007"
        },
        {
            "abstract": "The design of Boolean functions with properties of cryptographic significance is a hard task. In this paper, we adopt an unorthodox approach to the design of such functions. Our search space is the set of functions that possess the required properties. It is \u201cBoolean-ness \u201d that is evolved. Key words: Q1 1.",
            "group": 2302,
            "name": "10.1.1.101.8066",
            "keyword": "",
            "title": "Almost Boolean Functions: The Design of Boolean Functions by Spectral Inversion"
        },
        {
            "abstract": "Abstract \u2014 This paper presents a decentralized negotiation protocol for cooperative economic scheduling in a supply chain environment. For this purpose we designed autonomous agents that maximize their profits by optimizing their local schedule and offer side payments to compensate other agents for lost profit or extra expense if cumulative profit is achievable. To further increase their income the agents have to apply a randomized local search heuristic to prevent the negotiation from stopping in locally optimal contracts. We show that the welfare could be increased by using a search strategy similar to Simulated Annealing. Unfortunately, a na\u00efve application of this strategy makes the agents vulnerable to exploitation by untruthful partners. We develop and test a straightforward mechanism based on trust accounts to protect the agents against systematic exploitation. This \u201cTrusted \u201d Simulated Annealing mechanism fosters the agents to truthfully reveal their opportunity cost situation, which is used as the basis for the calculation of side payments. I.",
            "group": 2303,
            "name": "10.1.1.101.8815",
            "keyword": "",
            "title": "A Trust-based Negotiation Mechanism for Decentralized Economic Scheduling"
        },
        {
            "abstract": "Abstract. One of the main questions concerning learning in a Multi-Agent System\u2019s environment is: \u201c(How) can agents benefit from mutual interaction during the learning process? \u201d This paper describes a technique that enables a heterogeneous group of Learning Agents (LAs) to improve its learning performance by exchanging advice. This technique uses supervised learning (backpropagation), where the desired response is not given by the environment but is based on advice given by peers with better performance score. The LAs are facing problems with similar structure, in environments where only reinforcement information is available. Each LA applies a different, well known, learning technique. The problem used for the evaluation of LAs performance is a simplified traffic-control simulation. In this paper the reader can find a summarized description of the traffic simulation and Learning Agents (focused on the advice-exchange mechanism), a discussion of the first results obtained and suggested techniques to overcome the problems that have been observed. 1",
            "group": 2304,
            "name": "10.1.1.101.9538",
            "keyword": "",
            "title": "Cooperative learning using advice exchange"
        },
        {
            "abstract": "Abstract. As the complexity of systems increases, so does the need of examining the nature of complexity itself. This work discusses the domain of physical swarm problems, in which a swarm of mobile agents is employed for solving physical graph problems (where a certain amount of travel effort in required for every movement along the graph\u2019s edges). A new kind of complexity scheme, suitable for this domain, is discussed by examining a central problem of this domain \u2014 the physical k-clique problem. In this problem, a swarm comprising of mobile agents travels along the vertices of a physical graph G, searching for a clique of size k. Thus, the complexity of the problem is measured in travel efforts (instead of in computation resources). In order to share information between the agents, two communication models are discussed \u2014 a complete knowledge sharing (referred to as centralized shared memory) and a distributed shared memory model, where the mobile agents can store and extract information using the graph\u2019s vertices. The work presents a search algorithm for the agents, and discusses its performance under each communication model. The major contribution of this work is demonstrating the strength of the distributed shared memory model. Although this model is much easier to implement and maintain, is highly fault tolerant and has high scalability, the quality of the results it produces is very high, compared to the strongest model of complete knowledge sharing.",
            "group": 2305,
            "name": "10.1.1.102.604",
            "keyword": "K-CliquePattern MatchingSwarm AlgorithmsSwarm IntelligenceDistributed Knowledge Sharing",
            "title": "On the Complexity of Physical Problems and a Swarm Algorithm for k-Clique Search in Physical Graphs"
        },
        {
            "abstract": "National Taiwan University In this article, we introduce a new placement problem motivated by the Dynamically Reconfigurable FPGA (DRFPGA) architectures. Unlike traditional placement, the problem for DRFPGAs must consider the precedence constraints among logic components. For the placement, we develop an effective metric that can consider wirelength, register requirement, and power consumption simultaneously. With the considerations of the new metric and the precedence constraints, we then present a three-stage scheme of partitioning, initial placement generation, and placement refinement to solve the new placement problem. Experimental results show that our placement scheme with the new metric achieves respective improvements of 17.2, 27.0, and 35.9 % in wirelength, the number of registers, and power consumption requirements, compared with the list scheduling method.",
            "group": 2306,
            "name": "10.1.1.102.883",
            "keyword": "Categories and Subject DescriptorsB.7.1 [Integrated CircuitsTypes and Design Styles\u2014Gate arraysB.7.2 [Integrated CircuitsDesign Aids\u2014Placement and routingJ.6 [Computer ApplicationsComputer-Aided Engineering General TermsAlgorithmsDesignExperimentationMeasurementPerformance Additional Key Words and PhrasesComputer-aided design of VLSIdynamically reconfigurablelayout",
            "title": "Realtek Semiconductor Corp.,"
        },
        {
            "abstract": "In this paper we provide a detailed and comprehensive survey of proposed approaches for network design, charting the evolution of models and techniques for the automatic planning of cellular wireless services. These problems present themselves as a trade-off between commitment to infrastructure and quality of service, and have become increasingly complex with the advent of more sophisticated protocols and wireless architectures. Consequently these problems are receiving increased attention from researchers in a variety of fields who adopt a wide range of models, assumptions and methodologies for problem solution. We seek to unify this dispersed and fragmented literature by charting the evolution of centralised planning for cellular systems. 1",
            "group": 2307,
            "name": "10.1.1.102.974",
            "keyword": "",
            "title": "Evolution of planning for wireless communication systems"
        },
        {
            "abstract": "Abstract \u2014 Broadcasting in wireless networks, unlike wired networks, inherently reaches several nodes with a single transmission. For omnidirectional wireless broadcast to a node, all nodes closer will also be reached. This property can be used to compute routing trees which minimize the sum of the transmitter powers. In this paper we present a simulated annealing algorithm for the problem. Extensive experimental results are presented. They show that the algorithm we propose is capable of improving the results of state-of-the-art algorithms for most of the problems considered. The solutions provided by the simulated annealing algorithm can be improved by applying a very fast post-optimization procedure. This leads to the best known mean results for the problems considered. Keywords\u2014Wireless networks, minimum power broadcast, simulated annealing. I.",
            "group": 2308,
            "name": "10.1.1.102.1547",
            "keyword": "",
            "title": "The minimum power broadcast problem in wireless networks: a simulated annealing approach"
        },
        {
            "abstract": "We go through the main results from the article by Kate Larson and Tuomas Sandholm, published in Artificial Intelligence [2]. A normative theory of interaction for analyzing the non-cooperative game of two self-interested rational agents with limited computational capabilities is presented. Both agents have an intractable optimization problem, and a limited amount of time to carry out the computations. The agents can benefit from pooling the problems and implementing the joint problem. If no advantage is gained from pooling agents implement their individual solutions. The equilibria for the game differ based on what kind of uncertainties there are in the game, and whether the performance profiles are deterministic or stochastic. We conclude with some considerations about the algorithms Larson and Sandholm presented.",
            "group": 2309,
            "name": "10.1.1.102.2089",
            "keyword": "Abstract............................................................................................",
            "title": "Bargaining with limited computation: Deliberation equilibrium"
        },
        {
            "abstract": "An important stage in circuit design is placement, where components are assigned to physical locations on a chip. A popular contemporary method for placement is the use of simulated annealing. While this approach has been shown to produce good placement solutions, recent work in genetic algorithms has produced promising results. The purpose of this study is to determine which approach will result in better placement solutions. A simpli ed model of the placement problem, circuit partitioning, was tested on three circuits with both a genetic algorithm and a simulated annealing algorithm. When compared with simulated annealing, the genetic algorithm was found to produce similar results for one circuit, and better results for the other two circuits. Based on these results, genetic algorithms may also yield better results than simulated annealing when applied to the placement problem.",
            "group": 2310,
            "name": "10.1.1.102.4916",
            "keyword": "",
            "title": "Genetic algorithms vs. simulated annealing: a comparison of approaches for solving the circuit partioning problem"
        },
        {
            "abstract": "Protocol security is important. So are ef\u00aeciency and cost. This paper provides an early framework for handling such aspects in a uniform way based on combinatorial optimisation techniques. The belief logic of Burrows, Abadi and Needham \ufffdBAN logic) is viewed as both a speci\u00aecation andproof system andas a `protocol programming language'. The paper shows how simulatedannealing andgenetic algorithms can be usedto generate correct andef\u00aecient BAN protocols. It also investigates the use of parsimonious andredundant representations.",
            "group": 2311,
            "name": "10.1.1.102.6016",
            "keyword": "Secure protocolsBelief logicSimulatedannealingGenetic algorithms",
            "title": "Protocols are programs too: the meta-heuristic search for security protocols"
        },
        {
            "abstract": "Abstract. This work explores the effect of adding a new partitioning step into the traditional complex programmable logic device (CPLD) CAD flow. A novel algorithm based on Rent\u2019s rule and simulated annealing partitions a design before it enters the place and route stage in CPLD CAD. The resulting partitions are then placed using an enhanced placement tool. Experiments conducted on Altera\u2019a APEX20K chips indicate that a partitioned placement can provide an average performance gain of 7 % over flat placements. 1",
            "group": 2312,
            "name": "10.1.1.102.6451",
            "keyword": "",
            "title": "Toronto, ON"
        },
        {
            "abstract": "Abstract \u2014 In this paper, we introduce the concept of the P*-admissible representation and propose a P*-admissible, transitive closure graph-based representation for general floorplans, called TCG, and show its superior properties. TCG combines the advantages of popular representations such as sequence pair, BSG, and B*-tree. Like sequence pair and BSG, but unlike O-tree, B*-tree, and CBL, TCG is P*-admissible. Like B*-tree, but unlike sequence pair, BSG, O-tree, and CBL, TCG does not need to construct additional constraint graphs for the cost evaluation during packing, implying faster runtime. Further, TCG supports incremental update during operations and keeps the information of boundary modules as well as the shapes and the relative positions of modules in the representation. More importantly, the geometric relation among modules is transparent not only to the TCG representation but also to its operations, facilitating the convergence to a desired solution. All these properties make TCG an effective and flexible representation for handling the general floorplan/placement design problems with various constraints. Experimental results show the promise of TCG.",
            "group": 2313,
            "name": "10.1.1.102.7850",
            "keyword": "FloorplanningLayoutPhysical DesignTransitive Closure Graph",
            "title": "TCG: A transitive closure graph-based representation for non-slicing floorplans"
        },
        {
            "abstract": "Motivation: Recent advances in cell-free protein expression systems allow specific labeling of proteins with amino acids containing stable isotopes ( 15 N, 13 C, and 2 H), an important feature for protein structure determination by nuclear magnetic resonance (NMR) spectroscopy. Given this labeling ability, we present a mathematical optimization framework for designing a set of protein isotopomers, or labeling schedules, to reduce the congestion in the NMR spectra. The labeling schedules, which are derived by the optimization of a cost function, are tailored to a specific protein and NMR experiment. Results: For 2D 15 N- 1 H HSQC experiments, we can produce an exact solution using a dynamic programming algorithm in under two hours on a standard desktop machine. Applying the method to a standard benchmark protein, calmodulin, we are able to reduce the number of overlaps in the 500MHz HSQC spectrum from ten to one using four samples with a true cost function, and ten to four if the cost function is derived from statistical estimates. On a set of 448 curated proteins from the BMRB database, we are able to reduce the relative percent congestion by 84.9 % in their HSQC spectra using only four samples. Our method can be applied in a high-throughput manner on a proteomic scale using the server we developed. On a 100 node cluster, optimal schedules can be computed for every protein coded for in the human genome in less than a month. Availability: A server for creating labeling schedules for 15 N-1 H HSQC experiments as well as results for each of the individual 448 proteins used in the test set is available at",
            "group": 2314,
            "name": "10.1.1.102.8030",
            "keyword": "",
            "title": "Minimizing The Overlap Problem In Protein NMR: A Computational Framework For Precision Amino Acid Labeling"
        },
        {
            "abstract": "This paper addresses the problem of reliably setting genetic algorithm parameters for consistent labelling problems. Genetic algorithm parameters are notoriously difficult to determine. This paper proposes a robust empirical framework, based on the analysis of factorial experiments. The use of a graeco-latin square permits an initial study of a wide range of parameter settings. This is followed by fully crossed factorial experiments with narrower ranges, which allow detailed analysis by logistic regression. The empirical models thus derived can be used first to determine optimal algorithm parameters, and second to shed light on interactions between the parameters and their relative importance. The initial models do not extrapolate well. However, an advantage of this approach is that the modelling process is under the control of the experimenter, and is hence very flexible. Refined models are produced, which are shown to be robust under extrapolation to up to triple the problem size.",
            "group": 2315,
            "name": "10.1.1.102.8481",
            "keyword": "",
            "title": "Empirical Modeling of Genetic Algorithms"
        },
        {
            "abstract": "Abstracf-Most digital systems at some time during use have areas (modules) that are \u201cdead \u201d in the sense that they do not contain valid data, i.e., the data that was processed or generated by that area has been passed on to a subsequent stage and will not be required (read) again. In a synthesized system, where the flow of data is determined explicitly by an on-chip (synthesized) controller, the question of which arras will be dead or not (and when) is known in advance. There are areas and times when the \u201cuse \u201d is data-dependent, but then the use is known to the controller at that time. This deadtime can be exploited to run a test pattern (either complete or in part) through the unused area, thereby giving the ability to continuously monitor the \u201chealth \u201d of the overall system with very little (sometimes zero) impact on the processing capability. This has obvious applications in situations where reliability is a concern. There exist systems where an area is so heavily used that it is impossible to perform any testing at a serious rate; in this case the area may either be partially tested (or tested at a lower rate) or the processing of \u201creal \u201d data periodically halted to allow a more thorough test to take place with concomitant throughput degradation. This paper describes a behavioral synthesis system that can detect and exploit dead areas for automatic testing. Pertinent aspects of the controller are described, and a number of dead area statistics (including \u201ctest throughput\u201d) generated from real designs are reported. Index Terms-Synthesis, test.",
            "group": 2316,
            "name": "10.1.1.102.9130",
            "keyword": "I. INTR~DU~I~N",
            "title": "Online testing of statically and dynamically scheduled synthesized systems"
        },
        {
            "abstract": "A modified simulated annealing algorithm for estimating solute transport parameters in streams from tracer experiment data",
            "group": 2317,
            "name": "10.1.1.102.9341",
            "keyword": "Solute transport in streamSimulated annealingDispersion coefficientParameter estimation",
            "title": "Availability: freely available by e-mailing your"
        },
        {
            "abstract": "Abstract \u2013 This paper summarizes the recent works of the author and his team in the methodology for solving the intractable combinatorial problems, which occur in VLSI and SoC physical design. The optimal circuit reduction method has proved to be an effective tool to identify the hierarchical cluster structure of the circuit. The author reviews the applicability of this method to solving a wide spectrum of various problems, including hierarchical clustering, partitioning, packaging, and placement. He develops a general approach to these problems based on the recursive use of high quality algorithms of global and local optimization for unique, not very large size problems. Experiments confirm the high effectiveness of this approach. For some well-known test cases the optimal results were achieved for the first time, while for many other cases improved results were obtained. 1",
            "group": 2318,
            "name": "10.1.1.102.9924",
            "keyword": "",
            "title": "The Optimal Circuit Reduction Method as an Effective Tool to Solve Large and Very Large Size Intractable Combinatorial VLSI Physical Design Problems"
        },
        {
            "abstract": "Abstract-h this paper, we attack the figure-ground discrim-ination problem from a combinatorial optimization perspective. In general, the solutions proposed in the past solved this prob-lem only partially: Either the mathematical model encoding the figure-ground problem was too simple, the optimization methods that were used were not efficient enough, or they could not guarantee that the global minimum of the cost function describing the figure-ground model would be found. The method that we devised and is describe-d in this paper is tailored around three main contributions. First, we suggest a mathematical model encoding the figure-ground discrimination problem that makes explicit a definition of shape (or figure) based on cocircularity, smoothness, proximity, and contrast. This model consists of building a cost function on the basis of image element interactions. Moreover, this cost func-tion fits the constraints of an interacting spin system that, in turn, is a well suited physical model that solves hard combinatorial optimization problems Second, we suggest two combinatorial optimization methods for solving the figure-ground problem, namely i) mean field annealing, which combines mean field approximation theory and annealing, and ii) microcanonical annnealing. Mean field annealing may well be viewed as a deterministic approximation of stochastic methods such as simulated annealing. We describe, in detail, the theoretical bases of these methods, derive computa-tional models, and provide practical algorithms. Third, we provide a comparison of the efficiency of mean field annealing, simulated annealing, and microcanonical annealing algorithms. Within the framework of such a comparison, the figure-ground problem may well be viewed as a benchmark. Index Terms-Feature grouping, figure-ground discrimination, low-level vision, mean field annealing, microcanonical anneal-ing, recursive neural networks, simulated annealing, stochastic optimization theory, thresholding. T",
            "group": 2319,
            "name": "10.1.1.103.233",
            "keyword": "",
            "title": "Figure-Ground Discrimination: A Combinatorial Optimization Approach"
        },
        {
            "abstract": "We describe a synthesis system that takes operating range constraints and inter- and intra- circuit parametric manufacturing variations into account while designing a sized and biased analog circuit. Previous approaches to CAD for analog circuit synthesis have concentrated on nominal analog circuit design, and subsequent optimization of these circuits for statistical fluctuations and operating point ranges. Our approach simultaneously synthesizes and optimizes for operating and manufacturing variations by mapping the circuit design problem into an Infinite Programming problem and solving it using an annealing within annealing formulation. We present circuits designed by this integrated synthesis system, and show that they indeed meet their operating range and parametric manufacturing constraints. 1",
            "group": 2320,
            "name": "10.1.1.103.246",
            "keyword": "",
            "title": "Abstract Synthesis of Manufacturable Analog Circuits"
        },
        {
            "abstract": "A mobile user location management mechanism is introduced that incorporates a distance based location update scheme and a paging mechanism that satis\ufffdes prede\ufffdned delay requirements. An analytical model is developed which captures the mobility and call arrival pattern of a terminal. Given the respective costs for location update and terminal paging, the average total location update and terminal paging cost is determined. An iterative algorithm is then used to determine the optimal location update threshold distance that results in the minimum cost. Analytical results are also obtained to demonstrate the relative cost incurred by the proposed mechanism under various delay requirements. 1",
            "group": 2321,
            "name": "10.1.1.103.868",
            "keyword": "",
            "title": "A mobile user location update and paging mechanism under delay constraints"
        },
        {
            "abstract": "",
            "group": 2322,
            "name": "10.1.1.103.1937",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": "This paper describes the radiotherapy patient scheduling problem of minimising waiting times. Like many other service industry problems, radiotherapy patient scheduling may be solved by first modelling and formulating it into a shop scheduling problem. Over the years, these shop scheduling models have been researched and solved using various approaches. This paper typifies radiotherapy patient scheduling into a job shop problem. In addition, exact and metaheuristic approaches of solving job shop scheduling problems are also reviewed and comparatively analysed. 1",
            "group": 2323,
            "name": "10.1.1.103.2253",
            "keyword": "radiotherapyjob shop problempatient schedulingmetaheuristic",
            "title": "A REVIEW OF SCHEDULING PROBLEMS IN RADIOTHERAPY"
        },
        {
            "abstract": null,
            "group": 2324,
            "name": "10.1.1.103.2593",
            "keyword": "",
            "title": "Student: Quang-Huy NGUYEN"
        },
        {
            "abstract": "View management, a relatively new area of research in Augmented Reality (AR) applications, is about the spatial layout of 2D virtual annotations in the view plane. This paper represents the first study in an actual AR application of a specific view management task: evaluating the placement of 2D virtual labels that identify information about real counterparts. Here, we objectively evaluated four different placement algorithms, including a novel algorithm for placement based on identifying existing clusters. The evaluation included both a statistical analysis of traditional metrics (e.g. counting overlaps) and an empirical user study guided by principles from human cognition. The numerical analysis of the three real-time algorithms revealed that our new cluster-based method recorded the best average placement accuracy while requiring only relatively moderate computation time. Measures of objective readability from the user study demonstrated that in practice, human subjects were able to read labels fastest with the algorithms that most quickly prevented overlap, even if placement wasn\u2019t ideal. 1.",
            "group": 2325,
            "name": "10.1.1.103.3670",
            "keyword": "",
            "title": "Evaluating Label Placement for Augmented Reality View Management"
        },
        {
            "abstract": "Abstract. Optimization problems are very important in many fields. To the present, many optimization algorithms based on computational intelligence have been proposed, such as the Genetic Algorithm, Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO). In this paper, a new optimization algorithm, namely, Cat Swarm Optimization (CSO) is proposed. CSO is generated by observing the behavior of cats, and composed of two sub-models by simulating the behavior of cats. According to the experiments, the results reveal that CSO is superior to PSO.",
            "group": 2326,
            "name": "10.1.1.103.3698",
            "keyword": "Cat swarm optimizationSwarm intelligenceSoft computingEvolutionary",
            "title": "COMPUTATIONAL INTELLIGENCE BASED ON THE BEHAVIOR OF CATS"
        },
        {
            "abstract": "To my grandmother, the late V. Mae Teeters, whose integrity, tenacity, resource-fulness, intelligence and love inspired me. I miss you. iii",
            "group": 2327,
            "name": "10.1.1.103.3942",
            "keyword": "",
            "title": "ii"
        },
        {
            "abstract": "",
            "group": 2328,
            "name": "10.1.1.103.4120",
            "keyword": "",
            "title": "3.2 Deterministic Approach..................... 8"
        },
        {
            "abstract": "Abstract. The lattice approach to biological structural analysis was made popular by the HP model for protein folding, but had not been used previously for RNA secondary structure prediction. We introduce the Delta toolset for the structural analysis of biological sequences on a 3D triangular lattice. The Delta toolset includes a proof-of-concept RNA folding program that is both fast and accurate in predicting the secondary structures with pseudoknots of short RNA sequences. 1",
            "group": 2329,
            "name": "10.1.1.103.4900",
            "keyword": "",
            "title": "Delta: a Toolset for the Structural Analysis of Biological Sequences on a 3D Triangular Lattice \u22c6"
        },
        {
            "abstract": "This paper describes a complete design flow for multiprocessor systems-on-chips (SoCs) covering the design phases from system-level modeling to FPGA prototyping. The design of complex heterogeneous systems is enabled by raising the abstraction level and providing several system-level design automation tools. The system is modeled in a UML design environment following a new UML profile that specifies the practices for orthogonal application and architecture modeling. The design flow tools are governed in a single framework that combines the subtools into a seamless flow and visualizes the design process. Novel features also include an automated architecture exploration based on the system models in UML, as well as the automatic back and forward annotation of information in the design flow. The architecture exploration is based on the global optimization of systems that are composed of subsystems, which are then locally optimized for their particular purposes. As a result, the design flow produces an optimized component allocation, task mapping, and scheduling for the described application. In addition, it implements the entire system for FPGA prototyping board. As a case study, the design flow is utilized in the integration of state-of-the-art technology approaches, including a wireless terminal architecture, a network-on-chip, and multiprocessing utilizing RTOS in a SoC. In this study, a central part of a WLAN terminal is modeled,",
            "group": 2330,
            "name": "10.1.1.103.5059",
            "keyword": "Categories and Subject DescriptorsI.6.0 [Simulation and ModelingGeneralB.8.2 [Performance and ReliabilityPerformance Analysis and Design Aids\u2014SimulationC.2.1 [Computer- Communication NetworkNetwork Architecture Design\u2014Wireless communication General TermsDesignPerformanceVerification Additional Key Words and PhrasesUML 2.0design flowarchitecture exploration",
            "title": "Nokia Technology Platforms"
        },
        {
            "abstract": "Co-occurrence constraints play an important role in rule-based systems such as natural language processing. The constraints appear in many forms including agreement restrictions (e.g.. number agreement between subjects and predicates), selectional restrictions on complement types, and filler-gap movement dependencies. A general account of such constraints is given in a parallel execution model for rule-based systems \u2014 Active Production Networks (APNs). The APN model is similar to connectionist (or spreading activation) models, but explicitly provides a functional interpretation of rule-based phenomena such as variable binding, multiple instantiations (including recursion), and contextual expectations. Co-occurrence constraints are represented by a theory of coindexing and trace capture, based on a feedback mechanism. Several examples of constraint processing are presented which, surprisingly, also include phenomena such as phonological nulls and contraction.",
            "group": 2331,
            "name": "10.1.1.103.6042",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Multiplicative weight-update algorithms such as Winnow and Weighted Majority have been studied extensively due to their on-line mistake bounds \u2019 logarithmic dependence on N, the total number of inputs, which allows them to be applied to problems where N is exponential. However, a large N requires techniques to efficiently compute the weighted sums of inputs to these algorithms. In special cases, the weighted sum can be exactly computed efficiently, but for numerous problems such an approach seems infeasible. Thus we explore applications of Markov chain Monte Carlo (MCMC) methods to estimate the total weight. Our methods are very general and applicable to any representation of a learning problem for which the inputs to a linear learning algorithm can be represented as states in a completely connected, untruncated Markov chain. We give theoretical worst-case guarantees on our technique and then apply it to two problems: learning DNF formulas using Winnow, and pruning classifier ensembles using Weighted Majority. We then present empirical results on simulated data indicating that in practice, the time complexity is much better than what is implied by our worst-case theoretical analysis.",
            "group": 2332,
            "name": "10.1.1.103.8492",
            "keyword": "1",
            "title": "Efficiently approximating weighted sums with exponentially many terms"
        },
        {
            "abstract": "Abstract\u2014A novel search principle for optimal feature subset selection using the Branch & Bound method is introduced. Thanks to a simple mechanism for predicting criterion values, a considerable amount of time can be saved by avoiding many slow criterion evaluations. We propose two implementations of the proposed prediction mechanism that are suitable for use with nonrecursive and recursive criterion forms, respectively. Both algorithms find the optimum usually several times faster than any other known Branch & Bound algorithm. As the algorithm computational efficiency is crucial, due to the exponential nature of the search problem, we also investigate other factors that affect the search performance of all Branch & Bound algorithms. Using a set of synthetic criteria, we show that the speed of the Branch & Bound algorithms strongly depends on the diversity among features, feature stability with respect to different subsets, and criterion function dependence on feature set size. We identify the scenarios where the search is accelerated the most dramatically (finish in linear time), as well as the worst conditions. We verify our conclusions experimentally on three real data sets using traditional probabilistic distance criteria.",
            "group": 2333,
            "name": "10.1.1.103.9065",
            "keyword": "Index Terms\u2014Subset search",
            "title": "Fast branch & bound algorithms for optimal feature selection"
        },
        {
            "abstract": "Proposed working title: Co-Evolution of Programs and Software Tests A software program is implemented from a set of specifications that defines its behaviour. However, proving that a program fulfils its specifications is not straightforward. Although software tests cannot prove that a program is correct, they are widely used to find defects in software programs and to increase their reliability. Both the tasks of generating programs and tests have been tried to be automated for several decades, with promising results but at the same time with many limits. Although the idea is not completely new, studying how to co-evolve programs and tests together has never been appropriately studied yet. With an analogy to the natural world, the tests can be seen as \u201cparasites \u201d that attack the programs. Both the programs and the tests evolve influencing each others. The programs that are able to defend from the parasites (i.e., pass the tests) are more likely to be chosen to generate the next population, whereas the parasites that infect (i.e., make fail) more programs will be chosen to generate the new parasites. I believe that such a co-evolution might help to shed light on and solve some issues in the Automatic Programming field. Furthermore, investing different software testing techniques for the evolution of the",
            "group": 2334,
            "name": "10.1.1.103.9638",
            "keyword": "Contents",
            "title": "Name of supervisor: prof. Xin Yao"
        },
        {
            "abstract": "The purpose of this research is to extend the theory of uncertain reasoning over time through integrated, multi-strategy learning. Its focus is on decomposable, concept learning problems for classification of spatiotemporal sequences. Systematic methods of task decomposition using attribute-driven methods, especially attribute partitioning, are investigated. This leads to a novel and important type of unsupervised learning in which the feature construction (or extraction) step is modified to account for multiple sources of data and to systematically search for embedded temporal patterns. This modified technique is combined with traditional cluster definition methods to provide an effective mechanism for decomposition of time series learning problems. The decomposition process interacts with model selection from a collection of probabilistic models such as temporal artificial neural networks and temporal Bayesian networks. Models are chosen using a new quantitative (metric-based) approach that estimates expected performance of a learning architecture, algorithm, and mixture model on a newly defined subproblem. By mapping subproblems to customized configurations of probabilistic networks for time series learning, a hierarchical, supervised learning system with enhanced generalization quality can be automatically built. The system can improve data fusion",
            "group": 2335,
            "name": "10.1.1.103.9947",
            "keyword": "",
            "title": "Time Series Learning with Probabilistic Network Composites"
        },
        {
            "abstract": "paper presents an overview and analylds of learning in Artiftciul Neural System _ (ANS's). It bef_ns with a general introduction to neural networks and connection_t approaches to informatlon proceging. The basis for learning in ANS's is then described, and compared with classical machine learning. While similar in some ways, ANS learning deviates from tradition in its dependence on the modification of individual _eigh_a to bring about changes in a knowledge representation distributed across connections in a network. This unique form of learning is analysed from two aspects: the selection of an appropriate network architecture for representing the problem, and the choice of a suitable learning rule capable of reproducing the desired function within the given network. The various network architectures are classified, and then identified with explicit restrictions on the types of functions they are capable of representing. The learning rules, i.e., algorithms that specify how the network weights are modified, are similarly taxonomised, and where possible, the limitations inherent to specific classes of rules are outlined. 1.",
            "group": 2336,
            "name": "10.1.1.104.648",
            "keyword": "artificial neural systemsneural networksconnectionismlearning",
            "title": "Learning in Artificial Neural Systems"
        },
        {
            "abstract": "Abstract \u2014 MatrixExplorer is a network visualization system that uses two representations: node-link diagrams and matrices. Its design comes from a list of requirements formalized after several interviews and a participatory design session conducted with social science researchers. Although matrices are commonly used in social networks analysis, very few systems support the matrix-based representations to visualize and analyze networks. MatrixExplorer provides several novel features to support the exploration of social networks with a matrix-based representation, in addition to the standard interactive filtering and clustering functions. It provides tools to reorder (layout) matrices, to annotate and compare findings across different layouts and find consensus among several clusterings. MatrixExplorer also supports Node-link diagram views which are familiar to most users and remain a convenient way to publish or communicate exploration results. Matrix and node-link representations are kept synchronized at all stages of the exploration process. Index Terms \u2014 social networks visualization, node-link diagrams, matrix-based representations, exploratory process, matrix ordering, interactive clustering, consensus. Fig. 1. MatrixExplorer showing two synchronized representations of the same network: matrix on the left and node-link on the right. 1",
            "group": 2337,
            "name": "10.1.1.104.1055",
            "keyword": "",
            "title": "MatrixExplorer: a Dual-Representation System to Explore Social Networks"
        },
        {
            "abstract": "Abstract. Stochastic local search algorithms are proved to be one of the most effective approach for computing approximate solutions of hard combinatorial problems. Most of them are based on a typical randomness related to some uniform distributions for generating initial solutions. Particularly, Extremal Optimization is a recent meta-heuristic proposed for finding high quality solutions to hard optimization problems. In this paper, we introduce an algorithm based on another distribution, known as the Bose-Einstein distribution in quantum physics, which provides a new stochastic initialization scheme to an Extremal Optimization procedure. The resulting algorithm is proposed for the approximated solution to an instance of the weighted maximum satisfiability problem (MAX SAT). We examine its effectiveness by computational experiments on a large set of test instances and compare it with other existing meta-heuristic methods. Our results are remarkable and show that this approach is appropriate for this class of problems. 1",
            "group": 2338,
            "name": "10.1.1.104.1064",
            "keyword": "",
            "title": "Efficient initial solution to extremal optimization algorithm for weighted MAXSAT problem"
        },
        {
            "abstract": null,
            "group": 2339,
            "name": "10.1.1.104.1836",
            "keyword": "",
            "title": "Solving the converter placement problem in wdm ring networks using genetic algorithms"
        },
        {
            "abstract": "and why",
            "group": 2340,
            "name": "10.1.1.104.3232",
            "keyword": "",
            "title": "you"
        },
        {
            "abstract": "Abstract-Current technology provides a means to obtain sampled data that digitally describes three-dimensional surfaces and objects. Three-dimensional digitizing cameras can be used to obtain sampled data that maps the surface of three-dimensional figures and models. Data obtained from such sources enable accurate renderings ofthe original surface. However, the digitizing process often provides much more data than is needed to accurately recreate the surface or object. In order to use such data in real-time visual simulators, a significant reduction in the data needed to accurately render the sampled surfaces is required. The techniquespresented were developed to drastically reduce the number ofdata points required to depict an object without sacrificing the detail and accuracy inherent in the digitizing process. 1.",
            "group": 2341,
            "name": "10.1.1.104.3375",
            "keyword": "",
            "title": "@ l99l Pergamon Pre $ plc"
        },
        {
            "abstract": "Autonomous model building is a crucial trend in model based methods like AAMs. This paper introduces an approach that deals with non-linearities by detecting distinct sub-parts in the data. Sub-models each representing an individual sub-part are derived from a minimum description length criterion. Thereby the resulting clique of models is more compact and obtains a better generalization behavior than a single model. The proposed AAM clique generation deals with non-linearities in the data in a generic information theoretic manner reducing the necessity of user interaction during training. 1",
            "group": 2342,
            "name": "10.1.1.104.3534",
            "keyword": "",
            "title": "A Clique of Active Appearance Models by Minimum Description Length \u2217"
        },
        {
            "abstract": "Summary: Genome-wide association studies are now technically feasible and likely to become a fundamental tool in unraveling the ultimate genetic basis of complex traits. However, new statistical and computational methods need to be developed to extract the maximum information in a realistic computing time. Here we propose a new method for multiple association analysis via simulated annealing that allows for epistasis and any number of markers. It consists of finding the model with lowest Bayesian Information Criterion using simulated annealing. The data are described by means of a mixed model and new alternative models are proposed using a set of rules, e.g., new sites can be added (or deleted), or new epistatic interactions can be included between existing genetic factors. The method is illustrated with simulated and real data.",
            "group": 2343,
            "name": "10.1.1.104.3750",
            "keyword": "",
            "title": "Correspondence to: Departament de Ci\u00e8ncia Animal i del Aliments"
        },
        {
            "abstract": "System performance in multi-agent resource allocation systems can often improve if individual agents reduce their activity. Agents in such systems need a way to modulate their individual behavior in the light of the system\u2019s state, preferably in a way that does not require centralized control. We illustrate the problem of hyperactive agents in two domains related to resource allocation. We describe a simple, decentralized scheme, inspired by insect pheromones, that enables individual agents to adjust their level of activity as the system operates, and discuss a general approach to dealing with approaching deadlines. Then we demonstrate the effectiveness of these mechanisms in the two example domains.",
            "group": 2344,
            "name": "10.1.1.104.3760",
            "keyword": "complexity",
            "title": "Forthcoming at AAMAS'03 How to Calm Hyperactive Agents"
        },
        {
            "abstract": "for movement analysis",
            "group": 2345,
            "name": "10.1.1.104.3772",
            "keyword": "Image registrationmovement analysisskin textureNormalized Mutual InformationSimulated Annealing",
            "title": "Possibilities in using skin texture based image registration"
        },
        {
            "abstract": "A communication tree is a binomial tree embedded in a hypercube, whose communication direction is from its leaves to its root. If a problem to be solved is first divided into independent subproblems, then each subproblem can be solved by one of the hypercube processors, and all the subresults can be merged into the final results through tree communication. This paper uses two random search techniques, the genetic algorithm (GA) and simulated annealing (SA), to construct fault-tolerant communication trees with the minimum data transmission time. Experimental evaluation shows that, with reasonably low search time, the proposed GA and SA approaches are able to find more desirable communication trees (i.e., trees with less data transmission time) than the minimal cost approach can. A distributed approach which applies parallel search to communication subtrees in disjoint subcubes is also provided to reduce the search time of the proposed approaches.",
            "group": 2346,
            "name": "10.1.1.104.4024",
            "keyword": "fault-tolerant communication treeshypercubesgenetic algorithmssimulated annealing",
            "title": "Constructing Fault-Tolerant Communication Trees in Hypercubes *"
        },
        {
            "abstract": "The Variable Neighborhood Search (VNS) is a recent metaheuristic that combines series of random and improving local searches based on systematically changed neighborhoods. When a local minimum is reached, a shake procedure performs a random search. This determines a new starting point for running an improving search. The use of interchange moves provides a simple implementation of the VNS algorithm for the p-Median Problem. Several strategies for the parallelization of the VNS are considered and coded in C using OpenMP. They are compared in a shared memory machine with large instances. Key Words: parallel computing, VNS, p-median, OpenMP 1.",
            "group": 2347,
            "name": "10.1.1.104.4351",
            "keyword": "",
            "title": "The parallel variable neighborhood search for the p-median problem"
        },
        {
            "abstract": "A protein is identified by a finite sequence of amino acids, each of them chosen from a set of 20 elements. The Protein Structure Prediction Problem is the problem of predicting the 3D native conformation of a protein, when its sequence of amino acids is known. This problem is fundamental for biological and pharmaceutical research. All current mathematical models of the problem are affected by intrinsic computational limits. In particular, simulation-based techniques that handle every chemical interaction between all atoms in the amino acids (and the solvent) are not feasible due to the huge amount of computations involved. These programs are typically written in imperative languages and hard to be parallelized. Moreover, each approach is based on a particular energy function. In the literature there are various proposals and there is no common agreement on which is the most reliable. In this paper we present a novel framework for ab-initio simulations using Concurrent Constraint Programming. We are not aware of any other similar proposals in the literature. Each amino acid of an input protein is viewed as an independent process that communicates with the others. The framework allows a modular representation of the problem and it is easily extensible for further refinements. Simulations at this level of abstraction allow faster calculation. We provide a first preliminary working example in Mozart, to show the feasibility and the power of the method. The code is intrinsically concurrent and thus easy to be parallelized.",
            "group": 2348,
            "name": "10.1.1.104.7787",
            "keyword": "Key wordsComputational BiologyConcurrent Constraint Programming",
            "title": "Bio-CONCUR 2004 Preliminary Version Protein Folding Simulation in Concurrent Constraint Programming"
        },
        {
            "abstract": "Compared to the well understood macro networks, networks-onchip introduce novel design challenges. The characteristics of the system data flows and the knowledge of the required wire lengths can be exploited to optimize for speed and power consumption. A component library for flexible construction of interconnection architectures is being developed at the Tampere University of Technology to enable the creation of application development platforms. The overall design flow of these development platforms is reviewed in this paper. Network-on-chip topology optimization is addressed by describing the methodologies used by an effective design automation tool. The detailed cost functions of the tool capture the factors contributing to the speed and power consumption of asynchronous interconnections, while different abstraction level input information is supported. A case study into the application domain of industrial process control and monitoring is presented in order to evaluate the result quality.",
            "group": 2349,
            "name": "10.1.1.104.7988",
            "keyword": "Network-on-ChipTopology OptimizationPlatform Design",
            "title": "Topology optimization for application-specific networks-on-chip"
        },
        {
            "abstract": "Improving logic capacity by time-sharing, dynamically reconfigurable FP-GAs are employed to handle designs of high complexity and functionality. In this paper, we use a novel topological floorplan representation, named 3DsubTCG (3-Dimensional sub-Transitive Closure Graph) to deal with the 3dimensional (temporal) floorplanning/placement problem, arising from dynamically reconfigurable FPGAs. The 3D-subTCG uses three transitive closure graphs to model the temporal and spatial relations between modules. We derive the feasibility conditions for the precedence constraints induced by the execution of the dynamically reconfigurable FPGAs. Because the geometric relationship is transparent to 3D-subTCG and its induced operations, we can easily detect any violation of temporal precedence constraints on 3D-subTCG. We also derive important properties of the 3D-subTCG to reduce the solution space and shorten the running time for 3D (temporal) foorplanning/placement. Experimental results show that our 3D-subTCG based algorithm is very effective and efficient. 1",
            "group": 2350,
            "name": "10.1.1.104.8927",
            "keyword": "",
            "title": "Temporal floorplanning using 3d-subtcg"
        },
        {
            "abstract": "Abstract. Many software test generation techniques target on generating software test data. Only a few of them provide automatic way to verify if software behaves correctly using generated test data. We propose a testing technique, which uses UML modeling language extension OCL as imprecise test oracle. Imprecise OCL constraints can be viewed as expressions which define expected results within some ranges of possible values. When software is executed using generated test data the output is verified against imprecise OCL constraints. If output invalidates imprecise OCL constraints, a tester can assume with some probability that software has bugs. 1.",
            "group": 2351,
            "name": "10.1.1.104.9153",
            "keyword": "",
            "title": "THE USE OF MODEL CONSTRAINTS AS IMPRECISE SOFTWARE TEST ORACLES"
        },
        {
            "abstract": "First and foremost, I would like to express my gratitude to my advisor, Janos Sztipanovits. He has always been there whenever I needed help, guidance, or motivation. Without him, this work wouldn't have been successful. Ben Abbott, Ted Bapty, Csaba Biegl and Gabor Karsai form the core of the Measurement and Computing Systems Group. Their contribution to my education has been invaluable. Thanks are in order to the additional committee members, Benoit Dawant, Jerry Spinrad, and Mitch Wilkes for their helpful advices. Thanks to Hubertus Franke, Amit Misra, Michael Moore, James &quot;Bubba &quot; Davis and the rest of the group for putting up with me and my silly jokes. Many thanks to the organizations providing financial support to this research: Vanderbilt University for awarding me a University Graduate Fellowship, the US Air Force for providing funding, and the IBM T. J. Watson Research Center for its summer internship program. Thanks to the Department of Measurement and Instrument Engineering of the Technical University of Budapest for providing me with an excellent engineering education during my six years there. Most of all, I would like to thank my wife, J\u00falia, for her constant love and support. Without her, this wouldn't have been possible. I owe a great deal to my brother, Tam\u00e1s, who has always pushed me into the right direction. Thanks to my parents for their support and encouragement. ii",
            "group": 2352,
            "name": "10.1.1.104.9551",
            "keyword": "Page ACKNOWLEDGEMENTS........................................ ii",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "Recent years have seen growing interest in the problem of super-resolution restoration of video sequences. Whereas in the traditional single image restoration problem only a single input image is available for processing, the task of reconstructing super-resolution images from multiple undersampled and degraded images can take advantage of the additional spatiotemporal data available in the image sequence. In particular, camera and scene motion lead to frames in the source video sequence containing similar, but not identical information. The additional information available in these frames make possible reconstruction of visually superior frames at higher resolution than that of the original data. In this paper we review the current state of the art and identify promising directions for future research.",
            "group": 2353,
            "name": "10.1.1.105.551",
            "keyword": "",
            "title": "Spatial resolution Enhancement of Low-Resolution . . . "
        },
        {
            "abstract": "The linear arrangement minimization problem consists of finding a labeling or arrangement of the vertices of a graph that minimizes the sum of the absolute values of the differences between the labels of adjacent vertices. This is a well-known NP-hard problem that presents a challenge to solution methods based on heuristic optimization. Many linear arrangement reduction algorithms have recently been developed and applied to structural engineering, VLSI and software testing. We undertake the development of different heuristic procedures with the goal of uncovering the most effective designs to tackle this difficult but important problem. Specifically, we consider the adaptation of constructive, local search, GRASP and Path Relinking methods for the linear arrangement minimization. We perform computational experiments with previously reported instances to first study the effects of changes in critical search parameters and then to compare the efficiency of our proposal with previous solution procedures.",
            "group": 2354,
            "name": "10.1.1.105.1626",
            "keyword": "Key WordsGRASPPath RelinkingMetaheuristics",
            "title": "Heuristics for the Minimum Linear Arrangement Problem"
        },
        {
            "abstract": "Processor performance advances are increasingly in-hibit(ed by limitations in thermal power dissipation. Part of the problem is the lack of architectural power estimates before implementation. Although high-performance designs exist that dissipate low power, the method for finding these designs has bc:en through trial-and-error. This paper presents system-atic techniques to find low-power, high-performance superscalar processors tailored to specific user bench-marks. The model of power is novel because it sep-arates power into architectural and technology com-ponents. The architectural component is found via trace-driven simulation, which also produces perfor-mance estimates. An example technology model is presented that estimates the technology component, along with critical delay time and real estate usage. This model is bwed on case studies of actual designs. It is used to solve an important problem: increasing the duplication in superscalar execution units without excessive power consumption. Results are present#ed from runs using simulated annealing to maximize pro-cessor performance subject to power and area con-st#raints. The major contributions of this paper are the sep-aration of architectural and technology components of dynamic power, the use of trace-driven simulation for architectural power measurement, and the use of a near-optimal search t,o tailor a processor design to a benchmark. 1",
            "group": 2355,
            "name": "10.1.1.105.1650",
            "keyword": "",
            "title": "A technique to determine power-efficient, high-performance superscalar processors"
        },
        {
            "abstract": "Abstract. Visually pleasant texture reconstruction plays an important role in computer graphics. In this paper we propose special triangulations for texture reconstruction. We introduce two new algorithms for data-dependent triangulation. The new deterministic algorithm named image partitioning algorithm (IPA) shifts this reconstruction method closer to real usage. We present a new modification of the optimization technique simulated annealing with generalized look-ahead process (SALA). Also a new way of utilization of color information is presented. It supports achieving qualitative way of reconstruction of color images. Results obtained demonstrate both theoretical and practical superiority over another methods. This work is a part of the Virtual Bratislava research.",
            "group": 2356,
            "name": "10.1.1.105.1989",
            "keyword": "Image ReconstructionData-dependent TriangulationMinimum Weight Triangulation",
            "title": "Towards an optimal texture reconstruction"
        },
        {
            "abstract": "We propose a tuning method for MEMS gyroscopes based on evolutionary computation to efficiently increase the sensitivity of MEMS gyroscopes through tuning and, furthermore, to find the optimally tuned configuration for this state of increased sensitivity. The tuning method was tested for the second generation JPL/Boeing Post-resonator MEMS gyroscope using the measurement of the frequency response of the MEMS device in open-loop operation.",
            "group": 2357,
            "name": "10.1.1.105.2282",
            "keyword": "Track CategoryEvolvable Hardware Keywords Genetic AlgorithmsSimulated AnnealingDynamic Hill ClimbingEvolvable HardwareGyroscopeMEMS",
            "title": "Evolutionary Computation applied to the Tuning of MEMS gyroscopes."
        },
        {
            "abstract": "A novel artificial neural network heuristic (INN) for general constraint satisfaction problems is presented, extending a recently suggested method restricted to boolean variables. In contrast to conventional ANN methods, it employs a particular type of non-polynomial cost function, based on the information balance between variables and constraints in a mean-field setting. Implemented as an annealing algorithm, the method is numerically explored on a testbed of Graph Coloring problems. The performance is comparable to that of dedicated heuristics, and clearly superior to that of conventional mean-field annealing.",
            "group": 2358,
            "name": "10.1.1.105.3031",
            "keyword": "constraint satisfactiongraph coloringconnectionistartificial neural networkmean-field annealingheuristicinformation",
            "title": "An Information-Based Neural Approach to Generic Constraint"
        },
        {
            "abstract": "of primer design for the detection of",
            "group": 2359,
            "name": "10.1.1.105.3909",
            "keyword": "",
            "title": "variable genomic lesions"
        },
        {
            "abstract": "Proactive and reactive multi-dimensional histogram maintenance",
            "group": 2360,
            "name": "10.1.1.105.4552",
            "keyword": "",
            "title": "for selectivity estimation q"
        },
        {
            "abstract": "Abstract--Identifying corresponding points between two recordings of a point set has always been an important problem in stereo vision applications. We describe this matching problem in terms of cost minimization and present an algorithm to approach the minimal cost mapping using simulated annealing. The algorithm calculates the costs to match all possible point pairs and tries to minimize the sum of the costs of all matched points. Starting from an initial mapping, it uses a random rearrangement scheme to alter the mapping towards the optimal (minimal cost) mapping.",
            "group": 2361,
            "name": "10.1.1.105.4660",
            "keyword": "",
            "title": "Matching Point correspondences"
        },
        {
            "abstract": "A quantum algorithm for general combinatorial search that uses the underlying structure of the search space to increase the probability of finding a solution is presented. This algorithm shows how coherent quantum systems can be matched to the underlying structure of abstract search spaces, and is analytically simpler than previous structured search methods. The algorithm is evaluated empirically with a variety of search problems, and shown to be particularly effective for searches with many constraints. Furthermore, the algorithm provides a simple framework for utilizing search heuristics. It also exhibits the same phase transition in search difficulty as found for sophisticated classical search methods, indicating it is effectively using the problem structure. 1",
            "group": 2362,
            "name": "10.1.1.105.5010",
            "keyword": "",
            "title": "A framework for structured quantum search"
        },
        {
            "abstract": "For the traveling salesman problem various search algorithms have been suggested for decades. In the eld of genetic algorithms, many genetic operators have beenintroduced for the problem. Most genetic encoding schemes have some restrictions that cause more-or-less loss of information contained in problem instances. We suggest a new encoding/crossover pair which pursues minimal information loss in chromosomal encoding and minimal restriction in recombination for the 2D Euclidean traveling salesman problem. The most notable feature of the suggested crossover is that it is based on a totally new concept of encoding. We also prove the theoretical validity of the new crossover by an equivalence-class analysis. The proposed encoding/crossover pair outperformed both distance-preserving crossover and edge-assembly crossover, two state-of-the-art crossovers in the literature. 1",
            "group": 2363,
            "name": "10.1.1.105.6713",
            "keyword": "",
            "title": "The Natural Crossover for the 2D Euclidean TSP"
        },
        {
            "abstract": "We describe a programming methodology for computational science based on programming paradigms for multicomputers. Each paradigm is a class of algorithms that have the same control structure. For every paradigm, a general parallel program is developed. The general program is then used to derive two or more model programs, which solve specific problems in science and engineering. These programs have been tested on a Computing Surface and published with every detail open to scrutiny. We explain the steps involved in developing model programs and conclude that the study of programming paradigms provides an architectural vision of parallel scientific computing. 1",
            "group": 2364,
            "name": "10.1.1.105.6777",
            "keyword": "",
            "title": "Model Programs for Computational Science: A Programming Methodology"
        },
        {
            "abstract": "Abstract \u2014 When designing the reduction tree of a parallel multiplier, we can exploit a large intrinsic freedom for the interconnection order of partial products. The transition activities vary significantly for different internal partial products. In this work we propose a method for generation of power-efficient parallel multipliers in such a way that its partial products are connected to minimize activity. The reduction tree is designed progressively. A Simulated Annealing optimizer uses power cost numbers from a specially implemented probabilistic gate-level power estimator and selects a power-efficient solution for each stage of the reduction tree. VHDL simulation using ModelSim shows a significant reduction in the overall number of transitions. This reduction ranges from 15 % up to 32 % compared to randomly generated reduction trees and is achieved without any noticeable area or performance overhead. I.",
            "group": 2365,
            "name": "10.1.1.105.6793",
            "keyword": "",
            "title": "Power Optimized Partial Product Reduction Interconnect Ordering in Parallel Multipliers"
        },
        {
            "abstract": null,
            "group": 2366,
            "name": "10.1.1.105.7263",
            "keyword": "",
            "title": "Evolution-Based Scheduling of Computations and Communications on Distributed-Memory"
        },
        {
            "abstract": "Multiple DNA sequence alignment is an important research topic of bioinformatics. In this paper, we present a new method for multiple DNA sequence alignment, based on genetic simulated annealing techniques, to choose the best cutting point set of the multiple DNA sequence set. The experimental results show that the proposed method gets higher scores and more match columns than the method presented in [3] for dealing with the multiple DNA sequence alignment problem.",
            "group": 2367,
            "name": "10.1.1.105.7718",
            "keyword": "AlgorithmsGenetic Algorithms",
            "title": "Multiple DNA sequence alignment based on genetic algorithms and divide-and-conquer techniques"
        },
        {
            "abstract": "Logic emulation enables designers to functionally verify complex integrated circuits prior to chip fabrication. However, traditional FPGA-based logic emulators have poor inter-chip communication bandwidth, commonly limiting gate utilization to less than 20 percent. Global routing contention mandates the use of expensive crossbar and PC-board technology in a system of otherwise low-cost, commodity parts. Even with crossbar technology,current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). Virtual Wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. The resulting increase in bandwidth allows effective use of low dimension, direct interconnect. The size of the FPGA array can be decreased as well, resulting in low cost logic emulation. This paper covers major contributions of the MIT Virtual Wires project. In the context of a complete emulation system, we analyze phase-based static scheduling and routing algorithms, present Virtual Wires synthesis methodologies, and overview an operational prototype with 20Kgate boards. Results, including in-circuit emulation of a SPARC microprocessor, indicate that Virtual Wires eliminate the need for expensive crossbar technology while increasing FPGA utilization beyond 45 percent. Theoretical analysis predicts that Virtual Wires emulation scales with FPGA size and average routing distance, while traditional emulation does not. 1",
            "group": 2368,
            "name": "10.1.1.105.8085",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The exploitation of networked workstations as a set of under-utilized computational resources is an attrac-tive idea for cost effective parallel computing. The use of these resources, however, raises a number of in-teresting policy and performance questions (e.g, what impact will a parallel application have on the normal workstation user, what performance levels can be ex-pected when many users are logged on?). We present performance models for parallel computations execut-ing on shared, networked workstations that can help to address these questions. 1",
            "group": 2369,
            "name": "10.1.1.105.8507",
            "keyword": "",
            "title": "Proceedings of the 28th Annual Hawaii International Conference on System Sciences- 1995 Stealing Cycles: Can We Get Along?"
        },
        {
            "abstract": "by",
            "group": 2370,
            "name": "10.1.1.105.8831",
            "keyword": "",
            "title": "Abstract TOMOGRAPHIC RECONSTRUCTION OF LABEL IMAGES USING GIBBS"
        },
        {
            "abstract": "This paper discusses product variety design under optimization viewpoint. Product variety design means the challenge to simultaneously design multiple products toward higher optimality beyond ordinary design methods for a single product. The paper explores the possibilities of design optimization for product variety under fixed product architecture. Such optimization demands to determine the contents of modules and their combinations under modular architecture. This indicates that product variety optimization includes three classes of optimization problems, attribute assignment, module combination and simultaneous design of both. The paper formulates problem classification, domains and situations of such optimization problems. Further, the paper demonstrates two typical optimization examples through aircraft design for simultaneous attribute optimization and through design of television circuit boards for module combination, respectively. The paper concludes with the roles of problem classification and the direction of future works.",
            "group": 2371,
            "name": "10.1.1.106.491",
            "keyword": "Design optimizationProduct familyModular architectureMathematical modeling",
            "title": "Abstract PRODUCT VARIETY OPTIMIZATION UNDER MODULAR ARCHITECTURE"
        },
        {
            "abstract": "Finite element (FE) models are widely used to predict the dynamic characteristics of aerospace structures. These models often give results that differ from the measured results and therefore need to be updated to",
            "group": 2372,
            "name": "10.1.1.106.2172",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "This work analyzes the relative advantages of different metaheuristic approaches to the well known natural language processing problem of part-of-speech tagging. This consists of assigning to each word of a text its disambiguated part-of-speech according to the context in which the word is used. We have applied a classic genetic algorithm (GA), a CHC algorithm, and a Simulated Annealing (SA). Different ways of encoding the solutions to the problem (integer and binary) have been studied, as well as the impact of using parallelism for each of the considered methods. We have performed experiments on different linguistic corpora and compared the results obtained against other popular approaches plus a classic dynamic programming algorithm. Our results claim for the high performances achieved by the parallel algorithms compared to the sequential ones, and estate the singular advantages for every technique. Our algorithms and some of its components can be used to represent a new set of state-of-the-art procedures for complex tagging scenarios. Key words: Genetic algorithms, CHC algorithm, natural language processing, part-of-speech tagging, parallelism.",
            "group": 2373,
            "name": "10.1.1.106.2564",
            "keyword": "",
            "title": "Natural language tagging with parallel genetic algorithms"
        },
        {
            "abstract": "We consider the problem of coordinating the behavior of multiple self-interested agents. It involves constraint optimization problems that often can only be solved by local search algorithms. Using local search poses problems of incentivecompatibility and individual rationality. We thus define a weaker notion of bounded-rational incentive-compatibility where manipulation is made impossible with high probability through computational complexity. We observe that in real life, manipulation of complex situations is often impossible because the effect of the manipulation cannot be predicted with sufficient accuracy. We show how randomization schemes in local search can make predicting its outcome hard and thus form a bounded-rational incentive-compatible coordination algorithm. 1",
            "group": 2374,
            "name": "10.1.1.106.2705",
            "keyword": "",
            "title": "Multi-agent coordination using local search. IJCAI"
        },
        {
            "abstract": "Abstract. In order to design a quantum circuit that performs a desired quantum computation, it is necessary to find a decomposition of the unitary matrix that represents that computation in terms of a sequence of quantum gate operations. To date, such designs have either been found by hand or by exhaustive enumeration of all possible circuit topologjks. In this paper we propose an automated approach to quantum circuit design using search heuristics based on principles abstracted from evolutionary genetics, i.e. using a genetic programming algorithm adapted specially for this problem. We demonstrate the method on the task of discovering quantum circuit designs for quantum teleportation. We show that to find a given known circuit design (one which was hand-crafted by a human), the method considers roughly an order of magnitude fewer designs than naive enumeration. In addition, the method finds novel circuit designs superior to those previously known. 1",
            "group": 2375,
            "name": "10.1.1.106.3138",
            "keyword": "",
            "title": "Automated Design of Quantum Circuits"
        },
        {
            "abstract": "ii",
            "group": 2376,
            "name": "10.1.1.106.4779",
            "keyword": "",
            "title": "Acknowledgments"
        },
        {
            "abstract": "This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and \u2018blackbox \u2019 or \u2018oracle\u2019-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO. 1.",
            "group": 2377,
            "name": "10.1.1.106.4815",
            "keyword": "",
            "title": "Keywords: Monte Carlo Optimization, Black-box Optimization, Parametric Learning, Automated Annealing, Bias-variance-covariance Parametric Learning"
        },
        {
            "abstract": "Abstract. The timetabling problem consists in scheduling a sequence of lectures between teachers and students in a prefixed period of time (typically a week), satisfying a set of constraints of various types. A large number of variants of the timetabling problem have been proposed in the literature, which differ from each other based on the type of institution involved (university or school) and the type of constraints. This problem, that has been traditionally considered in the operational research field, has recently been tackled with techniques belonging also to Artificial Intelligence (e.g., genetic algorithms, tabu search, and constraint satisfaction). In this paper, we survey the various formulations of the problem, and the techniques and algorithms used for its solution. 1.",
            "group": 2378,
            "name": "10.1.1.106.5077",
            "keyword": "Key wordsTimetablingCombinatorial OptimizationSchedulingLocal Search",
            "title": "Heuristics"
        },
        {
            "abstract": "We show how coupling of local optimization processes can lead to better solutions than multi-start local optimization consisting of independent runs. This is achieved by minimizing the average energy cost of the ensemble, subject to synchronization constraints between the state vectors of the individual local minimizers. From an augmented Lagrangian which incorporates the synchronization constraints both as soft and hard constraints, a network is derived wherein the local minimizers interact and exchange information through the synchronization constraints. From the view-point of neural networks, the array can be considered as a Lagrange programming network for continuous optimization and as a cellular neural network (CNN). The penalty weights associated with the soft state synchronization constraints follow from the solution to a linear program. This expresses that the energy cost of the ensemble should maximally decrease. In this way successful local minimizers can implicitly impose their state to the others through a mechanism of master-slave dynamics re-sulting into a cooperative search mechanism. Improved information spreading within",
            "group": 2379,
            "name": "10.1.1.106.5358",
            "keyword": "",
            "title": "Running title: Intelligence and Cooperative Search Published in"
        },
        {
            "abstract": "Abstract \u2013 Optical packet switching over arbitrary physical topologies generally requires complex routing schemes coupled with contention resolving buffering. However, the relatively immature nature of optical logic devices and the limitations associated with optical buffering provide significant incentive to reduce the routing complexity and avoid optical domain contentions. This paper examines how the Manhattan Street Network (MSN) may be used to facilitate optical packet switching over arbitrary physical topologies. Several pertinent novel costs are introduced. Simulated annealing is used to deploy the MSN (near) optimally as a virtual topology in the physical topologies. Favourable and encouraging results are obtained, indicating that deploying the MSN is an attractive means to switch packets optically over arbitrary topologies. I.",
            "group": 2380,
            "name": "10.1.1.106.6721",
            "keyword": "",
            "title": "Ultrafast Optical Packet Switching over Arbitrary Physical Topologies using the Manhattan Street Network"
        },
        {
            "abstract": "This paper presents the results of a three year research program to develop an automated test-data generation framework to support the testing of safety-critical software systems. The generality of the framework comes from the exploitation of domain independent search techniques, allowing new test criteria to be addressed by constructing functions that quantify the suitability of test-data against the test-criteria. The paper presents four applications of the framework \u2014 specification falsification testing, structural testing, exception condition testing and worst-case execution time testing. The results of three industrial scale case-studies are also presented to show that the framework offers useful support in the development safety-critical software systems. 1",
            "group": 2381,
            "name": "10.1.1.106.7359",
            "keyword": "",
            "title": "A Search Based Automated Test-Data Generation Framework for High-Integrity Systems"
        },
        {
            "abstract": "Voxel coloring methods reconstruct a three-dimensional volumetric surface model from a set of calibrated twodimensional photographs taken of a scene. In this paper, we recast voxel coloring as an optimization problem, the solution of which strives to minimize reprojection error, which measures how well projections of the reconstructed scene reproduce the photographs. The reprojection error, defined in image space, guides the refinement of the scene reconstruction in object space. Unlike previous voxel coloring methods, ours makes better use of all color information from all viewpoints, and thereby produces higher quality reconstructions. In addition, it allows voxels to be added to, not just removed from, the scene at any time during reconstruction. We examine methods to minimize the reprojection error, including greedy and simulated annealing techniques. Reconstructions of both synthetic and real scenes are presented and analyzed. 1",
            "group": 2382,
            "name": "10.1.1.106.9091",
            "keyword": "",
            "title": "Improved voxel coloring via volumetric optimization"
        },
        {
            "abstract": "ii dedicated to Sandi",
            "group": 2383,
            "name": "10.1.1.106.9855",
            "keyword": "LIST OF FIGURES.....................................................................................",
            "title": "AND NAVIGATION OF AUTONOMOUSAIRCRAFT"
        },
        {
            "abstract": "Abstract. This paper presents a greedy randomized adaptive search procedure (GRASP) to reconstruct aircraft routings in response to groundings and delays experienced over the course of the day. Whenever the schedule is disrupted, the immediate objective of the airlines is to minimize the cost of reassigning aircraft to flights taking into account available resources and other system constraints. Associated costs are measured by flight delays and cancellations. In the procedure, the neighbors of an incumbent solution are generated and evaluated, and the most desirable are placed on a restricted candidate list. One is selected randomly and becomes the incumbent. The heuristic is polynomial with respect to the number of flights and aircraft. This is reflected in our computational experience with data provided by Continental Airlines. Empirical results demonstrate the ability of the GRASP to quickly explore a wide range of scenarios and, in most cases, to produce an optimal or near-optimal solution.",
            "group": 2384,
            "name": "10.1.1.107.419",
            "keyword": "GRASPairline schedulingreal-time controlirregular operations",
            "title": "A GRASP for aircraft routing in response to groundings and delays"
        },
        {
            "abstract": "Abstract. In this paper, two heuristic optimization techniques are tested and compared in the application of motion planning for autonomous agricultural vehicles: Simulated Annealing and Genetic Algorithms. Several preliminary experimentations are performed for both algorithms, so that the best neighborhood definitions and algorithm parameters are found. Then, the two tuned algorithms are run extensively, but for no more than 2000 cost function evaluations, as run-time is the critical factor for this application. The comparison of the two algorithms showed that the Simulated Annealing algorithm achieves the better performance and outperforms the Genetic Algorithm. The final optimum found by the Simulated Annealing algorithm is considered to be satisfactory for the specific motion planning application.",
            "group": 2385,
            "name": "10.1.1.107.449",
            "keyword": "Key wordsAutonomous agricultural vehiclesGenetic algorithmsHeuristic optimizationMotion planningSimulated annealing",
            "title": "\u00a9 2002 Kluwer Academic Publishers. Printed in the Netherlands. Heuristic optimization methods for motion planning of autonomous agricultural vehicles"
        },
        {
            "abstract": null,
            "group": 2386,
            "name": "10.1.1.107.1327",
            "keyword": "",
            "title": "An Introduction to Dimensionality Reduction Using Matlab"
        },
        {
            "abstract": "Abstract. In this paper we present a Monte Carlo localization algorithm that exploits 3D information obtained by a trinocular stereo camera. First, we obtain a 3D map by estimating the optimal transformations between two consecutive views of the environment through the minimization of an energy function. Then, we use a particle-filter algorithm for addressing the localization in the map. For that purpose we define the likelihood of each sample as depending not only on the compatibility of its 3D perception with that of the observation, but also depending on its compatibility in terms of visual appearance. Our experimental results show the success of the algorithm both in easy and quite ambiguous settings, and they also show the speed-up in convergence when visual appearance is added to depth information. 1",
            "group": 2387,
            "name": "10.1.1.107.1833",
            "keyword": "",
            "title": "Robot Vision Group Departamento de Ciencia de la Computaci\u00f3n e Inteligencia Artificial"
        },
        {
            "abstract": "Metaheuristics are very useful methods because they can find (approximate) solutions of a great variety of problems. One of them, which interests us, is graph partitioning. We present a new metaheuristic based on nuclear fusion and fission of atoms. This metaheuristic, called fusion fission, is compared to other classical algorithms. First, we present spectral and multilevel algorithms which are used to solve partitioning problems. Secondly, we present two metaheuristics applied to partitioning problems: simulated annealing and ant colony algorithms. We will show that fusion fission gives good results, compared to the other algorithms. We demonstrate on a problem of Air Traffic Control that metaheuristics methods can give better results than specific methods. 1 The partitioning problem Graph partitioning is a fundamental problem for many scientists. This problem arises in many graphs applications and consists in dividing the vertices into several sets of roughly equal \u201csize \u201d in such a way that the weight of edges between sets is as small as possible. A classical application of graph partitioning is parallel computing, to reduce the communication between processors. But applications of graph partitioning include also VLSI design, data clustering, image segmentation, and mesh partitioning of a 2D surface of an airfoil. A new partitioning problem consists of a new organization of the European airspace. This is the Functional",
            "group": 2388,
            "name": "10.1.1.107.3301",
            "keyword": "",
            "title": "A metaheuristic based on fusion and fission for partitioning problems"
        },
        {
            "abstract": "Numerical modelling is now used routinely to make predictions about the behaviour of environmental systems. Model calibration remains a critical step in the modelling process and different approaches have been taken to develop guidelines to support engineers and scientists in this task. This article reviews currently available guidelines for a river hydraulics modeller by dividing them into three types: on the calibration process, on hydraulic parameters, and on the use of hydraulic simulation codes. The article then presents an integration of selected guidelines within a knowledge-based calibration support system. A prototype called CaRMA-1 (Calibration of River Model Assistant) has been developed for supporting the calibration of models based on a specific 1D code. Two case studies illustrate the ability of the prototype to face operational situations in river hydraulics engineering, for which both data quality and quantity are not sufficient for an optimal calibration. Using CaRMA-1 allows the modeller to achieve the calibration task in accordance with good calibration practice implemented in the knowledge base. Relevant reasoning rules can easily be added to the knowledge base to extend the prototype range of applications. This study thus provides a framework for building operational support tools from various types of existing engineering guidelines.",
            "group": 2389,
            "name": "10.1.1.107.3478",
            "keyword": "River hydraulicsModel calibrationKnowledge-based systemGood practiceDecision support",
            "title": "Abstract River model calibration, from guidelines to operational support tools"
        },
        {
            "abstract": "Copyright 2003, 2004 by Daniel Barker. Permission is granted to copy and use this document provided that no fee is charged for it and provided that this copyright notice is not removed. Supplementary Information for the paper: Barker, D., 2004. LVB: parsimony and simulated annealing in the search for phylogenetic trees. Bioinformatics, 20, 274\u2013275. Why use a heuristic? The average size of phylogenetic analyses is increasing. Research is being performed on relationships among large numbers of species and large gene families. However, many of the popular methods of evaluating trees, including parsimony (Fitch 1971) and maximum likelihood (Felsenstein 1981), are so computationally intensive that an exact solution is not possible for more than about 20 sequences. For larger numbers of sequences, heuristic methods may give a result similar to the exact solution, but in much shorter time. Heuristics used in phylogenetic inferrence include a greedy algorithm (stepwise addition, Swofford 1993), hill climbing (Croes 1958), simulated annealing (Kirkpatrick et al. 1983, Lundy 1985, Salter and Pearl 2001) and genetic algorithms (Holland 1975, Lewis 1998). Why use simulated annealing? A popular heuristic for seeking parsimonious trees is stepwise addition followed by hill climbing. This combination is implemented by, for example, DNAPARS in the PHYLIP package (Felsenstein 1989) and PAUP (Swofford 1993, 2002). However, as the \u2018search space \u2019 becomes more complicated, many random re-starts are required, since each search may become \u2018stuck \u2019 in a local optimum. Simulated annealing is able to \u2018jump out \u2019 of local optima in a way that stepwise addition and hill climbing cannot, by occasionally accepting a worse tree during the course of the search. One or a small number of simulated annealing searches may be more economical than stepwise addition and hill climbing with many re-starts. Implementation LVB 2.0 is written in ANSI C. Internal documentation is provided by structured comments in POD format, which may be extracted to HTML files by pod2html (e.g., by using the supplied Makefile). Data matrix input code is re-used from PHYLIP <http://evolution.genetics.washington.edu/phylip.html>. LVB\u2019s source distribution includes documentation of some PHYLIP internals. LVB has an automated test suite, which uses Perl to launch LVB and collate results. The test suite is expected to grow",
            "group": 2390,
            "name": "10.1.1.107.5879",
            "keyword": "",
            "title": "LVB: parsimony and simulated annealing in the search for phylogenetic trees"
        },
        {
            "abstract": "We propose a new method for training an ensemble of neural networks. A population of networks is created and maintained such that more probable networks replicate and less probable networks vanish. Each individual network is updated using random weight changes. This produces a diversity among the networks which is important for the ensemble prediction using the population. The method is compared against Bayesian learning for neural networks, Bagging and a simple neural network ensemble, on three datasets. The results show that the population method can be used as an efficient neural network learning algorithm. I.",
            "group": 2391,
            "name": "10.1.1.107.8492",
            "keyword": "",
            "title": "A New Learning Scheme for Neural Network Ensembles"
        },
        {
            "abstract": "Abstract\u2014Statistical-mechanical modeling of digital halftoning is proposed. The digital halftoning dealt with here is achieved by making use of the threshold mask, and for each pixel, the halftoned pixel is determined as black (set to one) if the original grayscale pixel is greater than or equal to the mask value and is determined as white (set to zero) vice versa. Basically, our method is a kind of the so-called model-based digital halftoning technique and we use the information about the original grayscale image to generate the threshold mask. To determine the optimal value of the mask on each pixel for a given original grayscale image, we first assume that the human-eyes might recognize the black and white binary halftoned image as the corresponding grayscale one by linear filters. Then, the energy function is constructed as the distance between the original and the recognized images which is written in terms of the threshold mask. By minimizing the energy function via simulated annealing, we obtain the optimal threshold mask and the resultant halftoned binary dots simultaneously. From the power-spectrum analysis, we find that the resultant binary dots image is physiologically plausible from the view point of human-eyes modulation properties. Key words\u2014Image processing, Model-based digital halftoning",
            "group": 2392,
            "name": "10.1.1.108.462",
            "keyword": "techniqueStatistical mechanicsMarkov Chain Monte Carlo methodsSimulated annealingComputer visionCombinatorial optimization problemPower-spectrum",
            "title": "Statistical-Mechanical Modeling of Digital Halftoning"
        },
        {
            "abstract": "Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard. We describe deterministic annealing (Rose et al., 1990) as an appealing alternative to the Expectation-Maximization algorithm (Dempster et al., 1977). Seeking to avoid search error, DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1",
            "group": 2393,
            "name": "10.1.1.108.506",
            "keyword": "",
            "title": "2004. Annealing techniques for unsupervised statistical language learning"
        },
        {
            "abstract": "",
            "group": 2394,
            "name": "10.1.1.108.618",
            "keyword": "",
            "title": "Bayesian Ranking of Biochemical System Models"
        },
        {
            "abstract": "Abstract: Real world timetabling applications are usually diverse in their problem structures, constraints and algorithms used. We have developed an object oriented timetabling framework that adapts to varied problem structures, and allows for easy and flexible maintenance of timetabling algorithms and constraints. We use the Unified Modelling Language for problem structure representations and Object Constraint Language for constraint expressions. The model is easier to understand as compared to mathematical formulations and results in coherent development from problem specification to software constructions. We also describe an application that we have instantiated from this framework. 1",
            "group": 2395,
            "name": "10.1.1.108.1794",
            "keyword": "",
            "title": "Singapore Technologies Electronics Limited"
        },
        {
            "abstract": "to solve inferential problems with complex models and problematic data. This is an enormously powerful set of tools based on replacing difficult or impossible analytical work with simulated empirical draws from the distributions of interest. Although practitioners are generally aware of the importance of convergence of the Markov chain, many are not fully aware of the difficulties in fully assessing convergence across multiple dimensions. In most applied circumstances, every parameter dimension must be converged for the others to converge. The usual culprit is slow mixing of the Markov chain and therefore slow convergence towards the target distribution. This work demonstrates the partial convergence problem for the two dominant algorithms and illustrates these issues with empirical examples. 1",
            "group": 2396,
            "name": "10.1.1.108.2694",
            "keyword": "",
            "title": "Political Analysis Advance Access published August 19, 2007 Is Partial-Dimension Convergence a Problem for Inferences from MCMC Algorithms?"
        },
        {
            "abstract": "Summary. The Nested Partitions (NP) method has been proven to be a useful framework for effectively solving large-scale discrete optimization problems. In this paper, we provide a brief review of the NP method and its applications. We then present a hybrid algorithm that integrates mathematical programming lower bound into the sampling procedure of the NP framework. The efficiency of the hybrid algorithm is demonstrated by the Intermodal Hub Location Problem (IHLP), a class of discrete facility location problems. Computational results show that the hybrid approach is superior to the integer programming approach and the Lagrangian relaxation approach.",
            "group": 2397,
            "name": "10.1.1.108.3516",
            "keyword": "Key wordsDiscrete OptimizationNested PartitionsIntermodal Hub Location Problem 1 Discrete Optimization",
            "title": "Nested Partitions and Its Applications to the Intermodal Hub Location Problem"
        },
        {
            "abstract": "The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training neural networks to evaluate Go positions via temporal difference (TD) learning. Our approach is based on neural network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by self-play alone. A network with less than 500 weights learned within 3 000 games of 9x9 Go a position evaluation function superior to that of a commercial Go program.",
            "group": 2398,
            "name": "10.1.1.108.4486",
            "keyword": "",
            "title": "Learning To Evaluate Go Positions Via Temporal Difference Methods"
        },
        {
            "abstract": "I had the great pleasure of working with Dr. Anamitra Makur during my stay at the Institute. But for him, this thesis would not have seen light. I would like to thank him for the help he has rendered during this work. I was fortunate to have had Gaggan, Mouli, Suba, Selvi, Doss and Vinay as labmates who had been excellent people to work with. I would like to thank Sitaram, Vasuki, Ganesh and Suryan for their help. I would also like to thank Venn, Gopal, Snajay, Kathy, Delay, Pattar, Babu, Manoj, Jyothish and Jemlin for making my stay at the Institute a happy and memorable one. I would also like to thank Ramu, Pozhan, Eapen, Uncle, Vappan, Vaman, Anil and Sunil, Prahh and Mala who had been always helpful to me. I dso remember many a afternoon (Saturday special) sessions with Babu, Kannan, Padi-yar, Madhu, Jeby, Joy and Ajith. Finally I would like to thank my parents for their constant encouragement throughout the course.",
            "group": 2399,
            "name": "10.1.1.108.4844",
            "keyword": "",
            "title": "Acknowledgements"
        },
        {
            "abstract": "www.elsevier.com/locate/cpc A new stochastic clustering algorithm is introduced that aims to locate all the local minima of a multidimensional continuous and differentiable function inside a bounded domain. The accompanying software (MinFinder) is written in ANSI C++. However, the user may code his objective function either in C++, C or Fortran 77. We compare the performance of this new method to the performance of Multistart and Topographical Multilevel Single Linkage Clustering on a set of benchmark problems. Program summary",
            "group": 2400,
            "name": "10.1.1.108.5011",
            "keyword": "",
            "title": "MinFinder: Locating all the local minima of a function \u2729"
        },
        {
            "abstract": "This paper presents a systematic model, two-phase optimization algorithms (TPOA), for Mastermind. TPOA is not only able to efficiently obtain approximate results but also effectively discover results that are getting closer to the optima. This systematic approach could be regarded as a general improver for heuristics. That is, given a constructive heuristic, TPOA has a higher chance to obtain results better than those obtained by the heuristic. Moreover, it sometimes can achieve optimal results that are difficult to find by the given heuristic. Experimental results show that (i) TPOA with parameter setting (k, d) 5 (1, 1) is able to obtain the optimal result for the game in the worst case, where k is the branching factor and d is the exploration depth of the search space. (ii) Using a simple heuristic, TPOA achieves the optimal result for the game in the expected case with (k, d) 5 (180, 2). This is the first approximate approach to achieve the optimal result in the expected case.  ",
            "group": 2401,
            "name": "10.1.1.108.5190",
            "keyword": "",
            "title": "  A Two-Phase Optimization Algorithm For Mastermind"
        },
        {
            "abstract": "Some of the most influential decisions about a software system are made in the early phases of the software development life cycle. Those decisions about requirements and design are generally made by teams of software engineers and domain experts who must weigh the complex interactions among requirements and the associated developmental and operational risks of those requirements. Some of these early life cycle decisions are more influential, or perhaps fateful, to subsequent software design and development than are others. When debating about complex systems with a large number of options, humans can often be slower than an AI system at identifying the clusters of key decisions that give the most leverage. By focusing a group of human domain experts or software engineers on these key decision clusters, more time can be devoted to these pivotal decisions and less time is wasted on irrelevancies. 1",
            "group": 2402,
            "name": "10.1.1.108.7746",
            "keyword": "",
            "title": "Improved software engineering decision support through automatic argument reduction tools"
        },
        {
            "abstract": "VLSI standard cell placement is the process of arranging circuit components (modules) on a silicon layout. The cell placement problem is a proven NP hard combinatorial optimization problem. The complexity of this problem increases when multiple optimization objectives are considered simultaneously. In this paper, a novel technique is presented to address this hard problem, while optimizing multiple objectives. A major difficulty with such multi-objective combinatorial optimization problems is the existence of a very large solution search space, within which is the desired optimal solution. Simulated Evolution (SE) a general iterative heuristic is used to traverse the large search space, while fuzzy logic",
            "group": 2403,
            "name": "10.1.1.108.8501",
            "keyword": "",
            "title": "Mixed Signal Systems &Verification,"
        },
        {
            "abstract": "Simulated annealing for multi-robot hierarchical task allocation with flexible constraints and",
            "group": 2404,
            "name": "10.1.1.108.8636",
            "keyword": "multi-robottask allocation",
            "title": "objective"
        },
        {
            "abstract": "biannually in May and November. Publication is via digital media and available for viewing or download from the journal\u2019s web site at",
            "group": 2405,
            "name": "10.1.1.108.8894",
            "keyword": "www.ijamt.org",
            "title": "Applied Management and Technology"
        },
        {
            "abstract": " ",
            "group": 2406,
            "name": "10.1.1.108.9310",
            "keyword": "",
            "title": "On the Acceleration of Simulated Annealing"
        },
        {
            "abstract": "In this paper, a functions localized network with branch gates (FLN-bg) is studied, which consists of a basic network and a branch gate network. The branch gate network is used to determine which intermediate nodes of the basic network should be connected to the output node with a gate coefficient ranging from 0 to 1. This determination will adjust the outputs of the intermediate nodes of the basic network depending on the values of the inputs of the network in order to realize a functions localized network. FLN-bg is applied to function approximation problems and a two-spiral problem. The simulation results show that FLN-bg exhibits better performance than conventional neural networks with comparable complexity.",
            "group": 2407,
            "name": "10.1.1.109.685",
            "keyword": "Universal learning networksNeural networksFuzzy networksFunctions localizationBranch gate",
            "title": "A functions localized neural network with branch gates"
        },
        {
            "abstract": "This paper reports on an investigation into the feasibility of using active and passive means of vibration control in aerospace structures. In particular, attention is focused on controlling vibration transmission through light weight satellite structures at medium frequencies. The initial structure under test here is a 4.5-meter long satellite boom consisting of 10 identical bays with equilateral triangular cross sections. This structure is typical of those that might be used in space telescopes, space stations or synthetic aperture radar systems. Such a structure is typically used to support sensitive instruments in precise alignments spaced tens of metres apart. While a great deal of work has been done on this problem at low frequencies, relatively little has been achieved to date at medium frequencies (here taken to be between 150 Hz and 250 Hz). Nonetheless, this is of importance to new space missions. Using the techniques described here, an overall reduction in vibration transmission of 31.0 dB is achieved in an essentially undamped structure using passive means alone. The amounts of attenuation achievable for active control with one, two and three actuators are found to be 15.1 dB, 26.1 dB and 33.5 dB, respectively. With the combined passive control (using 10 % geometric deviations) and active control (using",
            "group": 2408,
            "name": "10.1.1.109.725",
            "keyword": "Structural vibrationPassive controlActive controlOptimizationGenetic Algorithm",
            "title": "The integration of advanced active and passive structural noise control methods"
        },
        {
            "abstract": "Probabilistic roadmap (PRM) planner applied to the robot with multiple degrees of freedom moving in static environment with static obstacles is proved to be efficient. But it is clumsy to environment changes, and the achieved path is usually not optimal and not smooth enough. In order to make the planner optimal and more flexible, this paper integrates the PRM planner with simulated annealing (SA) to constitute a two-stage path planner: at first a path is generated by PRM planner, then this original path is optimized by SA. If the original path becomes infeasible because of environment changes, a feasible path can also be obtained from the original path by SA. The simulations are carried out to verify the efficiency of the proposed planner.",
            "group": 2409,
            "name": "10.1.1.109.1954",
            "keyword": "Path PlanningProbabilistic RoadmapSimulated Annealing",
            "title": "An Optimal Two-Stage Path Planner"
        },
        {
            "abstract": "Optimal cost design of water distribution networks",
            "group": 2410,
            "name": "10.1.1.109.2073",
            "keyword": "Water distribution networkHarmony searchMeta-heuristic algorithm",
            "title": "using"
        },
        {
            "abstract": "The problem of the reconstruction of binary matrices from their fan-beam projections is investigated here. A fan-beam projection model is implemented and afterwards employed in systematic experiments to determine the optimal parameter values for a data acquisition and reconstruction algorithm. The fan-beam model, the reconstruction algorithm which uses the optimization method of Simulated Annealing, the simulation experiments, and the results are then discussed in turn. 1",
            "group": 2411,
            "name": "10.1.1.109.2131",
            "keyword": "",
            "title": "Reconstruction of binary matrices from fan-beam projections"
        },
        {
            "abstract": "This paper presents a GA algorithm for function optimization that combines the features of Simplex crossover and Simulated Aannealing. Hybridization with a local search tool like Simplex directs the GA to more promising search spaces while preventing premature local minima convergence. The use of Simulated Annealing(SA) as a mutation operator further enhances the GA's overall capability. This elitist real-coded multi-parent model is able to address the issue of slow convergence rate very well and is robust in optimizing epistatic multimodal problems. In addition to test functions, our model was applied to the resolution of overlapping signals.",
            "group": 2412,
            "name": "10.1.1.109.3230",
            "keyword": "Genetic AlgorithmsSimplexSimulated AnnealingHybrid AlgorithmsOverlapping Signals",
            "title": "A Hybrid Optimization Method Using a Real-coded Multi-parent GA, Simplex & Simulated Annealing with Applications in Resolution of Overlapped Signals*1*"
        },
        {
            "abstract": "Abstract. In this paper a Simulated Annealing algorithm (SA) for solving the Protein Folding Problem (PFP) is presented. This algorithm has two phases: quenching and annealing. The first phase is applied at very high temperatures and the annealing phase is applied at high and low temperatures. The temperature during the quenching phase is decreased by an exponential function. We run through an efficient analytical method to tune the algorithm parameters. This method allows the change of the temperature in accordance with solution quality, which can save large amounts of execution time for PFP.",
            "group": 2413,
            "name": "10.1.1.109.3300",
            "keyword": "PeptideProtein Folding",
            "title": "Analytically Tuned Simulated Annealing applied to the Protein Folding Problem"
        },
        {
            "abstract": "The combination of local search operators, problem specific information and a genetic algorithm has provided very good results in certain scheduling problems, particularly in timetabling and maintenance scheduling problems. The resulting algorithm from this hybrid approach has been termed a Memetic Algorithm. This paper investigates the use of such an algorithm for the scheduling of transmission line maintenance for a known problem that has been addressed in the literature using a combination of a genetic algorithm and greedy optimisers. This problem is concerned with the scheduling of maintenance for an electricity transmission network where every transmission line must be maintained once within a specified time period. The objective is to avoid situations where sections of the network are disconnected, and to minimise the overloading of lines which are in service. In this paper we look at scheduling maintenance for the South Wales region of the national transmission network. We present and discuss, in some detail, a memetic algorithm that incorporates local search operators including tabu search and simulated annealing. A comparison is made both with the results from previous work, and against a selection of optimising techniques. The approach presented in this paper shows a significant improvement over previously published results on previously tackled problems. We also present results on another problem which has not been tackled in the literature but which is closer to the real world maintenance scheduling problems faced by such companies as The National Grid Company plc using the South Wales region.",
            "group": 2414,
            "name": "10.1.1.109.3650",
            "keyword": "Categories and Subject DescriptorsI.2.8 [Artificial IntelligenceProblem SolvingControl Methodsand Search\u2014Heuristic methicsScheduling General TermsMaintenance Scheduling Additional Key Words and PhrasesHeuristicsmemetic algorithmstabu searchsimulated annealinghill climbing NameE. K. Burke",
            "title": "The RSA Security Inc - http://www.rsasecurity.com"
        },
        {
            "abstract": "This dissertation concerns the principles and techniques for scalable evolutionary computation to achieve better solutions for larger problems with more computational resources. It suggests that many of the limitations of existent evolutionary algorithms, such as premature convergence, stagnation, loss of diversity, lack of reliability and efficiency, are derived from the fundamental convergent evolution model, the oversimplified \u201csurvival of the fittest\u201d Darwinian evolution model. Within this model, the higher the fitness the population achieves, the more the search capability is lost. This is also the case for many other conventional search techniques. The main result of this dissertation is the introduction of a novel sustainable evolution model, the Hierarchical Fair Competition (HFC) model, and corresponding five sustainable evolutionary algorithms (EA) for evolutionary search. By maintaining individuals in hierarchically organized fitness levels and keeping evolution going at all fitness levels, HFC transforms the conventional convergent evolutionary computation model into a sustainable search framework by ensuring a continuous supply and incorporation of low-level building blocks and by culturing and maintaining building blocks of intermediate levels with its",
            "group": 2415,
            "name": "10.1.1.109.4637",
            "keyword": "",
            "title": " SUSTAINABLE EVOLUTIONARY ALGORITHMS AND SCALABLE EVOLUTIONARY SYNTHESIS OF DYNAMIC SYSTEMS "
        },
        {
            "abstract": "Geospatial data is often used to predict or recommend movements of robots, people, or animals (\u2018walkers\u2019). Analysis of such systems can be combinatorially explosive. Each decision that a walker makes generates a new set of possible future decisions, and the tree of possible futures grows exponentially. Complete enumeration of alternatives is out of the question. One approach that we have found promising is to instantiate a large population of simple computer agents that explore possible paths through the landscape. The aggregate behaviour of this swarm of agents estimates the likely behaviour of the real-world system. This paper will discuss techniques that we have found useful in swarming geospatial reasoning, illustrate their behaviour in specific cases, compare them with existing techniques for path planning, and discuss the application of such systems.",
            "group": 2416,
            "name": "10.1.1.109.5799",
            "keyword": "Swarmingsoftware agentspath planningmovement prediction",
            "title": "Forthcoming in International Journal of Geographic Information Science Swarming methods for geospatial reasoning"
        },
        {
            "abstract": "Since the appearance of Shor\u2019s factoring algorithm in 1994, the search for novel quantum computer algorithms has proved surprisingly difficult. Two design approaches that have yielded some progress are quantum walks and adiabatic computing. The former has been shown to speed up algorithms whose complexity is related to the classical hitting time of a symmetric Markov chain, and there is evidence that the latter speeds up simulated annealing algorithms for computing ground states of classical Hamiltonians. In this thesis, we look into the possibility of obtaining a quantum speedup for the mixing time of a symmetric Markov chain. We prove that by subjecting a quantum walk to a small amount of decoherence (typically the adversary of a quantum computer), it can be forced to mix to the correct stationary distribution, often considerably faster than its classical counterpart. A more general theorem to this effect would imply quantum speedups for a variety of approximation algorithms for #P-complete problems. We conclude with some observations on adiabatic computing \u2013 a time-dependent generalization of the quantum walk framework \u2013 and the problem of estimating the ground state energy of a quantum Hamiltonian with local spin interactions. ii Acknowledgements I would like to thank Mario Szegedy for his support and guidance of my thesis research. His range and creativity as a researcher are as inspiring to me as they are impressive. I have benefited greatly from the instruction, advice, and good nature of the Rutgers",
            "group": 2417,
            "name": "10.1.1.109.5937",
            "keyword": "",
            "title": "ABSTRACT OF THE DISSERTATION Quantum walks and ground state problems"
        },
        {
            "abstract": "Abstract\u2013\u2013In this paper, a corner block list \u2014 a new efficient topological representation for non-slicing floorplan is proposed with applications to VLSI floorplan and building block placement. Given a corner block list, it takes only linear time to construct the floorplan. Unlike the O-tree structure, which determines the exact floorplan based on given block sizes, corner block list defines the floorplan independent of the block sizes. Thus, the structure is better suited for floorplan optimization with various size configurations of each block. Based on this new structure and the simulated annealing technique, an efficient floorplan algorithm is given. Soft blocks and the aspect ratio of the chip are taken into account in the simulated annealing process. The experimental results demonstrate the algorithm is quite promising. 1.",
            "group": 2418,
            "name": "10.1.1.109.7193",
            "keyword": "",
            "title": "Corner block list: An effective and efficient topological representation of non-slicing floorplan"
        },
        {
            "abstract": "Location area (LA) planning plays an important role in cellular networks because of the tradeoff caused by paging and registration signalling. The upper boundary for the size of an LA is the service area of a Mobile services Switching Center (MSC). In that extreme case, the cost of paging is at its maximum but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal but the cost of registration is the largest. Between these extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we seek to determine the location areas in an optimum fashion. Cell to switch assignments are also determined to achieve the minimization of the network cost. For that purpose, we use the available network information to formulate a realistic optimization problem, and propose an algorithm based on simulated annealing (SA) for its solution. Then, we investigate the quality of the SA-based technique by comparing it to greedy search, random generation methods and a heuristic algorithm.",
            "group": 2419,
            "name": "10.1.1.109.7405",
            "keyword": "Index Terms Location AreaLocation ManagementLocation TrackingLocation UpdatePaging AreaSimulated Annealing",
            "title": "IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, TO APPEAR 1 Location Area Planning and Cell-to-Switch Assignment in Cellular Networks"
        },
        {
            "abstract": "Artificial Intelligence can be thought of as the study of machines that are capable of solving problems that require human level intelligence. It has frequently been concerned with game playing. In this thesis we shall focus on the areas of search and complexity, with respect to single-player games. These types of games are called puzzles. Puzzles have received research attention for some decades. Consequently, inter-esting insights into some of these puzzles, and into the approaches for solving them, have emerged. However, many of these puzzles have been neglected by the artificial intelligence research community. Therefore, we survey these puzzles in the hope that we can motivate research towards them so that further interesting insights might emerge in the future. We describe research on a puzzle called LEMMINGS that is derived from a game called Lemmings, which itself is in NP-Complete. We attempt to find the first success-ful approach for LEMMINGS. We report on a successful approach to a sub-problem",
            "group": 2420,
            "name": "10.1.1.109.7654",
            "keyword": "",
            "title": "The Lemmings Puzzle: Computational Complexity of an Approach and Identification of Difficult Instances"
        },
        {
            "abstract": "How can an intelligent agent learn an effective representation of its world? This dissertation applies the psychological principle of cognitive economy to the problem of representation in reinforcement learning. Psychologists have shown that humans cope with difficult tasks by simplifying the task domain, focusing on relevant features and generalizing over states of the world which are \u201cthe same\u201d with respect to the task. This dissertation defines a principled set of requirements for representations in reinforcement learning, by applying these principles of cognitive economy to the agent's need to choose the correct actions in its task.\r\n\r\nThe dissertation formalizes the principle of cognitive economy into algorithmic criteria for feature extraction in reinforcement learning. To do this, it develops mathematical definitions of feature importance, sound decisions, state compatibility, and necessary distinctions, in terms of the rewards expected by the agent in the task. The analysis shows how the representation determines the apparent values of the agent's actions, and proves that the state compatibility criteria presented here result in representations which satisfy a criterion for task learnability.\r\n\r\nThe dissertation reports on experiments that illustrate one implementation of these ideas in a system which constructs its representation as it goes about learning the task. Results with the puck-on-a-hill task and the pole-balancing task show that the ideas are sound and can be of practical benefit. The principal contributions of this dissertation are a new framework for thinking about feature extraction in terms of cognitive economy, and a demonstration of the effectiveness of an algorithm based on this new framework.",
            "group": 2421,
            "name": "10.1.1.110.1058",
            "keyword": "",
            "title": "Cognitive Economy and the Role of Representation in On-Line Learning"
        },
        {
            "abstract": "convex optimization in the bandit setting:",
            "group": 2422,
            "name": "10.1.1.110.1171",
            "keyword": "",
            "title": "gradient descent without a"
        },
        {
            "abstract": "4 December 2005 This article presents the empirical evaluation of several simple metaheuristics applied to solve the Generalized Steiner Problem (GSP). This problem models the design of high-reliability communication networks, demanding a variable number of independent paths linking each pair of terminal nodes. GSP solutions are built using intermediate nodes for guaranteeing path redundancy, while trying to minimize the design total cost. The GSP is a NP-hard problem, and few algorithms have been proposed to solve it. In this work, we present the resolution of several GSP instances whose optimal solutions are known, using metaheuristic techniques. The comparative analysis shows promising results for some of the studied techniques.",
            "group": 2423,
            "name": "10.1.1.110.2038",
            "keyword": "metaheuristicsGeneralized Steiner Problem",
            "title": ""
        },
        {
            "abstract": "Abstract \u2014 In this paper, we describe a novel localization algorithm for ad hoc wireless sensor networks. Accurate selforganization and localization capability is a highly desirable characteristic of wireless sensor networks. Many researchers have approached the localization problem from different perspectives. A major problem in wireless sensor network localization is the flip ambiguity, which introduces large errors in the location estimates. In this paper, we propose a two phase localization method based on the simulated annealing technique to address the issue. Simulated annealing is a technique for combinatorial optimization problems and unlike the gradient search method, it is robust against being trapped into local minima. In this paper we show that our simulated annealing based localization method can be used in ad hoc wireless sensor networks to estimate the location of nodes accurately. In the first phase of our algorithm, simulated annealing is used to obtain an accurate estimate of location. Then a second phase of optimization is performed only on those nodes that are likely to have flip ambiguity problem. Based on the neighborhood information of nodes, those nodes likely to have been affected by flip ambiguity are identified and moved to the correct position. The proposed scheme is tested using simulation on a sensor network of 200 nodes whose distance measurements are corrupted by Gaussian noise. Simulation results show that the proposed novel scheme gives accurate and consistent location estimates of the nodes, and mitigate errors due to flip ambiguity. The performance of the proposed algorithm is better than the performance of some well-known schemes such as DVhop method and convex optimization based semi-definite programming method. Index Terms \u2014 wireless sensor network, localization, flip ambiguity, simulated annealing I.",
            "group": 2424,
            "name": "10.1.1.110.2833",
            "keyword": "",
            "title": "Simulated Annealing based Wireless Sensor Network Localization"
        },
        {
            "abstract": "Abstract. New global optimization heuristics, Simulated Annealing,(SA) and Tabu",
            "group": 2425,
            "name": "10.1.1.110.4156",
            "keyword": "",
            "title": "Inversion Process Driven by Global Optimization Heuristics Coupled with MESSINE, a Fast-Running forward Model, volume IV of Electromagnetic Nondestructive Evaluation, pages 175\u2013180. S.S. Upda et al"
        },
        {
            "abstract": "In this paper we develop a scientific approach to control inter-country conflict. This system makes use of a neural network and a feedback control approach. It was found that by controlling the four controllable inputs: Democracy, Dependency, Allies and Capability simultaneously, all the predicted dispute outcomes could be avoided. Furthermore, it was observed that controlling a single input Dependency or Capability also avoids all the predicted conflicts. When the influence of each input variable on conflict is assessed, Dependency, Capability, and Democracy emerge as key variables that influence conflict.",
            "group": 2426,
            "name": "10.1.1.110.4879",
            "keyword": "Artificial intelligenceControlDecision support systemsInterstate conflict",
            "title": "An Integrated Human-Computer System for Controlling Interstate Disputes"
        },
        {
            "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at",
            "group": 2427,
            "name": "10.1.1.110.5995",
            "keyword": "",
            "title": "Markov chains for exploring posterior distributions"
        },
        {
            "abstract": "Network virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding\u2014mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research restricted the problem space to allow efficient solutions, or focused on designing heuristic algorithms. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtualnetwork topologies. Our simulation experiments show that path splitting, path migration, and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks.",
            "group": 2428,
            "name": "10.1.1.110.6715",
            "keyword": "Categories and Subject Descriptors C.2.5 [Computer-Communication NetworksLocal and Wide-Area NetworksG.1.6 [Numerical AnalysisOptimization General Terms AlgorithmsDesign Keywords Virtual Network EmbeddingPath SplittingPath MigrationNetwork VirtualizationOptimization",
            "title": "Rethinking Virtual Network Embedding: Substrate Support for Path Splitting and Migration"
        },
        {
            "abstract": "Abstract: In this work, the synthesis of shaped beam pattern of linear antenna arrays is presented. Three intelligent optimization techniques, genetic, simulated annealing and tabu search, are used to determine the excitations of array elements. Examples of the cosecant pattern with the restricted sidelobe levels are given to illustrate the performance of the algorithms.",
            "group": 2429,
            "name": "10.1.1.110.6947",
            "keyword": "Key WordsAntenna array pattern synthesisbeam shapinggenetic algorithmsimulated annealingtabu search. ZEK\u0130 OPT\u0130M\u0130ZASYON TEKN\u0130KLER\u0130 KULLANARAK L\u0130NEER ANTEN D\u0130Z\u0130LER\u0130N\u0130N",
            "title": "SHAPED BEAM PATTERN SYNTHESIS OF LINEAR ANTENNA ARRAYS BY USING INTELLIGENT OPTIMIZATION TECHNIQUES: COSECANT PATTERN EXAMPLE"
        },
        {
            "abstract": "Abstract. In this paper we present two algorithms for the floorplan design problem. The algorithms are quite similar in spirit. They both use Polish expressions to represent floorplans and employ the search method of simulated annealing. The first algorithm is for the case where all modules are rectangular, and the second one is for the case where the modules are either rectangular or L-shaped. Our algorithms consider simultaneously the interconnection information as well as the area and shape information for the modules. Experimental results indicate that our algorithms perform well for many test problems. Key Words. VLSI circuit layout, Floorplan design, Simulated annealing.",
            "group": 2430,
            "name": "10.1.1.110.7399",
            "keyword": "",
            "title": "Floorplan design of VLSI circuits"
        },
        {
            "abstract": "This un-edited manuscript has been accepted for publication in Biophysical Journal and is freely available on BioFast at",
            "group": 2431,
            "name": "10.1.1.111.106",
            "keyword": "",
            "title": "Optimization of a Stochastically-Simulated Gene Network Model via Simulated Annealing"
        },
        {
            "abstract": "z/Sm3 2--",
            "group": 2432,
            "name": "10.1.1.111.726",
            "keyword": "",
            "title": "Learning to Read Aloud: A Neural Network Approach Using Sparse Distributed Memory"
        },
        {
            "abstract": "The Dissertation Committee for Anubrati Mukherjee Certifies that this is the approved version of the following dissertation:",
            "group": 2433,
            "name": "10.1.1.111.1235",
            "keyword": "",
            "title": "To my parents"
        },
        {
            "abstract": "Teknillinen korkeakoulu Konetekniikan osasto Energiatekniikan ja ymp\u00e4rist\u00f6nsuojelun laboratorio Distribution:",
            "group": 2434,
            "name": "10.1.1.111.1426",
            "keyword": "ISBN (pdf) 951-22-8097-3",
            "title": "TKK-DISS-2110"
        },
        {
            "abstract": "A Boltzmann Machine is a network of symmetrically connected, neuronlike units that make stochastic decisions about whether to be on or off. Boltzmann machines have a simple learning algorithm that allows them to discover interesting features in datasets composed of binary vectors. The learning algorithm is very slow in networks with many layers of feature detectors, but it can be made much faster by learning one layer of feature detectors at a time. Boltzmann machines are used to solve two quite different computational problems. For a search problem, the weights on the connections are fixed and are used to represent the cost function of an optimization problem. The stochastic dynamics of a Boltzmann machine then allow it to sample binary state vectors that represent good solutions to the optimization problem. For a learning problem, the Boltzmann machine is shown a set of binary data vectors and it must find weights on the connections so that the data vectors are good solutions to the optimization problem defined by those weights. To solve a learning problem, Boltzmann machines make many small updates to their weights, and each update requires them to solve many different search problems. The stochastic dynamics of a Boltzmann machine When unit i is given the opportunity to update its binary state, it first computes its total input, zi, which is the sum of its own bias, bi, and the weights on connections coming from other active units: zi = bi + \ufffd",
            "group": 2435,
            "name": "10.1.1.111.2786",
            "keyword": "",
            "title": "Spiking Boltzmann machines"
        },
        {
            "abstract": "Abstract\u2014We propose a new image segmentation technique called strings. A string is a variational deformable model that is learned from a collection of example objects rather than built from a priori analytical or geometrical knowledge. As opposed to existing approaches, an object boundary is represented by a one-dimensional multivariate curve in functional space, a feature function, rather than by a point in vector space. In the learning phase, feature functions are defined by extraction of multiple shape and image features along continuous object boundaries in a given learning set. The feature functions are aligned, then subjected to functional principal components analysis and functional principal regression to summarize the feature space and to model its content, respectively. Also, a Mahalanobis distance model is constructed for evaluation of boundaries in terms of their feature functions, taking into account the natural variations seen in the learning set. In the segmentation phase, an object boundary in a new image is searched for with help of a curve. The curve gives rise to a feature function, a string, that is weighted by the regression model and evaluated by the Mahalanobis model. The curve is deformed in an iterative procedure to produce feature functions with minimal Mahalanobis distance. Strings have been compared with active shape models on 145 vertebra images, showing that strings produce better results when initialized close to the target boundary, and comparable results otherwise. Index Terms\u2014Machine learning, deformable models, energy minimization, multivariate statistics, shape analysis, functional data analysis, chemometrics, active shape models. 1",
            "group": 2436,
            "name": "10.1.1.111.4950",
            "keyword": "",
            "title": "Strings: Variational deformable models of multivariate ordered features"
        },
        {
            "abstract": null,
            "group": 2437,
            "name": "10.1.1.111.6278",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "In this paper, we study the area-balanced multi-way partitioning problem of VLSI circuits based on a new dual netlist representation named the hybrid dual netlist (HDN), and propose a general paradigm for multi-way circuit partitioning based on dual net transformation. Given a netlist we first compute a K-way partitioning of nets based on the HDN representation, and then transform the K-way net partition into a K-way module partitioning solution. The main contri-bution of our work is in the formulation and solution of the K-way module contention (K-MC) problem, which determines the best assignment of the modules in contention to partitions while maintaining user-specified area requirements, when we transform the net partition into a module partition. Under a natural definition of binding factor between nets and modules, and preference function between partitions and modules, we show that the K-MC problem can be reduced to a min-cost max-flow problem. We present an efficient solution to the K-MC problem based on network flow computation. We apply our dual transformation paradigm to the well-known K-way FM partitioning algorithm (K-FM) and show that the new algorithm, named K-DualFM, reduces the net cutsize by 20 % to 31 % compared with the K-FM algorithm. We also apply the same paradigm to the K-MFFC-FM algorithm, a K-FM algorithm based on maximum fanout-free cone (MFFC) clustering reported in [CoLB94], and show that the resulting algorithm, K-DualMFFC-FM reduces the net cut-size by 15 % to 26 % compared with K-MFFC-FM. Furthermore, we compare the K-DualFM algorithm with EIG1 [HaKa91] and Paraboli [RiDJ94], two recently proposed spectral-based bipartitioning algorithms. We showed that K-DualFM reduces the net cutsize by 56 % on average when compared with EIG1 and produces comparable results with Paraboli. 1.",
            "group": 2438,
            "name": "10.1.1.111.7185",
            "keyword": "",
            "title": "Multi-Way VLSI Circuit Partitioning Based on Dual Net Representation"
        },
        {
            "abstract": "Abstract \u2014 The spherical k-means algorithm, i.e., the k-means algorithm with cosine similarity, is a popular method for clustering high-dimensional text data. In this algorithm, each document as well as each cluster mean is represented as a high-dimensional unit-length vector. However, it has been mainly used in batch mode. That is, each cluster mean vector is updated, only after all document vectors being assigned, as the (normalized) average of all the document vectors assigned to that cluster. This paper investigates an online version of the spherical k-means algorithm based on the well-known Winner-Take-All competitive learning. In this online algorithm, each cluster centroid is incrementally updated given a document. We demonstrate that the online spherical k-means algorithm can achieve significantly better clustering results than the batch version, especially when an annealing-type learning rate schedule is used. We also present heuristics to improve the speed, yet almost without loss of clustering quality. I.",
            "group": 2439,
            "name": "10.1.1.111.8125",
            "keyword": "",
            "title": "Efficient Online Spherical K-means Clustering"
        },
        {
            "abstract": "Finite Element (FE) model updating entails tuning the model so that it can better reflect the measured data from the physical structure being modeled [1]. One fundamental characteristic of an FE model is that it can never be a true reflection of the",
            "group": 2440,
            "name": "10.1.1.111.8262",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "A protein is identified by a finite sequence of amino acids, each of them chosen from a set of 20 elements. The Protein Structure Prediction Problem is the problem of predicting the 3D native conformation of a protein, when its sequence of amino acids is known. This problem is fundamental for biological and pharmaceutical research. All current mathematical models of the problem are affected by intrinsic computational limits. In particular, simulation-based techniques that handle every chemical interaction between all atoms in the amino acids (and the solvent) are not feasible due to the huge amount of computations involved. These programs are typically written in imperative languages and hard to be parallelized. Moreover, each approach is based on a particular energy function. In the literature there are various proposals and there is no common agreement on which is the most reliable. In this paper we present a novel framework for ab-initio simulations using Concurrent Constraint Programming. We are not aware of any other similar proposals in the literature. Each amino acid of an input protein is viewed as an independent process that communicates with the others. The framework allows a modular representation of the problem and it is easily extensible for further refinements. Simulations at this level of abstraction allow faster calculation. We provide a first preliminary working example in Mozart, to show the feasibility and the power of the method. The code is intrinsically concurrent and thus easy to be parallelized.",
            "group": 2441,
            "name": "10.1.1.111.8602",
            "keyword": "Key wordsComputational BiologyConcurrent Constraint Programming",
            "title": "Bio-CONCUR 2004 Preliminary Version Protein Folding Simulation in Concurrent Constraint Programming"
        },
        {
            "abstract": "We examine the problem of transmembrane protein structure determination. Like many questions that arise in biological research, this problemcannot be addressed generally by traditional laboratory experimentation alone. Instead, an approach that integrates experiment and computation is required. We formulate the transmembrane protein structure determination problem as a bound-constrained optimization problem using a special empirical scoring function, called Bundler, as the objective function. In this paper, we describe the optimization problem and its mathematical properties, and we examine results obtained using two different derivative-free optimization algorithms. Key words: optimization; computational biology; nonlinear programming; parallel algorithm; protein structure; Bundler scoring function",
            "group": 2442,
            "name": "10.1.1.111.9365",
            "keyword": "",
            "title": "\u00a9 2004 INFORMS Optimizing an Empirical Scoring Function for Transmembrane Protein Structure Determination"
        },
        {
            "abstract": "RESCHEOULING WITH N93-15288 (NASA) 17 p uncl.is G3/63 0135322 t /f",
            "group": 2443,
            "name": "10.1.1.111.9982",
            "keyword": "FunctionPlanning and Scheduling DomainSpace Shuttle Ground Processing KnowledgeConstraints *Recom Technologies?Lockheed Artificial Intelligence Center",
            "title": "April-_1992 Rescheduling with Iterative Repair"
        },
        {
            "abstract": "The importance of high performance algorithms for tackling difficult optimization problems cannot be understated, and in many cases the only available methods are metaheuristics. When designing a metaheuristic, it is preferable that it be simple, both conceptually and in practice. Naturally,",
            "group": 2444,
            "name": "10.1.1.112.767",
            "keyword": "",
            "title": "Iterated Local Search"
        },
        {
            "abstract": "The domain of Field-Programmable Gate Arrays (FPGAs) has undergone a dramatic revolution in the past decade. What started as a cheap alternative to custom design for implementing non-critical functions has now progressed into a new model of computation. The programmability of the device at a bit level combined with an application specific flow of data between operations offer the silicon reusability of general purpose processor with the implementation efficiency of application specific integrated circuits. One of the major drawbacks of FPGAs that has not been addressed is the poor energy efficiency. This factor has excluded the full utilization of its potential in energy sensitive domains like portable computing. Even the application domains that do not require low energy consumption will feel the consequence as the process technology moves into the sub-0.1\u00b5m region. This is because of the higher frequencies and larger chips that will be seen in the future process technologies. ",
            "group": 2445,
            "name": "10.1.1.112.2463",
            "keyword": "",
            "title": "Low Energy Field-Programmable Gate Array "
        },
        {
            "abstract": "An MCM's increased throughput and dense circuitry can easily result in failure if the board contains \ufffdhot spots&quot;. Therefore, an accurate thermal model of an MCM was needed in the development ofanew placement algorithm designed to consider both total net length and heat constraints. This algorithm uses a combination of simulated evolution and simulated annealing in an iterative approach. The \ufffdtness method evaluates the maximum heat for each chip, considering the chip's own heat and the heat from surrounding chips at its hottest point. Results are shown and comparisons are drawn to other placement algorithms. 1",
            "group": 2446,
            "name": "10.1.1.112.3686",
            "keyword": "",
            "title": "Object-Oriented Thermal Placement Using an Accurate Heat Model"
        },
        {
            "abstract": "Abstract. We present a simple model of distributed multi-agent multi-issued contract negotiation for open systems where interactions are competitive and information is private and not shared. We then investigate via simulations two different approximate optimization strategies and quantify the contribution and costs of each towards the quality of the solutions reached. To evaluate the role of knowledge the obtained results are compared to more cooperative strategies where agents share more information. Interesting social dilemmas emerge that suggest the design of incentive mechanisms. 1",
            "group": 2447,
            "name": "10.1.1.112.7472",
            "keyword": "",
            "title": "England Complex Systems"
        },
        {
            "abstract": "ABSTRACT By rearranging naturally occurring genetic components, gene networks can be created that display novel functions. When designing these networks, the kinetic parameters describing DNA/protein binding are of great importance, as these parameters strongly influence the behavior of the resulting gene network. This article presents an optimization method based on simulated annealing to locate combinations of kinetic parameters that produce a desired behavior in a genetic network. Since gene expression is an inherently stochastic process, the simulation component of simulated annealing optimization is conducted using an accurate multiscale simulation algorithm to calculate an ensemble of network trajectories at each iteration of the simulated annealing algorithm. Using the three-gene repressilator of Elowitz and Leibler as an example, we show that gene network optimizations can be conducted using a mechanistically realistic model integrated stochastically. The repressilator is optimized to give oscillations of an arbitrary specified period. These optimized designs may then provide a starting-point for the selection of genetic components needed to realize an in vivo system.",
            "group": 2448,
            "name": "10.1.1.113.82",
            "keyword": "",
            "title": "Optimization of a Stochastically Simulated Gene Network Model via Simulated Annealing"
        },
        {
            "abstract": "In this work we focus on efficient heuristics for solving a class of stochastic planning problems that arise in a variety of business, investment, and industrial applications. The problem is best described in terms of future buy and sell contracts. By buying less reliable, but less expensive, buy (supply) contracts, a company or a trader can cover a position of more reliable and more expensive sell contracts. The goal is to maximize the expected net gain (profit) by constructing a close to optimum portfolio out of the available buy and sell contracts. This stochastic planning problem can be formulated as a two-stage stochastic linear programming problem with recourse. However, this formalization leads to solutions that are exponential in the number of possible failure combinations. Thus, this approach is not feasible for large scale problems. In this work we investigate heuristic approximation techniques alleviating the efficiency problem. We primarily focus on the clustering approach and devise heuristics for finding clusterings leading to good approximations. We illustrate the quality and feasibility of the approach through experimental data. 1",
            "group": 2449,
            "name": "10.1.1.113.1225",
            "keyword": "",
            "title": "A clustering approach to solving large stochastic matching problems"
        },
        {
            "abstract": "ad agenti",
            "group": 2450,
            "name": "10.1.1.113.2193",
            "keyword": "Computational BiologyAgent-Based TechnologiesConcurrent Constraint Programming",
            "title": "Protein Folding Simulation"
        },
        {
            "abstract": "Abstract- Simulated annealing (SA) is an effective general heuristic method for solving many combinatorial optimization problems. This paper deals with two problems in SA. One is the long computational time of the numerical annealings, and the solution to it is the parallel processing of SA. The other one is the determination of the appropriate temperature schedule in SA, and the solution to it is the introduction of an adaptive mechanism for changing the temperature. The multiple SA processes are performed in multiple processors, and the temperatures in the SA processes are determined by a genetic algorithms. The proposed method is applied to solve many TSPs (Traveling Salesman Problems) and JSPs (Jobshop Scheduling Problems), and it is found that the method is very useful and effective.",
            "group": 2451,
            "name": "10.1.1.113.2366",
            "keyword": "Key WordsSimulated AnnealingGenetic AlgorithmAdaptive TemperatureTraveling Salesman ProblemsJobshop Scheduling Problems",
            "title": "Adaptive Temperature Schedule Determined by Genetic Algorithm for Parallel Simulated Annealing"
        },
        {
            "abstract": "Many problems in Natural Language Processing (NLP) involves an efficient search for the best derivation over (exponentially) many candidates. For example, a parser aims to find the best syntactic tree for a given sentence among all derivations under a grammar, and a machine translation (MT) decoder explores the space of all possible translations of the source-language sentence. In these cases, the concept of packed forest provides a compact representation of huge search spaces by sharing common sub-derivations, where efficient algorithms based on Dynamic Programming (DP) are possible. Building upon the hypergraph formulation of forests and well-known 1-best DP algorithms, this dissertation develops fast and exact k-best DP algorithms on forests, which are orders of magnitudes faster than previously used methods on state-of-theart parsers. We also show empirically how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications. We then extend these algorithms to approximate search when the forests are",
            "group": 2452,
            "name": "10.1.1.113.2387",
            "keyword": "",
            "title": "A Search in the Forest: Efficient Algorithms for Parsing and Machine Translation based on Packed Forests"
        },
        {
            "abstract": "National Taiwan University Due to the layout complexity in modern VLSI designs, integrated circuit blocks may not be rectangular. However, literature on general rectilinear block placement is still quite limited. In this article, we present approaches for handling the placement for arbitrarily shaped rectilinear blocks using B*-trees [Chang et al. 2000]. We derive the feasibility conditions of B*-trees to guide the placement of rectilinear blocks. Experimental results show that our algorithm achieves optimal or near-optimal block placement for benchmarks with various shaped blocks.",
            "group": 2453,
            "name": "10.1.1.113.2440",
            "keyword": "Categories and Subject DescriptorsB7.2 [Integrated CircuitsDesign Aids\u2014placement and routingJ.6 [Computer ApplicationsComputer-Aided Engineering General TermsAlgorithmsDesignExperimentationMeasurementPerformance Additional Key Words and PhrasesComputer-aided design of VLSIfloorplanninglayout",
            "title": "Realtek Semiconductor Corp."
        },
        {
            "abstract": "This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speedup factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned. ",
            "group": 2454,
            "name": "10.1.1.113.3059",
            "keyword": "",
            "title": "Real-Time Object Detection for \"Smart\" Vehicles"
        },
        {
            "abstract": "The LPASSO method for regression regularization Linear models are often built to understand how a set of input data affects output data and predict its value. A linear regression model assumes",
            "group": 2455,
            "name": "10.1.1.113.3207",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Performance and data from some cognitive models suggested that emotions, experienced during problem solving, should be taken into account. Moreover, it is proposed that the cognitive science approach using both theoretical and experimental data may lead to a better understanding of the phenomena. A closer investigation of ACT-R cognitive architecture (Anderson 1993) revealed some properties analogous to phenomena known from the activation theory of emotion. A model of the classical Yerkes-Dodson experiment was built to test the predictions. The study explained such psychological phenomena as arousal, motivation and confidence within the mathematical notation. The influence of changes in these motivational states, controlled by emotion, on information processing has been investigated and it is shown that the dynamics corresponds to the well-known optimisation methods, such as best-first search and simulated annealing. 1",
            "group": 2456,
            "name": "10.1.1.113.3565",
            "keyword": "",
            "title": "The Role of Emotion in Problem Solving"
        },
        {
            "abstract": "We consider the inverse scattering problem of retrieving the surface profile function from far-field angle-resolved intensity data. The problem is approached as a nonlinear constrained optimization problem. The surface, assumed one-dimensional and perfectly conducting, is also assumed to be a realization of a Gaussian random process with a Gaussian correlation function with known standard deviation of heights (\u03b4) and correlation length (a). Starting from rigorously calculated far-field angleresolved scattered data, we search for the optimum profile using evolutionary strategies. Examples that illustrate the proposed scheme are presented. Aspects of the convergence and lack of uniqueness of the solution are discussed. 1.",
            "group": 2457,
            "name": "10.1.1.113.4393",
            "keyword": "",
            "title": "Inverse scattering with far-field intensity data: random surfaces that belong to a well-defined statistical class"
        },
        {
            "abstract": "f(x)",
            "group": 2458,
            "name": "10.1.1.113.4527",
            "keyword": "\u2022 Exact methods",
            "title": "\u2022 Optimization Problems"
        },
        {
            "abstract": "consolidation can benefit from the prediction of VM page miss rate at each candidate memory size. Such prediction is challenging for the hypervisor (or VM monitor) due to a lack of knowledge on VM memory access pattern. This paper explores the approach that the hypervisor takes over the management for part of the VM memory and thus all accesses that miss the remaining VM memory can be transparently traced by the hypervisor. For online memory access tracing, its overhead should be small compared to the case that all allocated memory is directly managed by the VM. To save memory space, the hypervisor manages its memory portion as an exclusive cache (i.e., containing only data that is not in the remaining VM memory). To minimize I/O overhead, evicted data from a VM enters its cache directly from VM memory (as opposed to entering from the secondary storage). We guarantee the cache correctness by only caching memory pages whose current contents provably match those of corresponding storage locations. Based on our design, we show that when the VM evicts pages in the LRU order, the employment of the hypervisor cache does not introduce any additional I/O overhead in the system. We implemented the proposed scheme on the Xen para-virtualization platform. Our experiments with microbenchmarks and four real data-intensive services (SPECweb99, index searching, TPC-C, and TPC-H) illustrate the overhead of our hypervisor cache and the accuracy of cache-driven VM page miss rate prediction. We also present the results on adaptive VM memory allocation with performance assurance. 1",
            "group": 2459,
            "name": "10.1.1.113.6142",
            "keyword": "",
            "title": "Virtual Machine Memory Access Tracing With Hypervisor Exclusive Cache \u2217 Abstract"
        },
        {
            "abstract": "\u2014anonymous Advances in technology have made it possible to build ad hoc sensor networks using inexpensive nodes consisting of a low power processor, a modest amount of memory, a wireless network transceiver and a sensor board; a typical node",
            "group": 2460,
            "name": "10.1.1.113.6167",
            "keyword": "LocationLocationLocation",
            "title": "1 Localization in Sensor Networks"
        },
        {
            "abstract": "Abstract \u2014 Computing has recently reached an inflection point with the introduction of multi-core processors. On-chip threadlevel parallelism is doubling approximately every other year. Concurrency lends itself naturally to allowing a program to trade performance for power savings by regulating the number of active cores, however in several domains users are unwilling to sacrifice performance to save power. We present a prediction model for identifying energy-efficient operating points of concurrency in well-tuned multithreaded scientific applications, and a runtime system which uses live program analysis to optimize applications dynamically. We describe a dynamic, phase-aware performance prediction model that combines multivariate regression techniques with runtime analysis of data collected from hardware event counters to locate optimal operating points of concurrency. Using our model, we develop a prediction-driven, phase-aware runtime optimization scheme that throttles concurrency so that power consumption can be reduced and performance can be set at the knee of the scalability curve of each program phase. The use of prediction reduces the overhead of searching the optimization space while achieving near-optimal performance and power savings. A thorough evaluation of our approach shows a reduction in power consumption of 10.8 % simultaneous with an improvement in performance of 17.9%, resulting in energy savings of 26.7%. Index Terms \u2014 Modeling and prediction, Application-aware adaptation, Energy-aware systems",
            "group": 2461,
            "name": "10.1.1.113.6395",
            "keyword": "",
            "title": "Prediction-based Power-Performance Adaptation of Multithreaded Scientific Codes"
        },
        {
            "abstract": "The research presented here examines topological drawing, a new mode of constructing and interacting with mathematical objects in three-dimensional space. In topological drawing, issues such as adjacency and connectedness, which are topological in nature, take precedence over purely geometric issues. Because the domain of application is mathematics, topological drawing is also concerned with the correct representation and display of these objects on a computer. By correctness we mean that the essential topological features of objects are maintained during interaction. We have chosen to limit the scope of topological drawing to knot theory, a domain that consists essentially of one class of object (embedded circles in three-dimensional space) yet is rich enough to contain a wide variety of difficult problems of research interest. In knot theory, two embedded circles (knots) are considered equivalent if one may be smoothly deformed into the other without any cuts or self-intersections. This notion of equivalence may be thought of as the heart of knot theory. We present methods for the computer construction and interactive manipulation of a",
            "group": 2462,
            "name": "10.1.1.113.6799",
            "keyword": "",
            "title": "Interactive Topological Drawing"
        },
        {
            "abstract": "Abstract: We present a survey of different techniques to approximate a color image using a piecewise linear interpolation induced by a triangulation of the image domain. We also include a detailed description of a method called guided simulated annealing (GSA) we designed that reduces computation time significantly. Finally, we give a short overview of possible extensions. 1",
            "group": 2463,
            "name": "10.1.1.113.9076",
            "keyword": "",
            "title": "Survey of Techniques for Data-dependent Triangulations Approximating Color Images"
        },
        {
            "abstract": "Abstract. The Bayesian approach together with Markov chain Monte Carlo techniques has provided an attractive solution to many important bioinformatics problems such as multiple sequence alignment, microarray analysis and the discovery of gene regulatory binding motifs. The employment of such methods and, more broadly, explicit statistical modeling, has revolutionized the field of computational biology. After reviewing several heuristicsbased computational methods, this article presents a systematic account of Bayesian formulations and solutions to the motif discovery problem. Generalizations are made to further enhance the Bayesian approach. Motivated by the need of a speedy algorithm, we also provide a perspective of the problem from the viewpoint of optimizing a scoring function. We observe that scoring functions resulting from proper posterior distributions, or approximations to such distributions, showed the best performance and can be used to improve upon existing motif-finding programs. Simulation analyses and a real-data example are used to support our observation.",
            "group": 2464,
            "name": "10.1.1.113.9167",
            "keyword": "Key words and phrasesGene regulationmotif discoveryBayesian",
            "title": "1. THE BIOLOGY OF TRANSCRIPTION REGULATION"
        },
        {
            "abstract": "\u201cIf you built a piece of software that was as tightly coupled as Extreme Programming, you\u2019d be fired.\u201d It was late 1999, and I was sitting at lunch with Pragmatic Dave Thomas and the rest of the North Texas XP interest group. It\u2019s",
            "group": 2465,
            "name": "10.1.1.113.9388",
            "keyword": "Agile software developmentProcess customization",
            "title": "A Simple Model of Agile Software Processes \u2013 or \u2013 Extreme Programming Annealed Categories and Subject Descriptors D.2.9 [Software Engineering]: Management \u2013 software process"
        },
        {
            "abstract": "Abstract. We present a randomized adaptive layout algorithm for nicely drawing undirected graphs that is based on the spring-embedder paradigm and contains several new heuristics to improve the convergence, including local temperatures, gravitational forces and the detection of rotations and oscillations. The proposed algorithm achieves drawings of high quality on a wide range of graphs with standard settings. Moreover, the algorithm is fast, being thus applicable on general undirected graphs of substantially larger size and complexity than before [9, 6, 3]. Aesthetically pleasing solutions are found in most cases. We give empirical data for the running time of the algorithm and the quality of the computed layouts. 1",
            "group": 2466,
            "name": "10.1.1.113.9565",
            "keyword": "",
            "title": "A fast adaptive layout algorithm for undirected graphs"
        },
        {
            "abstract": "Simultaneous Localization and Mapping (SLAM) is concurrently gathering features of the environment and building a map accordingly, to determine your current position. We applied this technique on a Pioneer robot using Growing Neural Gas (GNG). GNG is an algorithm which is comparable to Kohonen, but it is capable of dynamically adding or removing nodes from the neural network. We set out to investigate how well this system of SLAM and GNG will perform when information from camera images, sonar and odometry sensors is used. None of these sensors give precise or complete information, but our aim is that they might be able to fill each other\u2019s gaps. We show that our system performs best when information from all the sensors is used, but also that when sonar is not used, the system still performs well. The system uses data that was gathered before learning occurred and is therefore not yet a true SLAM system. Future research can improve the system by collecting features from the data (using computer vision techniques to extract features from the camera images) and by implementing it as a true SLAM system. 1",
            "group": 2467,
            "name": "10.1.1.113.9874",
            "keyword": "",
            "title": "Implementation of a Simultaneous Localization and Mapping system using Growing Neural Gas"
        },
        {
            "abstract": "Statistical modeling of sequences is a central paradigm of machine learning that \ufffd nds multiple uses in computational molecular biology and many other domains. The probabilistic automata typically built in these contexts are subtended by uniform,  \ufffd xed-memory Markov models. In practice, such automata tend to be unnecessarily bulky and computationally imposing both during their synthesis and use. Recently, D. Ron, Y. Singer, and N. Tishby built much more compact, tree-shaped variants of probabilistic automata under the assumption of an underlying Markov process of variable memory length. These variants, called Probabilistic Suf \ufffd x Trees (PSTs) were subsequently adapted by G. Bejerano and G. Yona and applied successfully to learning and prediction of protein families. The process of learning 2 the automaton from a given training set of sequences requires worst-case time, where is the total length of the sequences in and is the length of a longest substring of to be considered for a candidate state in the automaton. Once the automaton is built, predicting the likelihood of a query sequence of characters may cost time 2 in the worst case. The main contribution of this paper is to introduce automata equivalent to PSTs but having the following properties: Learning the automaton, for any, takes time. Prediction of a string of symbols by the automaton takes time. Along the way, the paper presents an evolving learning scheme and addresses notions of empirical probability and related ef \ufffd cient computation, which is a by-product possibly of more general interest. Key words: amnesic automata, probabilistic suf \ufffd x trees, variable memory Markovian models, protein families, protein classi \ufffd cation. 1",
            "group": 2468,
            "name": "10.1.1.113.9924",
            "keyword": "",
            "title": "Optimal amnesic probabilistic automata or how to learn and classify proteins in linear time and space"
        },
        {
            "abstract": "In this contribution we derive the cost function corresponding to the linear complexity Subtractive Interference Cancellation with tangent hyperbolic tentative decisions. We use the cost function to anal-yse the fix-points of solving the Subtractive Interference Cancellation equations. The analysis show that we can control the slope of the tangent hyperbolic functions so that the corresponding cost function is convex. We also show that increasing the slope can make the cost non-convex. Going from the convex regime into the non-convex regime, we prove that the bifurcation of the fix-points, for non-singular sig-nal correlation matrices, consist of the fix-point of interest together with a saddle node bifurcation. This proves that tracking the solution from low slopes, with a convex cost, gradually increasing to higher slopes, with non-convex cost, can bring us to the best solution being very close to the optimal deter-mined by enumeration. This tracking is the idea behind annealing. We show Monte Carlo studies with a substantial signal to noise ratio gain compared to not using annealing. We also show how annealing can be used to increase capacity at a given target bit error rate. In fact this capacity gain is the same obtained by Improved Parallel Interference Cancellation making us believe that the latter includes a mechanism to avoid local minima similar to annealing.",
            "group": 2469,
            "name": "10.1.1.114.219",
            "keyword": "Index Terms Multiple Access TechniqueSubtractive Interference CancellationLocal MinimaBifurcationMean-Field Annealingfix-point analysis",
            "title": "Analysis of Mean Field Annealing in Subtractive Interference Cancellation"
        },
        {
            "abstract": "This paper proposes a novel strategy that uses hypergraph partitioning and K-way iterative mapping-refinement heuristics for scheduling a batch of data-intensive tasks with batch-shared I/O behavior on heterogeneous collections of storage and compute clusters. The strategy formulates file sharing among tasks as a hypergraph to minimize the I/O overheads due to duplicate file transfers and employs a K-way iterative mapping-refinement scheme to adapt to the heterogeneity of compute clusters and storage networks in the system. We evaluate the proposed approach through real experiments and simulations on application scenarios from two application domains; satellite data processing and biomedical imaging. Our experimental results show that our approach can achieve significant performance improvement over algorithms such as HPS, Shortest Job First, MinMin, MaxMin and Sufferage for workloads with high degree of shared I/O among tasks. 1",
            "group": 2470,
            "name": "10.1.1.114.593",
            "keyword": "",
            "title": "Scheduling of Tasks with Batch-shared I/O on Heterogeneous Systems \u2217"
        },
        {
            "abstract": "The need to identify a few important variables that affect a certain outcome of interest commonly arises in various industrial engineering applications. The genetic algorithm (GA) appears to be a natural tool for solving such a problem. In this article we first demonstrate that the GA is actually not a particularly effective variable selection tool, and then propose a very simple modification. Our idea is to run a number of GAs in parallel without allowing each GA to fully converge, and to consolidate the information from all the individual GAs in the end. We call the resulting algorithm the parallel genetic algorithm (PGA). Using a number of both simulated and real examples, we show that the PGA is an interesting as well as highly competitive and easy-to-use variable selection tool.",
            "group": 2471,
            "name": "10.1.1.114.5830",
            "keyword": "KEY WORDSAkaike information criterionBayesian information criterionCentral limit theoremCombinatorial optimizationEarly stoppingMajority voteStepwise selectionStochastic search",
            "title": "Darwinian Evolution in Parallel Universes: A Parallel Genetic Algorithm for Variable Selection"
        },
        {
            "abstract": "Abstract. Multi-agent systems offer a new stage in the evolution of combat simulation. Originally, warfighters simulated combat manually to explore alternatives and plan their campaigns. The first applications of computers to combat simulation used algorithms that aggregated the warriors on each side, such as differential equations or game theory, effectively modeling the entire battlespace with a single process. Entity-based models such as OOS and Combat XXI assign a single agent to each entity, following the standard MAS agenda. A new modeling construct, the polyagent, takes this trend one step further, and uses several agents to model each construct. This approach addresses several challenges that face the traditional MAS approach, including fitting, closure, dynamism, and singularity. This chapter surveys the history of combat modeling, gives two examples of polyagent systems (one for planning, the other for adversarial prediction), and discusses how this construct addresses the challenges. 1.",
            "group": 2472,
            "name": "10.1.1.114.7749",
            "keyword": "",
            "title": "MAS Combat Simulation"
        },
        {
            "abstract": "A template-matching approach to registration of volumetric images is described. The process automatically selects about a dozen highly detailed and unique templates (cubic or spherical subvolumes) from the target volume and locates the templates in the reference volume. The centroids of the \\best &quot; four correspondences are then used to determine the transformation matrix that resamples the target volume to overlay thereference volume. Di erent similarity measures used in template matching are discussed and preliminary results are presented. The proposed registration method produces a median error of 2.8 mm when registering Venderbilt image data sets, with average registration time of 2.5 minutes on a 400 MHz PC.",
            "group": 2473,
            "name": "10.1.1.114.8876",
            "keyword": "Image registrationvolumetric imagetemplate matchingmutual informationchamfer matching",
            "title": "Volumetric image registration by template matching"
        },
        {
            "abstract": "First and foremost, I would like to thank Dr. Christina L. Bloebaum for introducing me to the field of MDO, and for more than five years of support, guidance, advisement, and friendship. I would next like to thank my parents, family, and friends for their constant support and understanding. I would further like to thank my committee members, Dr. Kemper Lewis, Dr. Roger Mayne, and Dr. Rakesh Nagi, for their respective inputs to this dissertation. I would also like to recognize my colleagues in the MODEL laboratory, in particular, Ken English, Eliot Winer, Collin McCulley, Bryan Moulton, Chen-Hung Huang, Mohamad Kasim Abdul-Jalil, Omar Conteh, and Yuji Nozaki. It has been a privilege to work alongside each of you. Last, but not least \u2013 a tip of the hat to lunchie. ii",
            "group": 2474,
            "name": "10.1.1.114.9241",
            "keyword": "",
            "title": "Acknowledgments"
        },
        {
            "abstract": "on microsatellites and amplified fragment length",
            "group": 2475,
            "name": "10.1.1.115.302",
            "keyword": "",
            "title": "Linkage map of birch, Betula pendula Roth, based"
        },
        {
            "abstract": "The use of population-based techniques to solve optimization problems is continuously increas-ing, as these kind of problems appears in a number of situations in real life. Many of these problems are of a combinatorial nature, this means that we have to find the ideal permutation of the parameters involved to reach the optimal solution. As the number of parameters increases, the difficulty to find the optimal solution becomes harder, and if we are conscious that a large amount of problems have more than one objective to optimize, this task grows to be too much harder. Population-based techniques, as any other heuristic, do not guarantee to obtain the optimal solutions, but good approximations, as they have the ability to find not only one, but a set of solutions in a single run. This is the reason why these methods have turned out to be a very successful and popular tool nowadays. This is also the cause why we have considered them for solving the combinatorial problem called Vehicle Routing Problem, which has many applications in real world problems and has several variants that lack of investigation. In this document we are proposing the design of at least two algorithms based in such techniques to solve a couple of those variants, considering some topics that current publications have excluded and we believe they are relevant. 1 1",
            "group": 2476,
            "name": "10.1.1.115.368",
            "keyword": "",
            "title": "Proposed working title: Population-based techniques for multi-objective optimization"
        },
        {
            "abstract": "Power consumption is a crucial concern in nanometer chip design. Researchers have shown that multiple supply voltage (MSV) is an effective method to reduce power consumption. The underlying idea behind MSV is the trade-off between power saving and performance. In this paper, we present an effective voltage assignment technique based on dynamic programming. Given a netlist without reconvergent fanouts, the dynamic programming can guarantee an optimal solution for the voltage assignment in linear time. We then generate a level shifter for the net that connects two blocks in different voltage domains, and perform power-network aware floorplanning for the MSV design. Experimental results show that our floorplanner is very effective in optimizing power consumption under timing constraints. 1.",
            "group": 2477,
            "name": "10.1.1.115.1124",
            "keyword": "",
            "title": "Timing-Constrained Voltage Island Assignment and Floorplanning for Power Optimization \u2217"
        },
        {
            "abstract": "APPLICATION OF STOCHASTIC OPTIMIZATION METHOD FOR AN URBAN CORRIDOR This paper presents a stochastic traffic signal optimization method that consists of the CORSIM microscopic traffic simulation model and a heuristic optimizer. For the heuristic optimizer, the performance of three widely used optimization methods (i.e., genetic algorithm, simulated annealing and OptQuest Engine) was compared using a real world test corridor with 12 signalized intersections in Fairfax, Virginia, USA. The performance of the proposed stochastic optimization method was compared with an existing signal timing optimization program, SYNCHRO, under microscopic simulation environment. The results indicated that the genetic algorithm-based optimization method outperforms the SYNCHRO program as well as the other stochastic",
            "group": 2478,
            "name": "10.1.1.115.1840",
            "keyword": "",
            "title": "Proceedings of the 2006 Winter Simulation Conference"
        },
        {
            "abstract": "and to solve complex dynamic problems",
            "group": 2479,
            "name": "10.1.1.115.2466",
            "keyword": "",
            "title": "Engineering complex systems: Ant colony optimization to model and to solve complex dynamic problems"
        },
        {
            "abstract": "Abstract\u2014In this paper, a stochastic optimization algorithm is used to characterize the polarization states of a nonpolarimetric radar transmitter and receiver antennas for optimal target classification. Specifically, the optimized solution is sought when a multitude of targets are to be categorized. It is shown that the objective function of the optimization problem is highly nonlinear and discontinuous, hence, classical optimization algorithms fail to provide satisfactory results. The stochastic optimization algorithm used in this paper is based on a genetic algorithm (GA) which operates on a discretized form of the parameter space and searches globally for the optimum point. In this process, it is assumed that the polarimetric responses of the targets are known a priori. The optimization algorithm is applied to two sets of data: 1) a synthetic backscatter data for four point targets with similar radar cross sections (RCS\u2019s) and 2) a set of polarimetric backscatter measurements of asphalt surfaces under different physical conditions at 94 GHz. The purpose of the latter study is to come up with the optimal design for polarization states of an affordable millimeter-wave radar sensor that can assess traction of road surfaces. Index Terms\u2014Genetic algorithms, object detection. I.",
            "group": 2480,
            "name": "10.1.1.115.3795",
            "keyword": "",
            "title": "Characterization of Optimum Polarization for Multiple Target Discrimination Using Genetic Algorithms"
        },
        {
            "abstract": "The one \u2013 and two-sided bipartite graph drawing problem aims to find a layout of a bipartite graph, with vertices of the two parts placed on parallel imaginary lines, that has the minimum number of edge-crossings. Vertices of one part are in fixed positions for the one-sided problem, whereas all vertices are free to move along their lines in the two-sided version. Many different heuristics exist for finding approximations to these problems, which are NP-hard. New sequential and parallel methods for producing drawings with low edge-crossings are investigated and compared to existing algorithms, notably Penalty Minimisation and Sifting, the current leaders. For the one-sided problem, new methods that include those based on simple stochastic hill-climbing, simulated annealing and genetic algorithms were tested. The new block-crossover genetic algorithm produced very good results with lower crossings than existing methods, although it tended to be slower. However,",
            "group": 2481,
            "name": "10.1.1.115.4740",
            "keyword": "",
            "title": "Sequential and Parallel Algorithms for Low-Crossing Graph Drawing"
        },
        {
            "abstract": "Abstract\u2014Proteins or genes that have similar sequences are likely to perform the same function. One of the most widely used techniques for sequence comparison is sequence alignment. Sequence alignment allows mismatches and insertion/deletion, which represents biological mutations. Sequence alignment is usually performed only on two sequences. Multiple sequence alignment, is a natural extension of two-sequence alignment. In multiple sequence alignment, the emphasis is to find optimal alignment for a group of sequences. Several applicable techniques were observed in this research, from traditional method such as dynamic programming to the extend of widely used stochastic optimization method such as Genetic Algorithms (GAs) and Simulated Annealing. A framework with combination of Genetic Algorithm and Simulated Annealing is presented to solve Multiple Sequence Alignment problem. The Genetic Algorithm phase will try to find new region of solution while Simulated Annealing can be considered as an alignment improver for any near optimal solution produced by GAs.",
            "group": 2482,
            "name": "10.1.1.115.5004",
            "keyword": "Genetic AlgorithmSequence alignmentMultiple Sequence Alignment. A",
            "title": ""
        },
        {
            "abstract": "This paper provides a general formulation of probabilistic model-based clustering with deterministic annealing (DA), which leads to a unifying analysis of k-means, EM clustering, soft competitive learning algorithms (e.g., self-organizing map), and information bottleneck. The analysis points out an interesting yet not well-recognized connection between the k-means and EM clustering\u2014they are just two different stages of a DA clustering process, with different temperatures. Demonstrated relationships between modelbased clustering, competitive learning, and information bottleneck, can potentially generate a series of new algorithms. 1",
            "group": 2483,
            "name": "10.1.1.115.5521",
            "keyword": "",
            "title": "An Analysis of Model-based Clustering, Competitive Learning, and Information Bottleneck"
        },
        {
            "abstract": "by Arturo J. Pacheco-Vega This dissertation investigates enhancement in accuracy of heat rate predictions in compact fin-tube heat exchangers. The sources of error from a conventional approach based on correlating heat transfer coefficients, sometimes of 25\u201330%, are studied first. These include the idealized assumptions in the procedure by which correlations are found, the data compression that occurs through the correlation process, and the multiplicity of solutions for a proposed correlating function obtained using local regression. To remove the non-uniqueness of regression results, a methodology based on global optimization techniques that include genetic algorithms, simulated annealing and interval analysis is introduced. Applications of globally-determined correlations to single-phase and condensing coils demonstrate improved accuracy with errors of only 3%. The issue of degradation due to the idealized assumptions in the procedure",
            "group": 2484,
            "name": "10.1.1.115.6165",
            "keyword": "",
            "title": "SIMULATION OF COMPACT HEAT EXCHANGERS USING GLOBAL REGRESSION AND SOFT COMPUTING"
        },
        {
            "abstract": "For several years we have been employing a riskbased decision process to guide development and application of advanced technologies, and for research and technology portfolio planning. The process is supported by custom software, in which visualization plays an important role. During requirements gathering, visualization is used to help scrutinize the status (completeness, extent) of the information. During decision making based on the gathered information, visualization is used to help decisionmakers understand the space of options and their consequences. In this paper we summarize the visualization capabilities that we have employed, indicating when and how they have proven useful. 1.",
            "group": 2485,
            "name": "10.1.1.115.6571",
            "keyword": "",
            "title": "Experiences using Visualization Techniques to Present Requirements, Risks to Them, and Options for Risk Mitigation"
        },
        {
            "abstract": "I hereby declare that the seminar report entitled \u201cDigital Phase Locked Loop Interface Design \u201d is an authentic record of my own work carried out as requirements",
            "group": 2486,
            "name": "10.1.1.115.7799",
            "keyword": "H.O.DE.C.E.DTIET-Patiala Dean of Academic AffairsTIET-Patiala",
            "title": "DECLARATION"
        },
        {
            "abstract": "parameter estimation and optimal supervisory control of chilled water plants by barrett a. flake",
            "group": 2487,
            "name": "10.1.1.115.9937",
            "keyword": " i",
            "title": ""
        },
        {
            "abstract": "An on-line planning problem is one where an agent must optimise some objective criterion by making a sequence of action selection decisions, where the time and resources used in making decisions count in assessing overall solution quality. Typically in these problems it is not possible to find an optimal complete solution before an initial action must be executed, instead to maximise its performance the agent must interleave decision making and execution. This thesis investigates using decision theoretic techniques to solve these problems by equipping the agent with the ability to reason about the \u201ccomplexity induced \u201d uncertainties in its information and the costs of computation. The basic thinking/execution interleaving is provided by incremental search, where decisions are made incrementally based upon a guided partial search through the local space of possible solutions. The major sub-problems such an agent must solve are; i) decision making\u2014how to make decisions whilst in a state of \u201ccomplexity induced uncertainty\u201d, ii) search control\u2014which node to expand next, iii) stopping\u2014when to stop searching. Decision making is treated as a value estimation problem. By representing the agent\u2019s uncertainty in probabilistic terms this can be solved using decision theoretic techniques. Existing decision making systems are analysed and",
            "group": 2488,
            "name": "10.1.1.116.225",
            "keyword": "",
            "title": "UNIVERSITY OF SOUTHAMPTON Incremental Search Algorithms for On-Line Planning."
        },
        {
            "abstract": "2. Levinthal\u2019s paradox and energy landscapes 115 2.1 Including randomness in the energy function 121 2.2 Some effects of energetic correlations between structurally similar states 126 3. Resolution of problems by funnel theory 128",
            "group": 2489,
            "name": "10.1.1.116.284",
            "keyword": "",
            "title": "DOI: 10.1017/S0033583502003761 Printed in the United Kingdom Understanding protein folding with energy landscape theory Part I: Basic concepts"
        },
        {
            "abstract": "0.1 Nature of the problem 0.1.1 Introduction This chapter is about searching for the extremal values of an objective f defined on a domain \\Omega, possibly a large finite set, and equally important, for where these values occur. The methods used for this problem can be analyzed as finite Markov Chains which are either homogeneous or non-homogeneous or as renewal processes. By an optimal value we mean globally optimal, for example f\\Lambda is the minimal value and x\\Lambda 2 \\Omega a minimizer if f (x\\Lambda)  = f\\Lambda; and f\\Lambda  ^ f (x); x 2 \\Omega: Although we strive for the optimal value, this enterprise brings forth methods which rapidly find acceptably good values. Moreover, often knowing whether a value is the optimum or not cannot be answered with certainty. More generally, one might establish a goal for the search. It could of course be finding a global optimizer or it might be finding an x for which f (x) is within a certain fraction of the optimum or it could be based on other criteria.",
            "group": 2490,
            "name": "10.1.1.116.417",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract\u2014In this paper, we deal with arbitrarily shaped rectilinear module placement using the transitive closure graph (TCG) representation. The geometric meanings of modules are transparent to TCG as well as its induced operations, which makes TCG an ideal representation for floorplanning/placement with arbitrary rectilinear modules. We first partition a rectilinear module into a set of submodules and then derive necessary and sufficient conditions of feasible TCG for the submodules. Unlike most previous works that process each submodule individually and thus need to perform post processing to fix deformed rectilinear modules, our algorithm treats a set of submodules as a whole and thus not only can guarantee the feasibility of each perturbed solution but also can eliminate the need for the postprocessing on deformed modules, implying better solution quality and running time. Experimental results show that our TCG-based algorithm is capable of handling very complex instances; further, it is very efficient and results in better area utilization than previous work. Index Terms\u2014Floorplanning, placement, transitive closure graph. I.",
            "group": 2491,
            "name": "10.1.1.116.1474",
            "keyword": "",
            "title": "Arbitrarily Shaped Rectilinear Module Placement Using the Transitive Closure Graph Representation"
        },
        {
            "abstract": "We propose a model of self-organization of synaptic connections in V1, emphasizing lateral interactions. Subject to Hebbian learning with decay, evolution of synaptic strengths proceeds to a stable state in which all synapses are either saturated, or have minimum pre/ post-synaptic coincidence. The most stable configuration gives rise to anatomically realistic \u2018\u2018local maps\u2019\u2019, each of macro-columnar size, and each organized as Mobius projections of retinotopic space. A tiling of V1, constructed of approximately mirror-image reflections of each local map by its neighbors is formed, accounting for orientation-preference singularities, linear zones, and saddle points\u2014with each map linked by connections between sites of common orientation preference. Ocular dominance columns are partly explained as a special case of the same process. The occurrence of direction preference fractures always in odd numbers around singularities is a specific feature explained by the Mobius configuration of the local map. Effects of stimulus velocity, orientation relative to direction of motion, and extension, upon orientation preference, which are not accounted for by spatial filtering, are explained by interactions between the classic receptive field and global V1.",
            "group": 2492,
            "name": "10.1.1.116.1495",
            "keyword": "Primary visual cortexLateral interactionsNeural networksOrientationNeuroanatomy",
            "title": "Abstract Contribution of lateral interactions in V1 to organization of response properties"
        },
        {
            "abstract": "In this article we introduce a molecular docking algorithm called MolDock. MolDock is based on a new heuristic search algorithm that combines differential evolution with a cavity prediction algorithm. The docking scoring function of MolDock is an extension of the piecewise linear potential (PLP) including new hydrogen bonding and electrostatic terms. To further improve docking accuracy, a re-ranking scoring function is introduced, which identifies the most promising docking solution from the solutions obtained by the docking algorithm. The docking accuracy of MolDock has been evaluated by docking flexible ligands to 77 protein targets. MolDock was able to identify the correct binding mode of 87 % of the complexes. In comparison, the accuracy of Glide and Surflex is 82 % and 75%, respectively. FlexX obtained 58 % and GOLD 78 % on subsets containing 76 and 55 cases, respectively.",
            "group": 2493,
            "name": "10.1.1.116.2126",
            "keyword": "",
            "title": "MolDock: A New Technique for High-Accuracy Molecular Docking"
        },
        {
            "abstract": "Stacking and blocking (S&B) is an important and complex process in building design. Stacking is the process that decides which spaces should go to which floors in a multi-story building. Blocking is a process to decide which spaces should go to which zones for a given floor. As S&B designs a building on a broad scale, it directs the form and general organization of a building. The S&B process is a complex one that necessitates the consideration of many design requirements. While some requirements are related, each design requirement is fundamentally different from the others and thus cannot be handled in the same way. S&B is also a combinatorial problem, in which the number of possible arrangements of spaces is so large that they cannot all be examined individually. Accordingly, S&B is a challenging and extremely critical aspect of building design. Computerized S&B exists but there is room for improvement. Currently there are three state-ofthe-art S&B programs, but their algorithms are weak, and they can handle only a single design requirement, namely that of functional adjacency. These limitations significantly hamper their ability to assist designers in the S&B process.",
            "group": 2494,
            "name": "10.1.1.116.2786",
            "keyword": "",
            "title": "Functional Decomposition in Architecture Ph.D. Dissertation"
        },
        {
            "abstract": "Research Reports 1 1 Introduction Finite state controllers (FSCs) are a common representation for policies in reinfor-cment learning. They are particularly appropriate for partially observable domains, where memory is necessary to act optimally, as the amount of memory can be directlycontrolled by the number of states in the controller. Each problem requires some minimum amount of memory. Too little memory prevents the problem from being solved,but too much memory causes learning to take longer than necessary. In general the optimal amount of memory is not known in advance so it is desireable to adapt it tothe problem. We describe one approach to adjusting the memory size to the problem, using simulated annealing to search over the space of finite state controllers. 2 Reinforcement Learning Reinforcement learning is a sequential decision making problem. A reinforcementlearning problem requires an agent that can act in and sense an environment, and a reward signal that the agent receives. The agent's goal is to optimise the reward signal, orat least attain reward above a certain threshold (known as satisficing). This framework can represent many tasks, such as a animal foraging for food, or a student trying tocomplete a PhD.",
            "group": 2495,
            "name": "10.1.1.116.2880",
            "keyword": "",
            "title": "THE UNIVERSITYOF BIRMINGHAM Learning Variable Size Finite StateControllers for POMDPs via"
        },
        {
            "abstract": "Abstract. We present Sancta, a flexible control architecture for multirobot teams. It is fully written in Ada 2005, except for the reuse of some C libraries. In this paper we highlight the architectural elements of our implementation and also present our experiences using the cutting-edge 2005 implementation from GNAT, through its GPL 2005 and 2006 iterations. We expect to exemplify the kind of advantages and challenges that developers can find in using the new Ada 2005 features. Since this architecture makes use of a wide range of Ada capabilities, from low level hardware interaction to graphical user interfaces, we believe it is a good example of a successful mid-size project using Ada 2005 in academy.",
            "group": 2496,
            "name": "10.1.1.116.2908",
            "keyword": "Key wordsmulti-robotcontrol architectureAda 2005GNAT GPL",
            "title": "Architecture for Mobile Robotics Research"
        },
        {
            "abstract": "This paper presents a novel technique to perform dynamic high-level exploration of a behavioral specification that is being partitioned for a multi-device architecture. The technique, unlike in traditional HLS, performs a global search on the four-dimensional design space formed by multiple partition segments of the behavior. Hence, the proposed technique effectively satisfies the global latency constraint on the entire design, as well as the area constraints on the individual partition segments. Since the technique is based on a rigorous exploration model, it employs an efficient lowcomplexity heuristic instead of an exhaustive search. We have provided a number of results by integrating the exploration technique with two popular partitioning algorithms: (i) simulated annealing and (ii) fiducciamattheyses. The proposed technique is highly effective in guiding any partitioning algorithm to a constraint satisfying solution, and in a fairly short execution time. At tight constraint values, the proposed technique has the ability to generate solutions that do not exist in search space of traditional HLS exploration techniques. 1",
            "group": 2497,
            "name": "10.1.1.116.4540",
            "keyword": "",
            "title": "A Technique for Dynamic High-Level Exploration During Behavioral-Partitioning for Multi-Device Architectures"
        },
        {
            "abstract": "Accurate localisation is a key requirement for most autonomous robots. Increasingly, mobile robots use camera sensors. We describe an algorithm that uses edge features in a visual image to implement a sensor model for a given environment. The algorithm (NightOwl) was developed to help localise a Sony legged AIBO robot on a RoboCup soccer field, but has more general applicability. The NightOwl sensor model is optimised for efficient use of computing resources, is effective even when edge features are partially occluded and can be used with a variety of Bayes filter localisation methods.",
            "group": 2498,
            "name": "10.1.1.116.5920",
            "keyword": "",
            "title": "A Fast Vision Sensor Model: Matching Edges with NightOwl"
        },
        {
            "abstract": "Optimization techniques based on graph cuts have become a standard tool for many vision applications. These techniques allow to minimize efficiently certain energy functions corresponding to pairwise Markov Random Fields (MRFs). Currently, there is an accepted view within the computer vision community that graph cuts can only be used for optimizing a limited class of MRF energies (e.g. submodular functions). In this survey we review some results that show that graph cuts can be applied to a much larger class of energy functions (in particular, non-submodular functions). While these results are well-known in the optimization community, to our knowledge they were not used in the context of computer vision and MRF optimization. We demonstrate the relevance of these results to vision on the problem of binary texture restoration.",
            "group": 2499,
            "name": "10.1.1.116.6849",
            "keyword": "I.4.6.c Markov Random FieldsI.2.10.i Texture Index termsEnergy minimizationMarkov Random Fieldsquadratic pseudoboolean optimization",
            "title": "Minimizing non-submodular functions with graph cuts - a review"
        },
        {
            "abstract": "Every year, the Consortium for Mathematics and its Applications (C OMA OMAP) sponsors the Math-ematical Contest in Modeling (MCM), an international contest for undergraduates. We will discuss our strategy for developing models, writing the paper, the contest timeline, and team dynamics.",
            "group": 2500,
            "name": "10.1.1.116.7053",
            "keyword": "",
            "title": "Quest Quest of the THE Quest of the MCM Conquering the Math Contest in Modeling"
        },
        {
            "abstract": "Abstract \u2014 We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality in fitness landscape. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push the evolvability to increase. The search process switches between two phases: Conquest-of-the-Waters and Invasion-of-the-Land. A comparative study of the new algorithm and standard local search heuristics on the NKq-landscapes has shown advantage and limit of the scuba search. To enlighten qualitative differences between neutral search processes, the space is changed into a connected graph to visualize the pathways that the search is likely to follow. I.",
            "group": 2501,
            "name": "10.1.1.116.7755",
            "keyword": "",
            "title": "Scuba Search: when selection meets innovation"
        },
        {
            "abstract": "Design representations",
            "group": 2502,
            "name": "10.1.1.116.8009",
            "keyword": "",
            "title": "Structural"
        },
        {
            "abstract": " ",
            "group": 2503,
            "name": "10.1.1.116.8266",
            "keyword": "",
            "title": "Flexible Product Platforms"
        },
        {
            "abstract": "Absfrucf-This paper describes the implementation of the Genetic",
            "group": 2504,
            "name": "10.1.1.116.8496",
            "keyword": "",
            "title": "Algorithm for Standard-cell Placement (GASP). Unlike the other"
        },
        {
            "abstract": "Optimization of systems that involve uncertainty pose several challenges in computer science. This work revolves around three strands that are part of these challenges. The first is the optimization of large scale systems that involve uncertainty, the second is the attainment of meaningful description of the uncertainty and finally the computation of the global optimum of a function. In the first part, we consider decomposition approaches for the solution of multistage stochastic programs that appear in financial applications. In particular, we discuss the performance of two algorithms which we test on the mean-variance portfolio optimization problem. The first algorithm is based on a regularized version of Benders decomposition, we discuss its extension to the quadratic case. The second algorithm is an augmented Lagrangian method. In the second part, we consider the problem of recovering a compactly supported multidi-mensional probability distribution from the knowledge of a finite number of its moments. A maximum entropy distribution is postulated as the solution to the moment problem and",
            "group": 2505,
            "name": "10.1.1.116.9053",
            "keyword": "",
            "title": "Algorithms in Stochastic Optimization"
        },
        {
            "abstract": "Design representations",
            "group": 2506,
            "name": "10.1.1.116.9312",
            "keyword": "Structural",
            "title": "Physical"
        },
        {
            "abstract": "This work addresses the distortions in Functional Magnetic Resonance Images (FMRI) caused by subject motion. FMRI is a non-invasive technique which shows great promise in providing researchers and clinicians with neurological information both about healthy subjects and clinical patients by mapping functional activation within the brain using Echo Planar Imaging (EPI). If reliable information is to be obtained from these images, motion correction must be carried out in order to remove or suppress the artefacts arising from subject movement. This work begins by using exploratory data techniques to describe these artefacts so that they can be characterised according to their origin and spatio-temporal manifestation. Based on testing of the accuracy and consistency of existing rigid-body motion correction methods on FMRI data, a new registration algorithm \u2014 Motion Correction using the FMRIB Linear Image Registration Tool (MCFLIRT)  \u2014 has been developed. It is shown that while MCFLIRT is both more accurate and more",
            "group": 2507,
            "name": "10.1.1.116.9455",
            "keyword": "",
            "title": "Motion Correction for Functional Magnetic Resonance Images"
        },
        {
            "abstract": "Abstract. The aerospace industry has been investigating integrated modular avionics (IMA) for some years. IMA offers greater flexibility in the use of computing resources by reconfiguring the software to employ different processors and communications, in order to recover from failure and to redistribute workload. Such reconfiguration offers benefits, but poses difficulties for certification since current certification practice requires assessment of each configuration. The approach we have adopted is to seek means of clearing a configuration of a system and to identify a number of \u201cequivalent \u201d configurations. This requires us to establish \u201csafe\u201d reconfigurations for the IMA system. Technically, we have formulated the search for a set of \u201cequivalent \u201d configurations as a multiobjective optimisation problem. Pragmatically, the search produces configuration tables which could be used by the IMA operating system to make a \u201csafe \u201d change to an \u201cequivalent \u201d configuration, when necessary.",
            "group": 2508,
            "name": "10.1.1.116.9652",
            "keyword": "",
            "title": "Approaches to Certification of Reconfigurable IMA Systems"
        },
        {
            "abstract": "Modern engineering design problems often involve computation-intensive analysis and simulation processes. Design optimization based on such processes is desired to be efficient, informative, and transparent. This work proposes a rough set based approach that can identify multip,le sub-regions in a design space, within which all of the design points are expected to have a performance value equal to or less than a given level. The rough set method is applied iteratively on a growing sample set. A novel termination criterion is also developed to ensure a modest number of total expensive function evaluations to identify these sub-regions and search for the global optimum. The significances of the proposed method are two folds. First, it provides an intuitive method to establish the mapping from the performance space to the design space; given a performance level, its corresponding design region(s) can be identified. Such a mapping can be used to explore and visualize the entire design space. Second, it can be naturally extended to a global optimization method. It also bears potentials for more abroad applications to problems such as robust design optimization. The proposed method was tested with a number of test problems and compared with a few well-known global optimization algorithms.",
            "group": 2509,
            "name": "10.1.1.116.9838",
            "keyword": "rough setdesign optimizationspace explorationglobal optimization",
            "title": "Abstract Space Exploration and Global Optimization for Computationally Intensive Design Problems: A Rough Set Based Approach"
        },
        {
            "abstract": "Abstract \u2013 This paper presents an enhancement made to a high dimensional variant of a growing self organizing map called the High Dimensional Growing Self Organizing Map (HDGSOM) that enhances the clustering of the algorithm. The enhancement is based on randomness that expedites the self organizing process by moving the inputs out from local minima producing better clusters within a shorter training time. The enhancement is described in detail and several experiments on very large text datasets illustrating the effect of the enhancement are also presented.",
            "group": 2510,
            "name": "10.1.1.116.9894",
            "keyword": "Growing Feature MapsHigh DimensionsHDGSOMrHDGSOMGSOM",
            "title": "ENHANCING CLUSTERING PERFORMANCE OF FEATURE MAPS USING RANDOMNESS"
        },
        {
            "abstract": "LVB: parsimony and simulated annealing in the",
            "group": 2511,
            "name": "10.1.1.117.634",
            "keyword": "",
            "title": "search for phylogenetic"
        },
        {
            "abstract": "We propose a new approach to the problem of generating a simple topologically-c losed geometric model from a point-sampled volume data set. We call such a model a Geometrically Deformed Model or GDM. A GDM is created by placing a \u2018seed \u2019 model in the volume data set. The model is then deformed by a relaxation process that minimizes a set of constraints that provides a measure of bow well (be model tits the features in the data. Constraints are associated with each vertex in the model that control local deformation, interaction between the model and tbe data set. and the shape and topology of tbe model. Once generated, a GDM can be used for visualization. shape recognition, geometric measurements. or subjected to a series of geometric operations. This technique is of special importance because of the advent of nondestructive sensing equipment (CT, MRI) that generates point samples of true three-dimensional objects. CR Categories: 1.3.3 [Computer Graphics]: Picture/fmage Generation \u2014 di.rp/uy u/gori//vns \u2014 iie~\u2019ing a/gorirtms: 1.3.5",
            "group": 2512,
            "name": "10.1.1.117.1611",
            "keyword": "ModellingVolume Visualization. Volume ModellingConstraint Minimization",
            "title": "GeometricallyDeformed Models: A Method for Extracting Closed Geometric Models from Volume Data"
        },
        {
            "abstract": "The world we live in is a complex socio-technical system. Although social, organizational and policy analysts have long recognized that groups, organizations, institutions and the societies in which they are embedded are complex systems; it is only recently that we have had the tools for systematically thinking about, representing, modelling and analyzing these systems. These tools include multi-agent computer models and the body of statistical tools and measures in social networks. This paper uses social network analysis and multi-agent models to discuss how to destabilize networks. In addition, we illustrate the potential difficulty in destabilizing networks that are large, distributed, and composed of individuals linked on a number of socio-demographic dimensions. The specific results herein are generated, and our ability to think through such systems is enhanced, by using a multi-agent network approach to complex systems. Such an illustration is particularly salient in light of the tragic events",
            "group": 2513,
            "name": "10.1.1.117.2264",
            "keyword": "",
            "title": "\u00a9 2002 INSNA Destabilizing Networks 1"
        },
        {
            "abstract": "Simulation-based quantitative performance evaluation using specific applications is indispensable for developing architectures of self-reconfigurable devices since static analysis is difficult to estimate their performance. In order to generate configuration data needed for simulating various target architectures, we developed a synthesis tool which can be retargeted to various self-reconfigurable devices specified by architecture parameters. Given an application in C-language, our tool automatically executes data-flow analysis, technology mapping, and layout synthesis. Our tool enables us to perform efficient design-space exploration, and its retargetability helps fair evaluation of the devices on the same platform. This paper also shows architecture evaluation examples using our tool to demonstrate the advantage of our tool. Key words: coarse-grain, ALU-based reconfigurable architecture, high-level synthesis, layout synthesis 1.",
            "group": 2514,
            "name": "10.1.1.117.3659",
            "keyword": "",
            "title": "A Retargetable Compiler for Cell-Array-Based Self-Reconfigurable Architecture"
        },
        {
            "abstract": "Simulated annealing routing and wavelength",
            "group": 2515,
            "name": "10.1.1.117.4999",
            "keyword": "",
            "title": "multiplexing"
        },
        {
            "abstract": "This paper presents an efficient method to determine minimum System-On-Chip (SOC) test schedules with precedence and power constraints based on simulated annealing. The problem is solved using a partitioned testing scheme with run to completion that minimizes the number of idle test slots. The method can handle SOC test scheduling with and without power constraints in addition to precedence constraints that preserve desirable orderings among tests. We present experimental results for various SOC examples that demonstrate the effectiveness of the method. The method achieved optimal test schedules in all attempted cases in a short CPU time.",
            "group": 2516,
            "name": "10.1.1.117.5126",
            "keyword": "Embedded Core TestingTest SchedulingSimulated Annealing",
            "title": "A Simulated Annealing Algorithm for System-On-Chip Test Scheduling With Power and Precedence Constraints \u2217"
        },
        {
            "abstract": "planning for high-performance ASICs Design planning is emerging as a solution to some of the most difficult challenges of the deep-submicron VLSl design era. Reducing design turnaround time for extremely large designs with ever-increasing clock speeds, while ensuring first-pass implementation success, is exhausting the capabilities of traditional design tools. To solve this problem, we have designed and implemented a hierarchical design planning system that consists of a tightly integrated set of design and analysis tools. The integrated run-time environment, with its rich set of hierarchical, timing-driven design planning and implementation functions, provides an advanced platform for realizing a variety of ASIC and custom methodologies. One of the system's particular strengths is its tight integration with an incremental, static timing engine that assists in achieving timing closure in high-performance designs. The design planner is in production use at IBM internal and at external ASIC design centers. by J. Y. Sayah",
            "group": 2517,
            "name": "10.1.1.117.5691",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "",
            "group": 2518,
            "name": "10.1.1.117.6173",
            "keyword": "",
            "title": "Learning to act using real-time dynamic programming"
        },
        {
            "abstract": "Abstract \u2014 Computing has recently reached an inflection point with the introduction of multi-core processors. On-chip threadlevel parallelism is doubling approximately every other year. Concurrency lends itself naturally to allowing a program to trade performance for power savings by regulating the number of active cores, however in several domains users are unwilling to sacrifice performance to save power. We present a prediction model for identifying energy-efficient operating points of concurrency in well-tuned multithreaded scientific applications, and a runtime system which uses live program analysis to optimize applications dynamically. We describe a dynamic, phase-aware performance prediction model that combines multivariate regression techniques with runtime analysis of data collected from hardware event counters to locate optimal operating points of concurrency. Using our model, we develop a prediction-driven, phase-aware runtime optimization scheme that throttles concurrency so that power consumption can be reduced and performance can be set at the knee of the scalability curve of each program phase. The use of prediction reduces the overhead of searching the optimization space while achieving near-optimal performance and power savings. A thorough evaluation of our approach shows a reduction in power consumption of 10.8 % simultaneous with an improvement in performance of 17.9%, resulting in energy savings of 26.7%. Index Terms \u2014 Modeling and prediction, Application-aware adaptation, Energy-aware systems",
            "group": 2519,
            "name": "10.1.1.117.6999",
            "keyword": "",
            "title": "Prediction-based Power-Performance Adaptation of Multithreaded Scientific Codes"
        },
        {
            "abstract": "Abstract In this chapter the physical aspects of the global optimization of the geometry of atomic clusters are elucidated. In particular, I examine the structural principles that determine the nature of the lowest-energy structure, the physical reasons why some clusters are especially difficult to optimize and how the basin-hopping transformation of the potential energy surface enables these difficult clusters to be optimized.",
            "group": 2520,
            "name": "10.1.1.117.7020",
            "keyword": "atomic clustersbasin-hoppingmultiple funnels",
            "title": "Chapter 1 PHYSICAL PERSPECTIVES ON THE GLOBAL OPTIMIZATION OF ATOMIC CLUSTERS"
        },
        {
            "abstract": "Exponential growth in number of possible strategies with the increase in number of relations in a query has been identified as a major problem in the field of query optimization of relational databases. Present database systems use exhaustive search to find the best possible strategy. But as the size of a query grows, exhaustive search method itself becomes quite expensive. Other algorithms like A * algorithm, Simulated Annealing etc. have been suggested as a solution. However, all these algorithms fail to produce the best results; necessarily required for query execution. We did some modifications to the A * algorithm to produce a randomized form of the algorithm and compared it with the original A * algorithm and exhaustive search. The comparison results have shown improved A * algorithm to be almost equivalent in output quality along with a colossal decrease in search space in comparison to exhaustive search method. I.",
            "group": 2521,
            "name": "10.1.1.117.7907",
            "keyword": "",
            "title": "Query Processing, A * algorithm ABSTRACT"
        },
        {
            "abstract": "Abstract \u2014 We present in this paper a new algorithm to co-optimize the problems of test scheduling and core wrapper design under power constraints for core-based SoC (System on Chip) designs. The problem of test scheduling is first transformed into a floorplanning problem with a given maximum height (test access mechanism width) constraint. Then, we apply the B*-tree based floorplanning technique to solve the SoC test scheduling problem. Experimental results based on the ITC\u201902 benchmarks show that our method is very effective and efficient---our method obtains the best results ever reported for SoC test scheduling with power constraint in every efficient running time. Compared with recent works, our method achieves average improvements of 4.7 % to 20.1%. I.",
            "group": 2522,
            "name": "10.1.1.117.7943",
            "keyword": "",
            "title": "SOC Test Scheduling Using B*-tree Based Floorplanning Technique"
        },
        {
            "abstract": "Computationalorigami is a parallel-processing concept in which a regular array of processors can be folded along any dimension so that it can be simulated by a smaller number of processors. The problem of assigning functions to each of the processors is very much like the generalized electrical circuit layout problem. This paper presents a simple, polynomial time algorithm for placing and routing functions in an origami architecture. Empirical results are analyzed and optimizations suggested. 1",
            "group": 2523,
            "name": "10.1.1.117.8120",
            "keyword": "",
            "title": "A Simple Placement and Routing Algorithm for a Two-Dimensional Computational Origami Architecture"
        },
        {
            "abstract": "I would like to thank my advisors Professors Enrico",
            "group": 2524,
            "name": "10.1.1.117.8520",
            "keyword": "INTRODUZIONE IN ITALIANO...................................................................... v",
            "title": "To Luigina and Ugo"
        },
        {
            "abstract": "Mixture models form one of the most fundamental classes of generative models for clustered data. Specific application examples include text classification problems, image segmentation and motion detection, collaborative filtering and many others. However, quite surprisingly, very little had been known about algorithms which have provable performance guarantees within the framework of mixture models. This is the topic we study in this work. Our contribution is twofold. First, for the canonical problem of separating mixtures of continuous distributions in the high-dimensional Euclidean space, we provide the first algorithm that can learn distributions with heavy tails, including those with infinite variance and expectation. We formulate necessary conditions and provide an algorithm which guarantees that the underlying mixture model can be learned by observing only polynomially many samples. We also show that for many classes of distributions, our separation conditions are necessary for any algorithm which guarantees accurate reconstruction. Second for the case of discrete mixture models we give an efficient polynomial time algorithm with provable performance guarantees. Recasting of our algorithm for the text classification problem immediately results in a very fast unsupervised learning method, with an excellent classification accuracy. BIOGRAPHICAL SKETCH",
            "group": 2525,
            "name": "10.1.1.117.8860",
            "keyword": "ACKNOWLEDGEMENTS",
            "title": "ALGORITHMS FOR MIXTURE MODELS"
        },
        {
            "abstract": "In this report we continue development of a Bayesian CBIR system by considering the issue of an image display strategy. This is a decision problem and so within the Bayesian paradigm is to be solved by decision theory. We show how different display strategies may be quantified by an appropriate utility function, and compare different optimisation strategies. Examples are given. 1",
            "group": 2526,
            "name": "10.1.1.117.8879",
            "keyword": "",
            "title": "Decision-Theoretic Approaches to Display Strategies in Content Based Image Retrieval"
        },
        {
            "abstract": "0 This article presents a statistical method for detecting recombination in DNA sequence alignments, which is based on combining two probabilistic graphical models: (1) a taxon graph (phylogenetic tree) representing the relationship between the taxa, and (2) a site graph (hidden Markov model) representing interactions between different sites in the DNA sequence alignments. We adopt a Bayesian approach and sample the parameters of the model from the posterior distribution with Markov chain Monte Carlo, using a Metropolis-Hastings and Gibbs-within-Gibbs scheme. The proposed method is tested on various synthetic and real-world DNA sequence alignments, and we compare its performance with the established detection methods RECPARS, PLATO, and TOPAL, as well as with two alternative parameter estimation schemes. 1",
            "group": 2527,
            "name": "10.1.1.117.9193",
            "keyword": "DNA sequence alignmentrecombinationhidden Markov modelsMarkov",
            "title": "Detecting Recombination in 4-Taxa DNA Sequence Alignments with Bayesian Hidden Markov Models and"
        },
        {
            "abstract": "Abstract\u2014In this article, we evaluate the performance of three clustering algorithms, hard K-Means, single linkage, and a simulated annealing (SA) based technique, in conjunction with four cluster validity indices, namely Davies-Bouldin index, Dunn\u2019s index, Calinski-Harabasz index, and a recently developed index I. Based on a relation between the index I and the Dunn\u2019s index, a lower bound of the value of the former is theoretically estimated in order to get unique hard K-partition when the data set has distinct substructures. The effectiveness of the different validity indices and clustering methods in automatically evolving the appropriate number of clusters is demonstrated experimentally for both artificial and real-life data sets with the number of clusters varying from two to ten. Once the appropriate number of clusters is determined, the SA-based clustering technique is used for proper partitioning of the data into the said number of clusters.",
            "group": 2528,
            "name": "10.1.1.117.9297",
            "keyword": "Index Terms\u2014Unsupervised classificationEuclidean distanceK-Means",
            "title": "Performance Evaluation of Some Clustering Algorithms and Validity Indices"
        },
        {
            "abstract": "This paper provides a pure simulation approach to solving maximum expected utility (MEU) problems. MEU problems require both integration, to compute the expected utility, and optimization, to find the optimal decision. In most cases of interest, the expected utility does not have a analytical solution, even for a given value of the decision. One must apply gradient methods around numerical estimates of expected utility and its required derivatives. This issue arises in multi-period problems, but also in discret-time single-period problems, with parameter uncertainty. The simulation approach proposed here generates draws of the decision rule that concentrate on the optimal decision without the need for gradient methods. It can be applied to functional optimization and sequential decision problems with Bayesian learning. We illustrate the methodology in an asset allocation framework. The resulting MCMC algorithm is very efficient, nearly reducing to a pure Gibbs sampler, due to the properties of utility kernels.",
            "group": 2529,
            "name": "10.1.1.118.393",
            "keyword": "Key WordsMCMCexpected utilityportfolio choiceasset allocationsequential decisionslatent variablesoptimizationsimulated annealingevolutionary Monte CarloBayesian learning",
            "title": "Maximum Expected Utility via MCMC"
        },
        {
            "abstract": "As the logic capacity of FPGAs continues to increase with deep submicron technology, performing a full recompilation for small iterative changes in a large design is an extremely time-consuming and costly process. To address this issue, this thesis presents a new incremental placement algorithm for FPGAs named \u201ciPlace \u201d that significantly reduces the time required for recompilation. The iPlace algorithm is based on shifting, compaction, and annealing. Key ideas from the algorithm include a placement super-grid that is larger than the physical size of the FPGA. The super-grid allows insertion of additional CLBs into areas with no free locations by CPU-efficient shifting. This is followed by a compaction scheme to re-legalize CLBs that are shifted to illegal locations outside of the physical size of the FPGA. The algorithm ends with a low-temperature anneal to improve quality. This algorithm is capable of handling multiple design changes across large regions of a FPGA. This is especially useful for hierarchical designs where sub-circuits are re-used multiple times. If one such sub-circuit is modified, iPlace can quickly produce a high quality incremental placement solution. For a single region of",
            "group": 2530,
            "name": "10.1.1.118.462",
            "keyword": "",
            "title": "INCREMENTAL PLACEMENT FOR FIELD-PROGRAMMABLE GATE ARRAYS"
        },
        {
            "abstract": "ABSTRACT The computational identification of the optimal three-dimensional fold of even a small peptide chain from its sequence, without reference to other known structures, is a complex problem. There have been several attempts at solving this by sampling the potential energy surface of the molecule in a systematic manner. Here we present a new method to carry out the sampling, and to identify low energy conformers of the molecule. The method uses mutually orthogonal Latin squares to select (of the order of) n 2 points from the multidimensional conformation space of size m n, where n is the number of dimensions (i.e., the number of conformational variables), and m specifies the fineness of the search grid. The sampling is accomplished by first calculating the value of the potential energy function at each one of the selected points. This is followed by analysis of these values of the potential energy to obtain the optimal value for each of the n-variables separately. We show that the set of the n-optimal values obtained in this manner specifies a low energy conformation of the molecule. Repeated application of the method identifies other low energy structures. The computational complexity of this algorithm scales as the fourth power of the size of the molecule. We applied this method to several small peptides, such as the neuropeptide enkephalin, and could identify a set of low energy conformations for each. Many of the structures identified by this method have also been previously identified and characterized by experiment and theory. We also compared the best structures obtained for the tripeptide (Ala)3 by the present method, with those obtained by an exhaustive grid search, and showed that the algorithm is successful in identifying all the low energy conformers of this molecule.",
            "group": 2531,
            "name": "10.1.1.118.795",
            "keyword": "1999Klein et al1998Howard and Kollman1988). Often",
            "title": "Enhanced Sampling of the Molecular Potential Energy Surface Using Mutually Orthogonal Latin Squares: Application to Peptide Structures"
        },
        {
            "abstract": "You are welcome to use the material under the license provided at",
            "group": 2532,
            "name": "10.1.1.118.1259",
            "keyword": "",
            "title": "1. TABLE OF CONTENTS"
        },
        {
            "abstract": "Abstract. This paper is the result of a literature study carried out by the authors. It is a review of the different attempts made to solve the Travelling Salesman Problem with Genetic Algorithms. We present crossover and mutation operators, developed to tackle the Travelling Salesman Problem with Genetic Algorithms with different representations such as: binary representation, path representation, adjacency representation, ordinal representation and matrix representation. Likewise, we show the experimental results obtained with different standard examples using combination of crossover and mutation operators in relation with path representation.",
            "group": 2533,
            "name": "10.1.1.118.2597",
            "keyword": "Key wordsTravelling Salesman ProblemGenetic Algorithmsbinary representationpath",
            "title": "c 1999 Kluwer Academic Publishers. Printed in the Netherlands. Genetic Algorithms for the Travelling Salesman Problem: A Review of Representations and Operators"
        },
        {
            "abstract": "Network virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding\u2014 mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research has focused on two main approaches: (i) significantly restricting the problem space to allow efficient solutions or (ii) proposing heuristic algorithms that do not use the substrate resources efficiently. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtual-network topologies. Our simulation experiments show that path splitting, path migration, and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks. 1.",
            "group": 2534,
            "name": "10.1.1.118.2811",
            "keyword": "",
            "title": "ABSTRACT Rethinking Virtual Network Embedding: Substrate Support for Path Splitting and Migration"
        },
        {
            "abstract": "Orthogonal arrays are used widely in manufacturing and high-technology industries for quality and productivity improvement experiments. For reasons of run size economy or \ufffd exibility, nearly-orthogonal arrays are also used. The construction of orthogonal or nearly-orthogonal arrays can be quite challenging. Most existing methods are complex and produce limited types of arrays. This article describes a simple and effective algorithm for constructing mixed-level orthogonal and nearly-orthogonal arrays that can construct a variety of small-run designs with good statistical properties ef \ufffd ciently. KEY WORDS: D-optimality; Exchange algorithm; Interchange algorithm; J 2-optimality. 1.",
            "group": 2535,
            "name": "10.1.1.118.5124",
            "keyword": "",
            "title": "An algorithm for constructing orthogonal and nearly-orthogonal arrays with mixed levels and small runs"
        },
        {
            "abstract": null,
            "group": 2536,
            "name": "10.1.1.118.5298",
            "keyword": "",
            "title": "1 Introduction Query Optimization"
        },
        {
            "abstract": "A continuous location problem in which a firm wants to set up two or more new facilities in a competitive environment is considered. Other facilities offering the same product or service already exist in the area. Both the locations and the qualities of the new facilities are to be found so as to maximize the profit obtained by the firm. This is a global optimization problem, with many local optima. In this paper we analyze several approaches to solve it, namely, three multi-start local search heuristics, a multistart simulated annealing algorithm and two variants of an evolutionary algorithm. Through a comprehensive computational study it is shown that the evolutionary algorithms are the heuristics which provide the best solutions. Furthermore, using a set of problems for which the optimal solutions are known, only the evolutionary algorithms were able to find the optimal solutions for all the instances. The evolutionary strategies presented in this paper can be easily adapted to handle other continuous location problems.",
            "group": 2537,
            "name": "10.1.1.118.5610",
            "keyword": "continuous",
            "title": "Solving the Multiple Competitive Facilities Location and Design Problem on the Plane"
        },
        {
            "abstract": "This paper investigates the design space for techniques that enable runtime, autonomic program adaptation for high-performance and low-power execution via event-driven performance prediction. The emerging multithreaded and multicore processor architectures enable applications to trade performance for reduced power consumption via regulating concurrency. At the same time however, power and performance adaptation opportunities for multithreaded programs in high-end computing environments are constrained by the fact that users are unwilling to compromise performance for saving power. Runtime systems that enable autonomous program adaptation are an appealing solution in the specific context, due to the challenges that arise in statically identifying the optimal energy-efficient operating points in each program, and the concerns of delegating the complexity of this task to end-users or application developers. System software needs to identify and exploit the power saving opportunities that arise due to the inability of code to effectively utilize all the available resources in the system, or the inability of the system to overcome scalability bottlenecks in parallel code. The techniques investigated in this paper fall into a broader class of methods that collect",
            "group": 2538,
            "name": "10.1.1.118.5968",
            "keyword": "",
            "title": "On the Design of Online Predictors for Autonomic Power-Performance Adaptation of Multithreaded Programs"
        },
        {
            "abstract": "Despite the success of conventional Sanger sequencing, significant regions of many genomes still present major obstacles to sequencing. Here we propose a novel approach with the potential to alleviate a wide range of sequencing difficulties. The technique involves extracting target DNA sequence from variants generated by introduction of random mutations. The introduction of mutations does not destroy original sequence information, but distributes it amongst multiple variants. Some of these variants lack problematic features of the target and are more amenable to conventional sequencing. The technique has been successfully demonstrated with mutation levels up to an average 18 % base substitution and has been used to read previously intractable poly(A), AT-rich and GC-rich motifs.",
            "group": 2539,
            "name": "10.1.1.118.6568",
            "keyword": "",
            "title": " Unlocking Hidden Genomic Sequence"
        },
        {
            "abstract": "Abstract \u2014 In recently introduced \u201cmulti-cell access\u201d schemes, cells (rather than users) compete for the spectral resource. In a previous paper [1], a framework for such a scheme was proposed, using only local channel information. In the framework each cell competes for access through a credit that is a function of the signal to noise ratio of its scheduled user. Access is then given to a cell with a probability dependent on the access function. In [1] an ad-hoc choice of function was formulated. In this work we investigate which access function actually optimizes the system capacity, utilizing a numerical optimization procedure. We obtain a surprsingly simple solution which also corroborates previous results. For a realistic path loss model we find that our multicell distributed access scheme gives a 19 % gain compared to having all cells on, and more than a 50 % increase in system capacity compared to keeping a traditional static spectral reuse scheme. I.",
            "group": 2540,
            "name": "10.1.1.118.6634",
            "keyword": "",
            "title": "Probabilistic Access Functions for Multi-Cell Wireless Schemes"
        },
        {
            "abstract": "We review the papers devoted to solution of reconstruction tomographic prob-lems by using neural networks. Recent developments in the solution of linear and non-linear tomographic problems in various types of tomography are sur-veyed.",
            "group": 2541,
            "name": "10.1.1.119.45",
            "keyword": "tomographytomographic problemneural networkperceptronsen- sorfiber optic",
            "title": "Neural network methods of reconstruction tomography problem solutions"
        },
        {
            "abstract": "Motivation: De novo protein structure prediction can be formulated as search in a high-dimensional space. One of the most frequently used computational tools to solve such search problems is the Monte Carlo method. We present a novel search technique, called model-based search. This method samples the high-dimensional search space to build an approximate model of the underlying function. This model is incrementally refined in areas of interest, while areas that are not of interest are excluded from further exploration. Model-based search derives its efficiency from the fact that the information obtained during the exploration of the search space is used to guide further exploration. In contrast, Monte Carlo-based techniques are memory-less and exploration is performed based on random walks, ignoring the information obtained in previous steps. Results: Model-based search is applied to protein structure prediction, where search is employed to find the global minimum of the protein\u2019s energy landscape. We show that model-based search uses computational resources more efficiently to find lower-energy conformations of proteins when compared to one of the leading protein structure prediction methods, which relies on a tailored Monte Carlo method to perform search. The performance improvements become more pronounced as the dimensionality of the search space increases. We show that model-based search enables more accurate protein structure prediction than previously possible. Furthermore, we believe that similar performance improvements can be expected in other problems that are currently solved using Monte Carlo-based search methods. Availability: An implementation of model-based search can be obtained by contacting the authors. Contact:",
            "group": 2542,
            "name": "10.1.1.119.666",
            "keyword": "",
            "title": "BIOINFORMATICS Improving Protein Structure Prediction with Model-Based Search"
        },
        {
            "abstract": "Design representations Represents functionality but not implementation",
            "group": 2543,
            "name": "10.1.1.119.3146",
            "keyword": "",
            "title": "Structural"
        },
        {
            "abstract": "Abstract \u2014 This paper introduces the research on possibility of global optimization elements and \u03b5-insensitive learning techniques integration in aim of fuzzy if-then rules extraction quality increase. The new learning algorithm of neuro-fuzzy system with parameterized consequents is introduced. It consists in integration of deterministic annealing and \u03b5 iterative quadratic programming method. The proposed algorithm indicates generalization ability and outliers robustness improvement comparing with zero-tolerance learning procedures. To show usefulness of introduced method two numerical experiments concerning system identification and chaotic time series prediction problems are",
            "group": 2544,
            "name": "10.1.1.119.3900",
            "keyword": "Index Terms \u2014 Neuro-fuzzy systemsrules extractiondeterministic annealing\u03b5-insensitive",
            "title": "Fuzzy If-Then Rules Extraction by Means of \u03b5-Insensitive Learning Techniques Integrated with Deterministic Annealing Optimization Method"
        },
        {
            "abstract": "In a mobile environment, each mobile host should have a home agent on its home network that maintains a registry of the current location of the mobile host. This registry is normally updated every time a mobile host moves from one subnet to another. We study the tradeoff between the cost of updating the registry and the cost of searching for a mobile host while it is away from home. Using a set of special agents, called proxy agents, which implement a two-tier update process, the cost of updates could be reduced; however, the search cost might increase. We study different approaches to identify a set of proxy agents that minimizes the cost of search. In this paper, we use mathematical programming to obtain optimal solutions to the problem. We consider two situations: the cost of search measured by the sum of all search message costs, and the cost of search measured by the maximum cost of such messages. For these two respective cases we formulate the minimization of the cost of search as Min-Sum and Min-Max problems. For large networks in which the optimization problem may be intractable, we study three different approximate approaches: 1) clustering, 2) genetic algorithms, and 3) simulated annealing. Results of a large set of experiments are presented. 1. Introduction and Related",
            "group": 2545,
            "name": "10.1.1.119.3981",
            "keyword": "",
            "title": "Abstract"
        },
        {
            "abstract": "Except where reference is made to the work of others, the work described in this thesis is my own or was done in collaboration with my advisory committee. This thesis does not include proprietary or classified information. Certificate of Approval:",
            "group": 2546,
            "name": "10.1.1.119.5663",
            "keyword": "shown to solve largecomplex and",
            "title": "Parallel Simulated Annealing With Load Balancing On A Temporally Heterogeneous Cluster Of Workstations"
        },
        {
            "abstract": "effect of phenotypic plasticity on evolution in multipeaked fitness landscapes",
            "group": 2547,
            "name": "10.1.1.119.5705",
            "keyword": "",
            "title": "The"
        },
        {
            "abstract": "Abstract: This paper presents a new stochastic approach for solving combinatorial optimization problems by using a new selection method, i.e. SA-selection, in genetic algorithm (GA). This approach combines GA with simulated annealing (SA) to improve the performance of GA. GA and SA have complementary strengths and weaknesses. While GA explores the search space by means of population of search points, it suffers from poor convergence properties. SA, by contrast, has good convergence properties, but it cannot explore the search space by means of population. However, SA does employ a completely local selection strategy where the current candidate and the new modification are evaluated and compared. To verify the effectiveness of the proposed method, the optimization of a fuzzy controller for balancing an inverted pendulum on a cart is considered.",
            "group": 2548,
            "name": "10.1.1.119.5707",
            "keyword": "problems become more and more complex. For",
            "title": "SA-selection-based genetic algorithm for the design of fuzzy controller"
        },
        {
            "abstract": "This report proposes a double simulated annealing (DSA) for functional magnetic resonance image (fMRI) analysis. The first simulated annealing (SA) is used to disconnect the brain from the skull. The second SA is applied to locate the activation area of the fMRIs. The performance evaluation of this approach includes receiver-operating characteristic (ROC) analysis, similarity analysis, and comparisons with other analytical methods such as classical SA (CSA), a t-Test, cross-correlation (CCR), and a general linear model (GLM). Experimental results show that the proposed algorithm can get better solutions than other approaches.",
            "group": 2549,
            "name": "10.1.1.119.5801",
            "keyword": "functional magnetic resonance imagingsimulated annealingreceiver-operating characteristic",
            "title": "DOUBLE SIMULATED ANNEALING FOR FUNCTIONAL MRI ANALYSIS"
        },
        {
            "abstract": "Constraints of artificial neural networks for rainfall-runoff modelling: trade-offs in hydrological state representation and model evaluation",
            "group": 2550,
            "name": "10.1.1.119.5859",
            "keyword": "",
            "title": "Sciences Discussions"
        },
        {
            "abstract": "Simulated annealing has earlier been found successful in sparse array optimization. However, finding the array pattern requires a discrete Fourier transform which makes the algorithm time consuming. A faster evaluation of the array pattern allows more element configurations to be searched. This gives the algorithm a larger degree of stochastic freedom, which improves the peak sidelobe level of the final array. Several new sparse arrays are presented, which are an improvement of those found in the literature. 1.",
            "group": 2551,
            "name": "10.1.1.119.6218",
            "keyword": "",
            "title": "OPTIMIZATION OF SPARSE ARRAYS BY AN IMPROVED SIMULATED ANNEALING ALGORITHM"
        },
        {
            "abstract": "Abstract\u2014Previous work in antenna optimization has primarily focused on applications of optimization algorithms in conjunction with problem-specific or semi-analytic tools. However, recent developments in fast algorithms now offer the possibility of designs and moreover allow for full flexibility in material specification across three dimensions. As an example, this paper combines genetic algorithms (GA) and simulated annealing (SA) with fast hybrid finite-element boundary integral simulations to develop full three-dimensional (3-D) antenna designs using shape, topology, as well as material optimization. To illustrate these optimization methods as well as compare between GA and SA, three different antenna designs are considered. First, a folded-slot antenna is optimized for broad-band performance, followed by an irregular-shaped dual-band antenna design. As a third design, which combines shape and material optimization, a bandgap substrate is designed to substantially increase the bandwidth of a patch residing on the optimized substrate. Index Terms\u2014Antennas, electromagnetic bandgap structures, design optimization, periodic substrates. I.",
            "group": 2552,
            "name": "10.1.1.119.6349",
            "keyword": "",
            "title": "Design Optimization of Conformal Antennas by Integrating Stochastic Algorithms With the Hybrid Finite-Element Method"
        },
        {
            "abstract": "The Bayesian Ying-Yang (BYY) harmony learning acts as a general statistical learning framework, featured by not only new regularization techniques for parameter learning but also a new mechanism that implements model selection either automatically during parameter learning or via a new class of model selection criteria used after parameter learning. In this paper, further advances on BYY harmony learning by considering modular inner representations are presented in three parts. One consists of results on unsupervised mixture models, ranging from Gaussian mixture based Mean Square Error (MSE) clustering, elliptic clustering, subspace clustering to NonGaussian mixture based clustering not only with each cluster represented via either Bernoulli\u2013Gaussian mixtures or independent real factor models, but also with independent component analysis implicitly made on each cluster. The second consists of results on supervised mixture-ofexperts (ME) models, including Gaussian ME, Radial Basis Function nets, and Kernel regressions. The third consists of two strategies for extending the above structural mixtures into self-organized topological maps. All these advances are introduced with details on three issues, namely, (a) adaptive learning algorithms, especially elliptic, subspace, and structural rival penalized competitive learning algorithms, with model selection made automatically during learning; (b) model selection criteria for being used after parameter learning, and (c) how these",
            "group": 2553,
            "name": "10.1.1.119.7460",
            "keyword": "",
            "title": "2002 Special Issue BYY harmony learning, structural RPCL, and topological self-organizing on mixture models q"
        },
        {
            "abstract": "under the direction of his Thesis Advisor and approved by his Thesis Committee, has been presented to and accepted by the Dean of Graduate Studies, in partial",
            "group": 2554,
            "name": "10.1.1.119.8028",
            "keyword": "",
            "title": "DEANSHIP OF GRADUATE STUDIES"
        },
        {
            "abstract": "gates on a single integrated circuit, so you have to partition the gates between separate circuits. Let\u2019s assume that you are forced to place exactly n/2 gates each on two integrated circuits. The connections between the gates across the partition are slow, energy consuming, and heat producing, while the cost associated with connections inside an integrated circuit are negligible. So, you want to divide the network of gates such that the cost function C, the number of connections cutting across the partition, is minimized (see Figure 1). Because a million computers will be running almost nonstop for 10 years, removing even one costly connection would be worthwhile. Fortunately, this (simplified) problem can be mapped onto the well-known graph-bipartitioning problem. In this problem, the n gates are the vertices of a graph with edges between two connected gates. Each vertex is a Boolean variable, with state \u201c0 \u201d if placed on the left integrated circuit and state \u201c1 \u201d if placed on the right integrated circuit. Although the graph of connections is fixed, the vertices can be moved so that we may obtain a good partition. Unfortunately, optimizing the equal partition is NP-hard; that is, the computations needed to find the global optimum with certainty for even the cleverest algo-",
            "group": 2555,
            "name": "10.1.1.119.8167",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Experiments over a variety of optimization problems indicate that scale-effective convergence is an emergent behavior of certain computer-based agents, provided these agents are organized into an asynchronous team (A-Team). An A-Team is a problem-solving architecture in which the agents are autonomous and cooperate by modifying one another\u2019s trial-solutions. These solutions circulate continually. Convergence is said to occur if and when a persistent solution appears. Convergence is said to be scale-effective if the quality of the persistent solution increases with the number of agents, and the speed of its appearance increases with the number of computers. This paper uses a traveling salesman problem to illustrate scale-effective behavior and develops Markov models that explain its occurrence in A-Teams, particularly, how autonomous agents, without strategic planning or centralized coordination, can converge to solutions of arbitrarily high quality. The models also predict two properties that remain to be experimentally confirmed: \u2022 construction and destruction are dual processes. In other words, adept destruction can compensate for inept construction in an A-Team, and vice-versa. (Construction refers to the process of creating or changing solutions, destruction, to the process of erasing solutions.) \u2022 solution-quality is independent of agent-phylum. In other words, A-Teams provide an organizational framework in which humans and autonomous mechanical agents can cooperate effectively. 2 1.",
            "group": 2556,
            "name": "10.1.1.119.8685",
            "keyword": "",
            "title": "Asynchronous Teams: Cooperation Schemes for Autonomous Agents"
        },
        {
            "abstract": "Software is increasingly complex and is used in increasingly critical applications. Sophisticated techniques are available for verifying that software systems work correctly, but these techniques can be very difficult and expensive to use. Researchers have developed tools to automatically verify software models, but using these tools can still be very costly, in terms of manual effort and expertise required to build accurate models and to formally specify required properties, and also in terms of the time and memory required to run these tools. Much work has been done to simplify the process of building software models and to improve the performance of verification tools, resulting in a variety of different modeling languages, each with features designed to reduce effort or improve performance for certain types of input models, and a range of verification tools, each with a different set of strategies available for reducing time and memory requirements. It can be difficult to determine which verification strategy is best for a particular software system. Others have observed complementary relationships between tools and have argued that there is no single best tool\u2014that as users\u2019 needs change the choice of tool should change as well. This dissertation provides further evidence for complementary relationships between verification tools, specifically considering tools available for specifications of synchronous software systems",
            "group": 2557,
            "name": "10.1.1.119.9964",
            "keyword": "",
            "title": " Combining Complementary Formal Verification Strategies to Improve Performance and Accuracy"
        },
        {
            "abstract": "Abstract\u2014Three-dimensional (3-D) packaging via system-onpackage (SOP) is a viable alternative to system-on-chip (SOC) to meet the rigorous requirements of today\u2019s mixed signal system integration. In this article, we present the first physical design algorithms for thermal and power supply noise-aware 3-D placement and crosstalk-aware 3-D global routing. Existing approaches consider the thermal distribution, power supply noise, and crosstalk issues as an afterthought, which may require an expensive cooling scheme, more decoupling capacitors @adecapA, and additional routing layers. Our goal is to overcome this problem with our thermal/decap/crosstalk-aware 3-D layout automation tools. The traditional design objectives such as performance, area, wirelength, and via are considered simultaneously to ensure high quality results. The related experimental results demonstrate the effectiveness of our approaches. Index Terms\u2014Crosstalk, mixed-signal CAD, placement and routing, power supply noise, system-on-package (SOP), thermal distribution, three-dimensional (3-D) packaging. I.",
            "group": 2558,
            "name": "10.1.1.120.1081",
            "keyword": "",
            "title": "Placement and Routing for 3D System-On-Package Designs"
        },
        {
            "abstract": "This paper proposes a novel strategy that uses hypergraph partitioning and K-way iterative mapping-refinement heuristics for scheduling a batch of data-intensive tasks with batch-shared I/O behavior on heterogeneous collections of storage and compute clusters. The strategy formulates file sharing among tasks as a hypergraph to minimize the I/O overheads due to duplicate file transfers and employs a K-way iterative mapping-refinement scheme to adapt to the heterogeneity of compute clusters and storage networks in the system. We evaluate the proposed approach through real experiments and simulations on application scenarios from two application domains; satellite data processing and biomedical imaging. Our experimental results show that our approach can achieve significant performance improvement over algorithms such as HPS, Shortest Job First, MinMin, MaxMin and Sufferage for workloads with high degree of shared I/O among tasks. 1",
            "group": 2559,
            "name": "10.1.1.120.1105",
            "keyword": "",
            "title": "Scheduling of Tasks with Batch-shared I/O on Heterogeneous Systems \u2217"
        },
        {
            "abstract": "of high complexity data for the inversion of metric InSAR in urban environments Vom Fachbereich Elektrotechnik und Informatik der",
            "group": 2560,
            "name": "10.1.1.120.1161",
            "keyword": "",
            "title": "zur Erkl\u00e4rung des akademischen Grades"
        },
        {
            "abstract": "consolidation can benefit from the prediction of VM page miss rate at each candidate memory size. Such prediction is challenging for the hypervisor (or VM monitor) due to a lack of knowledge on VM memory access pattern. This paper explores the approach that the hypervisor takes over the management for part of the VM memory and thus all accesses that miss the remaining VM memory can be transparently traced by the hypervisor. For online memory access tracing, its overhead should be small compared to the case that all allocated memory is directly managed by the VM. To save memory space, the hypervisor manages its memory portion as an exclusive cache (i.e., containing only data that is not in the remaining VM memory). To minimize I/O overhead, evicted data from a VM enters its cache directly from VM memory (as opposed to entering from the secondary storage). We guarantee the cache correctness by only caching memory pages whose current contents provably match those of corresponding storage locations. Based on our design, we show that when the VM evicts pages in the LRU order, the employment of the hypervisor cache does not introduce any additional I/O overhead in the system. We implemented the proposed scheme on the Xen para-virtualization platform. Our experiments with microbenchmarks and four real data-intensive services (SPECweb99, index searching, TPC-C, and TPC-H) illustrate the overhead of our hypervisor cache and the accuracy of cache-driven VM page miss rate prediction. We also present the results on adaptive VM memory allocation with performance assurance. 1",
            "group": 2561,
            "name": "10.1.1.120.2356",
            "keyword": "",
            "title": "Virtual Machine Memory Access Tracing With Hypervisor Exclusive Cache \u2217 Abstract"
        },
        {
            "abstract": "calculations can be formulated as optimization problems with or without restrictions. As indicated by Henderson et al. [1] the formulation of thermodynamic calculations for optimization problems offers some advantages: a) the use of a robust optimization method, b) the possibility of using a direct optimization method which requires only calculations of the objective function and c) the use of an iterative procedure whose convergence is almost independent on the initial guesses. Some examples of these calculations are phase stability analysis, phase equilibrium problems, parameter estimation in thermodynamic models, calculation of critical points, among others. These problems are non-linear, multivariable and the objective function used as optimization criterion is non-convex with several local optimums. By consequence, its solving with local optimization methods is not reliable because they generally converge to local optimums. During the last years, the development and application of global optimization strategies have increased in many areas of Chemical Engineering. Global optimization methods can be classified as deterministic and stochastic [2]. The first class offers a guarantee to find the global optimum of the objective function [21, 32, 24]. However, these strategies often require high computational time (generally more time than stochastic methods) and in some cases the problem reformulation is needed. In the other hand, stochastic optimization methods are robust numerical tools that present a reasonable computational effort in the optimization of multivariable functions; they are applicable to ill-structure or unknown structure problems and can be used with all thermodynamic models [3].",
            "group": 2562,
            "name": "10.1.1.120.2745",
            "keyword": "",
            "title": "Thermodynamic Calculations Using a Simulated Annealing Optimization Algorithm"
        },
        {
            "abstract": "Vol. 21 Suppl. 1 2005, pages i66\u2013i74 doi:10.1093/bioinformatics/bti1029 Improving protein structure prediction with model-based search",
            "group": 2563,
            "name": "10.1.1.120.3234",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "I would like to extend my sincere gratitude to my advisor, Dr. Don Bouldin, for his support and guidance in this project. Without him, this work could never have been completed. Special thanks to Dr. Chandra Tan for his effort and assistance which contributed greatly to this work. I would also like to thank Dr. Mike Langston, Dr. Danny Newport, and Dr. Dan Koch for serving as members of my thesis committee. I also",
            "group": 2564,
            "name": "10.1.1.120.5491",
            "keyword": "",
            "title": "Development and Experimental Evaluation of Partitioning Algorithms for Adaptive Computing Systems"
        },
        {
            "abstract": "The world is covered with millions of webcams, many transmit everything in their field of view over the Internet 24 hours a day. A web search finds public webcams in airports, intersections, classrooms, parks, shops, ski resorts, and more. Even more private surveillance cameras cover many private and public facilities. Webcams are an endless resource, but most of the video broadcast will be of little interest due to lack of activity. We propose to generate a short video that will be a synopsis of an endless video streams, generated by webcams or surveillance cameras. We would like to address queries like \u201cI would like to watch in one minute the highlights of this camera broadcast during the past day\u201d. The process includes two major phases: (i) An online conversion of the video stream into a database of objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user\u2019s query. To include maximum information in a short synopsis we simultaneously show activities that may have happened at different times. The synopsis video can also be used as an index into the original video stream. 1.",
            "group": 2565,
            "name": "10.1.1.120.6352",
            "keyword": "",
            "title": "Webcam synopsis: Peeking around the world"
        },
        {
            "abstract": "Search algorithms are biased because many factors related to the algorithm design \u2013 such as representation, decision points, search control, memory usage, heuristic guidance, and stopping criteria \u2013 or the problem instance characteristics impact how a search algorithm performs. The variety of algorithms and their parameterized variants make it difficult to select the most efficient algorithm for a given problem instance. It seems natural to apply learning to the algorithm selection problem of allocating computational resources among a portfolio of algorithms that may have complementing (or competing) search technologies. Such selection is called the portfolio strategy. This research exam studies the state-of-the-art in portfolio strategy by examining five recent papers, listed below, from the AI community. The specific focus of the exam will identify the key issues in the mechanism and evaluation of the portfolio strategy. But the discussion of these papers will also include a summary of the author\u2019s primary findings, a more specific context within its community, the implications to the specific community where the paper is published, and the implications to the larger AI community. This set of papers is representative, but not exhaustive, of recent work in portfolios. All selected portfolios model the runtime behavior of various algorithms for combinatorial problems. The papers also",
            "group": 2566,
            "name": "10.1.1.120.6486",
            "keyword": "",
            "title": "Harnessing Algorithm Bias A Study of Selection Strategies and Evaluation for Portfolios of Algorithms"
        },
        {
            "abstract": "VLSI standard cell placement is the process of arranging circuit components (modules) on a silicon layout. The cell placement problem is a proven NP hard combinatorial optimization problem. The complexity of this problem increases when multiple optimization objectives are considered simultaneously. In this paper, a novel technique is presented to address this hard problem, while optimizing multiple objectives. A major difficulty with such multi-objective combinatorial optimization problems is the existence of a very large solution search space, one of which is the desired optimal solution. Simulated Evolution (SE) a general iterative heuristic is used to traverse the large search space, while fuzzy logic is resorted to assist in multi-criteria decision making and overcome the imprecise nature of design information at placement stage. New fuzzy aggregation functions are proposed. SE is hybridized with force directed algorithm to speed-up the search. The proposed schemes are compared with previously presented SE based heuristics. The implementations exhibit considerable improvement in terms of both solution quality and runtime. 1",
            "group": 2567,
            "name": "10.1.1.120.7764",
            "keyword": "",
            "title": "Fast Fuzzy Force-Directed/Simulated Evolution Metaheuristic for Multiobjective VLSI Cell Placement"
        },
        {
            "abstract": "In this paper, we deal with arbitrary convex and concave rectilinear module packing using the Transitive Closure Graph (TCG) representation. The geometric meanings of modules are transparent to TCG and its induced operations, which makes TCG an ideal representation for floorplanning/placement with arbitrary rectilinear modules. We first partition a rectilinear module into a set of submodules and then derive necessary and sufficient conditions of feasible TCG for the submodules. Unlike most previous works that process each submodule individually and thus need post processing to fix deformed rectilinear modules, our algorithm treats a set of submodules as a whole and thus not only can guarantee the feasibility of each perturbed solution but also can eliminate the need of the post processing on deformed modules, implying better solution quality and running time. Experimental results show that our TCG-based algorithm is capable of handling very complex instances; further, it is very efficient and results in better area utilization than previous work. 1",
            "group": 2568,
            "name": "10.1.1.120.8257",
            "keyword": "",
            "title": "Arbitrary convex and concave rectilinear module packing using TCG"
        },
        {
            "abstract": "Rescheduling This paper describes an iterative repair search method called constraint-based simulated annealing. Simulated annealing is a hill climbing search technique capable of escaping local minima. We demonstrate the utility of our constraint-based framework by comparing search performance with and without the constraint framework on a suite of randomly generated problems. We also show results of applying the technique to the NASA Space Shuttle ground processing problem. These experiments demonstrate that the search method scales to complex, real-world problems and reflects interesting anytime behavior. 1 introduction Iterative repair scheduling techniques typically hill-climb through a space of complete sched-ules repeatedly making patches or fixes to the schedule's weak points [Zwe90, Ming0, Bie91]. The informedness of individual repairs can range from weak, random repairs to very knowl-edge intensive repairs. This paper presents a search framework for incorporating repair",
            "group": 2569,
            "name": "10.1.1.120.8799",
            "keyword": "",
            "title": "II Iterative Repair for Scheduling and"
        },
        {
            "abstract": "... for external access to five of JPL\u2019s real-world requirements models, anonymized to conceal proprietary information, but retaining their computational nature. Experimentation with these models, reported herein, demonstrates a dramatic speedup in the computations performed on them. These models have a well defined goal: select mitigations that retire risks which, in turn, increases the number of attainable requirements. Such a non-linear optimization is a well-studied problem. However identification of not only (a) the optimal solution(s) but also (b) the key factors leading to them is less well studied. Our technique, called KEYS, shows a rapid way of simultaneously identifying the solutions and their key factors. KEYS improves on prior work by several orders of magnitude. Prior experiments with simulated annealing or treatment learning took tens of minutes to hours to terminate. KEYS runs much faster than that; e.g for one model, KEYS ran 13,000 times faster than treatment learning (40 minutes versus 0.18 seconds). Processing these JPL models is a non-linear optimization problem: the fewest mitigations must be selected while achieving the most requirements. Non-linear optimization is a well studied problem. With this paper, we challenge other members of the PROMISE community to improve on our results with other techniques.  ",
            "group": 2570,
            "name": "10.1.1.120.9012",
            "keyword": "collarsclumpsDDP",
            "title": "  Optimizing Requirements Decisions With KEYS"
        },
        {
            "abstract": "consolidation can benefit from the prediction of VM page miss rate at each candidate memory size. Such prediction is challenging for the hypervisor (or VM monitor) due to a lack of knowledge on VM memory access pattern. This paper explores the approach that the hypervisor takes over the management for part of the VM memory and thus all accesses that miss the remaining VM memory can be transparently traced by the hypervisor. For online memory access tracing, its overhead should be small compared to the case that all allocated memory is directly managed by the VM. To save memory space, the hypervisor manages its memory portion as an exclusive cache (i.e., containing only data that is not in the remaining VM memory). To minimize I/O overhead, evicted data from a VM enters its cache directly from VM memory (as opposed to entering from the secondary storage). We guarantee the cache correctness by only caching memory pages whose current contents provably match those of corresponding storage locations. Based on our design, we show that when the VM evicts pages in the LRU order, the employment of the hypervisor cache does not introduce any additional I/O overhead in the system. We implemented the proposed scheme on the Xen para-virtualization platform. Our experiments with microbenchmarks and four real data-intensive services (SPECweb99, index searching, TPC-C, and TPC-H) illustrate the overhead of our hypervisor cache and the accuracy of cache-driven VM page miss rate prediction. We also present the results on adaptive VM memory allocation with performance assurance. 1",
            "group": 2571,
            "name": "10.1.1.120.9353",
            "keyword": "",
            "title": "Virtual Machine Memory Access Tracing With Hypervisor Exclusive Cache \u2217 Abstract"
        },
        {
            "abstract": "As the machine learning community tackles more complex and harder problems, the graph-ical models needed to solve such problems become larger and more complicated. As a result performing inference and learning exactly for such graphical models become ever more expen-sive, and approximate inference and learning techniques become ever more prominent. There are a variety of techniques for approximate inference and learning in the literature. This thesis contributes some new ideas in the products of experts (PoEs) class of models (Hin-ton, 2002), and the Bethe free energy approximations (Yedidia et al., 2001). For PoEs, our contribution is in developing new PoE models for continuous-valued do-mains. We developed RBMrate, a model for discretized continuous-valued data. We applied it to face recognition to demonstrate its abilities. We also developed energy-based models (EBMs)  \u2013 flexible probabilistic models where the building blocks consist of energy terms com-puted using a feed-forward network. We show that standard square noiseless independent components analysis (ICA) (Bell and Sejnowski, 1995) can be viewed as a restricted form of EBMs. Extending this relationship with ICA, we describe sparse and over-complete represen-tations of data where the inference process is trivial since it is simply an EBM. For Bethe free energy approximations, our contribution is a theory relating belief propaga-tion and iterative scaling. We show that both belief propagation and iterative scaling updates can be derived as fixed point equations for constrained minimization of the Bethe free energy. This allows us to develop a new algorithm to directly minimize the Bethe free energy, and to ap-ply the Bethe free energy to learning in addition to inference. We also describe improvements to the efficiency of standard learning algorithms for undirected graphical models (Jirou\u02c7sek and",
            "group": 2572,
            "name": "10.1.1.120.9534",
            "keyword": "",
            "title": "Abstract Bethe Free Energy and Contrastive Divergence Approximations for Undirected Graphical Models"
        },
        {
            "abstract": "Edge-based scoring and searching method for identifying condition-responsive protein\u2013protein interaction sub-network",
            "group": 2573,
            "name": "10.1.1.121.432",
            "keyword": "",
            "title": "BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm294 Systems biology"
        },
        {
            "abstract": "This is to certify that I have examined this copy of a doctoral dissertation by",
            "group": 2574,
            "name": "10.1.1.121.515",
            "keyword": "Signature Date",
            "title": "Abstract Practical and Efficient Internet Routing"
        },
        {
            "abstract": "FlexRay will very likely become the de-facto standard for in-vehicle communications. Its main advantage is the combination of high speed static and dynamic transmission of messages. In our previous work we have shown that not only the static but also the dynamic segment can be used for hard-real time communication in a deterministic manner. In this paper, we propose techniques for optimising the FlexRay bus access mechanism of a distributed system, so that the hard real-time deadlines are met for all the tasks and messages in the system. We have evaluated the proposed techniques using extensive experiments. 1.",
            "group": 2575,
            "name": "10.1.1.121.1954",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "We present an application of hidden Markov models (HMMs) to analysis of geode-tic time series in Southern California. Our model fitting method uses a regularized version of the deterministic annealing expectation-maximization algorithm to en-sure that model solutions are both robust and of high quality. Using the fitted models, we segment the daily displacement time series collected by 127 stations of the Southern California Integrated Geodetic Network (SCIGN) over a two year period. Segmentations of the series are based on statistical changes as identified by the trained HMMs. We look for correlations in state changes across multi-ple stations that indicate region-wide activity. We find that although in one case a strong seismic event was associated with a spike in station correlations, in all other cases in the study time period strong correlations were not associated with any seismic event. This indicates that the method was able to identify more subtle signals associated with aseismic events or long-range interactions between smaller events.",
            "group": 2576,
            "name": "10.1.1.121.4162",
            "keyword": "",
            "title": "Statistical Analysis of Geodetic Networks for Detecting Regional Events"
        },
        {
            "abstract": "Previous research on cluster-based servers has focused on homogeneous systems. However, real-life clusters are almost invariably heterogeneous in terms of the performance, capacity, and power consumption of their hardware components. In this paper, we describe a self-configuring Web server for a heterogeneous cluster. The self-configuration is guided by analytical models of throughput and power consumption. Our experimental results for a cluster comprised of traditional and blade nodes show that the modelbased server can consume 29 % less energy than an energyoblivious server, with only a negligible loss in throughput. The results also show that our server conserves more than twice as much energy as an energy-conscious server that we previously proposed for homogeneous clusters. 1",
            "group": 2577,
            "name": "10.1.1.121.4636",
            "keyword": "",
            "title": "Self-Configuring Heterogeneous Server Clusters \u00a3"
        },
        {
            "abstract": "Abstract \u2014 After more than a decade of research, there now exist several neural-network techniques for solving NP-hard combinatorial optimization problems. Hopfield networks and selforganizing maps are the two main categories into which most of the approaches can be divided. Criticism of these approaches includes the tendency of the Hopfield network to produce infeasible solutions, and the lack of generalizability of the self-organizing approaches (being only applicable to Euclidean problems). This paper proposes two new techniques which have overcome these pitfalls: a Hopfield network which enables feasibility of the solutions to be ensured and improved solution quality through escape from local minima, and a self-organizing neural network which generalizes to solve a broad class of combinatorial optimization problems. Two sample practical optimization problems from Australian industry are then used to test the performances of the neural techniques against more traditional heuristic solutions. Index Terms\u2014Assembly line, combinatorial optimization, Hopfield networks, hub location, NP-hard, self-organization, sequencing,",
            "group": 2578,
            "name": "10.1.1.121.4761",
            "keyword": "traveling",
            "title": "Neural techniques for combinatorial optimization with applications"
        },
        {
            "abstract": "www.sbg.org.br The main goal of this study is to find the most effective set of parameters for the Simplified Generalized Simulated Annealing algorithm, SGSA, when applied to distinct cost function as well as to find a possible correlation between the values of these parameters sets and some topological characteristics of the hypersurface of the respective cost function. The SGSA algorithm is an extended and simplified derivative of the GSA algorithm, a Markovian stochastic process based on Tsallis statistics that has been used in many classes of problems, in particular, in biological molecular systems optimization. In all but one of the studied cost functions, the global minimum was found in 100 % of the 50 runs. For these functions the best visiting parameter, q, belongs to the interval [1.2, 1.7]. Also, the V temperature decaying parameter, q, should be increased when better precision is required. Moreover, the similarity T in the locus of optimal parameter sets observed in some functions indicates that possibly one could extract topological information about the cost functions from these sets. Key words: optimization, generalized simulated annealing.",
            "group": 2579,
            "name": "10.1.1.121.6291",
            "keyword": "",
            "title": "Research Article Performance and parameterization of the algorithm Simplified Generalized Simulated Annealing"
        },
        {
            "abstract": "Abstract \u2014 In this paper, a pipelined version of genetic algorithm, called PLGA, and a corresponding hardware platform are described. The basic operations of conventional GA (CGA) are made pipelined using an appropriate selection scheme. The selection operator, used here, is stochastic in nature and is called SA-selection. This helps maintaining the basic generational nature of the proposed pipelined GA (PLGA). A number of benchmark problems are used to compare the performances of conventional roulette-wheel selection and the SA-selection. These include unimodal and multimodal functions with dimensionality varying from very small to very large. It is seen that the SA-selection scheme is giving comparable performances with respect to the classical roulette-wheel selection scheme, for all the instances, when quality of solutions and rate of convergence are considered. The speedups obtained by PLGA for different benchmarks are found to be significant. It is shown that a complete hardware pipeline can be developed using the proposed scheme, if parallel evaluation of the fitness expression is possible. In this connection a low-cost but very fast hardware evaluation unit is described. Results of simulation experiments show that in a pipelined hardware environment, PLGA will be much faster than CGA. In terms of efficiency, PLGA is found to outperform parallel GA (PGA) also. Keywords \u2014 Hardware evaluation, Hardware pipeline, Optimization, Pipelined genetic algorithm, SA-selection.",
            "group": 2580,
            "name": "10.1.1.121.6747",
            "keyword": "Malay",
            "title": "Generational PipeLined Genetic Algorithm (PLGA) using Stochastic Selection"
        },
        {
            "abstract": "In a recent result, Weitz [31] established equivalence between the marginal distribution of a node, say v, in any binary pair-wise Markov Random Field (MRF), say G, with the marginal distribution of the root node in the selfavoid walk tree of the G starting at v. In this paper, we exploit this remarkable connection to obtain insights in the performance of the widely popular Belief Propagation heuristic for computing marginal distribution (sum-product) and max-marginal distribution (max-product). We obtain a tight characterization of the size of self-avoiding walk tree for any connected graph as a function of number of edges. This may be of interest in its own right. I.",
            "group": 2581,
            "name": "10.1.1.121.8053",
            "keyword": "",
            "title": "Inference in binary pair-wise markov random field through self-avoiding walk"
        },
        {
            "abstract": "Power consumption is a crucial concern in nanometer chip design. Researchers have shown that multiple supply voltage (MSV) is an effective method for power consumption reduction. The underlying idea behind MSV is the trade-off between power saving and performance. In this paper, we present an effective voltage assignment technique based on dynamic programming. Given a netlist without reconvergent fanouts, the dynamic programming can guarantee an optimal solution for the voltage assignment. We then generate a level shifter for each net that connects two blocks in different voltage domains, and perform power-network aware floorplanning for the MSV design. Experimental results show that our floorplanner is very effective in optimizing power consumption under timing constraints. 1.",
            "group": 2582,
            "name": "10.1.1.121.8431",
            "keyword": "",
            "title": "Voltage island aware floorplanning for power and timing optimization"
        },
        {
            "abstract": "In recent years researchers have shown increasing interest in swarm intelligence as a promising approach to adaptive distributed problem solving. Swarm intelligence consists of techniques inspired by nature, especially social insects and aggregations of animals, and even human interactions. They are based on self-organization (a system\u2019s overall behavior emerges from the local interactions among its relatively simple components) and are often decentralized and massively distributed. Particle systems are an approach to swarm intelligence that focus on collective movements, and have been used successfully for applications such as computer animation in graphics and control of movements of autonomous robotic vehicle teams. However, particle system techniques have not been applied substantially to problem solving beyond merely collective navigational tasks. In this dissertation, I present an extension to particle systems that incorpo-rates top-down, high-level control to self-organizing mobile agents, thereby guiding the self-organizing process and making it possible for particle systems to undertake problem solving directed by goal-oriented behavior while retaining their decentral-",
            "group": 2583,
            "name": "10.1.1.121.8843",
            "keyword": "",
            "title": "ABSTRACT Title of dissertation: Guided Self-Organizing Particle Systems for Basic Problem Solving"
        },
        {
            "abstract": "This is a preprint of an article accepted for publication",
            "group": 2584,
            "name": "10.1.1.122.33",
            "keyword": "data generation",
            "title": "in Software Testing Verification and Reliability,"
        },
        {
            "abstract": "The goal of this article is to give an overview of numerical problems encountered when determining the electronic structure of materials and the rich variety of techniques used to solve these problems. The paper is intended for a diverse scienti\u00a3c computing audience. For this reason, we assume the reader does not have an extensive background in the related physics. Our overview focuses on the nature of the numerical problems to be solved, their origin, and on the methods used to solve the resulting linear algebra or nonlinear optimization problems. It is common knowledge that the behavior of matter at the nanoscale is, in principle, entirely determined by the Schr\u00f6dinger equation. In practice, this equation in its original form is not tractable. Successful, but approximate, versions of this equation, which allow one to study nontrivial systems, took about \u00a3ve or six decades to develop. In particular, the last two decades saw a \u00a4urry of activity in developing effective software. One of the main practical variants of the Schr\u00f6dinger equation is based on what is referred to as Density Functional Theory (DFT). The combination of DFT with pseudopotentials allows one to obtain in an ef\u00a3cient way the ground state con\u00a3guration for many materials. This article will emphasize pseudopotentialdensity",
            "group": 2585,
            "name": "10.1.1.122.179",
            "keyword": "",
            "title": "Numerical methods for electronic structure calculations of materials"
        },
        {
            "abstract": "Abstract\u2014We propose a new method for detecting activation in functional magnetic resonance imaging (fMRI) data. We project the fMRI time series on a low-dimensional subspace spanned by wavelet packets in order to create projections that are as non-Gaussian as possible. Our approach achieves two goals: it reduces the dimensionality of the problem by explicitly constructing a sparse approximation to the dataset and it also creates meaningful clusters allowing the separation of the activated regions from the clutter formed by the background time series. We use a mixture of Gaussian densities to model the distribution of the wavelet packet coefficients. We expect activated areas that are connected, and impose a spatial prior in the form of a Markov random field. Our approach was validated with in vivo data and realistic synthetic data, where it outperformed a linear model equipped with the knowledge of the true hemodynamic response. Index Terms\u2014Functional magnetic resonance imaging (fMRI), mixture of Gaussian densities, wavelet packets. I.",
            "group": 2586,
            "name": "10.1.1.122.897",
            "keyword": "",
            "title": "Classification of fMRI Time Series in a Low-Dimensional Subspace With a Spatial Prior"
        },
        {
            "abstract": "for external access to five of JPL\u2019s real-world requirements models, anonymized to conceal proprietary information, but retaining their computational nature. Experimentation with these models, reported herein, demonstrates a dramatic speedup in the computations performed on them. These models have a well defined goal: select mitigations that retire risks which, in turn, increases the number of attainable requirements. Such a non-linear optimization is a well-studied problem. However identification of not only (a) the optimal solution(s) but also (b) the key factors leading to them is less well studied. Our technique, called KEYS, shows a rapid way of simultaneously identifying the solutions and their key factors. KEYS improves on prior work by several orders of magnitude. Prior experiments with simulated annealing or treatment learning took tens of minutes to hours to terminate. KEYS runs much faster than that; e.g for one model, KEYS ran 13,000 times faster than treatment learning (40 minutes versus 0.18 seconds). Processing these JPL models is a non-linear optimization problem: the fewest mitigations must be selected while achieving the most requirements. Non-linear optimization is a well studied problem. With this paper, we challenge other members of the PROMISE community to improve on our results with other techniques. Categories and Subject Descriptors i.5 [learning]: machine learning; d.2.1 [requirements]: tools",
            "group": 2587,
            "name": "10.1.1.122.1025",
            "keyword": "collarsclumpsDDP",
            "title": "ABSTRACT Optimizing Requirements Decisions With KEYS"
        },
        {
            "abstract": "service levels in practice The work described in the literature on inventory and supply chain management has advanced greatly over the last few decades and now covers many aspects and challenges of applied supply chain management. In this paper we describe an approach that combines many of these academic aspects in a practical way to manage the spare parts logistics at a German automobile manufacturer. The basic problem is a single-echelon inventory problem with a systemwide service-level requirement and the possibility of issuing emergency orders. There exist two related optimization problems: One is to maximize the system-wide service level under the constraint of a given budget; the other is to minimize the budget for a given system-wide service level. The most important requirements and constraints considered are a detailed cost structure, different packaging sizes, capacity constraints, several storage zones, the decision whether or not to stock a product, stochastic lead times, highly sporadic demands, and the stability of the optimization result over time. Our approach has been implemented successfully in an automotive spare parts planning environment. The complete solution package integrates into the mySAP ERPt, the SAP",
            "group": 2588,
            "name": "10.1.1.122.1225",
            "keyword": "",
            "title": "Inventory budget optimization: Meeting system-wide"
        },
        {
            "abstract": "mixed branch length model of heterotachy improves",
            "group": 2589,
            "name": "10.1.1.122.2461",
            "keyword": "heterotachycovarionphylogeneticsmaximum likelihoodmixed modelevolutionary heterogeneitymixed branch length",
            "title": "Correspondence to:"
        },
        {
            "abstract": "www.cs.yorku.ca/~wolfgang/ Fractal terrains provide an easy way to generate realistic landscapes. There are several methods to generate fractal terrains, but none of those algorithms allow the user much flexibility in controlling the shape or properties of the final outcome. A few methods to modify fractal terrains have been previously proposed, both algorithm-based as well as by hand editing, but none of these provide a general solution. In this work, we present a new algorithm for fractal terrain deformation. We present a general solution that can be applied to a wide variety of deformations. Our approach employs stochastic local search to identify a sequence of local modifications, which deform the fractal terrain to conform to a set of specified constraints. The presented results show that the new method can incorporate multiple constraints simultaneously, while still preserving the natural look of the fractal terrain. Keywords: (according to ACM CCS): I.3.7 [Computer Graphics, Three-Dimensional Graphics and Realism]: Fractals, I.2.8 [Problem Solving, Control Methods, and Search] Graph and tree search strategies",
            "group": 2590,
            "name": "10.1.1.122.5463",
            "keyword": "",
            "title": "An Algorithm for Automated Fractal Terrain Deformation"
        },
        {
            "abstract": "{r92089, yangc, r91089}",
            "group": 2591,
            "name": "10.1.1.122.5471",
            "keyword": "B.8.2 [HardwarePERFORMANCE AND RELIABIL- ITY General Terms AlgorithmDesignPerformance Keywords ThermalArchitectural FloorplanningPerformance",
            "title": "Joint exploration of architectural and physical design spaces with thermal consideration"
        },
        {
            "abstract": "The ODO project is an inquiry into constraint-directed scheduling with the primary motivation of the development of a unified foundation for constraint-directed search techniques. Central to this foundation is the exploitation of the knowledge in the constraint representation, the use of commitment assertion and retraction as search operators, a generic model of scheduling strategies, and the use of texture measurements to distill constraint information for search guidance. Each of these components is discussed in-depth. The ODO framework, a commitment-based model of constraint-directed search with which many existing scheduling techniques can be modelled and implemented, is presented along with a selection of past, current, and future research using the framework. \ufffd 1998 John Wiley & Sons, Ltd. KEY WORDS: scheduling; constraints; search; heuristics; texture measurements; propagators; backtracking 1.",
            "group": 2592,
            "name": "10.1.1.122.6857",
            "keyword": "",
            "title": "CONSTRAINT-DIRECTED SCHEDULING"
        },
        {
            "abstract": "The world is covered with millions of webcams. Some are private, but many transmit everything in their field of view over the internet 24 hours a day. A web search finds public webcams in airports, intersections, classrooms, parks, shops, ski resorts, and more. These public webcams are an endless resource, and some sites are already mapping them by location or by functionality. But when a webcam is selected- most of the video broadcast will be of no interest due to lack of activity. We propose to generate a short video that will be a synopsis of an infinite video stream, such as generated by a webcam. We would like to address queries like \u201cI would like to watch in one minute the highlights of this camera broadcast during the past day\u201d. The two major phases are: (i) An online conversion of the video stream into a searchable structure based on objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user\u2019s query. To include maximum information in a short synopsis we simultaneously show activities that may have happened at different times. The synopsis video can also be used as an index into the original video stream, restoring the chronological order. 1.",
            "group": 2593,
            "name": "10.1.1.122.7204",
            "keyword": "",
            "title": "Webcam synopsis: Peeking around the world"
        },
        {
            "abstract": "In recent years approximation theory has found interesting applications in the elds of Arti cial Intelligence and Computer Science. For instance, a problem that ts very naturally in the framework of approximation theory is the problem of learning to perform a particular task from a set of examples. The examples are sparse data points in a",
            "group": 2594,
            "name": "10.1.1.122.7518",
            "keyword": "",
            "title": "Some extensions of radial basis functions and their applications in artificial intelligence"
        },
        {
            "abstract": "Abstract. An exact algorithm is proposed for minimum sum-of-squares nonhierarchical clustering, i.e., for partitioning a given set of points from a Euclidean m-space into a given number of clusters in order to minimize the sum of squared distances from all points to the centroid of the cluster to which they belong. This problem is expressed as a constrained hyperbolic program in 0-1 variables. The resolution method combines an interior point algorithm, i.e., a weighted analytic center column generation method, with branch-and-bound. The auxiliary problem of determining the entering column (i.e., the oracle) is an unconstrained hyperbolic program in 0-1 variables with a quadratic numerator and linear denominator. It is solved through a sequence of unconstrained quadratic programs in 0-1 variables. To accelerate resolution, variable neighborhood search heuristics are used both to get a good initial solution and to solve quickly the auxiliary problem as long as global optimality is not reached. Estimated bounds for the dual variables are deduced from the heuristic solution and used in the resolution process as a trust region. Proved minimum sum-of-squares partitions are determined for the first time for several fairly large data sets from the literature, including Fisher\u2019s 150 iris. Key words. classification and discrimination, cluster analysis, interior-point methods, combinatorial optimization",
            "group": 2595,
            "name": "10.1.1.122.8655",
            "keyword": "",
            "title": "An interior point algorithm for minimum sum of squares clustering"
        },
        {
            "abstract": "Foraging- and feeding-related behaviors across eumetazoans share similar molecular mechanisms, suggesting the early evolution of an optimal foraging behavior called area-restricted search (ARS), involving mechanisms of dopamine and glutamate in the modulation of behavioral focus. Similar mechanisms in the vertebrate basal ganglia control motor behavior and cognition and reveal an evolutionary progression toward increasing internal connections between prefrontal cortex and striatum in moving from amphibian to primate. The basal ganglia in higher vertebrates show the ability to transfer dopaminergic activity from unconditioned stimuli to conditioned stimuli. The evolutionary role of dopamine in the modulation of goal-directed behavior and cognition is further supported by pathologies of human goal-directed cognition, which have motor and cognitive dysfunction and organize themselves, with respect to dopaminergic activity, along the gradient described by ARS, from perseverative to unfocused. The evidence strongly supports the evolution of goal-directed cognition out of mechanisms initially in control of spatial foraging but, through increasing cortical connections, eventually used to forage for information.",
            "group": 2596,
            "name": "10.1.1.122.8992",
            "keyword": "Genetic algorithmStriatumBasal gangliaDopamineArea-restricted searchEvolutionAnimal foragingGoal-directed behaviorAttentionADHDSchizophreniaOCDCognitive flexibilityPriming",
            "title": "Animal Foraging and the Evolution of Goal-Directed Cognition"
        },
        {
            "abstract": "With advanced sub-micron technologies, the exponentially increasing number of tran-sistors on a VLSI chip causes placement within physical design automation to become more and more important and consequently extremely complicated and time consuming. This thesis addresses the placement for VLSI standard cell designs. A number of heuristic optimization techniques for placement are studied and implemented, in par-ticular, local search, Tabu Search, Simulated Annealing and Genetic Algorithm. The Tabu Search reduces wire length on average by 52.4 % while Simulated Annealing yields a 61 % improvement on average. Furthermore, two parallel island-based GA models are implemented on a loosely-coupled parallel computing architecture to pursue better per-formance. With the synchronous model, an average speedup of 6.2 was achieved by seven processors. On the other hand, the asynchronous model achieved a 7.6 speedup. The former obtained the speedups while maintaining equal or better quality of solutions than a serial GA. In addition to the above developed algorithms, preprocessing and postpro-cessing procedures were analyzed and developed to further enhance solution quality. 1",
            "group": 2597,
            "name": "10.1.1.122.9007",
            "keyword": "Contents",
            "title": "ABSTRACT SEQUENTIAL/PARALLEL HEURISTIC ALGORITHMS"
        },
        {
            "abstract": "",
            "group": 2598,
            "name": "10.1.1.122.9698",
            "keyword": "",
            "title": "Genetic generation of connection patterns for a dynamic artificial neural network"
        },
        {
            "abstract": "This paper is devoted to real life flexible industrial production systems using universal machines, which are able to perform different kinds of jobs. In such environments, jobs from various families require different sets of processing facilities, thus, some rearrangements are needed in case of switching between families. To solve this problem, we formulate this issue in the context of scheduling theory. However, the problem is NP-hard, therefore, it is highly unlikely to find an optimal solution in polynomial time, hence, we provide some heuristic algorithms and one based on a simulated annealing approach, together with computational verification of their effectiveness.",
            "group": 2599,
            "name": "10.1.1.123.1526",
            "keyword": "Industrial production systemsSchedulingHeuristics",
            "title": "\u2217\u2217 \u2217 Control Theory and Applications Centre, Coventry"
        },
        {
            "abstract": "ii Die selbst\u00e4ndige und eingenh\u00e4ndige Anfertigung versichere ich an Eides statt. Berlin, den Unterschrift iii",
            "group": 2600,
            "name": "10.1.1.123.2439",
            "keyword": "",
            "title": "2 Parallelising of Programs 3"
        },
        {
            "abstract": "Abstract Although it is not a newcomer in the combinatorial optimization literature, Local Search is an emerging paradigm for combinatorial search, which has been recently shown to be very effective for a large number of scheduling problems. A number of metaheuristics based on local search have been also proposed to address with success a variety of scheduling problems. In this tutorial, we survey the basic local search techniques proposed in the literature and implemented in many industrial scheduling systems: Simulated Annealing, Tabu Search, and various forms of Hill Climbing. We also illustrate some of the most promising improvements and variations of such basic techniques which are currently investigated. Finally, we propose the combination of local search with other solution paradigms, such as genetic algorithms and constructive heuristics. Throughout the tutorial we illustrate three case studies of successful applications of these techniques to real life problems: school timetabling, frequency assignment in mobile radio systems, and sport scheduling.",
            "group": 2601,
            "name": "10.1.1.123.2651",
            "keyword": "",
            "title": "Local Search Techniques for Scheduling Problems-- A Tutorial--"
        },
        {
            "abstract": "The number of possible cases for evaluating the predicate of the branching is drastically reduced in these special cases. By the commutativity of multiplication (for Case I), by the commutativity of addition (for Case 11), and by induction (for Case 111) there are i possible cases of the predicate evaluation at ith iteration. This reduced number of possible cases of the predicate evaluation and the constant coefficients naturally simplify the precomputation to set up the binary tree representation and the path selection from the tree. We parallelize these special cases after strip mining of &i itera-tions at a time with p processors. By taking i e,,u\u2019s at the zth level from its binary tree representation where U = 2k-1 (1 5 IC 5 i), the path selection for a partition can be done in approximately logp time with p processors. The precomputation for a partition can be done also in O ( log p) time, because there are &i first-order linear recurrences of size fi. So, these simple forms of first-order linear",
            "group": 2602,
            "name": "10.1.1.123.2817",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "service",
            "group": 2603,
            "name": "10.1.1.123.2965",
            "keyword": "",
            "title": "Computational advances in maximum likelihood methods for molecular phylogeny. Genome Research"
        },
        {
            "abstract": "This chapter is focused on the physical design for system-on-a-chip (SOC). Physical design refers to all synthesis",
            "group": 2604,
            "name": "10.1.1.123.3693",
            "keyword": "",
            "title": "Physical Design for System-On-a-Chip"
        },
        {
            "abstract": "Area cartograms are used for visualizing geographically distributed data by attaching measurements to regions of a map and scaling the regions such that their areas are proportional to the measured quantities. A continuous area cartogram is a cartogram that is constructed without changing the underlying map topology. We present a new algorithm for the construction of continuous area cartograms that was developed by viewing their construction as a constrained optimization problem. The algorithm uses a relaxation method that exploits hierarchical resolution, constrained dynamics, and a scheme that alternates goals of achieving correct region areas and adjusting region shapes. It is compared favorably to existing methods in its ability to preserve region shape recognition cues, while still achieving high accuracy.",
            "group": 2605,
            "name": "10.1.1.123.3953",
            "keyword": "cartogramvalue-by-area mapmap transformationanamorphosis",
            "title": "Abstract Continuous Cartogram Construction"
        },
        {
            "abstract": "The power of video over still images is the ability to represent dynamic activities. But video browsing and retrieval are inconvenient due to inherent spatio-temporal redundancies, where some time intervals may have no activity, or have activities that occur in a small image region. Video synopsis aims to provide a compact video representation, while preserving the essential activities of the original video. We present dynamic video synopsis, where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times. For example, we can create a \u201dstroboscopic movie\u201d, where multiple dynamic instances of a moving object are played simultaneously. This is an extension of the still stroboscopic picture. Previous approaches for video abstraction addressed mostly the temporal redundancy by selecting representative key-frames or time intervals. In dynamic video synopsis the activity is shifted into a significantly shorter period, in which the activity is much denser. Video examples can be found online in",
            "group": 2606,
            "name": "10.1.1.123.5439",
            "keyword": "",
            "title": "Making a long video short: Dynamic video synopsis"
        },
        {
            "abstract": "Abstract \u2014 Substitution boxes are important components in many modern day block and stream ciphers. Their study has attracted a great deal of attention over many years. The development of a variety of cryptosystem attacks over the years has lead to the development of criteria for resilience to such attacks. Some general criteria such as high non-linearity and low autocorrelation have been proposed as useful criteria (providing some protection against attacks such as linear cryptanalysis and differential cryptanalysis). There has been little application of evolutionary search to the development of S-boxes. In this paper we show how a cost function that has found excellent single output Boolean functions can be generalised to provide improved results for small S-boxes. I.",
            "group": 2607,
            "name": "10.1.1.123.7114",
            "keyword": "",
            "title": "The Design of S-Boxes by Simulated Annealing"
        },
        {
            "abstract": "Optimal process planning for a combined punch-and-laser cutting machine using ant colony optimization",
            "group": 2608,
            "name": "10.1.1.123.8142",
            "keyword": "Combination machineCombined punch-and-laser machineSheet metalProcess planningOptimizationAnt colony optimization",
            "title": "International Journal of Production Research,"
        },
        {
            "abstract": "Abstract\u2014The hybrid wireless-optical broadband-access network (WOBAN) is a promising architecture for future access networks. Recently, the wireless part of WOBAN has been gaining increasing attention, and early versions are being deployed as municipal access solutions to eliminate the wired drop to every wireless router at customer premises. This architecture saves on network deployment cost because the fiber need not penetrate each end-user, and it extends the reach of emerging optical-access solutions, such as passive optical networks. This paper first presents an architecture and a vision for the WOBAN and articulates why the combination of wireless and optical presents a compelling solution that optimizes the best of both worlds. While this discussion briefly touches upon the business drivers, the main arguments are based on technical and deployment considerations. Consequently, the rest of this paper reviews a variety of relevant research challenges, namely, network setup, network connectivity, and fault-tolerant behavior of the WOBAN. In the network setup, we review the design of a WOBAN where the back end is a wired optical network, the front end is managed by a wireless connectivity, and, in between, the tail ends of the optical part [known as optical network unit (ONU)] communicate directly with wireless base stations (known as \u201cgateway routers\u201d). We outline algorithms to optimize the placement of ONUs in a WOBAN and report on a survey that we conducted on the distribution and types of wireless routers in the Wildhorse residential neighborhood of North Davis, CA. Then, we examine the WOBAN\u2019s routing properties (network connectivity), discuss the pros and cons of various routing algorithms, and summarize the idea behind fault-tolerant design of such hybrid networks. Index Terms\u2014Architecture, broadband access, fault tolerance, optical network, routing, wireless network. I.",
            "group": 2609,
            "name": "10.1.1.123.9005",
            "keyword": "",
            "title": "Hybrid Wireless-Optical Broadband Access Network (WOBAN): A Review of Relevant Challenges"
        },
        {
            "abstract": "In this paper, we present a set of extensions to the deformable template model: Active Appearance Model (AAM) proposed by Cootes et al. AAMs distinguish themselves by learning a priori knowledge through observation of shape and texture variation in a training set. This is used to obtain a compact object class description, which can be employed to rapidly search images for new object instances. The proposed extensions concern enhanced shape representation, handling of homogeneous and heterogeneous textures, refinement optimization using Simulated Annealing and robust statistics. Finally, an initialization scheme is designed thus making the usage of AAMs fully automated. Using these extensions it is demonstrated that AAMs can segment bone structures in radiographs, pork chops in perspective images and the left ventricle in cardiovascular magnetic resonance images in a robust, fast and accurate manner. Subpixel landmark accuracy was obtained in two of the three cases.",
            "group": 2610,
            "name": "10.1.1.123.9144",
            "keyword": "Deformable Template ModelsSnakesRobust StatisticsInitializationMetacarpal RadiographsCardiovascular Magnetic Resonance ImagingSegmentation",
            "title": "Extending and applying active appearance models for automated, high precision segmentation in different image modalities"
        },
        {
            "abstract": "Based on recent work on Stochastic Partial Differential Equations (SPDEs), this paper presents a simple and well-founded method to implement the stochastic evolution of a curve. First, we explain why great care should be taken when considering such an evolution in a Level Set framework. To guarantee the well-posedness of the evolution and to make it independent of the implicit representation of the initial curve, a Stratonovich differential has to be introduced. To implement this differential, a standard Ito plus drift approximation is proposed to turn an implicit scheme into an explicit one. Subsequently, we consider shape optimization techniques, which are a common framework to address various applications in Computer Vision, like segmentation, tracking, stereo vision etc. The objective of our approach is to improve these methods through the introduction of stochastic motion principles. The extension we propose can deal with local minima and with complex cases where the gradient of the objective function with respect to the shape is impossible",
            "group": 2611,
            "name": "10.1.1.123.9554",
            "keyword": "",
            "title": "m\u00e9thode des Level Sets pour la Vision par Ordinateur: Contours"
        },
        {
            "abstract": "of the",
            "group": 2612,
            "name": "10.1.1.124.525",
            "keyword": "",
            "title": "Distribution Approximation, Combinatorial Optimization, and Lagrange-Barrier"
        },
        {
            "abstract": "Acknowledgments I wish to express my appreciation to Dr. Karlin R. Roth at NASA Ames Research Center for her valuable advice and guidance throughout this study. I would also like to extend my appreciation to Dr. James C. Ross, Dr. Stuart E. Rogers, and Dr. Charles C. Jorgensen at NASA Ames Research Center. This work would not have been possible without their helpful discussions and sugges-tions. official endorsement, either expressed or implied, of such products or manufacturers by the National Aeronautics and I I The Spaceuse Administration. of trademarks or names of manufacturers in this report is for accurate reporting and does not constitute an I",
            "group": 2613,
            "name": "10.1.1.124.739",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": "Abstract\u2014We have developed a new optimization paradigm for solving computationally intractable combinatorial optimization and synthesis problems. The technique, named probabilistic constructive, combines the advantages of both constructive and probabilistic optimization mechanisms. Since it is a constructive approach, it has a relatively short runtime and is amenable for the inclusion of insights through heuristic rules. The probabilistic nature facilitates a flexible tradeoff between runtime and the quality of solution, suitability for the superimposition of a variety of control strategies, and simplicity of implementation. After presenting the generic technique, we apply it to a generic NP-complete problem (maximum independent set) and a synthesis and compilation problem (sequential code covering). Extensive experimentation indicates that the new approach provides very attractive tradeoffs between the quality of solution and runtime, often outperforming the best previously published approaches. Index Terms\u2014Graph coloring, maximum independent set, optimization algorithm, sequence covering.",
            "group": 2614,
            "name": "10.1.1.124.782",
            "keyword": "",
            "title": "Probabilistic Constructive Optimization Techniques"
        },
        {
            "abstract": "This paper describes an extended framework for scenario based spatial decision support for constructing new network infrastructure and its application in the domains of telecommunication, forestry and energy. There is an increasing need to provide new planning paradigms to support very expensive strategic investment decisions in new network infrastructure in these domains. The planning processes there are still dominated by an expert approach based on empirical knowledge and manual implementation. With this conventional approach it is impossible to consider different planning scenarios within a reasonable cost und time frame and no cost-based optimization is possible. We combined the powerful analytical and visualization capabilities of a Geographic Information System (GIS) with mathematical methods of graph theory and combinatorial optimization. This approach extends the basic spatial decision support model with a knowledge based module for scenario parameterization and graph generation, a module for geodata integration and processing, an operations research optimization module and a multi-level visualization module supporting the need of different communication channels within the decision making process. Keywords: GIS & Operations Research; Network Infrastructure; Scenario-based planning, Investment simulation; 1",
            "group": 2615,
            "name": "10.1.1.124.1256",
            "keyword": "",
            "title": "Scenario-based Spatial Decision Support for Network Infrastructure Design Abstract"
        },
        {
            "abstract": "Abstract: A phenomenon-inspired meta-heuristic algorithm, harmony search, imitating music improvisation process, is introduced and applied to vehicle routing problem, then compared with one of the popular evolutionary algorithms, genetic algorithm. The harmony search algorithm conceptualized a group of musicians together trying to search for better state of harmony. This algorithm was applied to a test traffic network composed of one bus depot, one school and ten bus stops with demand by commuting students. This school bus routing example is a multi-objective problem to minimize both the number of operating buses and the total travel time of all buses while satisfying bus capacity and time window constraints. Harmony search could find good solution within the reasonable amount of time and computation. Key words: Harmony search, genetic algorithm, vehicle routing",
            "group": 2616,
            "name": "10.1.1.124.1629",
            "keyword": "",
            "title": "\u00a9 2005 Science Publications Application of Harmony Search to Vehicle Routing"
        },
        {
            "abstract": "We present a model of surveillance based on the detection of community structure in social networks. We examine the extent of network topology information an adversary is required to gather in order to obtain high quality intelligence about community membership. We show that selective surveillance strategies can improve the adversary\u2019s resource efficiency. However, the use of counter-surveillance defense strategies can significantly reduce the adversary\u2019s capability. We analyze two adversary models drawn from contemporary computer security literature, and explore the dynamics of community detection and hiding in these settings. Our results show that in the absence of counter-surveillance moves, placing a mere 8 % of the network under surveillance can uncover the community membership of as much as 50 % of the network. Uncovering all community information with targeted selection requires half the surveillance budget where parties use anonymous channels to communicate. Finally, the most determined covert community can escape detection by adopting decentralized counter-surveillance techniques even while facing an adversary with full topology knowledge- by investing in a small counter-surveillance budget, a rebel group can induce a steep increase in the false negative ratio. 1",
            "group": 2617,
            "name": "10.1.1.124.2833",
            "keyword": "",
            "title": "The Economics of Community Detection and Hiding"
        },
        {
            "abstract": "Recently there has been increased interest in the development of high-level architectural synthesis tools targeting power optimization. In this paper, we first present an overview of the various architecture synthesis tasks and analyze their influence on power consumption. A survey of previously proposed techniques is given, and areas of opportunity are identified. We next propose a new architecture synthesis technique for low-power implementation of real-time applications. The technique uses algorithm partitioning to preserve locality in the assignment of operations to hardware units. Preserving locality results in more compact layouts, reduced usage of long high-capacitance buses, and reduced power consumption in multiplexors and buffers. Experimental results show reductions in bus and multiplexor power of up to 80 % and 60%, respectively, resulting in 10-25 % reduction in total power. 1.",
            "group": 2618,
            "name": "10.1.1.124.2994",
            "keyword": "",
            "title": "Low-power architectural synthesis and the impact of exploiting locality"
        },
        {
            "abstract": "In this study, a new robotic task replanning architecture for a class of problems is proposed. The architecture supports replanning in deliberation layer in addition to the unexpected event handling of low level behaviours. The replanning mechanism is supported by vision feedback to update the internal state of the system at certain intervals for exegoneous event detection. The expected state is known after each plan step execution. High level plans are compiled into lowlevel behaviours for execution and some unexpected events that do not need deliberation layer interference are handled with low-level behaviours supported by sensory feedback.",
            "group": 2619,
            "name": "10.1.1.124.3262",
            "keyword": "roboticsunstructured robotic environmentsoperation safety and efficiencyplanningreplanningrobotic architecturesvision feedback",
            "title": "TAINN 2002 A PROPOSAL FOR A ROBOTIC PLANNING/REPLANNING FRAMEWORK"
        },
        {
            "abstract": "ii",
            "group": 2620,
            "name": "10.1.1.124.3806",
            "keyword": "",
            "title": "Acknowledgements"
        },
        {
            "abstract": "We design sequential and parallel genetic algorithms, simulated annealing algorithms and improved greedy algorithms for the shortest common superstring problem(SCS), which is to find the shortest string that contains all strings from a given set of strings. The SCS problem is NP-complete [7]. It is even MAX SNP hard [2] i.e. no polynomial-time algorithm exists, that can approximate the optimum to within a predetermined constant unless P=NP. We compare the above mentioned algorithms applied to several randomly generated test cases. The test results show the superiority of the parallel island genetic algorithm. 1",
            "group": 2621,
            "name": "10.1.1.124.3877",
            "keyword": "",
            "title": "Sequential and Parallel Algorithms for the Shortest Common Superstring Problem"
        },
        {
            "abstract": "Iterative mesh partitioning strategy for improving the efficiency of parallel substructure finite element computations Shang-Hsien Hsieh\u2020",
            "group": 2622,
            "name": "10.1.1.124.4031",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "SCCS-370b We present an overview of the state of the art and future trends in high performance parallel and distributed computing, and discuss techniques for using such computers in the simulation of complex problems in computational science. The use of high performance parallel computers can help improve our understanding of complex systems, and the converse is also true \u2014 we can apply techniques used for the study of complex systems to improve our understanding of parallel computing. We consider parallel computing as the mapping of one complex system \u2014 typically a model of the world \u2014 into another complex system \u2014 the parallel computer. We study static, dynamic, spatial and temporal properties of both the complex systems and the map between them. The result is a better understanding of which computer architectures are good for which problems, and of software structure, automatic partitioning of data, and the performance of parallel machines. 1",
            "group": 2623,
            "name": "10.1.1.124.4525",
            "keyword": "",
            "title": "Parallel computers and complex systems"
        },
        {
            "abstract": "Abstract: We have developed a chaotic neurodynamical searching method for solving lighting design problems. The goal of this method is to design interior lighting that satisfies required illuminance distribution. We can obtain accurate illuminance distribution by using the radiosity method to calculate interreflection of lights. We formulate the lighting design problem that considers the interreflection of lights as a combinatorial optimization problem, and construct a chaotic neural network which searches the optimum solution of the lighting design problem. The calculated illuminance distribution is visualized using computer graphics. We compare this optimization method with the conventional neural network with gradient dynamics, simulated annealing, and the genetic algorithm, and clarify the effectiveness of the proposed method based on the chaotic neural network.",
            "group": 2624,
            "name": "10.1.1.124.4625",
            "keyword": "chaotic neural networkslighting designcomputer graphicsradiosity methodcombinatorial optimization problemsimulated annealinggenetic algorithm",
            "title": "Optimization using chaotic neural networks and its application to lighting design,\u201d Contr. Cybern., submitted for publication"
        },
        {
            "abstract": "We present a new technique of annealing the EM algorithm to allow for its tractable application to fitting models which include graph structures like assignments. The method, which can be generally used to sparsify dependence models, is applied to solve the assignment problem for the shared-resources Gaussian mixture model (e.g. [4], [5],[9]), and is compared to (and contrasted to) the widely used technique of deterministic annealing (e.g. [8],[2]). 1",
            "group": 2625,
            "name": "10.1.1.124.5032",
            "keyword": "",
            "title": "NIPS*00 submission Algorithms and architectures Presentation preference: Oral Not previously submitted elsewhere Annealed Expectation-Maximization by Entropy Projection"
        },
        {
            "abstract": "This paper proposes a period representation for modelling the multidrug HIV therapies and an Adaptive Multimeme Algorithm (AMmA) for designing the optimal therapy. The period representation offers benefits in terms of flexibility and reduction in dimensionality compared to the binary representation. The AMmA is a memetic algorithm which employs a list of three local searchers adaptively activated by an evolutionary framework. These local searchers, having different features according to the exploration logic and the pivot rule, have the role of exploring the decision space from different and complementary perspectives and, thus, assisting the standard evolutionary operators in the optimization process. Furthermore, the AMmA makes use of an adaptation which dynamically sets the algorithmic parameters in order to prevent the stagnation and premature convergence. The",
            "group": 2626,
            "name": "10.1.1.124.5677",
            "keyword": "",
            "title": "An adaptive multimeme algorithm for designing HIV multidrug therapies"
        },
        {
            "abstract": "We present an improved \u201ccooling schedule \u201d for simulated annealing algorithms for combinatorial counting problems. Under our new schedule the rate of cooling accelerates as the temperature decreases. Thus, fewer intermediate temperatures are needed as the simulated annealing algorithm moves from the high temperature (easy region) to the low temperature (difficult region). We present applications of our technique to colorings and the permanent (perfect matchings of bipartite graphs). Moreover, for the permanent, we improve the analysis of the Markov chain underlying the simulated annealing algorithm. This improved analysis, combined with the faster cooling schedule, results in an O(n 7 log 4 n) time algorithm for approximating the permanent of a 0/1 matrix. 1",
            "group": 2627,
            "name": "10.1.1.124.5699",
            "keyword": "",
            "title": "Abstract Accelerating Simulated Annealing for the Permanent and Combinatorial Counting Problems"
        },
        {
            "abstract": "We assessed how well landscape metrics at 2, 5, and 10 km scales could explain the distribution of woodland bird species in the Mount Lofty Ranges, South Australia. We considered 31 species that have isolated or partially isolated populations in the region and used the Akaike Information Criterion to select a set of candidate logistic regression models. The 2 km distance was the most appropriate scale for a plurality of the species. While the total amount of area of native vegetation around a site was the most important determining factor, the effect of landscape configuration was also important for many species. Most species responded positively to area-independent fragmentation, but the responses to mean patch isolation and mean patch shape were more variable. Considering a set of candidate models for which there is reasonable support (Akaike weights> 0.10), 12 species responded negatively to landscapes with highly linear and isolated patches. No clear patterns emerged in terms of taxonomy or functional group as to how species respond to landscape configuration. Most of the species had models with relatively good discrimination (12 species had ROC values> 0.70), indicating that landscape pattern alone can explain their distributions reasonably well. For six species there were no models that had strong weight of evidence, based on the AIC and ROC criteria. This analysis shows the utility of the Akaike Information Criterion approach to model selection in landscape ecology. Our results indicate that landscape planners in the Mount Lofty Ranges must consider the spatial configuration of vegetation.",
            "group": 2628,
            "name": "10.1.1.124.5998",
            "keyword": "logistic regressionreceiver",
            "title": "Perspective Effects of Landscape Pattern on Bird Species Distribution in the Mt. Lofty Ranges, South"
        },
        {
            "abstract": "Abstract \u2014 NASA\u2019s space missions Dawn and JIMO will use low-thrust propulsion for multi-revolution orbit transfers around a central body. Here we address the problem of designing low-thrust orbit transfers between arbitrary orbits in an inverse-square gravity field by using evolutionary algorithms to drive parameter selection in a Lyapunov feedback control law (the Q-law). We develop an efficient and efficacious method to assess, with reasonable accuracy, the trade off between propellant mass and flight time (i.e., to find the Pareto front for these two quantities), and to provide the time history of the state variables and the thrust vector for any chosen",
            "group": 2629,
            "name": "10.1.1.124.6134",
            "keyword": "",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "Network virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding\u2014mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research restricted the problem space to allow efficient solutions, or focused on designing heuristic algorithms. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtualnetwork topologies. Our simulation experiments show that path splitting, path migration, and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks.",
            "group": 2630,
            "name": "10.1.1.124.6961",
            "keyword": "Categories and Subject Descriptors C.2.5 [Computer-Communication NetworksLocal and Wide-Area NetworksG.1.6 [Numerical AnalysisOptimization General Terms AlgorithmsDesign Keywords Virtual Network EmbeddingPath SplittingPath MigrationNetwork VirtualizationOptimization",
            "title": "ABSTRACT Rethinking Virtual Network Embedding: Substrate Support for Path Splitting and Migration"
        },
        {
            "abstract": "Every year, the Consortium for Mathematics and its Applications (C OMA OMAP) sponsors the Mathematical Contest in Modeling (MCM), an international contest for undergraduates. We will discuss our strategy for",
            "group": 2631,
            "name": "10.1.1.124.7678",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": "We present a method for maximizing the throughput of mobile ad hoc packet radio networks using broadcast transmission scheduling. In such networks, a terminal may become a bottleneck if it is not allocated enough transmission slots in the current transmission schedule to handle the traffic flowing through it. Topology induced bottlenecks may arise frequently in ad hoc networks due to uneven distributions of terminals. Terminals in sparse areas of a network may be required to forward a large amount of traffic to facilitate communication between dense areas of the network. We address this problem by modifying the broadcast transmission schedule so that terminals handling more traffic have more opportunities to transmit. First, we describe a theoretical framework for analyzing the performance of a given schedule in terms of end-to-end stable throughput; we also define the upper bound for this performance. Next, we introduce a centralized algorithm that uses a process similar to simulated annealing to generate schedules with near optimal performance. We conduct simulation studies to show that transmission schedules produced by the centralized algorithm offer greatly improved performance over a simple, collision-free transmission schedule in terms of end-to-end packet delay, throughput, and completion rate. These studies are performed on a variety of test networks to generalize results and demonstrate the wide applicability of these principles.",
            "group": 2632,
            "name": "10.1.1.124.8175",
            "keyword": "",
            "title": "Designing transmission schedules for wireless ad hoc networks to maximize network throughput"
        },
        {
            "abstract": "Nowadays, design of embedded systems is confronted with complex signal processing algorithms and a multitude of computational intensive multimedia applications, while time to product launch has been extremely reduced. Especially in the wireless domain, those challenges are stacked with tough requirements on power consumption and chip size. Unfortunately, design productivity did not undergo a similar progression, and therefore fails to cope with the heterogeneity of modern architectures. Electronic design automation tools exhibit deep gaps in the design flow like high-level characterization of algorithms, floating-point to fixed-point conversion, hardware/software partitioning, and virtual prototyping. This tutorial paper surveys several promising approaches to solve the widespread design problems in this field. An overview over consistent design methodologies that establish a framework for connecting the different design tasks is given. This is followed by a discussion of solutions for the integrated automation of specific design tasks. Copyright \u00a9 2006 M. Holzer et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1.",
            "group": 2633,
            "name": "10.1.1.124.9419",
            "keyword": "",
            "title": "Efficient Design Methods for Embedded Communication Systems"
        },
        {
            "abstract": "Protein structure prediction is one of the great challenges in structural biology. The ability to accurately predict the three-dimensional structure of proteins would bring about significant scientific advances and would facilitate finding cures and treatments for many diseases. We propose a novel computational framework for protein structure prediction. The novelty of the framework lies in its approach to conformation space search. Conformation space search is considered to be the primary bottleneck towards consistent, high-resolution prediction. The proposed approach to conformation space search represents a major conceptual shift in protein structure prediction, made possible by combining insights and algorithms from robotics and machine learning with techniques from molecular biology in an innovative manner. The key innovation comes from the insight that target-specific information can effectively guide conformation space search towards biologically relevant regions. We propose a framework for protein structure prediction that achieves biological accuracy and computational efficiency by guiding conformation space search using target-specific information. The proposed framework exploits information about the characteristics of the target\u2019s energy landscape acquired continuously during search. As search progresses, the continuous integration of these sources of information will tailor conformation space search to the particular characteristics of the target. This tailored conformation space exploration can overcome the current bottleneck, yielding highly accurate and efficient structure prediction. 1",
            "group": 2634,
            "name": "10.1.1.124.9665",
            "keyword": "",
            "title": "Predicting Protein Structure with Guided Conformation Space Search"
        },
        {
            "abstract": "Abstract. Previous work investigating the performance of genetic algorithms (GAs) has attempted to develop a set of fitness landscapes, called \u201cRoyal Roads \u201d functions, which should be ideally suited for search with GAs. Surprisingly, many studies have shown that genetic algorithms actually perform worse than random mutation hill-climbing on these landscapes, and several different explanations have been offered to account for these observations. Using a detailed stochastic model of genetic search on R1, we attempt to determine a lower bound for the required number of function evaluations, and then use it to evaluate the performance of an actual genetic algorithm on R1. 1",
            "group": 2635,
            "name": "10.1.1.124.9988",
            "keyword": "",
            "title": "The Royal Road Not Taken: A Re-Examination of the Reasons for GA Failure on R1"
        },
        {
            "abstract": "We present two new techniques for the inversion of first-arrival times to estimate velocity structure. These travel-time inversion techniques are unique in that they do not require the calculation of ray paths. First-arrival times are calculated using a finite-difference scheme that iteratively solves the eikonal equations for the position of the wavefront. The first inversion technique is a direct extension of linearized waveform inversion schemes. The nonlinear relationship between the observed first-arrival times and the model slowness is linearized using a Taylor series expansion and a solution is found by iteration. For a series of two-dimensional numerical tests, with and without random noise, this travel-time inversion procedure accurately reconstructed the synthetic test models. This iterative inversion procedure converges quite rapidly and remains stable with further iteration. The second inversion tech-nique is an application of simulated annealing to travel-time topography. The annealing algorithm is a randomized search through model space that can be shown to converge to a global minimum in well-posed problems. Our tests of simulated annealing travel-time topography indicate that, in the presence of less than ideal ray coverage, significant artifacts may be introduced into the solution. The linearized inversion scheme outperforms the nonlinear simulated annealing approach and is our choice for travel-time inversion problems. Both techniques are applicable to a variety of seismic problems including earth-quake travel-time tomography, reflection, refraction/wide-angle reflection, borehole, and surface-wave phase-velocity tomography.",
            "group": 2636,
            "name": "10.1.1.125.126",
            "keyword": "",
            "title": "TOMOGRAPHY WITHOUT RAYS"
        },
        {
            "abstract": "2 To my Parents because they were there when I was alone. To my Sister because she loves me without questions. To my Professors because they answer my questions. To my Friends because they were patient with me. Thank you 3",
            "group": 2637,
            "name": "10.1.1.125.780",
            "keyword": "LIST OF TABLES..................................... 10",
            "title": "ACKNOWLEDGMENTS"
        },
        {
            "abstract": "Estimations of population genetic parameters like allele frequencies, heterozygosities, inbreeding coefficients and genetic distances rely on the assumption that all sampled genotypes come from a randomly interbreeding population or sub-population. Here we show that small cross-generational samples may severely affect estimates of allele frequencies, when a small number of progenies dominate the next generation or the sample. A new estimator of allele frequencies is developed for such cases when the kin structure of the focal sample is unknown and has to be assessed simultaneously. Using Monte Carlo simulations it was demonstrated that the new estimator delivered significant improvement over the conventional allele-counting estimator. 1",
            "group": 2638,
            "name": "10.1.1.125.1052",
            "keyword": "",
            "title": "ESTIMATION OF POPULATION ALLELE FREQUENCIES FROM SMALL SAMPLES CONTAINING MULTIPLE GENERATIONS"
        },
        {
            "abstract": " ",
            "group": 2639,
            "name": "10.1.1.125.2750",
            "keyword": "",
            "title": "A Schema for Constraint Relaxation with Instantiations for Partial Constraint Satisfaction and Schedule Optimization"
        },
        {
            "abstract": "Abstract\u2014This paper proposes an improved adaptive approach involving Bacterial Foraging Algorithm (BFA) to optimize both the amplitude and phase of the weights of a linear array of antennas for maximum array factor at any desired direction and nulls in specific directions. The Bacteria Foraging Algorithm is made adaptive using principle of adaptive delta modulation. To show the improvement in making the algorithm adaptive, results for both adaptive and nonadaptive algorithms are given. It is found that Adaptive Bacteria Foraging Algorithm (ABFA) is capable of improving the speed of convergence as well as the precision in the desired result. 1.",
            "group": 2640,
            "name": "10.1.1.125.3615",
            "keyword": "",
            "title": "IMPROVED ADAPTIVE BACTERIA FORAGING ALGORITHM IN OPTIMIZATION OF ANTENNA ARRAY FOR FASTER CONVERGENCE"
        },
        {
            "abstract": "This paper describes an innovative framework, iFAO-Simo, which integrates optimization, simulation and GIS (geographic information system) techniques to handle complex spatial facility network optimization problems ever challenged from retailing, banking and logistics nowadays. At the top level of iFAO-Simo, an optimization engine serves to generate and test candidate solutions iteratively by use of optimization algorithms such as Tabu Search and Genetic Algorithms. For each scenario given by the candidate solutions, a discrete event simulation engine is triggered to simulate customer and facility behaviors based on a GIS platform to characterize and visualize the spatial, dynamic and indeterministic environments. As the result, the target measures can be easily calculated to evaluate the solution and feedback to the optimization engine. This paper studies a real case of banking branch network optimization problem, and the results show that iFAO-Simo provides a useful way to handle complex spatial optimization problems. 1",
            "group": 2641,
            "name": "10.1.1.125.5481",
            "keyword": "",
            "title": "Proceedings of the 2007 Winter Simulation Conference"
        },
        {
            "abstract": null,
            "group": 2642,
            "name": "10.1.1.125.6486",
            "keyword": "",
            "title": "1.1.1 Physical Construction......................... 11"
        },
        {
            "abstract": "In this paper we address the problem of mapping guitar music score into one of possible alternative fingering sequences on the fretboard grid. We use dynamic programming (DP) to model the decision process of a guitarist choosing the optimal fingering sequence. To estimate the DP cost functions based on examples of guitar fingering transcriptions (tablatures) we developed an original method named &quot;path difference learning\u201d employing a gradient descent search on the coefficients of the cost function. Features of the fingering alternatives that capture the essence of the mechanical difficulty and musical quality are used to reject impractical fingerings and thus reduce the DP search complexity. Our experiments for several classical guitar pieces showed consistent convergence of the path difference learning. The adaptation resulted in a significant decrease in error count compared to manually selecting the cost function weights. 1",
            "group": 2643,
            "name": "10.1.1.125.6552",
            "keyword": "",
            "title": "Path difference learning for guitar fingering problem"
        },
        {
            "abstract": "Due to the evolution of Geographical Information Systems, large collections of spatial data having various thematic contents are currently available. As a result, the interest of users is not limited to simple spatial selections and joins, but complex query types that implicate numerous spatial inputs become more common. Although several algorithms have been proposed for computing the result of pairwise spatial joins, limited work exists on processing and optimization of multiway spatial joins. In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.",
            "group": 2644,
            "name": "10.1.1.125.8608",
            "keyword": "General TermsAlgorithms Additional Key Words and PhrasesMultiway joinsquery processingspatial joins",
            "title": "Multiway Spatial Joins"
        },
        {
            "abstract": "Abstract-Quality is the hallmark of a competitive product. It is necessary to use inspection stations to check product quality and process performance. In this paper, we are concerned with the problem of location of inspection stations in a multistage man-ufacturing system. We present two stochastic search algorithms for solving this problem, one based on Simulated Annealing and the other on Genetic Algorithms. These algorithms are developed to determine the location of inspection stations resulting in a minimum expected total cost in a multistage manufacturing sys-tem. The total cost includes inspection, processing and scrapping cost at each stage of the production process. A penalty cost is also included in it to account for a defective item which is not detected by the inspection scheme. A set of test examples are solved using these algorithms. We also compare performance of these two algorithms. I.",
            "group": 2645,
            "name": "10.1.1.126.3294",
            "keyword": "",
            "title": "Inspection Allocation in Manufacturin Systems Using Stochastic Search Techni"
        },
        {
            "abstract": "This paper introduces a new learning algorithm for artificial neural networks, based on a fuzzy inference system ANBLIR. It is a computationally effective neuro-fuzzy system with parametrized fuzzy sets in the consequent parts of fuzzy if-then rules, which uses a conjunctive as well as a logical interpretation of those rules. In the original approach, the estimation of unknown system parameters was made by means of a combination of both gradient and least-squares methods. The novelty of the learning algorithm consists in the application of a deterministic annealing optimization method. It leads to an improvement in the neuro-fuzzy modelling performance. To show the validity of the introduced method, two examples of application concerning chaotic time series prediction and system identification problems are provided.",
            "group": 2646,
            "name": "10.1.1.126.5722",
            "keyword": "fuzzy systemsneural networksneuro-fuzzy systemsrules extractiondeterministic annealingprediction",
            "title": "Neuro-fuzzy modeling based on deterministic annealing approach"
        },
        {
            "abstract": "",
            "group": 2647,
            "name": "10.1.1.126.6013",
            "keyword": "",
            "title": "SEISMIC TRAVELTIME TOMOGRAPHY OF THE CRUST AND LITHOSPHERE"
        },
        {
            "abstract": "Computers continue to change the world of engineering and design, increasing the complexity of what can be designed and built as well as enhancing our imaginations and understanding. In",
            "group": 2648,
            "name": "10.1.1.126.6073",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "Abstract: In electrical impedance tomography (EIT), various image reconstruction algorithms have been used in order to compute the internal resistivity distribution of the unknown object with its electric potential data at the boundary. Mathematically, the EIT image reconstruction algorithm is a nonlinear ill-posed inverse problem. This paper presents a simulated annealing technique as a statistical reconstruction algorithm for the solution of the static EIT inverse problem. Computer simulations with 32 channels synthetic data show that the spatial resolution of reconstructed images by the proposed scheme is improved as compared to that of the mNR algorithm at the expense of increased computational burden.",
            "group": 2649,
            "name": "10.1.1.126.6510",
            "keyword": "Electrical impedance tomographyfinite element methodinverse problem",
            "title": "Image Reconstruction using Simulated Annealing Algorithm in EIT"
        },
        {
            "abstract": "In the past two decades, the simulated annealing technique has been considered as a powerful approach to handle many NP-hard optimization problems in VLSI designs. Recently, a new Monte Carlo and optimization technique, named simulated tempering, was invented and has been successfully applied to many scientific problems, from random field Ising modeling to the traveling salesman problem. It is designed to overcome the drawback in simulated annealing when the problem has a rough energy landscape with many local minima separated by high energy barriers. In this paper, we have successfully applied a version of relaxed simulated tempering to slicing floorplan design with consideration of both area and wirelength optimization. Good experimental results were obtained. 1",
            "group": 2650,
            "name": "10.1.1.126.6668",
            "keyword": "",
            "title": "Relaxed simulated tempering for VLSI floorplan design"
        },
        {
            "abstract": "Abstract. We consider the NP-hard label number maximization problem lnm: Given a set of rectangular labels, each of which belongs to a point feature in the plane, the task is to find a labeling for a largest subset of the labels. A labeling is a placement such that none of the labels overlap and each is placed so that its boundary touches the corresponding point feature. The purpose of this paper is twofold: We present a new force-based simulated annealing algorithm to heuristically solve the problem and we provide the results of a very thorough experimental comparison of the best known labeling methods on widely used benchmark sets. The design of our new method has been guided by the goal to produce labelings that are similar to the results of an experienced human performing the same task. So we are not only looking for a labeling where the number of labels placed is high but also where the distribution of the placed labels is good. Our experimental results show that the new algorithm outperforms the other methods in terms of quality while still being reasonably fast and confirm that the simulated annealing method is well-suited for map labeling problems. 1",
            "group": 2651,
            "name": "10.1.1.126.7312",
            "keyword": "",
            "title": "Label Number Maximization in the Slider Model (Extended Abstract)"
        },
        {
            "abstract": "This thesis examines methods for generation of heterogeneous reconfigurable hardware based on formal optimisation methods. As reconfigurable hardware has evolved, an increasing number of different embedded component types have been made available on reconfigurable devices. Existing techniques for generating hardware and analysing the advantages of different embedded com-ponents are heuristic based, and hence not provably optimal, indeed, even the meaning of \u201coptimal\u201d in such a setting is disputed. Using formal optimisa-tion in the form of integer linear programming, it is possible to determine the advantages of such components analytically for the first time. Moreover, it is possible to use such techniques to determine technology mapping and floor-planning of target designs and floorplanning of architectures concurrently and optimally. In addition, this thesis covers aspects concerning the scalability of integer linear programming. Heuristic methods to solve larger, more complex prob-lems are detailed. The heuristics have been developed so as to be scalable for large, complex designs. The methods introduced in this thesis allow reconfigurable architectures to be tailored to the specific needs of a given set of circuits. In this thesis, existing reconfigurable devices are modelled in order to prove the advantages of embedded components, and to indicate what type of improvement is achievable over existing commercially manufactured devices. ",
            "group": 2652,
            "name": "10.1.1.126.7575",
            "keyword": "",
            "title": "Heterogeneous Reconfigurable Architecture Design: An Optimisation Approach"
        },
        {
            "abstract": "Abstract. This paper considers the use of the EM-algorithm, combined with mean field theory, for parameter estimation in Markov random field models from unlabelled data. Special attention is given to the theoretical justification for this procedure, based on recent results from the machine learning literature. With these results established, an example is given of the application of this technique for analysis of single trial functional magnetic resonance (fMR) imaging data of the human brain. The resulting model segments fMR images into regions with different \u2018brain response \u2019 characteristics. 1",
            "group": 2653,
            "name": "10.1.1.126.9578",
            "keyword": "",
            "title": "Markov Random Field Modelling of fMRI Data Using a Mean Field EM-algorithm"
        },
        {
            "abstract": " ",
            "group": 2654,
            "name": "10.1.1.127.696",
            "keyword": "",
            "title": "FAT SUPPRESSION AND SEGMENTATION IN PHASE-CONTRAST MRI FLOW MEASUREMENTS "
        },
        {
            "abstract": "Automatic software testing tools are still far from ideal for real world Object-Oriented (OO) Software. The use of nature inspired search algorithms for this problem has been investigated recently. Testing complex data structures (e.g., containers) is very challenging since testing software with simple states is already hard. Because containers are used in almost every type of software, their reliability is of utmost importance. Hence, this paper focuses on the difficulties of testing container classes with nature inspired search algorithms. We will first describe how input data can be automatically generated for testing Java Containers. Input space reductions and a novel testability transformation are presented to aid the search algorithms. Different search algorithms are then considered and studied in order to understand when and why a search algorithm is effective for a testing problem. In our experiments, these nature inspired search algorithms seem to give better results than the traditional techniques described in literature. Besides, the problem of minimising the length of the test sequences is also addressed. Finally, some open research questions are given.",
            "group": 2655,
            "name": "10.1.1.127.1526",
            "keyword": "Software TestingObject-Oriented SoftwareContainersSearch AlgorithmsNature Inspired AlgorithmsSearch Based Software EngineeringTestability TransformationsWhite Box Testing",
            "title": "Search based software testing of object-oriented containers"
        },
        {
            "abstract": "Deterministic optimization programs are coming to be con-sidered as viable alternatives for the placement of very large sea-of-gate, gate array and standard cell designs. A nets-as-points placement program has been described which provides competitive results in comparison with non-deterministic placement, and at a fraction of the run time. A new pseudo Steiner tree model for the gate placement about the net-points, along with a virtual channel snap-to-grid procedure, provides results superior to the original nets-as-points place-ment program without requiring iterative improvement.",
            "group": 2656,
            "name": "10.1.1.127.2495",
            "keyword": "",
            "title": "Efficient Final Placement Based on Nets-As-Points"
        },
        {
            "abstract": "Abstract With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, iWarp, SmartMemories). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wireexposed architectures. In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, much analysis is needed to adapt a stream program to a parallel stream processor. We describe fission and fusion transformations that can be used to adjust the granularity of a stream graph, a layout algorithm for mapping a stream graph to a given network topology, and a scheduling algorithm for generating a fine-grained static communication pattern for each computational element. We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing optimizations. Using the cycle-accurate Raw simulator, we demonstrate that these optimizations can improve performance by up to 150%. We consider this work to be a first step towards a portable programming model for communication-exposed architectures. 1 Introduction As we approach the billion-transistor era, a number of emerging architectures are addressing the wire delay problem by replicating the basic processing unit and exposing the communication between units to a software layer (e.g., Raw [24], SmartMemories [13], TRIPS [19]). These machines are especially well-suited for streaming applications that have regular communication patterns and widespread parallelism.",
            "group": 2657,
            "name": "10.1.1.127.2569",
            "keyword": "",
            "title": "A Stream Compiler for Communication-Exposed Architectures"
        },
        {
            "abstract": "ad agenti",
            "group": 2658,
            "name": "10.1.1.127.2585",
            "keyword": "Computational BiologyAgent-Based TechnologiesConcurrent Constraint Programming",
            "title": "Protein Folding Simulation"
        },
        {
            "abstract": null,
            "group": 2659,
            "name": "10.1.1.127.3061",
            "keyword": "",
            "title": "c \u25cb 2003 Kluwer Academic Publishers. Manufactured in The Netherlands. An Introduction to MCMC for Machine Learning"
        },
        {
            "abstract": "This paper presents an automated partitioning strategy to divide a design into a set of partitions based on design hierarchy information. While the primary objective is to use these partitions in an Incremental Design flow for compile time reduction, the performance of the partitioned design should not be degraded after partitioning. Experimental results using the incremental design feature of Quartus CAD tool from Altera show that our algorithm can generate partitioning solutions comparable with a set of manually partitioned real industrial circuits and results in more than 50% compile time reduction. 1.",
            "group": 2660,
            "name": "10.1.1.127.3380",
            "keyword": "",
            "title": "MODULAR PARTITIONING FOR INCREMENTAL COMPILATION"
        },
        {
            "abstract": "analytic hierarchy process and heuristics. Silva Fennica 40(1): 143\u2013160. The management of low-volume roads has transitioned from focusing on maintenance designed to protect a capital investment in road infrastructure to also include environmental effects. In this study, two models using mathematical programming are applied to schedule forest road maintenance and upgrade activities involving non-monetary benefits. Model I uses a linear objective function formulation that maximizes benefit subject to budgetary constraints. Model II uses a non-linear objective function to maximize the sum of benefits divided by the sum of all costs in a period. Because of the non-linearity of the constraints and the requirements that the decision variables be binary, the solutions to both problem formulations are found using two heuristics, simulated annealing and threshold accepting. Simulated annealing was found to produce superior solutions as compared to threshold accepting. The potential benefit for completing a given road maintenance or upgrade project is determined using the Analytic Hierarchy Process (AHP), a multi-criterion decision analysis technique. This measure of benefit is combined with the economic cost of completing a given project to schedule maintenance and upgrade activities for 225 km (140 miles) of road in forested",
            "group": 2661,
            "name": "10.1.1.127.3826",
            "keyword": "threshold acceptingroad environmental impactsdecision supportAHP",
            "title": "Scheduling Forest Road Maintenance Using the Analytic Hierarchy Process and Heuristics"
        },
        {
            "abstract": "Sandia is a multiprogram laboratory operated by Sandia Corporation,",
            "group": 2662,
            "name": "10.1.1.127.4047",
            "keyword": "",
            "title": "National Nuclear Security Administration under Contract DE-AC04-94AL85000."
        },
        {
            "abstract": "Abstract \u2014 We describe a novel shape formation algorithm for ensembles of 2-dimansional lattice-arrayed modular robots, based on the manipulation of regularly shaped voids within the lattice (\u201choles\u201d). The algorithm is massively parallel and fully distributed. Constructing a goal shape requires time proportional only to the complexity of the desired target geometry. Construction of the shape by the modules requires no global communication nor broadcast floods after distribution of the target shape. Results in simulation show 97.3 % shape compliance in ensembles of approximately 60,000 modules, and we believe that the algorithm will generalize to 3D and scale to handle millions of modules. This paper is submitted to Invited Session: New Trends in Modular Robotics. I.",
            "group": 2663,
            "name": "10.1.1.127.4160",
            "keyword": "",
            "title": "Scalable shape sculpting via hole motion: Motion planning in lattice-constrained modular robots"
        },
        {
            "abstract": " Multihoming has been used by stub networks for several years as a form of redundancy, improving the availability of Internet access. More recently, Intelligent Route Control (IRC) products allow multihomed networks to dynamically switch parts of their egress or ingress traffic between ISPs, also improving cost and performance. IRC products assume that the set of upstream ISPs is given and fixed. Typically, however, a multihomed network has several ISP choices and the actual selection of ISPs can significantly affect cost, availability, and performance. In the first part of this work, we develop a methodology to select the best set of upstream ISPs, optimizing monetary cost and availability. Our results, based on measurements of actual Internet traffic and topology, show that the proposed algorithm selects the best possible set of ISPs in terms of resiliency to inter-AS single-link failures. The algorithm also performs well in the presence of double or triple link failures. In the second part of this work, we focus on the egress path selection problem. Specifically, we propose a stochastic search algorithm, based on simulated annealing, to allocate the network\u2019s egress traffic between upstream ISPs. The objectives are to minimize cost, also ensuring that the selected paths to the major destinations of egress traffic are congestion-free. Simulation results show that the proposed algorithm performs very well in meeting the previous objectives, when congestion-free paths exist.",
            "group": 2664,
            "name": "10.1.1.127.4489",
            "keyword": "MultihomingIntelligent RoutingPath DiversityNetwork MeasurementsSimulated Annealing",
            "title": "ISP and Egress Path Selection for Multihomed Networks"
        },
        {
            "abstract": "  This dissertation presents a new and efficient next-best-view algorithm for 3D reconstruction of indoor environments using active range sensing. A major challenge in range acquisition for 3D reconstruction is an efficient automated view planning algorithm to determine a sequence of scanning locations or views such that a set of acquisition constraints and requirements is satisfied and the object or environment of interest can be satisfactorily reconstructed. Due to the intractability of the view planning problem and the lack of global geometric information, a greedy approach is adopted to approximate the solution. A practical view metric is formulated to include many real-world acquisition constraints and reconstruction quality requirements. This view metric is flexible to allow trade-offs between different requirements of the reconstruction quality. A major contribution of this work is the application of a hierarchical approach to greatly accelerate the evaluation of the view metric",
            "group": 2665,
            "name": "10.1.1.127.4784",
            "keyword": "",
            "title": "View Planning for Range Acquisition of Indoor Environments "
        },
        {
            "abstract": " In this paper, we extend the concept of the P-admissible floorplan representation to that of the \u20ac-admissible one. A \u20ac-admissible representation can model the most general floorplans. Each of the currently existing \u20ac-admissible representations, sequence pair (SP), bounded-slicing grid, and transitive closure graph (TCG), has its strengths as well as weaknesses. We show the equivalence of the two most promising \u20ac-admissible representations, TCG and SP, and integrate TCG with a packing sequence (part of SP) into a representation, called TCG-S. TCG-S combines the advantages of SP and TCG and at the same time eliminates their disadvantages. With the property of SP, a fast packing scheme is possible. Inherited nice properties from TCG, the geometric relations among modules are transparent to TCG-S (implying faster convergence to a desired solution), placement with position constraints becomes much easier, and incremental update for cost evaluation can be realized. These nice properties make TCG-S a superior representation which exhibits an elegant solution structure to facilitate the search for a desired floorplan/placement. Extensive experiments show that TCG-S results in the best area utilization, wirelength optimization, convergence speed, and stability among existing works and is very flexible in handling placement with special constraints. ",
            "group": 2666,
            "name": "10.1.1.127.5153",
            "keyword": "",
            "title": "TCG-S: Orthogonal Coupling of P*-Admissible Representations for General Floorplans"
        },
        {
            "abstract": "Almost all analyses of time complexity of evolutionary algorithms (EAs) have been conducted for (1+1) EAs only. Theoretical results on the average computation time of population-based EAs are few. However, the vast majority of applications of EAs use a population size that is greater than one. The use of population has been regarded as one of the key features of EAs. It is important to understand in depth what the real utility of population is in terms of the time complexity of EAs, when EAs are applied to combinatorial optimization problems. This paper compares (1 + 1) EAs and (N + N) EAs theoretically by deriving their first hitting time on the same problems. It is shown that a population can have a drastic impact on an EA\u2019s average computation time, changing an exponential time to a polynomial time (in the input size) in some cases. It is also shown that the first hitting probability can be improved by introducing a population. However, the results presented in this paper do not imply that population-based EAs will always be better than (1 + 1) EAs for all possible problems. I.",
            "group": 2667,
            "name": "10.1.1.127.6004",
            "keyword": "",
            "title": "From an individual to a population: An analysis of the first hitting time of population-based evolutionary algorithms"
        },
        {
            "abstract": " ",
            "group": 2668,
            "name": "10.1.1.127.6653",
            "keyword": "",
            "title": "Anticipating urgent surgery in operating room departments"
        },
        {
            "abstract": "Abstract. Heuristic algorithms (or simply heuristics) are methods that seek for high quality solutions within a reasonable (limited) amount of time without being able to guarantee optimality. They often come out as a result of imitation of the real world (physics, nature, biology, etc.). In this paper, we give an overview of some heuristic algorithms for combinatorial optimization problems. At the beginning, some definitions related to combinatorial optimization, as well as the principle (framework) and basic features of the heuristics for combinatorial problems are concerned. Then, several popular heuristic algorithms are discussed, namely: descent local search, simulated annealing, tabu search, genetic algorithms, ant algorithms, and iterated local search. The unified paradigms of these heuristics are given. Finally, we present some results of comparisons of these algorithms for the well-known combinatorial problem, the quadratic assignment problem.",
            "group": 2669,
            "name": "10.1.1.127.7402",
            "keyword": "heuristic algorithmsdescent local searchsimulated annealingtabu searchgenetic algorithmsant algorithms",
            "title": "AN OVERVIEW OF SOME HEURISTIC ALGORITHMS FOR COMBINATORIAL OPTIMIZATION PROBLEMS"
        },
        {
            "abstract": "Maximum a Posteriori assignment (MAP) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a Bayesian network. MAP has been shown to be a NP-hard problem [22], even for constrained networks, such as polytrees [18]. Hence, previous approaches often fail to yield any results for MAP problems in large complex Bayesian networks. To address this problem, we propose AnnealedMAP algorithm, a simulated annealing-based MAP algorithm. The AnnealedMAP algorithm simulates a non-homogeneous Markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density. We tested this algorithm on several real Bayesian networks. The results show that, while maintaining good quality of the MAP solutions, the AnnealedMAP algorithm is also able to solve many problems that are beyond the reach of previous approaches. 1",
            "group": 2670,
            "name": "10.1.1.127.7419",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "This paper describes a simulated annealing based multiobjective optimization algorithm that incorporates the concept of archive in order to provide a set of tradeoff solutions for the problem under consideration. To determine the acceptance probability of a new solution vis-a-vis the current solution, an elaborate procedure is followed that takes into account the domination status of the new solution with the current solution, as well as those in the archive. A measure of the amount of domination between two solutions is also used for this purpose. A complexity analysis of the proposed algorithm is provided. An extensive comparative study of the proposed algorithm with two other existing and well-known multiobjective evolutionary algorithms (MOEAs) demonstrate the effectiveness of the former with respect to five existing performance measures, and several test problems of varying degrees of difficulty. In particular, the proposed algorithm is found to be significantly superior for many objective test problems (e.g., 4, 5, 10, and 15 objective problems), while recent studies have indicated that the Pareto ranking-based MOEAs perform poorly for such problems. In a part of the investigation, comparison of the real-coded version of the proposed algorithm is conducted with a very recent multiobjective simulated annealing algorithm, where the performance of the former is found to be generally superior to that of the latter.",
            "group": 2671,
            "name": "10.1.1.127.7529",
            "keyword": "SA",
            "title": " A Simulated Annealing-Based Multiobjective Optimization Algorithm: AMOSA"
        },
        {
            "abstract": "This paper describes a hybrid BIST methodology for testing systems-on-chip. In our hybrid BIST approach a test set is assembled, for each core, from pseudorandom test patterns that are generated on-line, and deterministic test patterns that are generated off-line and stored in the system. The deterministic test set is specially designed to shorten the pseudorandom test cycle and to target random resistant faults. To support such a test strategy, we have developed several hybrid BIST architectures that target different test scenarios. As the test lengths of the two test sequences is one of the important parameters in the final test cost, we have to find the most efficient combination of those two test sets without sacrificing the test quality. We describe methods for finding the optimal combination of pseudorandom and deterministic test sets of the whole system, consisting of multiple cores, under given memory constraints, so that the total test time is minimized. Our approach employs a fast estimation methodology in order to avoid exhaustive search and to speed up the calculation process. Experimental results have shown the efficiency of the algorithms to find a near-optimal solutions. ",
            "group": 2672,
            "name": "10.1.1.127.7718",
            "keyword": "",
            "title": "Hybrid BIST methodology for testing core-based systems"
        },
        {
            "abstract": "Abstract \u2014 Various computational intelligences are formulated based on max-plus algebra, especially, neural networks, wavelets, cellular automata including fractal/L-system/petri-nets, are formulated based on max-plus algebra. The advantages of ordered structure based computational intelligences are mainly correspond to high speed/parallel processing, analyzing information in terms of ordered structure, and treating only discrete information (no quantization error). Through application experiments, it is shown the potential of the ordered structure based computational intelligences. I.",
            "group": 2673,
            "name": "10.1.1.127.8235",
            "keyword": "",
            "title": "Ordered Structure Based Computational Intelligence and Its Application to Image Processing"
        },
        {
            "abstract": "Abstract \u2014 A huge application domain, in particular, wireless and handheld devices strongly requires flexible and powerefficient hardware with high performance. This can only be achieved with Application Specific Instruction-Set Processors (ASIPs). A key problem is to determine the instruction encoding of the processors for achieving minimum power consumption in the instruction bus and in the instruction memory. In this paper, a framework for determining powerefficient instruction encoding in RISC and VLIW architectures is presented. We have integrated existing and novel techniques in this framework and propose novel heuristic approaches. The framework accepts an existing processor\u2019s instruction-set and a set of implementations of various applications. The output, which is an optimized instruction encoding under the constraint of a well-defined cost model, minimizes the power consumption of the instruction bus and the instruction memory. This results in strong reduction of the overall power consumption. Case studies with commercial embedded processors show the effectiveness of this framework. Index Terms \u2014 power-efficient, instruction encoding, instruction memory, instruction bus, embedded processors I.",
            "group": 2674,
            "name": "10.1.1.127.9529",
            "keyword": "",
            "title": "Power-efficient Instruction Encoding Optimization for Various Architecture Classes"
        },
        {
            "abstract": "We propose a minimization algorithm that combines cost tables and simulated annealing. Unlike other minimization procedures that use cost tables, our method does not rely on the enumeration of all minterms, it works directly with the expression (a sum of product terms). We tested our method withacost table obtained from current mode CMOS. The algorithm can be readily adapted to any other technology by simply replacing the cost table. The technology dependency is limited to the cost table. Results from our implementation are very encouraging. 1",
            "group": 2675,
            "name": "10.1.1.127.9533",
            "keyword": "",
            "title": "Multiple-Valued Logic Minimization using Universal Literals and Cost Tables"
        },
        {
            "abstract": "Abstract. In this paper, we propose a new population-based framework for combining local search with global explorations to solve single-objective unconstrained numerical optimization problems. The idea is to use knowledge about local optima found during the search to a) locate promising regions in the search space and b) identify suitable step sizes to move from one optimum to others in each region. The search knowledge was maintained using a Cultural Algorithm-based structure, which is updated by behaviors of individuals and is used to actively guide the search. Some experiments have been carried out to evaluate the performance of the algorithm on well-known continuous problems. The test results show that the algorithm can get comparable or superior results to that of some current well-known unconstrained numerical optimization algorithms in certain classes of problems.",
            "group": 2676,
            "name": "10.1.1.128.363",
            "keyword": "Unconstrained Numerical OptimizationCultural AlgorithmIterated Local SearchEvolutionary Algorithm",
            "title": "Hybridizing Cultural Algorithms and Local Search"
        },
        {
            "abstract": "A brief introduction to high throughput technologies for measuring and analyzing gene expression is given. Various supervised and unsupervised data mining methods for analyzing the produced highdimensional data are discussed. The main emphasis is on supervised machine learning methods for classification and prediction of tumor gene expression profiles. Furthermore, methods to rank the genes according to their importance for the classification are explored. The approaches are illustrated by exploratory studies using two examples of retrospective clinical data from routine tests; diagnostic prediction of small round blue cell tumors (SRBCT) of childhood and determining the estrogen receptor (ER) status of sporadic breast cancer. The classification performance is gauged using blind tests. These studies demonstrate the feasibility of machine learning-based molecular cancer classification.",
            "group": 2677,
            "name": "10.1.1.128.652",
            "keyword": "Artificial neural networksBioinformaticsDiagnostic predictionDrug target identificationGenes",
            "title": "Analyzing tumor gene expression profiles"
        },
        {
            "abstract": "Abstract \u2014 We explore global optimizations\u2013Simulated Annealing (SA) and Hill-Climbing (HC)\u2013for placement of multiple Optical Network Units (ONU) in a hybrid opticalwireless access network. We compare the performance of SA and HC with a greedy algorithm (Greedy), proposed in [1], and conclude that results from Greedy are quite comparable to those from SA and HC, but at much lower processing requirements. I.",
            "group": 2678,
            "name": "10.1.1.128.1002",
            "keyword": "",
            "title": "Towards global optimization of multiple ONUs placement in hybrid optical-wireless broadband access networks"
        },
        {
            "abstract": "In this paper we study a bidding problem which can be modelled as a set packing problem. A simulated annealing heuristic with three local moves, including an embed-ded branch-and-bound move, is developed for the problem. We compared the heuristic with the CPLEX 8.0 solver and the current best non-exact method, Casanova, using the standard CATS benchmark and other realistic test sets. Results show that the heuristic outperforms CPLEX and Casanova.",
            "group": 2679,
            "name": "10.1.1.128.1859",
            "keyword": "biddingheuristicsartificial intelligence 1",
            "title": "Heuristics for a bidding problem"
        },
        {
            "abstract": "The approach proposed in this paper reduces power consumption in single-error correcting, double error-detecting checker circuits that perform memory ECC. Power is minimized with little or no impact on area and delay, using the degrees of freedom in selecting the parity check matrix of the error correcting code. The non-linear power optimization problem is solved using two methods, genetic algorithms and simulated annealing. Both the methods are applied to two SEC-DED codes: standard Hamming codes and odd-column-weight Hsiao codes. Experiments on actual memory traces of Spec and MediaBench benchmarks indicate that considering power along with area and delay when selecting the parity check matrix can result in power reductions of up to 27 % for Hsiao codes and up to 41 % for Hamming codes. Experiments are also performed to motivate the choice of parameters of the non-linear optimization algorithms, using sensitivity analysis of the low-power solutions to the choice of the different parameters of each algorithm.",
            "group": 2680,
            "name": "10.1.1.128.2306",
            "keyword": "Memory error correctionHsiao and Hamming codeslow powernon-linear optimization",
            "title": "N.A.: Selecting error correcting codes to minimize power in memory checker circuits"
        },
        {
            "abstract": "The methods for spatial data analysis are often based on the assumption of stationarity of the estimated parameters (Besag, 1974; Cressie, 1993). This hypothesis is patently violated when the data are characterized by information relative to predefined but unknown sub-groups of the reference population. It is clear that for spatial data which follow this hypothesis, the main analytic issue is, not to estimate the model parameters or to introduce a structural dependence among observations, but to identify the geographical units where the parameters model is stationary. We refer to such situation as local stationarity. The main aim of this paper is to present a spatial model for the forecast of agricultural data. In particular, we propose an approach based on the Simulated Annealing algorithm (Geman and Geman, 1984). Furthermore, we describe an application of the proposed algorithm for the forecasts of the yield of durum wheat in 2004. Our aim is to produce a map of potential yield for durum wheat through a regression using three purely geographical covariates, the x and y coordinates and the elevation, and on an agro-meteorological model estimated yield (SAM model). The assumption of non stationarity for agricultural data largely improves the obtained results in terms of R-squared index. 1.",
            "group": 2681,
            "name": "10.1.1.128.3294",
            "keyword": "",
            "title": "A spatial method for the forecast of agricultural data"
        },
        {
            "abstract": "Ideally, planning and managing the design and development of a complex systems should: \u2022 Consider the entire lifecycle (design, development, testing, integration, deployment, operation and decommissioning). \u2022 Take risk and reliability into account, as well as more traditional measures such as cost and performance, when making tradeoff decisions. \u2022 Provide understanding, and therefore motivation, as to the purpose of design artifacts (that piece is there to\u2026) and development activities (we\u2019ll be performing this test to A key enabler to all of the above is the ability to relate design and development choices to risk and reliability predictions. If the number of choices were small (e.g., selection between a handful of alternatives) it would suffice to",
            "group": 2682,
            "name": "10.1.1.128.3584",
            "keyword": "Key WordsRiskReliabilityDesignOptimization",
            "title": "Relating Risk and Reliability Predictions to Design and Development Choices"
        },
        {
            "abstract": "Autonomous range acquisition for 3D modeling requires reliable range registration, for both the precise localization of the sensor and combining the data from multiple scans for view-planning computation. We introduce and present a novel approach to improve the reliability and robustness of the ICP (Iterative Closest Point) 3D shape registration algorithm by smoothing the shape\u2019s surface into multiple resolutions. These smoothed surfaces are used in place of the original surface in a coarse-to-fine manner during registration, which allows the algorithm to avoid being trapped at local minima close to the global optimal solution. We used the technique of multiresolution analysis to create the smoothed surfaces efficiently. Besides being more robust, convergence is generally much faster, especially when combined with the point-to-plane error metric of Chen and Medioni. Since the point-to-plane error metric has no closedform solution, solving it can be slow. We introduce a variant of the ICP algorithm that has convergence rate close to it but still uses the closed-form solution techniques (SVD or unit quaternion methods) of the original ICP algorithm. ",
            "group": 2683,
            "name": "10.1.1.128.3724",
            "keyword": "",
            "title": "Reliable and Rapidly-Converging ICP Algorithm Using Multiresolution Smoothing"
        },
        {
            "abstract": "\ufffd Optimizing objective (cost) functions of complicated systems \ufffd Determination of global extremum of objective functions \ufffd Deterministic methods unrealistic as number of parameters becomes very large",
            "group": 2684,
            "name": "10.1.1.128.5115",
            "keyword": "",
            "title": "Reviewed by Amanda McCoy Combinatorial Optimization"
        },
        {
            "abstract": "Abstract. This paper describes a clustering method for unsupervised classification of objects in large data sets. The new methodology combines the mixture likelihood approach with a sampling and subsampling strategy in order to cluster large data sets efficiently. This sampling strategy can be applied to a large variety of data mining methods to allow them to be used on very large data sets. The method is applied to the problem of automated star/galaxy classification for digital sky data and is tested using a sample from the Digitized Palomar Sky Survey (DPOSS) data. The method is quick and reliable and produces classifications comparable to previous work on these data using supervised clustering.",
            "group": 2685,
            "name": "10.1.1.128.5928",
            "keyword": "clustering algorithmmixture likelihoodsamplingstar/galaxy classification",
            "title": "Sampling and subsampling for cluster analysis in data mining: With applications to sky survey data"
        },
        {
            "abstract": "We develop in this paper a two-timescale simultaneous perturbation stochastic approximation algorithm for simulation based parameter optimization over discrete sets. This algorithm is applicable in cases where the cost to be optimized is in itself the longrun average of certain cost functions whose noisy estimates are obtained via simulation. We present the convergence analysis of our algorithm. Next, we study applications of our algorithm to the problem of admission control in communication networks. We study this problem under two different experimental settings and consider appropriate continuous time queueing models in both settings. Our algorithm finds optimal threshold type policies within suitable parameterized classes of these. We show results of several experiments for different network parameters and rejection cost. We also study the sensitivity of our algorithm w.r.t. its parameters and step-sizes. The results obtained are along expected lines.",
            "group": 2686,
            "name": "10.1.1.128.6718",
            "keyword": "for correspondence",
            "title": "A discrete parameter stochastic approximation algorithm for simulation optimization"
        },
        {
            "abstract": "During recent years power optimisation has become one of the most challenging design goals in modern communication systems, particularly in the wireless domain. Many different approaches for task scheduling on single or multi-core systems exist, mostly addressing the minimisation of execution time or the number of processors used. The minimisation of the processor\u2019s clock frequency by adjusting the supply voltage or directly by frequency scaling according to the chosen task scheduling has shown good results in the reduction of power consumption. Most of the known approaches base their core algorithms on graph representations for multirate systems or synchronous data flow (SDF) graphs, in a single frequency domain. In many cases a signal processing system comprises several frequency domains, in which processes have to be fired according to their in- and output data rates as well as to their frequency domain. In this work the superposition of frequency domains and data dependencies is incorporated into the optimisation process and used as a another degree of freedom. Several algorithms have been implemented and evaluated to minimise the required processor\u2019s clock frequency, including a greedy, a simulated annealing, as well as a tabu search approach.",
            "group": 2687,
            "name": "10.1.1.128.7185",
            "keyword": "I.6 [Simulation and ModelingApplications General Terms AlgorithmsDesign Keywords Task SchedulingPower OptimisationFrequency ScalingMulti Frequency SystemsSynchronous Data Flow Graphs",
            "title": "Task Scheduling for Power Optimisation of Multi Frequency Synchronous Data Flow Graphs "
        },
        {
            "abstract": " This paper addresses the problem of device-level placement for analog layout, focusing mainly on symmetry-related aspects. Different from most of the existent analog placement approaches, employing basically simulated annealing optimization algorithms operating on flat (absolute) spatial representations [4], our model uses a more recent topological representation called sequence-pair [14], which has the advantage of not being restricted to slicing floorplan topologies. In this paper, we are explaining how specific features essential to analog placement, as the ability to deal with complex symmetry constraints (for instance, an arbitrary number of symmetry groups of cells), can be easily handled by employing the sequence-pair representation. Several analog examples substantiate the effectiveness of our placement tool, which is already in use in an industrial environment.  ",
            "group": 2688,
            "name": "10.1.1.128.7262",
            "keyword": "",
            "title": "Symmetry Within the Sequence-Pair Representation in the Context of Placement for Analog Design"
        },
        {
            "abstract": "Operational risk is an important quantitative topic as a result of the Basel II regulatory requirements. Operational risk models need to incorporate internal and external loss data observations in combination with expert opinion surveyed from business specialists. Following the Loss Distributional Approach, this article considers three aspects of the Bayesian approach to the modeling of operational risk. Firstly we provide an overview of the Bayesian approach to operational risk, before expanding on the current literature through consideration of general families of non-conjugate severity distributions, g-and-h and GB2 distributions. Bayesian model selection is presented as an alternative to popular frequentist tests, such as Kolmogorov-Smirnov or Anderson-Darling. We present a number of examples and develop techniques for parameter estimation for general severity and frequency distribution models from a Bayesian perspective. Finally we introduce and evaluate recently developed stochastic sampling techniques and highlight their application to operational risk through the models developed.",
            "group": 2689,
            "name": "10.1.1.128.7807",
            "keyword": "Approximate Bayesian ComputationBasel II Advanced Measurement ApproachBayesian InferenceCompound ProcessesLoss Distributional ApproachMarkov Chain Monte CarloOperational Risk",
            "title": "  Bayesian inference, Monte Carlo sampling and operational risk"
        },
        {
            "abstract": "Copyright by",
            "group": 2690,
            "name": "10.1.1.128.8713",
            "keyword": "",
            "title": "Acknowledgments"
        },
        {
            "abstract": "and the",
            "group": 2691,
            "name": "10.1.1.129.2389",
            "keyword": "1.1. Load Balancing Problems.................................. 5",
            "title": "Research performed under"
        },
        {
            "abstract": "sampling for environmental field estimation",
            "group": 2692,
            "name": "10.1.1.129.5023",
            "keyword": "Index Terms \u2014 Adaptive SamplingModelingField Estimation",
            "title": "using robotic sensors"
        },
        {
            "abstract": "This paper compares computational evolution using local and global evaluation functions in the context of solving two classical combinatorial problems on graphs: the k-coloring and minimum coloring problems. It is shown that the essential difference between traditional algorithms using local search (such as simulated annealing) and distributed algorithms (such as the Alife&AER model) lies in the evaluation function. Simulated annealing uses global information to evaluate the whole system state, which is called the global evaluation function (GEF) method. The Alife&AER model uses local information to evaluate the state of a single agent, which is called the local evaluation function (LEF) method. Computer experiment results show that the LEF methods are comparable to GEF methods (simulated annealing and greedy), and in some problem instances the LEF beats GEF methods. We present the consistency theorem which shows that a Nash equilibrium of an LEF is identical to a local optimum of a GEF when they are \u201cconsistent. \u201d This theorem explains why some LEFs can lead the system to a global goal. Some rules for constructing a consistent LEF are proposed.  ",
            "group": 2693,
            "name": "10.1.1.129.6096",
            "keyword": "",
            "title": "Local and Global Evaluation Functions for Computational Evolution"
        },
        {
            "abstract": "ABSTRACT A unified model of simulated annealing with locally enhanced sampling (LES) in a primary hydration shell (PHS) aqueous environment is developed and tested by predicting the structure of the tripeptide thyrotropin-releasing hormone (TRH) in solution. The model extends the formulation of the restraining force in the PHS method as a function of temperature, number of copies in the LES method, and shell thickness. The dependence of the restraining force on temperature can be shown to follow the relationship \ufffdc 1T \ufffd c 2, which can be derived from the expression for kinetic energy in molecular dynamics simulations. The calibration of the restraining force for different simulation conditions reveals the dependence of c 1 and c 2 on the number of copies in the LES method and the thickness of the PHS. The predicted structure of TRH is in very good agreement with results from NMR experiments and from a 10-ns PHS simulation at 300 K. The method promises to be useful in predicting structure of peptides and proteins in an aqueous environment.",
            "group": 2694,
            "name": "10.1.1.129.6299",
            "keyword": "",
            "title": "Application of the Primary Hydration Shell Approach to Locally Enhanced Sampling Simulated Annealing: Computer Simulation of Thyrotropin-Releasing Hormone in Water"
        },
        {
            "abstract": "Query optimisers are critical to the efficiency of modern relational database systems. If a query optimiser chooses a poor query execution plan, the performance of the database system in answering the query can be very poor. In fact, the differences in cost between the least and most expensive query execution plans can be several orders of magnitude. On the other hand, it can be prohibitively expensive for the query optimiser to search exhaustively for the least-cost (strictly optimal) query execution plan. Most query optimisers, therefore, compromise by using a reasonably cheap search to obtain a reasonably cheap query execution plan. Accurate, but inexpensive, query size estimation is fundamental to the success of real query optimisers. A number of studies [Christodoulakis 1984; Ioannidis and Christodoulakis 1991, 1993] have demonstrated that optimisers can select very expensive query execution plans if they are forced to rely on poor or inaccurate query size estimates. This thesis will address the problem of how to obtain reliable and accurate query size estimation for the cost calculation of query execution plans.",
            "group": 2695,
            "name": "10.1.1.129.6516",
            "keyword": "",
            "title": "Query Result Size Estimation Techniques in Database Systems"
        },
        {
            "abstract": "We describe an end-to-end real-time S&P futures trading system. Inner-shell stochastic nonlinear dynamic models are developed, and Canonical Momenta Indicators (CMI) are derived from a fitted Lagrangian used by outer-shell trading models dependent on these indicators. Recursive and adaptive optimization using Adaptive Simulated Annealing (ASA) is used for fitting parameters shared across these shells of dynamic and trading models. Ke ywords: Simulated Annealing; Statistical Mechanics; Trading Financial Markets \u00a92000 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked byeach author\u2019s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.",
            "group": 2696,
            "name": "10.1.1.129.6534",
            "keyword": "",
            "title": "and"
        },
        {
            "abstract": "We study optimal sample designs for prediction with estimated parameters. Recent advances in the infill asymptotic theory provides a deeper understanding of the finite sample behavior of prediction and estimation. By incorporating these known asymptotic results, we modify some existing design criteria for estimation of covariance function and best linear unbiased prediction. These modified criteria could significantly reduce the computation time necessary for finding an optimal design. We illustrate our approach through both a real experiment in agriculture and simulation.",
            "group": 2697,
            "name": "10.1.1.129.6723",
            "keyword": "Key wordsInfill asymptoticsMaximum likelihood estimationSimulated annealingSpatial",
            "title": "Spatial sampling design under the infill asymptotic framework"
        },
        {
            "abstract": "",
            "group": 2698,
            "name": "10.1.1.129.6826",
            "keyword": "",
            "title": "Stochastic Search Algorithms for Optimal Monitoring Network Designs"
        },
        {
            "abstract": "Vision tasks, such as segmentation, grouping, recognition, can be formulated as graph partition problems. The recent literature witnessed two popular graph cut algorithms: the Ncut using spectral graph analysis and the minimum-cut using the maximum flow algorithm. This paper presents a third major approach by generalizing the Swendsen-Wang method \u2013 a well celebrated algorithm in statistical mechanics. Our algorithm simulates ergodic, reversible Markov chain jumps in the space of graph partitions to sample a posterior probability. At each step, the algorithm splits, merges, or re-groups a sizable subgraph, and achieves fast mixing at low temperature enabling a fast annealing procedure. Experiments show it converges in 2-30 seconds in a PC for image segmentation. This is 400 times faster than the single-site update Gibbs sampler, and 20-40 times faster than the DDMCMC algorithm. The algorithm can optimize over the number of models and works for general forms of posterior probabilities, so it is more general than the existing graph cut approaches.  ",
            "group": 2699,
            "name": "10.1.1.129.7150",
            "keyword": "",
            "title": "Graph Partition by Swendsen-Wang Cuts"
        },
        {
            "abstract": "Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs\r\nand KFMs are limited in their \u201cexpressive power\u201d. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from sequential data.\r\nIn particular, the main novel technical contributions of this thesis are as follows: a way of representing\r\nHierarchical HMMs as DBNs, which enables inference to be done in O(T) time instead of O(T 3), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T) space instead of O(T); a simple way of using the junction tree algorithm for online inference in DBNs; new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the BK algorithm and loopy belief propagation; a way of\r\napplying Rao-Blackwellised particle filtering to DBNs in general, and the SLAM (simultaneous localization\r\nand mapping) problem in particular; a way of extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling.",
            "group": 2700,
            "name": "10.1.1.129.7714",
            "keyword": "RepresentationInference and Learning",
            "title": "Dynamic Bayesian Networks: Representation, Inference and Learning"
        },
        {
            "abstract": "One of the greatest perceived barriers to the widespread use of FPGAs in image processing is the difficulty for application specialists of developing algorithms on reconfigurable hardware. Minimum entropy deconvolution (MED) techniques have been shown to be effective in the restoration of star-field images. This paper reports on an attempt to implement a MED algorithm using simulated annealing, first on a microprocessor, then on an FPGA. The FPGA implementation uses DIME-C, a C-to-gates compiler, coupled with a low-level core library to simplify the design task. Analysis of the C code and output from the DIME-C compiler guided the code optimisation. The paper reports on the design effort that this entailed and the resultant performance improvements. 1.",
            "group": 2701,
            "name": "10.1.1.129.8719",
            "keyword": "",
            "title": "MINIMUM ENTROPY RESTORATION USING FPGAS AND HIGH-LEVEL TECHNIQUES"
        },
        {
            "abstract": "  This paper presents an algorithm for RNA secondary structure prediction based on Simulated Annealing (SA) and also studies the effect of using different types of annealing schedules. SA is known to be effective in solving many different types of minimization problems and for being able to approximate global minima in the solution space. Based on free energy minimization techniques, this permutation-based SA algorithm heuristically searches for the structure with a free energy value close to the minimum free energy \u2206G for that strand, within given constraints. Other contributions of this paper include the use of permutation-based encoding for RNA secondary structure and the swap mutation operator. Also, a detailed study of the convergence behavior of the algorithm is conducted and various annealing schedules are investigated. An evaluation of the performance of the new algorithm in terms of prediction accuracy is made via comparison with the dynamic programming algorithm mfold for thirteen individual known structures from four RNA classes (5S rRNA, Group I intron 23 rRNA, Group I intron 16S rRNA and 16S rRNA). Although dynamic programming algorithms for RNA folding are guaranteed to give the mathematically optimal (minimum energy) structure, the fundamental problem of this approach seems to be that the thermodynamic model is only accurate within 5 \u2212 10%. Therefore, it is difficult for a single sequence folding algorithm to resolve which of the plausible lowest-energy structure is correct. The new algorithm showed comparable results with mfold and demonstrated a slightly higher specificity.  ",
            "group": 2702,
            "name": "10.1.1.129.9426",
            "keyword": "",
            "title": "  SARNA-Predict: A Study of RNA Secondary Structure Prediction Using Different Annealing Schedules"
        },
        {
            "abstract": "Mask distortion due to thermal loading during lithography exposure has a tremendous impact on the overlay error budget and poses significant challenges for extending optical lithography to 65nm regime and beyond. In this thesis, the mask thermal distortion during the scanning exposure in 193nm lithography is numerically modeled, and its dependency on the distribution of the module chrome pattern density on the mask is investigated. Several numerical simulation methods are investigated for accurately predicting the transient and steady-state thermal and distortion response of the mask during scanning exposure. In particular, it is found that simulating an \u201ceffective \u201d continuous illumination power has the same thermal and distortion impact as the actual pulsed laser power delivery to the mask during scanning exposure. This approach dramatically reduces computational cost. The parametric analysis demonstrates that the magnitude of the thermal and distortion responses are closely related to the magnitude of the global pattern density and exposure dose. Furthermore, mask thermal distortion is found to be significantly",
            "group": 2703,
            "name": "10.1.1.129.9628",
            "keyword": "",
            "title": "Modeling of Mask Thermal Distortion during Optical Lithography and Its Dependence on Pattern Density Distribution M.S. Thesis"
        },
        {
            "abstract": "In this arttcle, we propose a hybrid algorithm for scheduling non-preemptive, single operation jobs in a multiclass production environment. The objective is to minimize the sum of the total weighted tardiness and setup costs of the schedule. We believe the problem is NP hard, and we use an eficient suboptimal algo-rithm based on Lagrangaan relaxation and simulated annealing. Our algorithm works well for a variety of scheduling problems. 1",
            "group": 2704,
            "name": "10.1.1.129.9702",
            "keyword": "",
            "title": "A Hybrid Scheduling Algorithm for Multiclass Production Systems with Setup Times"
        },
        {
            "abstract": "For several years we have been employing a riskbased decision process to guide development and application of advanced technologies, and for research and technology portfolio planning. The process is supported by custom software, in which visualization plays an important role. During requirements gathering, visualization is used to help scrutinize the status (completeness, extent) of the information. During decision making based on the gathered information, visualization is used to help decisionmakers understand the space of options and their consequences. In this paper we summarize the visualization capabilities that we have employed, indicating when and how they have proven useful. 1.",
            "group": 2705,
            "name": "10.1.1.130.2265",
            "keyword": "",
            "title": "Experiences using Visualization Techniques to Present Requirements, Risks to Them, and Options for Risk Mitigation"
        },
        {
            "abstract": "Optimization heuristics have been pursued in recent years as a viable approach in cryptanalysis. Even in simple ciphers where brute force method is successful, use of these techniques demonstrates their potential application in attacks of complex ciphers. This paper establishes the applicability of a couple of optimization heuristics to cryptanalysis studies; one based on thermostatistical persistency applied to simulated annealing and the other one based on particle swarm principle. Though both methods lead to successful attacks, our improvised version of group swarm optimization yields better performance. As a vehicle of demonstration of our concept, we choose simple yet representative block ciphers such as computationally tractable versions of DES, for our studies.",
            "group": 2706,
            "name": "10.1.1.130.3287",
            "keyword": "CryptanalysisDESheuristic optimizationparticle swarm optimizationsimulated annealingsimplified",
            "title": "Cryptanalysis of Block Ciphers via Improvised Particle Swarm Optimization and Extended Simulated Annealing Techniques Abstract"
        },
        {
            "abstract": "Abstract \u2014 Mapping an application on Multiprocessor Systemon-Chip (MPSoC) is a crucial step in architecture exploration. The problem is to minimize optimization effort and application execution time. Simulated annealing is a versatile algorithm for hard optimization problems, such as task distribution on MPSoCs. We propose a new method of automatically selecting parameters for a modified simulated annealing algorithm to save optimization effort. The method determines a proper annealing schedule and transition probabilities for simulated annealing, which makes the algorithm scalable with respect to application and platform size. Applications are modeled as static acyclic task graphs which are mapped to an MPSoC. The parameter selection method is validated by extensive simulations with 50 and 300 node graphs from the Standard Graph Set. I.",
            "group": 2707,
            "name": "10.1.1.130.3345",
            "keyword": "",
            "title": "Parameterizing Simulated Annealing for Distributing Task Graphs on Multiprocessor SoCs&quot;, International Symposium on System-on-Chip 2006"
        },
        {
            "abstract": "This revision of the ICML 2007 proceedings article corrects an error in Sec. 3. Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers of filled rank positions cause heterogeneity in the data. We propose a mixture approach for clustering of heterogeneous rank data. Rankings of different lengths can be described and compared by means of a single probabilistic model. A maximum entropy approach avoids hidden assumptions about missing rank positions. Parameter estimators and an efficient EM algorithm for unsupervised inference are derived for the ranking mixture model. Experiments on both synthetic data and real-world data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process. 1.",
            "group": 2708,
            "name": "10.1.1.130.3610",
            "keyword": "",
            "title": "Cluster Analysis of Heterogeneous Rank Data"
        },
        {
            "abstract": "",
            "group": 2709,
            "name": "10.1.1.130.3688",
            "keyword": "evolutionary diffusionmutil-agent systememergent behaviorautonomy oriented computationoptimization Abbreviated titleEvolutionary Multi-agent Diffusion 1",
            "title": "Evolutionary Multi-agent Diffusion Approach to Optimization"
        },
        {
            "abstract": "Predicting the future behavior of tropical cyclones is a problem of great importance for the atmospheric science community with concrete applications. Researchers understand enough about modeling storm systems to predict their track, but forecasting their future intensity remains elusive. In the present study, we formulate tropical storm intensification prediction as a supervised data mining problem; the objective being to produce accurate early warnings with respect to changes in wind speed of a particular storm. We examine two alternative approaches to discover classification rules on current hurricane data: particle swarm optimization and class association rules. Particle swarm optimization employs a population based search method to optimize a rule quality function and discover patterns in the data. Classification with association rules finds sufficiently supported trends in the data and transforms this knowledge into sets of rules. We examine both approaches in detail, present our findings and discuss their impact.",
            "group": 2710,
            "name": "10.1.1.130.4321",
            "keyword": "",
            "title": "Alternative Data Mining Techniques for Predicting Tropical Cyclone Intensification"
        },
        {
            "abstract": "Head of the school for Engineering and Computer Science",
            "group": 2711,
            "name": "10.1.1.130.6306",
            "keyword": "",
            "title": "A structural EM algorithm for phylogenetic inference"
        },
        {
            "abstract": "Sensor-actuator networks (SANs) are a new approach for the physically-based animation of objects. The user supplies the configurat\u00edon of a mechanical system that h\u00e1s been augmented with simple sensors and actuators. It is then possible to automatically discover many possible modes of locomotion for the given object. The SANs providing the control for these modes of locomotion are simple in structure and produce robust control. A SAN consists of a small non-linear network of weighted connections between sensors and actuators. A stochastic procedure for finding and then improving suitable SANs is given. Ten different creatures controlled by this method are presented.",
            "group": 2712,
            "name": "10.1.1.130.6962",
            "keyword": "Realism- animation1.6.3 [Simulation and Modeling]- Applications",
            "title": "Sensor-Actuator Networks"
        },
        {
            "abstract": "Lead me from the unreal to the real. Lead me from the darkness to light. Lead me from death to immortality- an invocation from the Upanishads I would like to dedicate this thesis to my parents and my brother for having supported me through my education and encouraging me to strive for excellence. i ACKNOWLEDGMENTS It all started the day the syllable research group met with Vapnik during the WS\u201997 work-",
            "group": 2713,
            "name": "10.1.1.130.7898",
            "keyword": "",
            "title": "Support Vector Machines for Speech recognition"
        },
        {
            "abstract": "Abstract \u2014 We present a simulation-based algorithm called \u201cSimulated Annealing Multiplicative Weights \u201d (SAMW) for solving large finitehorizon stochastic dynamic programming problems. At each iteration of the algorithm, a probability distribution over candidate policies is updated by a simple multiplicative weight rule, and with proper annealing of a control parameter, the generated sequence of distributions converges to a distribution concentrated only on the best policies. The algorithm is \u201casymptotically efficient, \u201d in the sense that for the goal of estimating the value of an optimal policy, a provably convergent finite-time upper bound for the sample mean is obtained. Index Terms \u2014 stochastic dynamic programming, Markov decision processes, simulation, learning algorithms, simulated annealing I.",
            "group": 2714,
            "name": "10.1.1.130.8242",
            "keyword": "",
            "title": "An asymptotically efficient simulation-based algorithm for finite horizon stochastic dynamic programming"
        },
        {
            "abstract": "",
            "group": 2715,
            "name": "10.1.1.130.8389",
            "keyword": "",
            "title": "Temperature-Aware Voltage Islands Architecting in System-on-Chip Design"
        },
        {
            "abstract": "Mission management, including on-board replanning, is a task that can benefit significantly from automation. On-board replanning is required to respond to departures from nominal plan execution that result from imperfect knowledge of and temporal variability in the mission environment Automation is particularly valuable in the high-risk Nap-Of-Earth (NOE) environment, where crew warkloads for tasks of immediate concern (such as obstacle avoidance and hat engagement) can be quite high. In this situation, an on-board, automated planning advisor can continuously monitor resource usage (i.e., fuel, ordinance, other expendables), assess risk along the cumnt mission plan and also suggest alternative plans that might better satisfy time, resource, and survivability constraints. The planning advisor requires an on-board environmental database of terrain, threat locations, and winds that is loaded preflight and updated during the mission by vehicle sensors and received communications. Also required is a mission database (also updated) indicating alternative objectives or bases, their locations, their relative values, and time of arrival constraints to ensure coordination with other force elements. Access to current vehicle",
            "group": 2716,
            "name": "10.1.1.130.8686",
            "keyword": "Mission management tasks can be d",
            "title": "Development and Demonstration of an On-Board Mission Planner for Helicopters&quot;, CSDL-R-2056"
        },
        {
            "abstract": "The most significant impediment for protein structure prediction is the inadequacy of conformation space search methods. Conformation space is too large and the energy landscape too rugged for existing search methods to consistently find near-optimal minima. To alleviate this problem, we present model-based search, a novel conformation space search method. Model-based search uses highly accurate information obtained during search to build an approximate, partial model of the energy landscape. Model-based search aggregates information in the model as it progresses, and in turn uses this information to guide exploration towards regions most likely to contain a near-optimal minimum. We validate our method by predicting the structure of 32 proteins, ranging in length from 49 to 213 amino acids. Our results demonstrate that model-based search is more effective at finding lowenergy conformations in high-dimensional conformation spaces than existing search methods. The lower-energy conformations found by our method also correspond to higher-accuracy structure predictions. 1",
            "group": 2717,
            "name": "10.1.1.130.8996",
            "keyword": "",
            "title": "Abstract Guiding Conformation Space Search Towards Biologically Relevant Regions Using All-Atom Energy Evaluations"
        },
        {
            "abstract": " We present a state-of-the-art survey of parallel meta-heuristic developments and results, discuss general design and implementation principles that apply to most meta-heuristic classes, instantiate these principles for the three meta-heuristic classes currently most extensively used - genetic methods, simulated annealing, and tabu search, and identify a number of trends and promising research directions.",
            "group": 2718,
            "name": "10.1.1.131.322",
            "keyword": "Key wordsParallel computationParallelization strategiesMeta-heuristicsGenetic methodsSimulated annealingTabu searchCo-operative search. 1 2",
            "title": "Parallel Strategies for Meta-heuristics"
        },
        {
            "abstract": " ",
            "group": 2719,
            "name": "10.1.1.131.1895",
            "keyword": "Contents",
            "title": "A distributed approach to memetic problem solving"
        },
        {
            "abstract": "The outlier detection problem has important applications in the eld of fraud detection, netw ork robustness analysis, and intrusion detection. Most suc h applications are high dimensional domains in whic hthe data can con tain hundreds of dimensions. Many recen t algorithms use concepts of pro ximity in order to nd outliers based on their relationship to the rest of the data. Ho w ever, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective ofproximity-based de nitions. Consequently, for high dimensional data, the notion of nding meaningful outliers becomes substantially more complex and non-obvious. In this paper, w e discuss new techniques for outlier detection whic h nd the outliers by studying the behavior of projections from the data set. 1.",
            "group": 2720,
            "name": "10.1.1.131.2385",
            "keyword": "",
            "title": "Outlier detection for high dimensional data"
        },
        {
            "abstract": "Within-die process variation causes individual cores in a Chip Multiprocessor (CMP) to differ substantially in both static power consumed and maximum frequency supported. In this environment, ignoring variation effects when scheduling applications or when managing power with Dynamic Voltage and Frequency Scaling (DVFS) is suboptimal. This paper proposes variation-aware algorithms for application scheduling and power management. One such power management algorithm, called LinOpt, uses linear programming to find the best voltage and frequency levels for each of the cores in the CMP \u2014 maximizing throughput at a given power budget. In a 20core CMP, the combination of variation-aware application scheduling and LinOpt increases the average throughput by 12\u201317 % and reduces the average ED 2 by 30\u201338 %  \u2014 all relative to using variation-aware scheduling together with a simple extension to Intel\u2019s Foxton power management algorithm. ",
            "group": 2721,
            "name": "10.1.1.131.3300",
            "keyword": "",
            "title": "Variation-Aware Application Scheduling and Power Management for Chip Multiprocessors "
        },
        {
            "abstract": "ABSTRACT A method is presented to determine the three-dimensional positions of immuno-labeled gold markers from tilted electron micrograph recordings by using image processing techniques. The method consists ofthree basicmodules: localization ofthe markers in the recordings, estimation ofthe motion parameters, and matching corresponding markers between theviews. Localization consists of a segmentation step based on edge detection and region growing. It also allows for the separation of (visually) aggregated markers. Initial estimates for the motion parameters are obtained from a small number of user-indicated correspondences.A matching algorithm based on simulated annealing is used to find corresponding markers. With the resulting mapping, the motion parameters are updated and used in anew matching step, etc.Oncethe parameters are stable, the marker depths are retrieved. The developed method has been applied to semithin resin sections of A431 cells labeled for DNA and detected by silver-enhanced ultrasmall gold particles. It represents a reliable method to analyze the three-dimensional distribution of gold markers in electron microscope samples.",
            "group": 2722,
            "name": "10.1.1.131.4824",
            "keyword": "",
            "title": "Three-Dimensional Localization of Immunogold Markers Using Two Tilted Electron Microscope Recordings"
        },
        {
            "abstract": "The previous research on cluster-based servers has focused on homogeneous systems. However, real-life clusters are almost invariably heterogeneous in terms of the performance, capacity, and power consumption of their hardware components. In this paper, we argue that designing efficient servers for heterogeneous clusters requires defining an efficiency metric, modeling the different types of nodes with respect to the metric, and searching for request distributions that optimize the metric. To concretely illustrate this process, we design a cooperative Web server for a heterogeneous cluster that uses modeling and optimization to minimize the energy consumed per request. Our experimental results for a cluster comprised of traditional and blade nodes show that our server can consume 42 % less energy than an energyoblivious server, with only a negligible loss in throughput. The results also show that our server conserves 45 % more energy than an energy-conscious server that was previously proposed for homogeneous clusters.",
            "group": 2723,
            "name": "10.1.1.131.6221",
            "keyword": "\u2217 This research has been supported by NSF under grants",
            "title": "Energy Conservation in Heterogeneous Server Clusters "
        },
        {
            "abstract": "The rest of this paper is organized as follows. Section 2 describes a particular type of probabilistic inference, namely the MAP problem, which is the focus of this work. Section 3 presents a discrete neural network model for MAP. Sections 4 and 5 extend the discrete model to simulated annealing and continuous models,",
            "group": 2724,
            "name": "10.1.1.131.7199",
            "keyword": "",
            "title": "A Neural Network Approach to MAP in Belief Networks"
        },
        {
            "abstract": "Droplet-based microfluidic biochips have recently gained much attention and are expected to revolutionize the biological laboratory procedure. As biochips are adopted for the complex procedures in molecular biology, its complexity is expected to increase due to the need of multiple and concurrent assays on a chip. In this paper, we formulate the placement problem of digital microfluidic biochips with a tree-based topological representation, called T-tree. To the best knowledge of the authors, this is the first work that adopts a topological representation to solve the placement problem of digital microfluidic biochips. Experimental results demonstrate that our approach is much more efficient and effective, compared with the previous unified synthesis and placement framework.",
            "group": 2725,
            "name": "10.1.1.131.7857",
            "keyword": "B.7.2 [Integrated CircuitsDesign Aids General Terms AlgorithmPerformanceDesign Keywords Microfluidicsbiochipplacementfloorplanning",
            "title": "Placement of digital microfluidic biochips using the T-tree formulation"
        },
        {
            "abstract": "",
            "group": 2726,
            "name": "10.1.1.131.8592",
            "keyword": "",
            "title": " Introduction to Learning Bayesian Networks from Data"
        },
        {
            "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9 % over state-of-the-art systems on two established information extraction tasks. 1",
            "group": 2727,
            "name": "10.1.1.131.8904",
            "keyword": "",
            "title": "Incorporating non-local information into information extraction systems by gibbs sampling"
        },
        {
            "abstract": "Floorplanning/placement allocates a set of modules into a chip so that no two modules overlap and some specified objective is optimized. To facilitate floorplanning/placement, we need to develop an efficient and effective representation to model the geometric relationship among modules. In this paper, we present a P-admissible representation, called corner sequence (CS), for nonslicing floorplans. CS consists of two tuples that denote the packing sequence of modules and the corners to which the modules are placed. CS is very effective and simple for implementation. Also, it supports incremental update during packing. In particular, it induces a generic worst case linear-time packing scheme that can also be applied to other representations. Experimental results show that CS achieves very promising results for a set of commonly used MCNC benchmark circuits.",
            "group": 2728,
            "name": "10.1.1.131.9828",
            "keyword": "",
            "title": "Corner Sequence -- A P-Admissible Floorplan Representation With a Worst Case Linear-Time Packing Scheme"
        },
        {
            "abstract": " ",
            "group": 2729,
            "name": "10.1.1.131.9861",
            "keyword": "",
            "title": "Probabilistic Model-Based Clustering of Complex Data"
        },
        {
            "abstract": "Simulated annealing has been shown to be a powerful stochastic method of tackling hard combinatorial optimisation problems, but it demands a vast amount of computation time to arrive at a good approximate solution. A lot of research has been done on the cooling schedule of simulated annealing to speed up its convergence, but only limited attention has been paid to the impact of the neighbourhood size on the performance of simulated annealing. It has been shown that the performance of simulated annealing can be improved by adopting a suitable neighbourhood size. However, previous studies usually assumed that the neighbourhood size was fixed during search after decided at the beginning. This paper presents a simulated annealing algorithm with a dynamic neighbourhood size which depends on the current \u201ctemperature \u201d value during search. A method of dynamically deciding the neighbourhood size by approximating a continuous probability distribution is given. Four continuous probability distributions are used in our experiments to generate neighbourhood sizes dynamically, and the results are compared.",
            "group": 2730,
            "name": "10.1.1.132.637",
            "keyword": "Combinatorial OptimisationNeighbourhood SizeTraveling Salesman Problem",
            "title": "Dynamic neighbourhood size in simulated annealing"
        },
        {
            "abstract": "Parameter estimation is a critical problem in modeling biological pathways. It is difficult because of the large number of parameters to be estimated and the limited experimental data available. In this paper, we propose a decompositional approach to parameter estimation. It exploits the structure of a large pathway model to break it into smaller components, whose parameters can then be estimated independently. This leads to significant improvements in computational efficiency. We present our approach in the context of Hybrid Functional Petri Net modeling and evolutionary search for parameter value estimation. However, the approach can be easily extended to other modeling frameworks and is independent of the search method used. We have tested our approach on a detailed model of the Akt and MAPK pathways with two known and one hypothesized crosstalk mechanisms. The entire model contains 84 unknown parameters. Our simulation results exhibit good correlation with experimental data, and they yield positive evidence in support of the hypothesized crosstalk between the two pathways. ",
            "group": 2731,
            "name": "10.1.1.132.1139",
            "keyword": "",
            "title": "  A Decompositional Approach to Parameter Estimation in Pathway Modeling: A Case Study of the Akt and MAPK Pathways and Their Crosstalk"
        },
        {
            "abstract": "Even the recognition of an individual whom we see every day is only possible as the result of an abstract idea of him formed by generalisation from his appearances in the past- James G. Frazer (in Malinowski & Bronislaw: Argonauts of the Western Pacific, 1922) The fundamental question studied in this thesis is how to evaluate and analyse supervised learning algorithms and classifiers. As a first step, we analyse current evaluation methods. Each method is described and categorised according to a number of properties. One conclusion of the analysis is that performance is often only measured in terms of accuracy, e.g., through cross-validation tests. However, some researchers have questioned the validity of using accuracy as the only performance metric. Also, the number of instances available for evaluation is usually very limited. In order to deal with these issues, measure functions have been suggested as a promising approach. However, a limitation of current measure functions is that they can only handle two-dimensional instance spaces. We present the design and implementation of a generalised multi-dimensional",
            "group": 2732,
            "name": "10.1.1.132.2903",
            "keyword": "",
            "title": "Contact information:"
        },
        {
            "abstract": "The problem of detecting hidden communities in large-scale networks has a long history in traffic analysis from secret societies evading detection to spy agencies datamining call record information looking for criminals. Graph theoretic partitioning is a well studied topic in both computer science and sociology. However, these methods are based on discovering communities by the narrow intuition of minimal edge cut-sets additionally we found these methods to have poor tolerance to random addition and removal of vertices and edges from the graph with an exponential rise in community mis-detection error. Recent work on partitioning techniques in the complex networks literature offer better tools to the covert community hunter. Based on a broader intuition of maximizing modularity, these methods can accurately delineate community boundaries, while tolerating random errors fairly well. While much attention has been paid to their role in increasing our knowledge about network structure and dynamics, lesser attention has been paid to the privacy and anonymity aspects of revealing social structure in large technological networks. We make a start on this problem, by",
            "group": 2733,
            "name": "10.1.1.132.3655",
            "keyword": "",
            "title": "On a dynamic topology of covert groups"
        },
        {
            "abstract": "As modern VLSI designs have become larger and more complicated, the computational re-quirements for design automation tools have also increased. As a result, the parallelization of these tools is of great importance. One of the more computationally intensive parts of the en-tire VLSI design process is the placement process. Simulated-annealing-based approaches have been the most popular and effective methods for cell placement. In this thesis, parallelization approaches to simulated-annealing-based standard cell placement are presented. In this work, four parallel algorithms have been investigated, with two that provide scalable behavior as well as acceptable quality. The first is the parallel moves approach based on work by Kim [1, 2]. The second algorithm is a multiple Markov chains approach that gives nearly linear speedups with very little loss of quality. This approach is suitable for small scale mul-tiprocessors and for circuits that are small enough to fit in the memory of a single node. The next algorithm is known as speculative computation and is not as effective. The final algorithm addresses the memory scalability problems by partitioning the circuit across the nodes. This circuit-partitioned approach provides speedups to larger numbers of processors with little loss of quality. All of the algorithms have been implemented using the ProperCAD II environment [3],",
            "group": 2734,
            "name": "10.1.1.132.3785",
            "keyword": "",
            "title": "PARALLEL ALGORITHMS FOR STANDARD CELL PLACEMENT USING SIMULATED ANNEALING"
        },
        {
            "abstract": "Abstract- In this paper we report the application and evaluation of the simulated annealing (SA) optimization method in parameter estimation for vapor-liquid equilibrium (VLE) modeling. We tested this optimization method using the classical least squares and error-in-variable approaches. The reliability and efficiency of the data-fitting procedure are also considered using different values for algorithm parameters of the SA method. Our results indicate that this method, when properly implemented, is a robust procedure for nonlinear parameter estimation in thermodynamic models. However, in difficult problems it still can converge to local optimums of the objective function. Keywords: Vapor-liquid equilibrium; Simulated annealing; Nonlinear parameter estimation; Error-in-variable method; Global optimization.",
            "group": 2735,
            "name": "10.1.1.132.5673",
            "keyword": "",
            "title": "THE PERFORMANCE OF SIMULATED ANNEALING IN PARAMETER ESTIMATION FOR VAPOR-LIQUID EQUILIBRIUM MODELING"
        },
        {
            "abstract": " ",
            "group": 2736,
            "name": "10.1.1.132.6925",
            "keyword": "",
            "title": " \t Word sense disambiguation using CIDE+"
        },
        {
            "abstract": "Tag clouds provide an aggregate of tag-usage statistics. They are typically sent as in-line HTML to browsers. However, display mechanisms suited for ordinary text are not ideal for tags, because font sizes may vary widely on a line. As well, the typical layout does not account for relationships that may be known between tags. This paper presents models and algorithms to improve the display of tag clouds that consist of in-line HTML, as well as algorithms that use nested tables to achieve a more general 2-dimensional layout in which tag relationships are considered. The first algorithms leverage prior work in typesetting and rectangle packing, whereas the second group of algorithms leverage prior work in Electronic Design Automation. Experiments show our algorithms can be efficiently implemented and perform well.",
            "group": 2737,
            "name": "10.1.1.132.7504",
            "keyword": "TagsVisualizationLayout",
            "title": "General Terms Algorithms, Experimentation, Measurements"
        },
        {
            "abstract": "We present a new approach to clustering, based on the physical properties of an inhomogeneous ferromagnet. No assumption is made regarding the underlying distribu-tion of the data. We assign a Potts spin to each data point and introduce an interaction between neighboring points, whose strength is a decreasing function of the distance between the neighbors. This magnetic system exhibits three phases. At very low tem-peratures it is completely ordered; i.e. all spins are aligned. At very high temperatures the system does not exhibit any ordering and in an intermediate regime clusters of relatively strongly coupled spins become ordered, whereas different clusters remain un-correlated. This intermediate phase is identified by a jump in the order parameters. The spin-spin correlation function is used to partition the spins and the corresponding data points into clusters. We demonstrate on three synthetic and three real data sets how the method works. Detailed comparison to the performance of other techniques clearly indicates the relative success of our method. 1 1",
            "group": 2738,
            "name": "10.1.1.132.7508",
            "keyword": "",
            "title": "Data clustering using a model granular magnet"
        },
        {
            "abstract": "",
            "group": 2739,
            "name": "10.1.1.132.8996",
            "keyword": "",
            "title": "  Declarative constraint modelling and specification-level reasoning"
        },
        {
            "abstract": "Given a graph G = (V, E), a clustering of G is a partitioning of V that induces a set of subgraphs C1, C2,..., Ck. These subgraphs are known as clusters. A good clustering induces clusters that are both dense and sparsely inter-connected, and represents a natural grouping of the vertices into highly interrelated sets. We present",
            "group": 2740,
            "name": "10.1.1.133.375",
            "keyword": "",
            "title": "An Efficient Cost-Based Graph Clustering Algorithm"
        },
        {
            "abstract": "Summary. The prediction of protein native conformations is still a big challenge in science, although a strong research activity has been carried out on this topic in the last decades. In this chapter we focus on ab-initio computational methods for protein fold predictions that do not rely heavily on comparisons with known protein structures and hence appear to be the most promising methods for determining conformations not yet been observed experimentally. To identify main trends in the research concerning protein fold predictions, we briefly review several ab-initio methods, including a recent topological approach that models the protein conformation as a tube having maximum thickness without any self-contacts. This representation leads to a constrained global optimization problem. We introduce a modification in the tube model to increase the compactness of the computed conformations, and present results of computational experiments devoted to simulating \u03b1-helices and all-\u03b1 proteins. A Metropolis Monte Carlo Simulated Annealing algorithm is used to search the protein conformational space.",
            "group": 2741,
            "name": "10.1.1.133.1943",
            "keyword": "Key wordsProtein fold predictionAb-initio methodsNative state topologyTube thicknessGlobal optimizationSimulated annealing",
            "title": "Computational Methods for Protein Fold Prediction: an Ab-initio Topological Approach"
        },
        {
            "abstract": "Ribonucleic Acid (RNA) plays fundamental roles in cellular processes and its structure is directly related to its functions. This paper describes and presents a novel algorithm for RNA secondary structure prediction based on Simulated Annealing (SA). SA is known to be effective in solving many different types of minimization problems and for finding the global minima in the solution space. Based on free energy minimization techniques, this permutation-based SA algorithm heuristically searches for the structure with a free energy value close to the minimum free energy \u2206G for that strand, within given constraints. A detailed study of the convergence behavior of the algorithm is conducted and various cooling schedules are investigated. An evaluation of the performance of the new algorithm in terms of prediction accuracy is made via comparison with the dynamic programming algorithm mfold for eight individual known structures from three RNA classes (5S rRNA, Group I intron 16S rRNA and 16S rRNA). The significant contribution of this algorithm is in showing comparable results with the most common dynamic programming prediction application mfold and surpassing results from an Evolutionary Algorithm (EA).",
            "group": 2742,
            "name": "10.1.1.133.3235",
            "keyword": "",
            "title": "SARNA-Predict: A Simulated Annealing Algorithm for RNA Secondary Structure Prediction"
        },
        {
            "abstract": "",
            "group": 2743,
            "name": "10.1.1.133.3678",
            "keyword": "",
            "title": "GRID RESOURCE MANAGEMENT  -- State of the art and future trends"
        },
        {
            "abstract": "First and foremost I wish to thank my advisor, Dr. Jose Principe. He allowed me the freedom to explore, while at the same time provided invaluable insight without which this dissertation would not have been possible. I also wish to thank the members of my committee, Dr. John Harris, Dr. Christiana Leonard, Dr. Joseph Wilson, and Dr. William Edmonson, for their insightful comments which improved the quality of this dissertation. I also wish to thank my wife Didem and my son Tugra for their patience and support during the long nights I have been working. iii",
            "group": 2744,
            "name": "10.1.1.133.5207",
            "keyword": "ACKNOWLEDGEMENTS............................................... iii ABSTRACT........................",
            "title": "TABLE OF CONTENTS"
        },
        {
            "abstract": "Field-Programmable Gate Arrays (FPGAs) are digital integrated circuits (ICs) that contain configurable logic and interconnect to provide a means for fast prototyping and also for a cost-effective chip design. The innovative development of FPGAs spurred the invention of a new field in which many different hardware algorithms could execute on a single device [16]. Efficient Computer Aided Design (CAD) tools are required to compile hardware descriptions into bit-stream files that are used to configure the target FPGA to implement the desired circuits. Currently, the compile time, which is dominated by placement and routing phases, can easily take hours or even days to complete for current large (over 8-million gate) FPGAs. Within the next few years the logic capacity of FPGAs will tend to increase dramatically (up to 40-million gates) that prohibitively long compile times may adversely affect instant manufacturability of FPGAs and become intolerable to users seeking very high speed compile. This paper presents several constructive and iterative improvement placement based heuristics that significantly reduce the amount of computation time required to achieve high-quality placements, compared with VPR [9], [8]. Cluster Seed, GRASP and Partitioning based approaches prove to be excellent candidates to generate good starting points in negligible amounts of time. The effectiveness of these constructive based methods are tested by implementing several local search based methods. Meta-heuristics in the form of Tabu Search and a hybrid Simulated Annealing with short-term memory are further implemented to explore and exploit the solution space effectively.",
            "group": 2745,
            "name": "10.1.1.133.5357",
            "keyword": "GRASPCluster SeedPartitioningSimple Local SearchTabu SearchSimulated Annealing with Memory",
            "title": "A Comparison of Heuristics for FPGA Placement"
        },
        {
            "abstract": "Abstract. The development and use of content-based retrieval techniques for 3-D models is a relatively new departure in multimedia retrieval. We have extended our existing multimedia museum information system to support content-, metadata- and concept-based retrieval of 3-D models of museum artifacts and in this paper we describe a \u201cclassifier agent \u201d to automatically assign associations between 3-D artifacts and concepts and metadata stored in a domain ontology. The context of the classifier agent is described, together with an overview of its architecture. Selecting appropriate parameters for the agent is an important activity and a comparison is made between manually selected parameters and the results of an automatic technique to determine \u201coptimal \u201d settings. 1",
            "group": 2746,
            "name": "10.1.1.133.5588",
            "keyword": "",
            "title": "Electronics and Computer Science, University of Southampton,"
        },
        {
            "abstract": "ABSTRACT The field of computational biology has been revolutionized by recent advances in genomics. The completion of a number of genome projects, including that of the human genome, has paved the way toward a variety of challenges and opportunities in bioinformatics and biological systems engineering. One of the first challenges has been the determination of the structures of proteins encoded by the individual genes. This problem, which represents the progression from sequence to structure (genomics to structural genomics), has been widely known as the structure-prediction-in-protein-folding problem. We present the development and application of ASTRO-FOLD, a novel and complete approach for the ab initio prediction of protein structures given only the amino acid sequences of the proteins. The approach exhibits many novel components and the merits of its application are examined for a suite of protein systems, including a number of targets from several critical-assessment-ofstructure-prediction experiments.",
            "group": 2747,
            "name": "10.1.1.133.5790",
            "keyword": "",
            "title": "Floudas. ASTRO-FOLD: A Combinatorial and Global Optimization Framework for Ab Initio Prediction of Three-Dimensional Structures of Proteins from the Amino Acid Sequence"
        },
        {
            "abstract": "Despite the increasing availability of parallel platforms, their wide-spread use in the solution of large computing problems remains restricted to a fairly narrow set of applications. This is due in part to the difficulty of parallel application development which is itself largely the result of a lack of sophisticated environments for parallel application development. Further, though the number of parallel platforms is increasing, the convergence of parallel architectures and operating systems does not appear to be similarly increasing. Given that most development environments are targeted towards a particular architecture, it is difficult to amortize development costs over a wide base of installed machines. In this research, we address these problems through the application of two significant technologies, object-oriented design techniques and the actor model of concurrent computation. Our approach is manifested in the ProperCAD II library, a C++ object library supporting actor concurrency on microprocessor-based parallel architectures and appropriate for applications demonstrating medium-grain parallelism. The development of the library has been driven in general by the requirements of large, unstructured problems, which have not proven amenable to techniques",
            "group": 2748,
            "name": "10.1.1.133.6733",
            "keyword": "PortableParallelObject-Oriented Programming",
            "title": "Approved for Public Release. Distribution Unlimited. ProperCAD II: A Run-Time Library for"
        },
        {
            "abstract": "We introduce a new technique for counting models of Boolean satisfiability problems. Our approach incorporates information obtained from sampling the solution space. Unlike previous approaches, our method does not require uniform or near-uniform samples. It instead converts local search sampling without any guarantees into very good bounds on the model count with guarantees. We give a formal analysis and provide experimental results showing the effectiveness of our approach. 1",
            "group": 2749,
            "name": "10.1.1.133.6777",
            "keyword": "",
            "title": "From sampling to model counting"
        },
        {
            "abstract": "On behalf of:",
            "group": 2750,
            "name": "10.1.1.133.6832",
            "keyword": "",
            "title": "KAMA: A Temperature-Driven Model of Mate-Choice using Dynamically Evolving Representations"
        },
        {
            "abstract": "We study adaptive approaches for the symmetric Euclidean Traveling Salesman problem. Specifically, we consider 2-opt, simulated annealing, and genetic algorithms with cycle crossover, ordered crossover, and edge recombination. Our algorithms are implemented using GALib. We note that difficulties associated with the TSP problem are representative of complexities in solving combinatorial optimization problems with adaptive methods in general.",
            "group": 2751,
            "name": "10.1.1.133.7106",
            "keyword": "",
            "title": "Pitfalls with Adaptive Methods for Combinatorial Optimization: The Traveling Salesman Problem -- A Case Study"
        },
        {
            "abstract": "The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has led to an increase in the deployment of complex enterprise applications. These applications typically run on Web Application Servers, which assume the burden of managing many tasks, such as concurrency, memory management, database access, etc., required by these applications. The performance of an Application Server depends heavily on appropriate configuration. Configuration is a difficult and error-prone task due to the large number of configuration parameters and complex interactions between them. We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem. We propose a Smart Hill-Climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling (LHS). The algorithm is efficient in both searching and random sampling. It consists of estimating a local function, and then, hill-climbing in the steepest descent direction. The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling. We have carried out extensive experiments with an online brokerage application running in a WebSphere environment. Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods. Categories and Subject Descriptors",
            "group": 2752,
            "name": "10.1.1.133.8169",
            "keyword": "Automatic TuningImportance SamplingSimulated AnnealingGradient Method Copyright is held by the author/owner(s",
            "title": "A smart hill-climbing algorithm for application server configuration"
        },
        {
            "abstract": "Abstract: Protein-protein interactions are important for biochemical processes in biological systems. The 3D structure of the macromolecular complex resulting from the proteinprotein association is a very useful source to understand its specific functions. This work focuses on computational study for protein-protein docking, where the individually crystallized structures of interacting proteins are treated as rigid, and the conformational space generated by the two interacting proteins is explored extensively. The energy function consists of intermolecular electrostatic potential, desolvation free energy represented by empirical contact potential, and simple repulsive energy terms. The conformational space is six dimensional, represented by translational vectors and rotational angles formed between two interacting proteins. The conformational sampling is carried out by the search algorithms such as simulated annealing (SA), conformational space annealing (CSA), and CSA combined with SA simulations (combined CSA/SA). Benchmark tests are performed on a set of 18 protein-protein complexes selected from various protein families to examine feasibility of these search methods coupled with the energy function above for protein docking study. Keywords: Global optimization; Protein-protein docking; Conformational space annealing; Simulated annealing; Combined CSA/SA; FastContact.",
            "group": 2753,
            "name": "10.1.1.133.8308",
            "keyword": "",
            "title": "Computational Study for Protein-Protein Docking Using Global Optimization and Empirical Potentials"
        },
        {
            "abstract": "A new method of parameter estimation for an artificial neural network inference system based on a logical interpretation of fuzzy if-then rules (ANBLIR) is presented. The novelty of the learning algorithm consists in the application of a deterministic annealing method integrated with \u03b5-insensitive learning. In order to decrease the computational burden of the learning procedure, a deterministic annealing method with a \u201cfreezing \u201d phase and \u03b5-insensitive learning by solving a system of linear inequalities are applied. This method yields an improved neuro-fuzzy modeling quality in the sense of an increase in the generalization ability and robustness to outliers. To show the advantages of the proposed algorithm, two examples of its application concerning benchmark problems of identification and prediction are considered.",
            "group": 2754,
            "name": "10.1.1.133.9862",
            "keyword": "fuzzy systemsneural networksneuro-fuzzy systemsrules extractiondeterministic annealing\u03b5-insensitive",
            "title": "EXTRACTION OF FUZZY RULES USING DETERMINISTIC ANNEALING INTEGRATED WITH \u03b5-INSENSITIVE LEARNING"
        },
        {
            "abstract": "Partitioning a large set of objects into homogeneous clusters is a fundamental operation in data mining. The k-means algorithm is best suited for implementing this operation because of its efficiency in clustering large data sets. However, working only on numeric values limits its use in data mining because data sets in data mining often contain categorical values. In this paper we present an algorithm, called k-modes, to extend the k-means paradigm to categorical domains. We introduce new dissimilarity measures to deal with categorical objects, replace means of clusters with modes, and use a frequency based method to update modes in the clustering process to minimise the clustering cost function. Tested with the well known soybean disease data set the algorithm has demonstrated a very good classification performance. Experiments on a very large health insurance data set consisting of half a million records and 34 categorical attributes show that the algorithm is scalable in terms of both the number of clusters and the number of records. 1",
            "group": 2755,
            "name": "10.1.1.134.83",
            "keyword": "",
            "title": "A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining"
        },
        {
            "abstract": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (01, 02, 04), the Lucent Technologies PAC/EPAC/MPAC, the Dolby1 AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.",
            "group": 2756,
            "name": "10.1.1.134.117",
            "keyword": "",
            "title": "Perceptual Coding of Digital Audio"
        },
        {
            "abstract": "Abstract \u2014 Computing has recently reached an inflection point with the introduction of multi-core processors. On-chip threadlevel parallelism is doubling approximately every other year. Concurrency lends itself naturally to allowing a program to trade performance for power savings by regulating the number of active cores, however in several domains users are unwilling to sacrifice performance to save power. We present a prediction model for identifying energy-efficient operating points of concurrency in well-tuned multithreaded scientific applications, and a runtime system which uses live program analysis to optimize applications dynamically. We describe a dynamic, phase-aware performance prediction model that combines multivariate regression techniques with runtime analysis of data collected from hardware event counters to locate optimal operating points of concurrency. Using our model, we develop a prediction-driven, phase-aware runtime optimization scheme that throttles concurrency so that power consumption can be reduced and performance can be set at the knee of the scalability curve of each program phase. The use of prediction reduces the overhead of searching the optimization space while achieving near-optimal performance and power savings. A thorough evaluation of our approach shows a reduction in power consumption of 10.8 % simultaneous with an improvement in performance of 17.9%, resulting in energy savings of 26.7%. Index Terms \u2014 Modeling and prediction, Application-aware adaptation, Energy-aware systems",
            "group": 2757,
            "name": "10.1.1.134.183",
            "keyword": "",
            "title": "Prediction-based Power-Performance Adaptation of Multithreaded Scientific Codes"
        },
        {
            "abstract": "Matsuka and Corter (2003b) presented evidence that people tend to utilize only the minimally necessary information for classification tasks. This approach for categorization was efficient and valid for the stimulus set used in the experiment, but might be considered a statistically or mathematically nonnormative approach. In the present paper, I hypothesized human category learning processes are biased toward simpler representation and/or conception rather than complex but normative ones. In particular, a few variants of \u201cbiased\u201d learning algorithms are introduced and applied to Matsuka and Corter\u2019s stochastic learning algorithm (2003a, 2004). The result of a simulation study showed that the biased learning models account for empirical results successfully.",
            "group": 2758,
            "name": "10.1.1.134.304",
            "keyword": "",
            "title": "Biased stochastic learning in computational model of category learning"
        },
        {
            "abstract": "informs \u00ae doi 10.1287/opre.1060.0367 \u00a9 2007 INFORMS Model reference adaptive search (MRAS) for solving global optimization problems works with a parameterized probabilistic model on the solution space and generates at each iteration a group of candidate solutions. These candidate solutions are then used to update the parameters associated with the probabilistic model in such a way that the future search will be biased toward the region containing high-quality solutions. The parameter updating procedure in MRAS is guided by a sequence of implicit probabilistic models we call reference models. We provide a particular algorithm instantiation of the MRAS method, where the sequence of reference models can be viewed as the generalized probability distribution models for estimation of distribution algorithms (EDAs) with proportional selection scheme. In addition, we show that the model reference framework can also be used to describe the recently proposed cross-entropy (CE) method for optimization and to study its properties. Hence, this paper can also be seen as a study on the effectiveness of combining CE and EDAs. We prove global convergence of the proposed algorithm in both continuous and combinatorial domains, and we carry out numerical studies to illustrate the performance of the algorithm.",
            "group": 2759,
            "name": "10.1.1.134.1980",
            "keyword": "",
            "title": "A Model Reference Adaptive Search Method for Global Optimization"
        },
        {
            "abstract": "",
            "group": 2760,
            "name": "10.1.1.134.2471",
            "keyword": "",
            "title": "Recent Directions in Netlist Partitioning: A Survey  "
        },
        {
            "abstract": "Adoption of advanced automated SE (ASE) tools would be favored if a business case could be made that these tools are more valuable than alternate methods. In theory, software prediction models can be used to make that case. In practice, this is complicated by the \u201dlocal tuning \u201d problem. Normally, predictors for software effort and defects and threat use local data to tune their predictions. Such local tuning data is often unavailable. This paper shows that assessing the relative merits of different SE methods need not require precise local tunings. STAR1 is a simulated annealer plus a Bayesian post-processor that explores the space of possible local tunings within software prediction models. STAR1 ranks project decisions by their effects on effort and defects and threats. In experiments with two NASA systems, STAR1 found that ASE tools were necessary to minimize effort / defect / threats.",
            "group": 2761,
            "name": "10.1.1.134.2976",
            "keyword": "Categories and Subject Descriptors I.6 [LearningMachine LearningD.2.8 [Software EngineeringMetrics\u2014product metricsprocess metrics General Terms EconomicsAlgorithmsManagement Keywords COCOMOCOQUALMOsimulated annealingBayes",
            "title": "The Business Case for Automated Software Engineering ABSTRACT"
        },
        {
            "abstract": "A number of algorithms and strategies and their variations are currently being used for solving complex optimization problems. Genetic algorithms (GAs) are one of the best strategies for solving such problems basically due to their inherent parallel search capability. Other methods found useful in diverse application areas are simulated annealing, evolution strategies etc. The searching ability of these algorithms can be improved by properly blending their characteristic features. In this paper an attempt is made to intermix the search properties of GA and SA, in order to develop a hybrid algorithm which is equally applicable and has a better searching ability and power to reach a near optimal solution. The author has incorporated an SA like selection criteria in a GA framework. The selection process need not have to wait for the generation of a complete pool of chromosomes. This leads to the development of a very fast hardware-software ensemble, designed with a pipelined architecture, to solve the most complicated types of optimization problems. A number of benchmark problems are used to compare the performances of the classical genetic algorithm (CGA) and the new hybrid algorithm.",
            "group": 2762,
            "name": "10.1.1.134.4679",
            "keyword": "Genetic algorithm (GASimulated annealing (SAHybrid genetic algorithm (HGAOptimizationProbabilistic selection",
            "title": "A hybrid genetic algorithm using probabilistic selection. Journal of the Institution of Engineers (India) 84: 23-30. AUTHOR BIOGRAPHIES AHMED GHANMI received a B.Sc. degree in engineering, a Master degree and a Ph.D. in applied mathematics from the Univer"
        },
        {
            "abstract": "Abstract\u2014A method is presented of performing geolocation of fixed emitters from a single, airborne platform using a twoelement, very long baseline interferometer (VLBI) from 10 to 100 feet in length. The interferometer baseline is precisely tracked through a differential GPS system using auxiliary antennas placed in close proximity to the VLBI pair. A lever arm correction is applied to arrive at the VLBI baseline from the GPS measurements. A batch least squares processing algorithm is presented that operates on the interferometric phase measurements directly and resolves the associated ambiguities through global search strategies. A method of eliminating receiver phase bias by performing a difference operation is shown. Simulation results and Cramer-Rao lower bounds are also presented. Index Terms\u2014Emitter location, Radio interferometry, Least squares methods, Radar position measurement, Radar signal processing.",
            "group": 2763,
            "name": "10.1.1.134.5296",
            "keyword": "",
            "title": "Airborne Very Long Baseline Interferometry and"
        },
        {
            "abstract": "Use of a simulated annealing algorithm to fit compartmental models with an application to fractal pharmacokinetics.",
            "group": 2764,
            "name": "10.1.1.134.5566",
            "keyword": "individual pharmacokineticssimulated",
            "title": ""
        },
        {
            "abstract": "ABSTRACT A method is proposed to restore ab initio low resolution shape and internal structure of chaotically oriented particles (e.g., biological macromolecules in solution) from isotropic scattering. A multiphase model of a particle built from densely packed dummy atoms is characterized by a configuration vector assigning the atom to a specific phase or to the solvent. Simulated annealing is employed to find a configuration that fits the data while minimizing the interfacial area. Application of the method is illustrated by the restoration of a ribosome-like model structure and more realistically by the determination of the shape of several proteins from experimental x-ray scattering data.",
            "group": 2765,
            "name": "10.1.1.134.6411",
            "keyword": "",
            "title": "Restoring low resolution structure of biological macromolecules from solution scattering using simulated annealing"
        },
        {
            "abstract": "Abstract\u2014An automated method is presented for selecting optimal parameter settings for vessel/neurite segmentation algorithms using the minimum description length principle and a recursive random search algorithm. It trades off a probabilistic measure of image-content coverage against its conciseness. It enables nonexpert users to select parameter settings objectively, without knowledge of underlying algorithms, broadening the applicability of the segmentation algorithm, and delivering higher morphometric accuracy. It enables adaptation of parameters across batches of images. It simplifies the user interface to just one optional parameter and reduces the cost of technical support. Finally, the method is modular, extensible, and amenable to parallel computation. The method is applied to 223 images of human retinas and cultured neurons, from four different sources, using a single segmentation algorithm with eight parameters. Improvements in segmentation quality compared to default settings using 1000 iterations ranged from 4.7%\u201321%. Paired-tests showed that improvements are statistically significant @ H HHHSA. Most of the improvement occurred in the first 44 iterations. Improvements in description lengths and agreement with the ground truth were strongly correlated @ aHUVA. Index Terms\u2014Image segmentation, minimum description length, optimization methods, segmentation evaluation. I.",
            "group": 2766,
            "name": "10.1.1.134.7132",
            "keyword": "",
            "title": "Automatic Selection of Parameters for Vessel/Neurite Segmentation Algorithms"
        },
        {
            "abstract": " ",
            "group": 2767,
            "name": "10.1.1.134.7326",
            "keyword": "",
            "title": "Prime: A Bottom-Up Approach to . . . "
        },
        {
            "abstract": "How can an intelligent agent learn an effective representation of its world? This dissertation applies the psychological principle of cognitive economy to the problem of representation in reinforcement learning. Psychologists have shown that humans cope with difficult tasks by simplifying the task domain, focusing on relevant features and generalizing over states of the world which are \u201cthe same\u201d with respect to the task. This dissertation defines a principled set of requirements for representations in reinforcement learning, by applying these principles of cognitive economy to the agent's need to choose the correct actions in its task.\r\n\r\nThe dissertation formalizes the principle of cognitive economy into algorithmic criteria for feature extraction in reinforcement learning. To do this, it develops mathematical definitions of feature importance, sound decisions, state compatibility, and necessary distinctions, in terms of the rewards expected by the agent in the task. The analysis shows how the representation determines the apparent values of the agent's actions, and proves that the state compatibility criteria presented here result in representations which satisfy a criterion for task learnability.\r\n\r\nThe dissertation reports on experiments that illustrate one implementation of these ideas in a system which constructs its representation as it goes about learning the task. Results with the puck-on-a-hill task and the pole-balancing task show that the ideas are sound and can be of practical benefit. The principal contributions of this dissertation are a new framework for thinking about feature extraction in terms of cognitive economy, and a demonstration of the effectiveness of an algorithm based on this new framework.",
            "group": 2768,
            "name": "10.1.1.134.7905",
            "keyword": "",
            "title": "Cognitive Economy and the Role of Representation in On-Line Learning"
        },
        {
            "abstract": "In a distributed stream processing system, streaming data are continuously disseminated from the sources to the distributed processing servers. To enhance the dissemination efficiency, these servers are typically organized into one or more dissemination trees. In this paper, we focus on the problem of constructing dissemination trees to minimize the average loss of fidelity of the system. We observe that existing heuristic-based approaches can only explore a limited solution space and hence may lead to sub-optimal solutions. On the contrary, we propose an adaptive and cost-based approach. Our cost model takes into account both the processing cost and the communication cost. Furthermore, as a distributed stream processing system is vulnerable to inaccurate statistics, runtime fluctuations of data characteristics, server workloads, and network conditions, we have designed our scheme to be adaptive to these situations: an operational dissemination tree may be incrementally transformed to a more cost-effective one. Our adaptive strategy employs distributed decisions made by the distributed servers independently based on localized statistics collected by each server at runtime. For a relatively static environment, we also propose two static tree construction algorithms relying on apriori system statistics. These static trees can also be used as initial trees in a dynamic environment. We apply our schemes to both single- and multi-object dissemination. Our extensive performance study shows that the adaptive mechanisms are",
            "group": 2769,
            "name": "10.1.1.134.8697",
            "keyword": "",
            "title": "   Disseminating Streaming Data in a Dynamic Environment: an Adaptive and Cost-Based Approach"
        },
        {
            "abstract": " ",
            "group": 2770,
            "name": "10.1.1.134.9360",
            "keyword": "fission",
            "title": "Metaheuristics versus spectral and multilevel methods applied on an Air Traffic Control problem"
        },
        {
            "abstract": "Tag clouds provide an aggregate of tag-usage statistics. They are typically sent as in-line HTML to browsers. However, display mechanisms suited for ordinary text are not ideal for tags, because font sizes may vary widely on a line. As well, the typical layout does not account for relationships that may be known between tags. This paper presents models and algorithms to improve the display of tag clouds that consist of in-line HTML, as well as algorithms that use nested tables to achieve a more general 2-dimensional layout in which tag relationships are considered. The first algorithms leverage prior work in typesetting and rectangle packing, whereas the second group of algorithms leverage prior work in Electronic Design Automation. Experiments show our algorithms can be efficiently implemented and perform well.",
            "group": 2771,
            "name": "10.1.1.135.144",
            "keyword": "TagsVisualizationLayout",
            "title": "General Terms Algorithms, Experimentation, Measurements"
        },
        {
            "abstract": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets. 1",
            "group": 2772,
            "name": "10.1.1.135.961",
            "keyword": "",
            "title": "Visualization of collaborative data"
        },
        {
            "abstract": "Abstract. We use Bayesian decision theory to address a variable selection problem arising in attempts to indirectly measure the quality of hospital care, by comparing observed mortality rates to expected values based on patient sickness at admission. Our method weighs data collection costs against predictive accuracy to find an optimal subset of the available admission sickness variables. The approach involves maximizing expected utility across possible subsets, using Monte Carlo methods based on random division of the available data into N modeling and validation splits to approximate the expectation. After exploring the geometry of the solution space, we compare a variety of stochastic optimization methods \u2014 including genetic algorithms (GA), simulated annealing (SA), tabu search (TS), threshold acceptance (TA), and messy simulated annealing (MSA)  \u2014 on their performance in finding good subsets of variables, and we clarify the role of N in the optimization. Preliminary results indicate that TS is somewhat better than TA and SA in this problem, with MSA and GA well behind the other three methods. Sensitivity analysis reveals broad stability of our conclusions.",
            "group": 2773,
            "name": "10.1.1.135.1952",
            "keyword": "Key wordsBayesian decision theoryCross-validationGenetic algorithmInput-output analysisLogistic regressionMaximization of expected utilityMessy simulated annealingMonte Carlo methodsPredictionQuality of health careSickness at hospital admissionSimulated annealingTabu searchThreshold acceptanceVariable selection",
            "title": "A Case Study of Stochastic Optimization in Health Policy: Problem Formulation and Preliminary Results"
        },
        {
            "abstract": "Abstract \u2014 We describe a novel shape formation algorithm for ensembles of 2-dimensional lattice-arrayed modular robots, based on the manipulation of regularly shaped voids within the lattice (\u201choles\u201d). The algorithm is massively parallel and fully distributed. Constructing a goal shape requires time proportional only to the complexity of the desired target geometry. Construction of the shape by the modules requires no global communication nor broadcast floods after distribution of the target shape. Results in simulation show 97.3 % shape compliance in ensembles of approximately 60,000 modules, and we believe that the algorithm will generalize to 3D and scale to handle millions of modules. This paper is submitted to Invited Session: New Trends in Modular Robotics. I.",
            "group": 2774,
            "name": "10.1.1.135.2000",
            "keyword": "",
            "title": "Scalable shape sculpting via hole motion: Motion planning in lattice-constrained modular robots"
        },
        {
            "abstract": "Molecular dynamics is discussed from a mathematical perspective. The recent history of method development is briefly surveyed with an emphasis on the use of geometric integration as a guiding principle. The recovery of statistical mechanical averages from molecular dynamics is then introduced, and the use of backward error analysis as a technique for analysing the accuracy of numerical averages is described. This article gives the first rigorous estimates for the error in statistical averages computed from molecular dynamics simulation based on backward error analysis. It is shown that molecular dynamics introduces an appreciable bias at stepsizes which are below the stability threshold. Simulations performed in such a regime can be corrected by use of a stepsize-dependent reweighting factor. Numerical experiments illustrate the efficacy of this approach. In the final section, several open problems",
            "group": 2775,
            "name": "10.1.1.135.5021",
            "keyword": "CONTENTS 1 Molecular dynamics 2",
            "title": "  Molecular dynamics and the accuracy of numerically computed averages"
        },
        {
            "abstract": "Abstract. The emergence in the past years of Bayesian analysis in many methodological and applied fields as the solution to the modeling of complex problems cannot be dissociated from major changes in its computational implementation. We show in this review how the advances in Bayesian analysis and statistical computation are intermingled. Key words and phrases: Monte Carlo methods, importance sampling, Markov chain Monte Carlo (MCMC) algorithms.",
            "group": 2776,
            "name": "10.1.1.135.5082",
            "keyword": "",
            "title": "Computational advances for and from Bayesian analysis"
        },
        {
            "abstract": "The computational requirements for high quality synthesis, analysis, and verification of VLSI designs have rapidly increased with the fast growing complexity of these designs. Research in the past has focused on the development of heuristic algorithms, special purpose hardware accelerators, or parallel algorithms for the numerous design tasks to decrease the tirn,e required for solution. In this thesis, we propose two new parallel algorithms for two VLSI synthesis tasks, standard cell placement and global routing. The first algorithm, a parallel algorithm for global routing, uses hierarchical tech-niques to decompose the routing problem into independent routing subproblems that are solved in parallel. Results are then presented which compare the routing quality to the results of other published global routers and which evaluate the speedups attained. The second algorithm, a parallel algorithm for cell placement and global routing, hierarchically integrates a quadrisection placement algorithrr{, a bisection placement algorithm, and the previous global routing algorithm. Unique partitioning techniques are used to decompose the various stages of the algorithm into independent tasks which can be evaluated in parallel. Finally, we present results which evaluate the various algo-rithm alternatives and compare the algorithm performance to other placement programs, and we present measurements on the parallel speedups available.  ",
            "group": 2777,
            "name": "10.1.1.135.5261",
            "keyword": "",
            "title": "PARALLEL ALGORITHMS FOR PLACEMENT AND ROUTING IN VLSI DESIGN"
        },
        {
            "abstract": "Creating high-quality label layouts in a particular visual style is a time-consuming process. Although automated labeling algorithms can aid the layout process, expert design knowledge is required to tune these algorithms so that they produce layouts which meet the designer\u2019s expectations. We propose a system which can learn a label layout style from a single example layout and then apply this style to new labeling problems. Because designers find it much easier to create example layouts than tune algorithmic parameters, our system provides a more natural workflow for graphic designers. We demonstrate that our system is capable of learning a variety of label layout styles from examples. ACM Classification I.2.6 [Artificial Intelligence]: Learning.",
            "group": 2778,
            "name": "10.1.1.135.5423",
            "keyword": "General Terms AlgorithmsDesign Keywordslearninglabelingdesignworkflows",
            "title": "Specifying Label Layout Styles by Example"
        },
        {
            "abstract": "",
            "group": 2779,
            "name": "10.1.1.135.5921",
            "keyword": "",
            "title": "Constraints of artificial . . . modelling: trade-offs in hydrological state representation and model evaluation"
        },
        {
            "abstract": "Abstract \u2013 This paper introduces a new dynamic neighborhood network for particle swarm optimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO) algorithm, each particle initially joins a default number of what we call \u2018clubs\u2019. Each particle is affected by its own experience and the experience of the best performing member of the clubs it is a member of. Clubs membership is dynamic, where the worst performing particles socialize more by joining more clubs to learn from other particles and the best performing particles are made to socialize less by leaving clubs to reduce their strong influence on other members. Particles return gradually to default membership level when they stop showing extreme performance. Inertia weights of swarm members are made random within a predefined range. This proposed dynamic neighborhood algorithm is compared with other two algorithms having static neighborhood topologies on a set of classic benchmark problems. The results showed superior performance for C-PSO regarding escaping local optima and convergence speed. I.",
            "group": 2780,
            "name": "10.1.1.135.6237",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "",
            "group": 2781,
            "name": "10.1.1.135.6918",
            "keyword": "",
            "title": "Support Vector Machines for Speech Recognition"
        },
        {
            "abstract": "The multi-product variable lot streaming (MPVLS) in a flow shop is to determine product sequence, to determine the machines to reallocate lot streaming for each product in the sequence, and to determine lot streaming for each machine, in order to minimize makespan for producing all products. In this paper, MPVLS is decomposed into three sub-problems: product sequence determination, lot streaming reallocation machine determination, and lot streaming range determination. After the combination of product sequence, lot streaming reallocation machines, and lot streaming ranges is determined, a mathematical formulation (Linear Programming, LP) for MPVLS with the specific combination information is developed and solved by LINDO. Since the search for all possible combinations is an NP problem, a heuristic method is proposed to solve the problem. First of all, an initial solution for MPVLS with a specific combination is determined. Then a hybrid heuristic combining tabu search (TS) with simulated annealing (SA) is used to improve the initial solution by searching for better combinations. The proposed heuristic method is tested and evaluated via simulation. The results show the proposed heuristic method is better than those existing methods in terms of average makespan deviation.",
            "group": 2782,
            "name": "10.1.1.135.8615",
            "keyword": "heuristic methodmulti-product variable lot streaming (MPVLSNP problemhybrid heuristictabu search",
            "title": "A HEURISTIC METHOD FOR MULTI-PRODUCT VARIABLE LOT STREAMING IN A FLOW SHOP"
        },
        {
            "abstract": "  ",
            "group": 2783,
            "name": "10.1.1.135.8662",
            "keyword": "translation fall in this category",
            "title": " Mathematical Methods  "
        },
        {
            "abstract": "Cellular Manufacturing is a successful application of Group Technology concepts: we want to group parts into families having similar characteristics and identify dedicated set of machines (cells) to process the different families while minimizing the number of parts that need to be processed by machines placed in different cells (Inter-Cell Flow). The proposed approach consists on iteratively running an algorithm for the cell formation. At each iteration, (i) we run a simulation for the cell configuration obtained in the optimization phase, (ii) we measure a set of performance indicators, and, (iii) on the ground of a suitably designed feedback indicator, we re-define similarity coefficients for the cell formation. Both in the optimization and feedback phases, techniques based on a greedy-like approach and Simulated Annealing are introduced. Simulation experiments are performed varying demand scenarios and material flow management rules. Encouraging results are presented and discussed.",
            "group": 2784,
            "name": "10.1.1.135.8740",
            "keyword": "Group TechnologyOptimizationSimulation",
            "title": "AN INTEGRATED APPROACH FOR THE DESIGN OF PLANT LAYOUT AND MATERIAL FLOWS"
        },
        {
            "abstract": "Motivation: Aligning protein structures is a highly relevant task. It enables the study of functional and ancestry relationships between proteins and is very important for homology and threading methods in structure prediction. Existing methods typically only partially explore the space of possible alignments and being able to efficiently handle permutations efficiently is rare. Results: A novel approach for structure alignment is presented, where the key ingredients are: (1) An error function formulation of the problem simultaneously in terms of binary (Potts) assignment variables and real-valued atomic coordinates. (2) Minimization of the error function by an iterative method, where in each iteration a mean field method is employed for the assignment variables and exact rotation/translation of atomic coordinates is performed, weighted with the corresponding assignment variables. The approach allows for extensive search of all possible alignments, including those involving arbitrary permutations. The algorithm is implemented using a C\u03b1-representation of the backbone and explored on different protein structure categories using the Protein Data Bank (Pdb) and is successfully compared with other algorithms. The approach performs very well with modest CPU consumption and is robust with respect to choice of parameters. It is extremely generic and flexible and can handle additional user-prescribed constraints easily. Furthermore, it allows for a probabilistic interpretation of the results. 2",
            "group": 2785,
            "name": "10.1.1.135.9336",
            "keyword": "protein structure alignmentpermutationmean field annealingfuzzy assignmentdatabase searching",
            "title": "A Novel Approach to Structure Alignment"
        },
        {
            "abstract": " ",
            "group": 2786,
            "name": "10.1.1.136.506",
            "keyword": "",
            "title": "Three new consensus QSAR models for the prediction of Ames genotoxicity"
        },
        {
            "abstract": "The effective visual exploration of large and complexly structured, abstract data requires sophisticated and interactive visualization techniques. Development of these techniques is the major discipline in information visualization. On the other hand, visualization of geospatial data is an important topic in cartography. The necessity to combine expertise from both fields has long been commonly recognized. In this paper, some considerations on the combination of arbitrary multivariate data visualizations, focus & context interaction techniques and thematic map displays are discussed that will allow the efficient combination of established techniques from both information visualization and cartography. 1.",
            "group": 2787,
            "name": "10.1.1.136.911",
            "keyword": "",
            "title": "Visualizing Abstract Data on Maps"
        },
        {
            "abstract": "We resolve in the affirmative a question of Boppana and Bui: whether simulated annealing can, with high probability and in polynomial time, find the optimal bisection of a random graph in Gnpr when p- r = O(n*-\u2019) for A 5 2. (The random graph model Gnpr specifies a \u201cplanted \u201d bisection of density r, separating two n/2-vertex subsets of slightly higher density p.) We show that simulated \u201cannealing \u201d at an appropriate fixed temperature (i.e., the Metropolis algorithm) finds the unique smallest bisection in O(n2+\u2018) steps with very high probability, provided A> 1116. (By using a slightly modified neighborhood structure, the number of steps can be reduced to O(n\u2019+\u2018).) We leave open the question of whether annealing is effective for A in the range 312 < A 5 1116, whose lower limit represents the threshold at which the planted bisection becomes lost amongst other random small bisections. It also remains open whether hillclimbing (i.e., annealing at temperature 0) solves the same problem. 1",
            "group": 2788,
            "name": "10.1.1.136.1625",
            "keyword": "",
            "title": "Simulated annealing for graph bisection"
        },
        {
            "abstract": "",
            "group": 2789,
            "name": "10.1.1.136.3194",
            "keyword": "",
            "title": "NUMB3RS Episode 401--Trust Metric"
        },
        {
            "abstract": "The inversion of pressure transient tests by cluster variable aperture (CVA) simulated annealing is used as an inversion technique for developing models of fluid flow in fractured formations. A three-dimensional (3D) fracture network system is represented as a filled regular lattice of fracture elements. The algorithm iteratively changes element apertures for a cluster of fracture elements, which are chosen randomly from a list of discrete apertures, in order to improve the match to observed pressure transients. In this technique, the finite element code TRINET is used as a subroutine to solve for the pressure distribution at each iteration. Aperture size is chosen randomly from a list of discrete apertures. The cluster size is held constant throughout the iterations. This technique is applied to a 3D synthetic model to invert a series of three injection tests. The inversion result shows that the high transmissivity distribution is clearly reconstructed by CVA simulated annealing. 1.",
            "group": 2790,
            "name": "10.1.1.136.3605",
            "keyword": "Key Wordspressure transientinversionsimulated",
            "title": "PRESSURE TRANSIENT TESTING INVERSION FOR FLUID FLOW MODELING IN FRACTURED ROCKS USING SIMULATED ANNEALING:-THREE DIMENSIONAL SYNTHETIC CASES-"
        },
        {
            "abstract": "In striving to construct higher level control representations for simulated characters or creatures, one must seek flexible control representations to build upon. We present a method for the synthesis of parameterized, physics-based motions. The method can be applied to both periodic and aperiodic motions. The basis of the method is a low-level control representation in which linear combinations of controllers generally produce predictable in-between motions.",
            "group": 2791,
            "name": "10.1.1.136.3702",
            "keyword": "",
            "title": "Synthesizing Parameterized Motions  "
        },
        {
            "abstract": "ABSTRACT We present an optimal solution to the problem of allocating communicating periodic tasks to heterogeneous processing nodes (PNs) in a distributed real-time system. The solution is optimal in the sense of minimizing the maximum normalized task response time, called the system hazard, subject to the precedence constraints resulting from intercommunication among the tasks to be allocated. Minimization of the system hazard ensures that the solution algorithm will allocate tasks so as to meet all task deadlines under an optimal schedule, whenever such an allocation exists. The task system is modeled with a task graph (TG), in which computation and communication modules, communication delays, and intertask precedence constraints are clearly described. Tasks described by this TG are assigned to PNs by using a branch-and-bound (B&B) search algorithm. The algorithm traverses a search tree whose leaves correspond to potential solutions to the task allocation problem. We use a bounding method that prunes, in polynomial time, non-leaf vertices that cannot lead to an optimal solution, while ensuring that the search path leading to an optimal solution will never be pruned. For each generated leaf vertex we compute the exact cost using the algorithm developed in [1]. The lowest-cost leaf vertex (one with the least system hazard) represents an optimal task allocation. Computational experiences and examples are provided to demonstrate the concept, utility, and power of the proposed approach.",
            "group": 2792,
            "name": "10.1.1.136.4036",
            "keyword": "",
            "title": "Assignment and scheduling of communicating periodic tasks in distributed real-time systems"
        },
        {
            "abstract": "We present an improved \u201ccooling schedule \u201d for simulated annealing algorithms for combinatorial counting problems. Under our new schedule the rate of cooling accelerates as the temperature decreases. Thus, fewer intermediate temperatures are needed as the simulated annealing algorithm moves from the high temperature (easy region) to the low temperature (difficult region). We present applications of our technique to colorings and the permanent (perfect matchings of bipartite graphs). Moreover, for the permanent, we improve the analysis of the Markov chain underlying the simulated annealing algorithm. This improved analysis, combined with the faster cooling schedule, results in an O(n 7 log 4 n) time algorithm for approximating the permanent of a 0/1 matrix. 1",
            "group": 2793,
            "name": "10.1.1.136.4841",
            "keyword": "",
            "title": "Abstract Accelerating Simulated Annealing for the Permanent and Combinatorial Counting Problems"
        },
        {
            "abstract": "The outlier detection problem has important applications in the eld of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to nd outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective ofproximity-based de nitions. Consequently, for high dimensional data, the notion of nding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which nd the outliers by studying the behavior of projections from the data set. 1.",
            "group": 2794,
            "name": "10.1.1.136.5765",
            "keyword": "",
            "title": "Outlier detection for high dimensional data"
        },
        {
            "abstract": "enl SIS 61 41 21",
            "group": 2795,
            "name": "10.1.1.136.7669",
            "keyword": "",
            "title": "CODEN: Author(s) Sponsoring organization"
        },
        {
            "abstract": "Performance and data from some cognitive models suggested that emotions, experienced during problem solving, should be taken into account. Moreover, it is proposed that the cognitive science approach using both theoretical and experimental data may lead to a better understanding of the phenomena. A closer investigation of ACT-R cognitive architecture (Anderson 1993) revealed some properties analogous to phenomena known from the activation theory of emotion. A model of the classical Yerkes-Dodson experiment was built to test the predictions. The study explained such psychological phenomena as arousal, motivation and confidence within the mathematical notation. The influence of changes in these motivational states, controlled by emotion, on information processing has been investigated and it is shown that the dynamics corresponds to the well-known optimisation methods, such as best-first search and simulated annealing. 1",
            "group": 2796,
            "name": "10.1.1.136.8127",
            "keyword": "",
            "title": "The Role of Emotion in Problem Solving"
        },
        {
            "abstract": "  An approach to Example-Based Machine Translation is presented which operates by extracting and recombining translation patterns from a bilingual corpus aligned at the level of the sentence. The translation patterns are extracted using a recursive machinelearning algorithm based on the principle of similar distributions of strings: source and target language lexical items that co-occur in the same two sentence-pairs are likely to be translations of each other. The translation patterns extracted represent generalisations of sentences that are translations of each other in that certain sequences of words are replaced by variables. The translation patterns resemble, to a certain extent, transfer rules but with less constraints since there is no concept of syntactic structure in this approach: translation patterns are extracted based on the",
            "group": 2797,
            "name": "10.1.1.136.8180",
            "keyword": "",
            "title": "Translation Pattern Extraction and Recombination for Example-Based Machine Translation"
        },
        {
            "abstract": "Abstract\u2014Since modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering techniques are useful to support the creation of these abstractions by producing architectural-level views of a system\u2019s structure directly from its source code. This paper examines the Bunch clustering system which, unlike other software clustering tools, uses search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph of the entities (e.g., classes) and relations (e.g., function calls) in the source code. Bunch uses a fitness function to evaluate the quality of graph partitions and uses search algorithms to find a satisfactory solution. This paper presents a case study to demonstrate how Bunch can be used to create views of the structure of significant software systems. This paper also outlines research to evaluate the software clustering results produced by Bunch. Index Terms\u2014Clustering, reverse engineering, reengineering, program comprehension, optimization, maintainability. 1",
            "group": 2798,
            "name": "10.1.1.136.8460",
            "keyword": "",
            "title": "On the automatic modularization of software systems using the bunch tool"
        },
        {
            "abstract": "Some of the most influential decisions about a software system are made in the early phases of the software development life cycle. Those decisions about requirements and design are generally made by teams of software engineers and domain experts who must weigh the complex interactions among requirements and the associated developmental and operational risks of those requirements. Some of these early life cycle decisions are more influential, or perhaps fateful, to subsequent software design and development than are others. When debating about complex systems with a large number of options, humans can often be slower than an AI system at identifying the clusters of key decisions that give the most leverage. By focusing a group of human domain experts or software engineers on these key decision clusters, more time can be devoted to these pivotal decisions and less time is wasted on irrelevancies. We are developing a tool based on an integration of: JPL\u2019s DDP group decision support tool [3] WVU\u2019s contrast set learners [7]) Miami University\u2019s cluster visualization tools Various component of this tool have been tested on case studies at JPL. 1",
            "group": 2799,
            "name": "10.1.1.136.8782",
            "keyword": "",
            "title": "Improved software engineering decision support through automatic argument reduction tools"
        },
        {
            "abstract": "Many dynamic-content online services are comprised of multiple interacting components and data partitions distributed across server clusters. Understanding the performance of these services is crucial for efficient system management. This paper presents a profile-driven performance model for cluster-based multi-component online services. Our offline constructed application profiles characterize component resource needs and intercomponent communications. With a given component placement strategy, the application profile can be used to predict system throughput and average response time for the online service. Our model differentiates remote invocations from fast-path calls between co-located components and we measure the network delay caused by blocking inter-component communications. Validation with two J2EE-based online applications show that our model can predict application performance with small errors (less than 13 % for throughput and less than 14% for the average response time). We also explore how this performance model can be used to assist system management functions for multi-component online services, with case examinations on optimized component placement, capacity planning, and cost-effectiveness analysis. 1",
            "group": 2800,
            "name": "10.1.1.136.9249",
            "keyword": "",
            "title": "Abstract Performance Modeling and System Management for Multi-component Online Services \u2217"
        },
        {
            "abstract": "Computer icons are small artificial images designed to be perceived with minimal ambiguity by the human visual system. In order to make them easier to perceive by visually impaired people, we propose a solution to the superresolution problem for color bitmap icons in a manner that exploits the unique characteristics of this medium versus that of generic low resolution natural imagery. We propose an MRF-based solution that incorporates local models of luminance and color perception which lays the basis for a snake-based vectorization of the icon and demonstrates encouraging performance on a diverse set of icons. 1.",
            "group": 2801,
            "name": "10.1.1.137.1041",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The principal objective of this research is to reduce the elapsed time of large-scale nonlinear dynamic structural finite element analysis using parallel computing techniques. The major tasks and contributions of this research are: (a) employing the general sparse matrix technique to reduce the computing time and storage requirements for both sequential and parallel substructure analysis (b) employing state-of-the-art matrix ordering methods to perform substructure matrix ordering to reduce the extra computations of substructure method, (c) proposing an iterative mesh partitioning approach for better load balance of parallel substructure analysis, (d) employing the multi-level substructure method for improving the efficiency of the parallel finite element analyses, and (e) designing and implementing a high-performance finite element environment, called FE2000, for linear or geometric nonlinear, static or implicit dynamic finite element structural analyses. This research",
            "group": 2802,
            "name": "10.1.1.137.1505",
            "keyword": "",
            "title": "PARALLEL COMPUTING FOR NONLINEAR DYNAMIC FINITE ELEMENT STRUCTURAL ANALYSIS WITH GENERAL SPARSE MATRIX TECHNOLOGY  "
        },
        {
            "abstract": "ABSTRACT We present a rate equation model for the TGF-b pathway in endothelial cells together with novel measurements. This pathway plays a prominent role in inter- and intracellular communication and subversion can lead to cancer, fibrosis vascular disorders, and immune diseases. The model successfully describes the kinetics of experimental data and also correctly predicts the behavior in experiments where the system is perturbed. A novel method in this context, simulated tempering, is used to fit the model parameters to the data. It provides an ensemble of high quality solutions, which are analyzed with clustering methods and display a hierarchical structure highlighting distinct parameter subspaces with biological interpretations. This analysis discriminates between different biological mechanisms to achieve a transient signal from a sustained TGF-b input, where one mechanism is to use a negative feedback to turn the signal off. Further analysis in terms of parameter sensitivity reveals that this negative feedback loop in TGF-b signaling renders the system global robustness. This sheds light upon the role of the Smad7 protein in this system.",
            "group": 2803,
            "name": "10.1.1.137.1769",
            "keyword": "",
            "title": "A Rate Equation Approach to Elucidate the Kinetics and Robustness of the TGF-b Pathway"
        },
        {
            "abstract": "Testing and fault-diagnosis of core-based systems are both difficult problems. Being able to identify which module in the core-based system is faulty has become very important. In this paper, we present algorithms to introduce test points for improving the diagnosability of a digital system. We define a measure of diagnosability known as module resolution which relates to the number of circuit modules that are suspected to be faulty after the diagnostic test procedure has been completed. We present a technique to partition the system into subsystems such that they can be tested in isolation. We also concurrently arrive at a test schedule which minimizes the overall effort in diagnostic testing. We have developed a tool called DEBIT for identifying the number, type, and location",
            "group": 2804,
            "name": "10.1.1.137.2299",
            "keyword": "",
            "title": "Abstract Improving the Diagnosability of Digital Circuits"
        },
        {
            "abstract": "Abstract\u2014Configuration similarity is a special form of content-based image retrieval that considers relative object locations. It can be used as a standalone method, or to complement retrieval based on visual or semantic features. The corresponding queries ask for sets of objects that satisfy some spatio-temporal constraints, e.g., \u201cfind all triplets of objects ( I, P, Q), such that I is northeast of P, which is inside Q. \u201d Exhaustive processing (i.e., retrieval of the best solutions) of configuration similarity queries, in general, has exponential complexity and fast search for sub-optimal solutions is the only way to deal with the vast amounts of multimedia information in several real-time applications. In this paper we first discuss the utilization of nonsystematic search heuristics, based on genetic algorithms, simulated annealing and hill climbing approaches. An extensive experimentation with real and synthetic datasets reveals that hill climbing techniques are the best for the current problem; therefore, as a subsequent step we study the search space, and develop improved variations of hill climbing that take advantage of the special structure of the problem to enhance speed. The proposed heuristic methods significantly outperform systematic search when there is only limited time for query processing. Index Terms\u2014Content-based retrieval, local search algorithms, spatial similarity. I.",
            "group": 2805,
            "name": "10.1.1.137.2372",
            "keyword": "",
            "title": "On the Retrieval of Similar Configurations"
        },
        {
            "abstract": "When explaining human behavior, anthropologists frequently distinguish the things that people do of their own free will from the things they do because they have to. In much of anthropology, and most American archaeology, this is",
            "group": 2806,
            "name": "10.1.1.137.2570",
            "keyword": "",
            "title": "Style, Function, and . . . "
        },
        {
            "abstract": "An effective partnership between industry and the university resulted in the system of design tools for the layout of HVAC systems presented in this paper and illustrated with the design of a heat pump. The system provides tools to assist in the placement of components and routing of tubes between the components. Traditional tubes, tubes that have minimized length and number of bends, and those that are impossible to route in the traditional manner, are generated. The paper provides insight on both the collaborative research interaction and the resulting set of tools. 1.",
            "group": 2807,
            "name": "10.1.1.137.2788",
            "keyword": "",
            "title": "HVAC CAD Layout Tools: A Case Study of University/Industry Collaboration"
        },
        {
            "abstract": "",
            "group": 2808,
            "name": "10.1.1.137.3060",
            "keyword": "",
            "title": "Analysis of microbial community composition using oligonucleotide fingerprinting of ribosomal RNA genes  "
        },
        {
            "abstract": "We consider the problem of mapping tasks onto processors in a reconfigurable array architecture. We assume a directed acyclic task graph as input. The node weights in the task graph represent their computational requirement; the weight on an edge (i, j> is an estimate of the communication requirement between tasks i and j. The problem is to (a) estimate the minimum number of processors p to execute all the tasks with the highest possible efficiency, (b) bind each task to a processor, (c) schedule the tasks within each processor, and (d) carry out link allocation among processors. We assume a realistic model of reconfigurable parallel processors, where each processor can be connected to at most d other processors through bidirectional links. The objective of the problem is to minimize the total overall execution time, which includes the time spent by the processors in computation, communication, and idling. The mapping problem is computationally hard, and we present two algorithms for obtaining near-optimal solutions. The fiit algorithm is a heuristic algorithm based on the critical path method and as soon IIS possible scheduling. The second algorithm uses the Boltzmam ~ machine model of artificial neural networks to solve the mapping problem. We have implemented both the algorithms on a Sun/SPARC workstation. Experimental results on a set of benchmark problems indicate that the neural algorithm generates better solutions than the heuristic algorithm, but takes significantly larger amounts of time than the latter. The number of neurons required in the algorithm is equal to n.p and hence the connection matrix is np X np; thus the neural algorithm is also memory intensive and I/O intensive due to swapping. We have devised a parallel divide-and-conquer algorithm which decomposes a large mapping problem into several smaller ones and solves the subproblems concurrently on a network of Sun workstations.",
            "group": 2809,
            "name": "10.1.1.137.3945",
            "keyword": "MappingSchedulingReconfigurable processorsNeural optimizationBoltzmann machines",
            "title": "  Heuristic and neural algorithms for mapping tasks to a reconfigurable array"
        },
        {
            "abstract": "Often times, individuals working together as a team can solve hard problems beyond the capability of any individual in the team. Cooperative optimization is a newly proposed general method for attacking hard optimization problems inspired by cooperation principles in team playing. It has an established theoretical foundation and has demonstrated outstanding performances in solving real-world optimization problems. With some general settings, a cooperative optimization algorithm has a unique equilibrium and converges to it with an exponential rate regardless initial conditions and insensitive to perturbations. It also possesses a number of global optimality conditions for identifying global optima so that it can terminate its search process efficiently. This paper offers a general description of cooperative optimization, addresses a number of design issues, and presents a case study to demonstrate its power. I.",
            "group": 2810,
            "name": "10.1.1.137.4449",
            "keyword": "",
            "title": "Cooperative Optimization for Energy Minimization: A Case Study of Stereo Matching"
        },
        {
            "abstract": " ",
            "group": 2811,
            "name": "10.1.1.137.5253",
            "keyword": "",
            "title": "Optimization by Stochastic Methods"
        },
        {
            "abstract": "Information theoretic justification ofBoltzmann selection and",
            "group": 2812,
            "name": "10.1.1.137.5411",
            "keyword": "",
            "title": "its generalization to Tsallis case"
        },
        {
            "abstract": "The Inverse Delayed (ID) model is a novel neural network system, which has been proposed by Prof. Nakajima et al. However, the learning method of the ID-model, which is an important feature of the neural network model, has not yet been developed. The ID model can be used for various networks, but why it is interesting for the Deterministic Boltzmann Machine (DBM) is because it is relatively suitable for realising a compact neurochip. DBM has proven to be much faster than the original Boltzmann Machine on some problems. Interest lies however in making it even faster. By changing DBM:s classical structure and let it be based on the ID model, this was investigated. The properties of the ID model, such as destabilisation of states and a speedup effect of state changes, due to negative resistance, bring hope that learning will become faster. In order to evaluate its performance, the XOR function was used and the results from the different network structures were compared. The performance of the networks was investigated with various network sizes and activation functions and the result was evaluated in terms of convergence likelihood but mainly by time span until stability. The ID model with negative resistance proved to be more successful than",
            "group": 2813,
            "name": "10.1.1.137.5632",
            "keyword": "",
            "title": "Supervisor at Nada was \u00d6rjan Ekeberg"
        },
        {
            "abstract": "The need for improvement in process operations, logistics and supply chain management has created a great demand for the development of optimization models for planning and scheduling. In this paper we first review the major classes of planning and scheduling models that arise in process operations, and establish the underlying mathematical structure of these problems. As will be shown, the nature of these models is greatly affected by the time representation (discrete or continuous), and is often dominated by discrete decisions. We then briefly review the major recent developments in mixed-integer linear and nonlinear programming, disjunctive programming and constraint programming, as well as general decomposition techniques for solving these problems. We present a general formulation for integrating planning and scheduling to illustrate the models and methods discussed in this paper.",
            "group": 2814,
            "name": "10.1.1.137.8702",
            "keyword": "SchedulingOptimizationMixed-integer programming",
            "title": "Discrete Optimization Methods and their Role in the Integration of Planning and Scheduling"
        },
        {
            "abstract": "Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling that factors in previously fixed locations- called anchor points- for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21 % average performance improvement over the best prior algorithm and comes within an average of 5 % of the annealed performance for our benchmarks.",
            "group": 2815,
            "name": "10.1.1.137.9739",
            "keyword": "General Terms AlgorithmPerformance Keywords Instruction schedulingpath schedulingsimulated annealingEDGE architecture",
            "title": "Appears in the\u00d8\ufffdInternational Conference on Architectural Support for Programming Languages and Operating Systems A Spatial Path Scheduling Algorithm for EDGE Architectures\u00a3 Abstract"
        },
        {
            "abstract": "",
            "group": 2816,
            "name": "10.1.1.138.150",
            "keyword": "",
            "title": "Designed to be used with Eclipse 3.1"
        },
        {
            "abstract": "Abstract. ProperCAD II is a C++ object oriented library supporting actor based parallel program design. The library easily allows the design of data structures with parallel semantics for use in irregular applications. Inheritance mechanisms allow creation of the distributed data structures from standard \u00a4\u00a6\u00a5\u00a7\u00a5 objects. This paper discusses the use of such distributed data structures in the context of a particular VLSI CAD application, standard cell placement. The library and associated runtime system currently run on a wide range of platforms. 1",
            "group": 2817,
            "name": "10.1.1.138.483",
            "keyword": "",
            "title": "Distributed object oriented data structures and algorithms for VLSI"
        },
        {
            "abstract": "Sparsely sampled irregular arrays and random arrays have been used or proposed in several fields such as radar, sonar, ultrasound imaging, and seismics. We start with an introduction to array processing and then consider the combinatorial problem of finding the best layout of elements in sparse 1-D and 2-D arrays. The optimization criteria are then reviewed: creation of beampatterns with low mainlobe width and low sidelobes, or as uniform as possible coarray. The latter case is shown here to be nearly equivalent to finding a beampattern with minimal peak sidelobes. We have applied several optimization methods to the layout problem, including linear programming, genetic algorithms and simulated annealing. The examples given here are both for 1-D and 2-D arrays. The largest problem considered is the selection of K = 500 elements in an aperture of 50 by 50 elements. Based on these examples we propose that an estimate of the achievable peak level in an algorithmically optimized array is inverse proportional to K and is close to the estimate of the average level in a random array. Active array systems use both a transmitter and receiver aperture and they need not necessarily be the same. This gives additional freedom in design of the thinning patterns, and favorable solutions can be found by using periodic patterns with different periodicity for the two apertures, or a periodic pattern in combination with an algorithmically optimized pattern with the condition that there be no overlap between transmitter and receiver elements. With the methods given here one has the freedom to choose a design method for a sparse array system using either the same elements for the receiver and the transmitter, no overlap between the receiver and transmitter or partial overlap as in periodic arrays.",
            "group": 2818,
            "name": "10.1.1.138.1389",
            "keyword": "",
            "title": "Sparse sampling in array processing"
        },
        {
            "abstract": "Abstract: As more research centers embark on sequencing new genomes, the problem of DNA fragment assembly for shotgun sequencing is growing in importance and complexity. Accurate and fast assembly is a crucial part of any sequencing project and since the DNA fragment assembly problem is NPhard, exact solutions are very difficult to obtain. Various heuristics were designed for solving the fragment assembly problem. But, in general, they are unable to sequence very large DNA molecules. In this work, we present several methods, a canonical genetic algorithm, a CHC method, a scatter search algorithm, and a simulated annealing, to solve accurately problem instances that are 77K base pairs long.",
            "group": 2819,
            "name": "10.1.1.138.1766",
            "keyword": "DNA Fragment AssemblyGenetic AlgorithmScatter SearchSimulated AnnealingCHC",
            "title": "Metaheuristics for the DNA Fragment Assembly Problem"
        },
        {
            "abstract": "Hidden Markov models, the expectation\u2013maximization algorithm, and the Gibbs sampler were introduced for biological sequence analysis in early 1990s. Since then the use of formal statistical models and inference procedures has revolutionized the field of computational biology. This chapter reviews the hidden Markov and related models, as well as their Bayesian inference procedures and algorithms, for sequence alignments and gene regulatory binding motif discoveries. We emphasize that the combination of Markov chain Monte Carlo and dynamic-programming techniques often results in effective algorithms for NP-hard problems in sequence analysis. In the past decade, we have witnessed the development of the likelihood approach to pairwise sequence alignments (Bishop and Thompson, 1986; Thorne et al., 1991); probabilistic models for RNA secondary structure (Zuker, 1989; Lowe and Eddy, 1997);",
            "group": 2820,
            "name": "10.1.1.138.1904",
            "keyword": "",
            "title": "Bayesian methods in biological sequence analysis"
        },
        {
            "abstract": "In this dissertation, we propose a general approach that can significantly reduce the com-plexity in solving discrete, continuous, and mixed constrained nonlinear optimization (NLP) problems. A key observation we have made is that most application-based NLPs have struc-tured arrangements of constraints. For example, constraints in AI planning are often lo-calized into coherent groups based on their corresponding subgoals. In engineering design problems, such as the design of a power plant, most constraints exhibit a spatial structure based on the layout of the physical components. In optimal control applications, constraints are localized by stages or time. We have developed techniques to exploit these constraint structures by partitioning the constraints into subproblems related by global constraints. Constraint partitioning leads to much relaxed subproblems that are significantly easier to solve. However, there exist global constraints relating multiple subproblems that must be resolved. Previous methods cannot exploit such structures using constraint partitioning because they cannot resolve inconsistent global constraints efficiently.",
            "group": 2821,
            "name": "10.1.1.138.3689",
            "keyword": "",
            "title": "Solving Nonlinear Constrained Optimization Problems Through Constraint Partitioning"
        },
        {
            "abstract": "",
            "group": 2822,
            "name": "10.1.1.138.4292",
            "keyword": "II. MAXIMUM LIKELIHOOD MODELING......................... 9",
            "title": "Maximum Likelihood Estimation of . . . "
        },
        {
            "abstract": "This paper proposes markovian models in portfolio theory and risk management. At first, we describe discrete time optimal allocation models. Then, we examine the investor\u2019s optimal choices either when the returns are uniquely determined by their mean and variance or when they are modeled by a Markov chain. We subject these models to back-testing on out-of-sample data, in order to assess their forecasting ability. Finally, we propose some models to compute VaR and CVaR when the returns are modeled by a Markov chain. Key words:",
            "group": 2823,
            "name": "10.1.1.138.4666",
            "keyword": "Markov chainPortfolio theoryVaR and CVaR models",
            "title": "Portfolio Selection and Risk Management with Markov Chains"
        },
        {
            "abstract": "",
            "group": 2824,
            "name": "10.1.1.138.5405",
            "keyword": "Unclassified",
            "title": "ASSET MANAGEMENT LITERATURE REVIEW AND POTENTIAL APPLICATIONS OF SIMULATION, OPTIMIZATION, AND DECISION ANALYSIS TECHNIQUES FOR RIGHT-OF-WAY AND TRANSPORTATION PLANNING AND PROGRAMMING"
        },
        {
            "abstract": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of \u201cno free lunch \u201d (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori \u201chead-to-head\u201d minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems\u2019 enforcing of a type of uniformity over all algorithms. ",
            "group": 2825,
            "name": "10.1.1.138.6606",
            "keyword": "",
            "title": "No Free Lunch Theorems for Optimization"
        },
        {
            "abstract": "The research presented here examines topological drawing, a new mode of constructing and interacting with mathematical objects in three-dimensional space. In topological drawing, issues such as adjacency and connectedness, which are topological in nature, take precedence over purely geometric issues. Because the domain of application is mathematics, topological drawing is also concerned with the correct representation and display of these objects on a computer. By correctness we mean that the essential topological features of objects are maintained during interaction. We have chosen to limit the scope of topological drawing to knot theory, a domain that consists essentially of one class of object (embedded circles in three-dimensional space) yet is rich enough to contain a wide variety of difficult problems of research interest. In knot theory, two embedded circles (knots) are considered equivalent if one may be smoothly deformed into the other without any cuts or self-intersections. This notion of equivalence may be thought of as the heart of knot theory. We present methods for the computer construction and interactive manipulation of",
            "group": 2826,
            "name": "10.1.1.138.7376",
            "keyword": "",
            "title": "Interactive Topological Drawing"
        },
        {
            "abstract": " ",
            "group": 2827,
            "name": "10.1.1.138.8831",
            "keyword": "CONTENTS LIST OF TABLES................................. v LIST OF FIGURES..........................",
            "title": "MAPPING UNSTRUCTURED GRID COMPUTATIONS TO MASSIVELY PARALLEL COMPUTERS"
        },
        {
            "abstract": "We present Fitness Expectation Maximization (FEM), a novel method for performing \u2018black box \u2019 function optimization. FEM searches the fitness landscape of an objective function using an instantiation of the well-known Expectation Maximization algorithm, producing search points to match the sample distribution weighted according to higher expected fitness. FEM updates both candidate solution parameters and the search policy, which is represented as a multinormal distribution. Inheriting EM\u2019s stability and strong guarantees, the method is both elegant and competitive with some of the best heuristic search methods in the field, and performs well on a number of unimodal and multimodal benchmark tasks. To illustrate the potential practical applications of the approach, we also show experiments on finding the parameters for a controller of the challenging non-Markovian double pole balancing task.",
            "group": 2828,
            "name": "10.1.1.139.1585",
            "keyword": "",
            "title": "Fitness Expectation Maximization"
        },
        {
            "abstract": "Finding reliable, meaningful patterns in data with high numbers of attributes can be extremely difficult. Feature selection helps us to decide what attributes or combination of attributes are most important for finding these patterns. In this chapter, we study feature selection methods for building classification models from high-throughput genomic (microarray) and proteomic (mass spectrometry) data sets. Thousands of feature candidates must be analyzed, compared and combined in such data sets. We describe the basics of four different approaches used for feature selection and illustrate their effects on an MS cancer proteomic data set. The closing discussion provides assistance in performing an analysis in high-dimensional genomic and proteomic data",
            "group": 2829,
            "name": "10.1.1.139.2045",
            "keyword": "",
            "title": "Feature Selection and Dimensionality Reduction in Genomics and Proteomics"
        },
        {
            "abstract": "In this paper, we address a driver-vehicle scheduling problem in a limousine rental company. Given a set of trips to be covered, the goal consists in finding a drivervehicle schedule that serves the maximum workload and optimizes several economic objectives while satisfying a set of imperative constraints. In this context, we propose a simultaneous scheduling of drivers and vehicles. The problem is modeled using the notion of partial consistent assignment. The solution approach is composed of two phases: the first one is based on constraint programming techniques and leads to the construction of an initial solution, improved in a second phase by a Simulated Annealing algorithm. Significant gains on the resulting solutions are systematically obtained in terms of quality, operational costs and elaboration time, compared to the current practice in the company.",
            "group": 2830,
            "name": "10.1.1.139.2067",
            "keyword": "Key wordsSimultaneous Vehicle and Driver SchedulingPartial Consistent AssignmentSimulated AnnealingConstraint Handling",
            "title": "Simultaneous Vehicle and Driver Scheduling: a Case Study in a Limousine Rental Company Abstract"
        },
        {
            "abstract": "Abstract \u2013 Genetic algorithms have been used successfully as a global optimization method when the search space is very large. To characterize and analyze the performance of genetic algorithms on a cluster of workstations, a parallel version of the GENESIS 5.0 was developed using PVM 3.3. This version, called VMGENESIS, was used to study a nonlinear leastsquares problem. Performance results show that linear speedups can be achieved if the basic distributed genetic algorithm is combined with a simple dynamic load-balancing mechanism. Results also show that the quality of search changes significantly with the number of processors involved in the computation and with the frequency of communication. 1",
            "group": 2831,
            "name": "10.1.1.139.2283",
            "keyword": "",
            "title": "A study of a non-linear optimization problem using a distributed genetic algorithm"
        },
        {
            "abstract": "In this paper, we present a self-organized computing approach to solving hard combinatorial optimization problems, e.g., the traveling salesman problem (TSP). First of all, we provide an analytical characterization of such an approach, by means of formulating combinatorial optimization problems into autonomous multi-entity systems and thereafter examining the microscopic characteristics of optimal solutions with respect to discrete state variables and local fitness functions. Next, we analyze the complexity of searching in the solution space based on the representation of fitness network and the observation of phase transition. In the second part of the paper, following the analytical characterization, we describe a decentralized, self-organized algorithm for solving combinatorial optimization problems. The validation results obtained by testing on a set of benchmark TSP instances have demonstrated the effectiveness and efficiency of the proposed algorithm. The link established between the microscopic characterization of hard computational systems and the design of self-organized computing methods provides a new way of studying and tackling hard combinatorial optimization problems.",
            "group": 2832,
            "name": "10.1.1.139.2377",
            "keyword": "",
            "title": "Self-Organized Combinatorial Optimization"
        },
        {
            "abstract": "Scheduling is concerned with allocating limited resources to tasks to optimize certain objective functions. Due to the popularity of the Total Quality Management concept, ontime delivery of jobs has become one of the crucial factors for customer satisfaction. Scheduling plays an important role in achieving this goal. Recent developments in scheduling theory have focused on extending the models to include more practical constraints. Furthermore, due to the complexity studies conducted during the last two decades, it is now widely understood that most practical problems are NP-hard. This is one of the reasons why local search methods have been studied so extensively during the last decade. In this paper, we review briefly some of the recent extensions of scheduling theory, the recent developments in local search techniques and the new developments of scheduling in practice. Particularly, we survey two recent extensions of theory: scheduling with a 1-job-on-r-machine pattern and machine scheduling with availability constraints. We also review several local search techniques, including simulated annealing, tabu search, genetic algorithms and constraint guided heuristic search. Finally, we study the robotic cell scheduling problem, the automated guided vehicles scheduling problem, and the hoist scheduling problem.",
            "group": 2833,
            "name": "10.1.1.139.2468",
            "keyword": "",
            "title": " Current trends in deterministic scheduling"
        },
        {
            "abstract": "Hiermit versichere ich, dass ich diese Diplomarbeit selbst\u00e4ndig verfasst habe. Ich habe dazu keine anderen als die angegebenen Quellen und Hilfsmittel verwendet.",
            "group": 2834,
            "name": "10.1.1.139.2845",
            "keyword": "",
            "title": "Prof. Dr. Luc de Raedt (Albert-Ludwigs-Universit\u00e4t Freiburg)"
        },
        {
            "abstract": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, to generate prices and derivatives commonly used by option traders.  ",
            "group": 2835,
            "name": "10.1.1.139.3348",
            "keyword": "High-resolution path-integral...-2- Lester Ingber",
            "title": "High-resolution path-integral development of financial options"
        },
        {
            "abstract": "In this paper, an improved Two-Stage Simulated Annealing algorithm is presented for the Minimum Linear Arrangement Problem for Graphs. This algorithm integrates several distinguished features including an efficient heuristic to generate good quality initial solutions, a highly discriminating evaluation function, a special neighborhood function and an effective cooling schedule. The algorithm is evaluated on a set of 30 well-known benchmark instances of the literature and compared with several state-of-the-art algorithms, showing improvements of 17 previous best results.",
            "group": 2836,
            "name": "10.1.1.139.3482",
            "keyword": "Key wordsLinear ArrangementEvaluation FunctionHeuristicsSimulated",
            "title": "An Effective Two-Stage Simulated Annealing Algorithm for the Minimum Linear Arrangement Problem Abstract"
        },
        {
            "abstract": " ",
            "group": 2837,
            "name": "10.1.1.139.3698",
            "keyword": "",
            "title": "RDLC2: The Ramp Model, . . . "
        },
        {
            "abstract": "We present a model of surveillance based on the detection of community structure in social networks. We examine the extent of network topology information an adversary is required to gather in order to obtain high quality intelligence about community membership. We show that selective surveillance strategies can improve the adversary\u2019s resource efficiency. However, the use of counter-surveillance defence strategies can significantly reduce the adversary\u2019s capability. We analyze two adversary models drawn from contemporary computer security literature, and explore the dynamics of community detection and hiding in these settings. Our results show that in the absence of counter-surveillance moves, placing a mere 8 % of the network under surveillance can uncover the community membership of as much as 50 % of the network. Uncovering all community information with targeted selection requires half the surveillance budget where parties use anonymous channels to communicate. Finally, the most determined covert community can escape detection by adopting decentralized counter-surveillance techniques even while facing an adversary with full topology knowledge- by investing in a small counter-surveillance budget, a rebel group can induce a steep increase in the false negative ratio. 1",
            "group": 2838,
            "name": "10.1.1.139.3790",
            "keyword": "",
            "title": "The Economics of Covert Community Detection and Hiding"
        },
        {
            "abstract": "Abstract \u2014 This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued \u2018black box \u2019 function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the \u2018vanilla \u2019 gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. I.",
            "group": 2839,
            "name": "10.1.1.139.4251",
            "keyword": "",
            "title": "Natural Evolution Strategies"
        },
        {
            "abstract": "",
            "group": 2840,
            "name": "10.1.1.139.4862",
            "keyword": "Key wordsBandwidth MinimizationHeuristicsSimulated Annealing",
            "title": "Eduardo Rodriguez-Tello a, \u2217,Jin-KaoHao a"
        },
        {
            "abstract": " The problem of automatically selecting simulation models for autonomous agents depending on their current intentions and beliefs is considered in this paper. The intended use of the models is for prediction, filtering, planning and other types",
            "group": 2841,
            "name": "10.1.1.139.5640",
            "keyword": "",
            "title": "Towards Automatic Model Generation by Optimization"
        },
        {
            "abstract": "...is an algorithm based on a simulated annealing algorithm (SA), a relatively recent algorithm for solving hard combinatorial optimization problems. The LEO algorithm was successfully applied to a facility layout problem, a scheduling problem and a line balancing problem. In this paper we will try to apply the LEO algorithm to the problem of optimizing a manufacturing simulation model, based on a Steelworks Plant. This paper also demonstrates the effectiveness and versatility of this algorithm. We compare the search effort of this algorithm with a Genetic Algorithm (GA) implementation of the same problem.  ",
            "group": 2842,
            "name": "10.1.1.139.5731",
            "keyword": "",
            "title": "  SIMULATION OPTIMIZATION WITH THE LINEAR MOVE AND EXCHANGE MOVE OPTIMIZATION ALGORITHM"
        },
        {
            "abstract": "Sensor-actuator networks (SANs) are a new approach for the physically-based animation of objects. The user supplies the configuration of a mechanical system that has been augmented with simple sensors and actuators. It is then possible to automatically discover many possible modes of locomotion for the given object. The SANs providing the control for these modes of locomotion are simple in structure and produce robust control. A SAN consists of a small non-linear network of weighted connections between sensors and actuators. A stochastic procedure for finding and then improving suitable SANs is given. Ten different creatures controlled by this method are presented.",
            "group": 2843,
            "name": "10.1.1.139.6252",
            "keyword": "CR CategoriesG.3 [Probability and StatisticsProbabilistic AlgorithmsI.2.6 [Artificial IntelligenceLearningRoboticsI.3.7 [Computer GraphicsThree-Dimensional Graphics and Realism- animationI.6.3 [Simulation and Modeling]- Applications",
            "title": "Sensor-Actuator Networks"
        },
        {
            "abstract": "Abstract: Conflict resolution is an important part of many intelligent systems such as production systems, planning tools and cognitive architectures. For example, the ACT\u2013R cognitive architectire [Anderson and Lebiere, 1998] uses a powerful conflict resolution theory that allowed for modelling many characteristics of human decision making. The results of more recent works, however, pointed to the need of revisiting the conflict resolution theory of ACT\u2013R to incorporate more dynamics. In the proposed theory the conflict is resolved using the estimates of the expected costs of production rules. The method has been implemented as a stand alone search program as well as an add\u2013on to the ACT\u2013R architecture replacing the standard mechanism. The method expresses more dynamic and adaptive behaviour. The performance of the algorithm shows that it can be successfully used as a search and optimisation technique. keywods: conflict resolution, decision making, search, optimisation, rule\u2013based systems, cognitive modelling. 1",
            "group": 2844,
            "name": "10.1.1.139.8161",
            "keyword": "",
            "title": "Conflict resolution by random estimated costs"
        },
        {
            "abstract": " ",
            "group": 2845,
            "name": "10.1.1.140.346",
            "keyword": "",
            "title": "A Heuristic Search Approach to Solving the . . . "
        },
        {
            "abstract": "Although a Ph.D. dissertation has a single name on the cover, it would be difficult to complete a Ph.D. program without a tremendous amount of support. There are many others who contributed to the success of this work. To these individuals I express my most sincere thanks. I would first like to recognize my advisor, S. Mancoridis. Dr. Mancoridis was a superior mentor, providing me with much needed encouragement, support, and guid-ance. I am also very proud to be the first member of Professor Mancoridis \u2019 research group, which now has more than 10 active members inclusive of undergraduate, grad-uate, and doctoral students. I would also like to thank the other members of my committee: J. Johnson, C. Rorres, A. Shokoufandeh, and R. Chen for their input and thoughtful direction. Finally, I want to recognize a former member of my committee, Dr. L. Perkovic, for his contributions to my research. During my Ph.D. studies I also worked full-time as a software architect. Special thanks to my current and former supervisors who provided me with flexibility and",
            "group": 2846,
            "name": "10.1.1.140.588",
            "keyword": "",
            "title": "Acknowledgements"
        },
        {
            "abstract": "Abstract- This paper presents and compares three heuristics for the combinatorial auction problem. Besides a simple greedy (SG) mechanism, two metaheuristics, a simulated annealing (SA), and a genetic algorithm (GA) approach are developed which use the combinatorial auction process to find an allocation with maximal revenue for the auctioneer. The performance of these three heuristics is evaluated in the context of a price controlled resource allocation process designed for the control and provision of distributed information services. Comparing the SG and SA method shows that depending on the problem structure the performance of the SA is up to \u00a0\u00a2\u00a1\u00a4\u00a3 higher than the performance of the sim-ple greedy allocation method. The proposed GA approach, using a random key encoding, results in a further improvement of the solution quality. Although the metaheuristic approaches result in higher search performance, the computational effort in terms of used CPU time is higher in comparison to the simple greedy mechanism. However, the absolute overall computation time is low enough to enable real-time execution in the considered IS application domain. 1",
            "group": 2847,
            "name": "10.1.1.140.2875",
            "keyword": "",
            "title": "Optimization heuristics for the combinatorial auction problem"
        },
        {
            "abstract": "Enormous progress has been made in recent years in the nanostructuring of materials, and a variety of techniques are available for fabricating bulk materials with a desired nanostructure. However, the higher levels of organization have been neglected, and nanostructured materials are assembled into macroscopic structures using techniques that are not essentially different from those used for conventional materials. We argue that the creation of complex hierarchical systems, with specific structures from the nanoscale up through the macroscale, and especially post-Moore\u2019s Law nanocomputers, will require a close alignment of computational and physical processes.",
            "group": 2848,
            "name": "10.1.1.140.3304",
            "keyword": "",
            "title": "Computation and Nanotechnology: Toward the Fabrication of Complex Hierarchical Structures"
        },
        {
            "abstract": "Proceedings",
            "group": 2849,
            "name": "10.1.1.140.4043",
            "keyword": "",
            "title": "BibTEX-Eintrag f\u00fcr Online-Proceedings:"
        },
        {
            "abstract": "Abstract. The Grid vision is to allow heterogeneous computational resources to be shared and utilised globally. Grid users are able to submit tasks to remote resources for execution. However, these resources may be unreliable and there is a risk that submitted tasks may fail or cost more than expected. The notion of trust is often used in agent-based systems to manage such risk, and in this paper we apply trust to the problem of resource selection in Grid computing. We propose a number of resource selection algorithms based upon trust, and evaluate their effectiveness in a simulated Grid. 1",
            "group": 2850,
            "name": "10.1.1.140.5125",
            "keyword": "",
            "title": "Experience-based trust: Enabling effective resource selection in a grid environment"
        },
        {
            "abstract": "thermodynamical model study for an energy saving algorithm",
            "group": 2851,
            "name": "10.1.1.140.5130",
            "keyword": "Fuzzy systemsHybrid Artificial Intelligence SystemsReal World ApplicationsThermodynamical ModellingElectric Energy saving",
            "title": "A"
        },
        {
            "abstract": "To my family. ii The motivation for this thesis has been to improve the robustness of image processing appli-cations to motion estimation failure and in particular applications for the restoration of archived film. The thesis has been divided into two parts. The first part is concerned with the development of an missing data detection algorithm that is robust to Pathological Motion (PM). PM can cause clean image data to be misdiagnosed as missing data. The proposed algorithm uses a probabilistic framework to jointly detect PM and missing data. A five frame window is employed to detect missing data instead of the typical three frame window. This allows the temporally impulsive intensity profile of blotches to be distinguished from the quasi-periodic profile of PM. A second diagnostic for PM is defined on the local motion fields of the five frame window. This follows the observation that Pathologi-cal Motion causes the Smooth Local Flow Assumption of motion estimators to be violated. A ground truth comparison with standard missing data detectors shows that the proposed algo-",
            "group": 2852,
            "name": "10.1.1.140.5333",
            "keyword": "",
            "title": "Motion Estimation Reliability and the Restoration of Degraded Archived Film"
        },
        {
            "abstract": "One of the central challenges of computer science is to get a computer to solve a problem without explicitly programming it. In particular, it would be desirable to have a problemindependent system whose input is a high-level statement of a problem's requirements and whose output is a working computer program that solves the given problem. The challenge is to make computers do what needs to be done, without having to tell the computer exactly how to do it. Alan Turing recognized that machine intelligence may be realized using a biological approach. In his 1948 essay \"Intelligent Machines \" [9], Turing made the connection between search techniques and the challenge of getting a computer to solve a problem without explicitly programming it. Further research into intelligence of machinery will probably be very greatly concerned with \u201csearches\u201d Turing then identified three broad approaches by which search techniques might be used to automatically create an intelligent computer program. One approach that Turing identified is a search through the space of integers representing candidate computer programs. This approach reflects the orientation of much of Turing\u2019s own work on the logical basis for computer algorithms. A second approach that Turing identified is the \"cultural search \" which relies on knowledge and expertise acquired over a period of years from others. This approach is akin to presentday knowledge-based systems and expert systems. The third approach that Turing identified is \u201cgenetical or evolutionary search. \u201d Turing said, There is the genetical or evolutionary search by which a combination of genes is looked for, the criterion being the survival value. The remarkable success of this search confirms to some extent the idea that intellectual activity consists mainly of various kinds of search. Turing did not specify in this essay how to conduct the \"genetical or evolutionary search\" for a computer program.",
            "group": 2853,
            "name": "10.1.1.141.174",
            "keyword": "",
            "title": "Genetic Programming: Turing's Third Way to . . . "
        },
        {
            "abstract": "Abstract\u2014The design of bit-interleaved coded continuous phase modulation (CPM) is characterized by the code rate, alphabet size, modulation index, and pulse shape. This paper outlines a methodology for determining the optimal values of these parameters under bandwidth and receiver complexity constraints. The cost function that is used to drive the optimization is the informationtheoretic minimum Eb/N0, which is found by evaluating the constrained channel capacity. The capacity can be estimated using Monte Carlo integration. A search for optimal parameters is conducted over a range of coded CPM parameters, bandwidth efficiencies, and channels. To limit complexity and allow any modulation index to be considered, the receiver is constrained to use a soft-output differential phase detector. Bit error rate curves using a binary turbo code confirm that the constrained capacity is a very good indicator of the performance of the complete system. Index Terms\u2014Bit-interleaved coded modulation (BICM), capacity, continuous phase modulation (CPM), differential phase detector. I.",
            "group": 2854,
            "name": "10.1.1.141.1002",
            "keyword": "",
            "title": "Capacity-Based Parameter Optimization of Bit-Interleaved Coded CPM With Differential Detection"
        },
        {
            "abstract": "This electronic book is copyrighted, and protected by the copyrightlaws of the United States. This (and all associated documents in the system) must contain the above copyright notice. If this electronic book is used anywhere other than the project's original system, CSEP must be noti ed in writing (email is acceptable) and the copyright noticemust remain intact.",
            "group": 2855,
            "name": "10.1.1.141.1449",
            "keyword": "2) list of computational methodsline searchsteepest descentconjugate gradientdiscrete",
            "title": "MO Mathematical Optimization"
        },
        {
            "abstract": "to make digital or hard copies of portions of this work for personal or classroom use is granted provided that the copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise requires prior specific permission by the publisher mentioned above.",
            "group": 2856,
            "name": "10.1.1.141.1984",
            "keyword": "",
            "title": "Massively Parallel All Atom Protein Folding in a Single Day"
        },
        {
            "abstract": "Abstract: The problem of deploying sensors in a large water distribution network is considered, in order to detect the malicious introduction of contaminants. It is shown that a large class of realistic objective functions\u2014such as reduction of detection time and the population protected from consuming contaminated water\u2014exhibits an important diminishing returns effect called submodularity. The submodularity of these objectives is exploited in order to design efficient placement algorithms with provable performance guarantees. The algorithms presented in this paper do not rely on mixed integer programming, and scale well to networks of arbitrary size. The problem instances considered in the approach presented in this paper are orders of magnitude \ufffda factor of 72 \ufffd larger than the largest problems solved in the literature. It is shown how the method presented here can be extended to multicriteria optimization, selecting placements robust to sensor failures and optimizing minimax criteria. Extensive empirical evidence on the effectiveness of the method presented in this paper on two benchmark distribution networks, and an actual drinking water distribution system of greater than 21,000 nodes, is presented.",
            "group": 2857,
            "name": "10.1.1.141.6653",
            "keyword": "",
            "title": "Efficient Sensor Placement Optimization for Securing Large Water Distribution Networks"
        },
        {
            "abstract": "mapping is an important but intractable optimization problem. In the paper, we use simulated annealing to tackle the mapping problem in 2D mesh NoCs. In particular, we combine a clustering technique with the simulated annealing to speed up the convergence to near-optimal solutions. The clustering exploits the connectivity and distance relation in the network architecture as well as the locality and bandwidth requirements in the core communication graph. The annealing is cluster-aware and may be dynamically constrained within clusters. Our experiments suggest that simulated annealing can be effectively used to solve the mapping problem with a scalable size, and the combined strategy improves over the simulated annealing in execution time by up to 30 % without compromising the quality of solutions. I.",
            "group": 2858,
            "name": "10.1.1.141.7605",
            "keyword": "",
            "title": "Cluster-based Simulated Annealing for Mapping Cores onto 2D Mesh Networks on Chip"
        },
        {
            "abstract": "The authors develop a two-timescale simultaneous perturbation stochastic approximation algorithm for simulation-based parameter optimization over discrete sets. This algorithm is applicable in cases where the cost to be optimized is in itself the long-run average of certain cost functions whose noisy estimates are obtained via simulation. The authors present the convergence analysis of their algorithm. Next, they study applications of their algorithm to the problem of admission control in communication networks.They study this problem under two different experimental settings and consider appropriate continuous time queuing models in both settings.Their algorithm finds optimal threshold-type policies within suitable parameterized classes of these. They show results of several experiments for different network parameters and rejection cost. The authors also study the sensitivity of their algorithm with respect to its parameters and step sizes. The results obtained are along expected lines.",
            "group": 2859,
            "name": "10.1.1.142.450",
            "keyword": "Discrete parameter optimizationstochastic approximation algorithmstwo-timescale SPSA",
            "title": "A discrete parameter stochastic approximation algorithm for simulation optimization\u201d, Simulation"
        },
        {
            "abstract": "A road network usually has to fulfill two requirements: (i) it should as far as possible provide direct connections between nodes, to avoid large detours, (ii) the costs for road construction and maintainance, which are assumed proportional to the total length of the roads, should be low. The optimal solution is a compromise between these contradicting demands, which in our model can be weighted by a parameter. The road optimization problem belongs to the class of frustrated optimization problems. In this paper, a special class of evolutionary strategies, such as the Boltzmann and Darwin and mixed strategies, are applied to find differently optimized solutions (graphs of varying density) for the road network in dependence on the degree of frustration. We show, that the optimization process occurs on two different time scales. In the asymptotic limit, a fixed relation between the mean connection distance (detour) and the total length (costs) of the network exists, which defines a range of possible compromises. Further, we investigate the density of states, which describes the number of solutions with a certain fitness value in the stationary regime. We find that the network problem belongs to a class of optimization problems, where more effort in optimization certainly yields better solutions. An analytical approximation for the relation between effort and improvement is derived.",
            "group": 2860,
            "name": "10.1.1.142.481",
            "keyword": "",
            "title": "Optimization of Road Networks Using Evolutionary Strategies"
        },
        {
            "abstract": "  Many combinatorial (optimisation) problems have natural models based on, or including, set variables and set constraints. This modelling device has been around for quite some time in the constraint programming area, and proved its usefulness in many applications. This paper introduces set variables and set constraints also in the local search area. It presents a way of representing set variables in the local search context, where we deal with concepts like transition functions, neighbourhoods, and penalty costs. Furthermore, some common set constraints and their penalty costs are defined. These constraints are later used to model three problems and some initial experimental results are reported.  ",
            "group": 2861,
            "name": "10.1.1.142.1023",
            "keyword": "",
            "title": "Set Variables and Local Search"
        },
        {
            "abstract": "Abstract\u2014In many environments where autonomous air or ground vehicles are used to collect information, there will be a known prioritization of areas of the environment where most valuable information will be found. Over time, priorities may change with areas losing value or suddenly becoming important. In this paper, we present an approach to planning paths for vehicles collecting information in such environments, such that they maximize the overall system information gain over time. A key feature of this path planning problem is that there is not a single or small set of goal points to which the vehicles should try to reach, instead information is collected over the entire path without a particular goal in mind. We present a planning approach, which rapidly expands a search tree, inspired by an RRT planner by choosing promising nodes to expand and expanding them randomly. Genetic algorithms are used to learn sets of configuration parameters for the planner, i.e., how to expand which nodes. Results show that the learned planner gets more substantially information than pre-defined paths in a variety of domains.",
            "group": 2862,
            "name": "10.1.1.142.1496",
            "keyword": "Path planning under uncertaintyRapidlyexploring Random TreeGenetic algorithmRandomized",
            "title": "Path Planning for Autonomous Information Collecting Vehicles"
        },
        {
            "abstract": "An introduction is presented to numerical methods, by which the behavior of complex metallic alloys can be simulated. We primarily consider the molecular dynamics (MD) technique as implemented in our software package IMD, where Newton\u2019s equations of motion are solved for all atoms in a solid. After a short discourse on integration algorithms, some possible types of interactions are addressed. Already simple model potentials, as for example the Lennard-Jones-Gauss potential, can give rise to complex structures, where the characteristic length scales of the order by far exceed the range of the pair interaction. Realistic interactions are modelled by highly parametrized effective potentials, like the EAM (Embedded Atom Method) potential. Our program potfit allows to fit the parameters such that data from experiment or from ab-initio calculations are well reproduced. Several applications of the methods are outlined, notably the simulation of aluminium diffusion in quasicrystalline d-Al-Ni-Co, the computation of the phonon dispersion via the dynamical structure factor of MgZn2, the propagation of cracks in NbCr2, and an order-disorder phase transition in CaCd6. ",
            "group": 2863,
            "name": "10.1.1.142.2071",
            "keyword": "",
            "title": "Simulating structure and physical properties of complex metallic alloys"
        },
        {
            "abstract": "This paper presents a method to simultaneously produce multiple solutions to unconstrained multi-objective optimization problems. The proposed methodology uses populations of sets instead of populations of individuals and iterative calls to a Genetic Algorithm (IGA) to obtain a set of solutions spread across the Pareto set in the objective space. The superiority of such an approach to single run, conventional population Pareto GAs is shown. The various difficulties of the algorithm and the methods used to overcome them are detailed. Finally, the paper expands upon how this method can be used with or without user inputs, and shows an analysis of its performance by applying it to a succession of increasingly difficult problems, identifying its range of application.",
            "group": 2864,
            "name": "10.1.1.142.2483",
            "keyword": "multi-objectiveParetooptimizationGenetic Algorithms",
            "title": "DETC99/DAC-8576 MULTIOBJECTIVE OPTIMIZATION BY ITERATIVE GENETIC ALGORITHM"
        },
        {
            "abstract": "",
            "group": 2865,
            "name": "10.1.1.142.4452",
            "keyword": "PART ONE............................................................................................",
            "title": "Spectral-Spatial Analysis of Remote Sensing Data: . . . "
        },
        {
            "abstract": "Requirements engineering for complex systems in resource-constrained environments produces an intricate set of dependencies. Finding ways to feasibly attain those requirements while adhering to resource limits can be extremely challenging. Here, we describe an approach that helps decisionmakers better explore this complex maze of data. The approach rests upon a combination of existing technologies drawn from the computer science milieu \u2013 most notably, heuristic search and clustering, coupled with appropriate visualizations. The context for this work is a requirements engineering method in use at the Jet Propulsion Laboratory, where its primary application area has been planning the development of spacecraft technologies. In this context there is a need to assess the range of risks that threaten to impede requirements attainment, and to plan for their satisfactory (i.e., cost-effective) mitigation. Because of cost and resource constraints, project managers are forced to make difficult choices among interrelated sets of mitigations activities. We show how the use of heuristic search reveals the overall cost-benefit space of requirements attainment, from knowledge of which the project managers can identify feasible solution neighborhoods. Within those neighborhoods clustering is then applied to distill from a plethora of solutions a manageable number of distinct solution categories. Visualizations are used to present this information in ways that assist managers to make their key decisions. 1.",
            "group": 2866,
            "name": "10.1.1.142.4767",
            "keyword": "",
            "title": "Mining Complex Requirements Specifications to Mitigate Risk via Clustering"
        },
        {
            "abstract": "We consider the problem of detecting minefields using aerial images. A first stage of image processing has reduced the image to a set of points, each one representing a possible mine. Our task is to decide which ones are actual mines. We assume that the minefield consists of approximately parallel rows of mines laid out according to a probability distribution that encourages evenly spaced, linear patterns. The noise points are assumed to be distributed as a Poisson process. We construct a Markov chain Monte Carlo algorithm to estimate the model and obtain posterior probabilities for each point being a mine. The algorithm performs well on",
            "group": 2867,
            "name": "10.1.1.142.7514",
            "keyword": "Probabilistic ClassificationSpatial point process",
            "title": "Detecting Mines in Minefields with Linear Characteristics"
        },
        {
            "abstract": "In this research we introduce the problem of the binary matrix partitioning in a biological context. Our idea is to use SNP matrix to construct a set of phylogenetic networks to retrieve underlying biological meanings and dependencies. We emphasize stochastic methods for matrix clustering and briefly describe the search algorithm. It will allow us to perform fast distributed search on huge biological data for calculating person\u2019s similarity. 1",
            "group": 2868,
            "name": "10.1.1.142.7669",
            "keyword": "",
            "title": "Stochastic Approach to Binary Matrix Partitioning for Phylogenetic Networks"
        },
        {
            "abstract": "Abstract A road network usually has to fulfill two requirements: (i) it should as far as possible provide direct connections between nodes, to avoid large detours, (ii) the costs for road construction and maintainance, which are assumed proportional to the total length of the roads, should be low. The optimal solution is a compromise between these contradicting demands, which in our model can be weighted by a parameter. The road optimization problem belongs to the class of frustrated optimization problems. In this paper, evolutionary strategies, such as the Boltzmann and Darwin strategy, are applied to find different optimized solutions (graphs of varying density) for the road network in dependence on the degree of frustration. We show, that the optimization process occurs on two different time scales. In the asymptotic limit, a fixed relation between the mean connection distance (detour) and the total length (costs) of the network exists, which defines a range of possible compromises. Further, we investigate the density of states, which describes the number of solutions with a certain fitness value in the stationary regime. We find that the network problem belongs to a class of optimization problems, where more effort in optimization certainly yields better solutions. An analytical approximation for the relation between effort and improvement is derived.",
            "group": 2869,
            "name": "10.1.1.142.8169",
            "keyword": "networkevolutionary strategyfrustrated problemcompromise",
            "title": "Optimization of road networks using evolutionary strategies"
        },
        {
            "abstract": "In physical, biological, technological and social systems, interactions between units give rise to intricate networks. These\u2014typically non-trivial\u2014structures, in turn, critically affect the dynamics and properties of the system. The focus of most current research on complex networks is, still, on global network properties. A caveat of this approach is that the relevance of global properties hinges on the premise that networks are homogeneous, whereas most real-world networks have a markedly modular structure. Here, we report that networks with different functions, including the Internet, metabolic, air transportation and protein interaction networks, have distinct patterns of connections among nodes with different roles, and that, as a consequence, complex networks can be classified into two distinct functional classes on the basis of their link type frequency. Importantly, we demonstrate that these structural features cannot be captured by means of often studied global properties.  ",
            "group": 2870,
            "name": "10.1.1.143.326",
            "keyword": "",
            "title": "Classes of complex networks defined by role-to-role connectivity profiles"
        },
        {
            "abstract": "ABSTRACT In high-dimensional data, one often seeks a few interesting low-dimensional projections which reveal important aspects of the data. Projection pursuit is a procedure for searching high-dimensional data for interesting low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Very few projection pursuit indices incorporate class or group information in the calculation, and hence can be adequately applied to supervised classification problems. We introduce new indices derived from linear discriminant analysis that can be used for exploratory supervised classification.",
            "group": 2871,
            "name": "10.1.1.143.982",
            "keyword": "Key WordsClassificationData miningGene expressionLinear discriminant analysisMicroarray data analysisMultivariate dataProjection pursuit",
            "title": "Projection pursuit for exploratory supervised classification"
        },
        {
            "abstract": " ",
            "group": 2872,
            "name": "10.1.1.143.3115",
            "keyword": "",
            "title": "A Study of Artificial Immune . . . "
        },
        {
            "abstract": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1",
            "group": 2873,
            "name": "10.1.1.143.3596",
            "keyword": "",
            "title": "Forest rescoring: Faster decoding with integrated language models"
        },
        {
            "abstract": "published in Parallel Computing:",
            "group": 2874,
            "name": "10.1.1.143.4817",
            "keyword": "",
            "title": "A Parallel Adaptive Algorithm to Improve Precision of Time Series Identification"
        },
        {
            "abstract": "Existing FPGA-based logic emulators suffer from limited inter-chip communication bandwidth, resulting in low gate utilization (10 to 20 percent). This resource imbalance increases the number of chips needed to emulate a particular logic design and thereby decreases emulation speed, since signals must cross more chip boundaries. Current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). These logical wires are not active simultaneouslyand are only switched at emulation clock speeds. Virtual wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. A virtual wire represents a connection from a logical output on one FPGA to a logical input on another FPGA. Virtual wires not only increase usable bandwidth, but also relax the absolute limits imposed on gate utilization. The resulting improvement in bandwidth reduces the need for global interconnect, allowing effective use of low dimension inter-chip connections (such as nearest-neighbor). Nearest-neighbor topologies, coupled with the ability of virtual wires to overlap communication with computation, can even improve emulation speeds. We present the concept of virtual wires and describe our first implementation, a \u201csoftwire \u201d compiler which utilizes static routing and relies on minimal hardware support. Results from compiling netlists for the 18K gate Sparcle microprocessor and the 86K gate Alewife Communications and Cache Controllerindicate that virtual wires can increase FPGA gate utilizationbeyond 80 percent withouta significant slowdown in emulation speed.",
            "group": 2875,
            "name": "10.1.1.143.5350",
            "keyword": "FPGAlogic emulationprototypingreconfigurable architecturesstatic routingvirtual wires",
            "title": "  Virtual Wires: Overcoming Pin Limitations in FPGA-based Logic Emulators"
        },
        {
            "abstract": "The parameter configuration of a network protocol can be formulated as a black-box optimization problem with network simulation evaluating the performance of the blackbox, i.e., the network. This paper proposes a unified search framework (USF) to handle such large-scale black-box optimization problems. The framework is designed to provides a general platform on which tailored optimization algorithms can be constructed easily for various types of problems. Therefore, it can be applied to the configuration of different network protocols. In the USF, various samplers are provided as basic building blocks and each of them implements a certain search technique. For a specific problem, a selection of samplers can be used to construct an appropriate search algorithm. These samplers are run in parallel and coordinated with various type of memories which selectively store the samples generated by samplers. The USF also include a resource management mechanism, which can manage parallel computing devices, for example, a network of workstations, and allocate the available computing resources to samplers according to the predefined allocation strategy. The benchmark tests are presented in this paper to demonstrate the flexibility and advantages of the USF. 1.",
            "group": 2876,
            "name": "10.1.1.143.7465",
            "keyword": "",
            "title": "A Unified Search Framework for Large-scale Black-box Optimization"
        },
        {
            "abstract": "To learn a language, the learners must first learn its words, the essential building blocks for utterances. The difficulty in learning words lies in the unavailability of explicit word boundaries in speech input. The learners have to infer lexical items with some innately endowed learning mechanism(s) for regularity detection- regularities in the speech normally indicate word patterns. With respect to Zipf's least-effort principle and Chomsky's thoughts on the minimality of grammar for human language, we hypothesise a cognitive mechanism underlying language learning that seeks for the least-effort representation for input data. Accordingly, lexical learning is to infer the minimal-cost representation for the input under the constraint of permissible representation for lexical items. The main theme of this thesis is to examine how far this learning mechanism can go in unsupervised lexical learning from real language data without any pre-defined (e.g., prosodic and phonotactic) cues, but entirely resting on statistical induction of structural patterns for the most economic representation for the data. We first review",
            "group": 2877,
            "name": "10.1.1.143.7571",
            "keyword": "",
            "title": "Unsupervised Lexical Learning as Inductive Inference"
        },
        {
            "abstract": "Zellner and Revankar in their paper \u201cGeneralized Production Functions \u201d introduced a production function, which was illustrated by fitting the generalized Cobb-Douglas function to the U.S. data for Transportation Equipment Industry. For estimating the production function, they used a method in which one of the parameters (theta) is repeatedly chosen at the trial basis and other parameters are estimated so as to obtain the global optimum of the likelihood function. We show that this method of Zellner and Revankar (ZR) is caught into a local optimum trap and the estimated parameters reported by ZR are somewhat sub-optimal. Using the Differential Evolution (DE) and the Repulsive Particle Swarm (RPS) methods, we re-estimate the parameters of the ZR production function with data used by ZR and show that our estimates of parameters are better than those of ZR. We also find that the returns to scale do not vary with the size of output in the manner reported by ZR.",
            "group": 2878,
            "name": "10.1.1.143.8132",
            "keyword": "",
            "title": "Estimation of Zellner-Revankar Production Function Revisited"
        },
        {
            "abstract": "Abstract-Self-Assembly is a powerful autopoietic mechanism ubiquitous throughout the natural world. It may be found at the molecular scale and also at astronomical scales. Self-assembly power lays in the fact that it is a distributed, not-necessarily synchronous, control mechanism for the bottom-up manufacture of complex systems. Control of the assembly process is shared across a myriad of elemental components, none of which has either the storage or the computation capabilities to know and follow a master plan for the assembly of the intended system. In this paper we present an evolutionary algorithm which is capable of programming the so called \"Wang Tiles \" for the self-assembly of two-dimensional squares. 1",
            "group": 2879,
            "name": "10.1.1.144.1752",
            "keyword": "",
            "title": "1808 Automated Tile Design for Self-Assembly Conformations"
        },
        {
            "abstract": "Simulated Annealing, a wide-spread technique for combinatorial optimisation, is employed to find the optimal candidate in a candidate set, as defined in Optimality Theory (OT). Being a heuristic techniques, simulated annealing does not guarantee to return the correct solution, and yet, some result is always returned within a constant time. Similarly to language production, this time framework can be diminished with the cost of diminishing correctness. We demonstrate how simulated annealing can model linguistic performance, built upon a competence theory, namely, OT. After having applied simulated annealing to OT, we attempt to reproduce empirical observations on metrical stress in Dutch fast speech. Simulated annealing necessitates defining a topology on the candidate set, as well as an exact formulation of the constraint OUTPUT-OUTPUT CORRESPONDENCE.",
            "group": 2880,
            "name": "10.1.1.144.2505",
            "keyword": "",
            "title": "When the Hothead Speaks Simulated Annealing Optimality Theory for Dutch Fast Speech"
        },
        {
            "abstract": "A model-based evaluation of inversions of atmospheric transport, using annual mean mixing ratios, as a tool to monitor fluxes of nonreactive trace substances like CO 2 on a continental scale",
            "group": 2881,
            "name": "10.1.1.144.2800",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "The final purpose of Reactive Search Optimization (RSO) is to simplify the life for the final user of optimization. While researchers enjoy designing algorithms, testing alternatives, tuning parameters and choosing solution schemes \u2014 in fact this is part of their daily life \u2014 the final users \u2019 interests are different: solving a problem in the",
            "group": 2882,
            "name": "10.1.1.144.2856",
            "keyword": "",
            "title": "Reactive Search Optimization: Learning while Optimizing"
        },
        {
            "abstract": "This is the second in a series of three papers that empirically examine the competitiveness of simulated annealing in certain well-studied domains of combinatorial optimization. Simulated annealing is a randomized technique proposed by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi for improving local optimization algorithms. Here we report on experiments at adapting simulated annealing to graph coloring and number partitioning, two problems for which local optimization had not previously been thought suitable. For graph coloring, we report on three simulated annealing schemes, all of which can dominate traditional techniques for certain types of graphs, at least when large amounts of computing time are available. For number partitioning, simulated annealing is not competitive with the differencing algorithm of N. Karmarkar and R. M. Karp, except on relatively small instances. Moreover, if running time is taken into account, natural annealing schemes cannot even outperform multiple random runs of the local optimization algorithms on which they are based, in sharp contrast to the observed performance of annealing on other problems. Simulated annealing is a new approach to the approximate solution of difficult combinational optimization problems. It was originally proposed by Kirkpatrick, Gelatt and Vecchi (1983) and Cerny (1985), who reported promising results based on sketchy experiments. Since then there has been an immense outpouring of papers on the topic, as documented in the extensive bibliographies of Collins, Eglese and Golden (1988) and Van Laarhoven and Aarts (1987). The question of how well annealing stacks up against its more traditional competition has remained unclear, however, for a variety of important applications. The series of papers, of which this is the second, attempts to rectify this situation.",
            "group": 2883,
            "name": "10.1.1.144.3113",
            "keyword": "",
            "title": "NUMBER PARTITIONING"
        },
        {
            "abstract": "(June 2008; comments welcome.)",
            "group": 2884,
            "name": "10.1.1.144.5589",
            "keyword": "",
            "title": "Optimising Monte Carlo Search Strategies for Automated Pattern Detection"
        },
        {
            "abstract": "Over the last thirty years designers have tried to cope with software complexity by organizing system entities into modules, i.e., groups of entities. However, the creation and organization of modules is not straightforward. The criterion with which these modules are built impacts in the maintainability and development of the system. Designers have different interests and personal views of the same system, views that are difficult to communicate and to extract from the code. Poor understanding of this organization increases the complexity of the system e.g., by favoring the addition of duplication and of unexpected rippling effects. This, in turn, lowers the flexibility of the system to changing requirements and leads to a sharp increase in their maintenance cost. To overcome these problems, we present a methodology to manage the locality in objectoriented systems. We develop a model that exploits the contextual information, i.e., the way objects are used by their clients, to understand and improve the organization of classes in the system. With our model we take full advantage of the contextual information of modules to evaluate their cohesion, find misplaced classes, detect hot spots and find the different views that its clients have.",
            "group": 2885,
            "name": "10.1.1.144.6413",
            "keyword": "",
            "title": "vorgelegt von"
        },
        {
            "abstract": "We generalize to general state spaces a Monte Carlo algorithm recently proposed by Wang and Landau (2001). The algorithm can be seen as an adaptive Markov Chain Monte Carlo algorithm where a partition of the state space is chosen and the target density sequentially reweighted is each component of the partition in such a way that at the limit the empirical occupation measure of the components are equal. We develop convergence results for the algorithm. We also show that the algorithm can be used to e ciently self-tune the weights (the so-called pseudo prior) in the simulated tempering. Applied to the reversible jump, the Wang-Landau algorithm can also be used to improve on the between-model acceptance rate of the algorithm. Simulations example are given to illustrate.",
            "group": 2886,
            "name": "10.1.1.144.6820",
            "keyword": "Key wordsMonte Carlo MethodsWang-Landau algorithmMarkov Chain Monte Carlo methodologyAdaptive Markov Chain Monte CarloSimulated TemperingReversible Jump MCMC MSC Numbers60C0560J2760J3565C40",
            "title": "The Wang-Landau Algorithm for Monte Carlo computation in general state spaces"
        },
        {
            "abstract": " ",
            "group": 2887,
            "name": "10.1.1.144.7523",
            "keyword": "",
            "title": "Energy Landscapes of Biopolymers"
        },
        {
            "abstract": "Adaptive resource management is critical to ensuring the quality of real-time distributed applications, particularly for energy-constrained mobile handheld devices. In this context, an optimization that simultaneously considers multiple layers (e.g., application, middleware, operating system) needs to be developed for continuous adaptation of system parameters. The tuning of system parameters greatly affects the system\u2019s ability to meet QoS requirements, and also directly affects the energy consumption and system robustness. We present a novel approach to developing cross-layer optimization for resource limited real-time distributed systems, based on a constraint refinement technique combined with formal specification and feedback from system implementation. Our approach tunes the parameters in a compositional manner allowing coordinated interaction among sub-layer optimizers that enables holistic cross-layer optimization. We present experiments on a realistic multimedia application which demonstrate that constraint refinement enables us to generate robust and near optimal parameter settings. The constraint language can be used as an interface for composition by encapsulating the details of local optimization algorithms. 1",
            "group": 2888,
            "name": "10.1.1.144.7571",
            "keyword": "",
            "title": "Constraint Refinement for Online Verifiable Cross-Layer System Adaptation"
        },
        {
            "abstract": "Abstract \u2014 The Fitts \u2019 law is used in order to predict the time for pointing a target with respect to its size and to the distance to cover. However, this law only predict the average time for pointing the target with a given difficulty. In this paper, we propose to use a fuzzy regression approach, named imprecise regression, which allows us to predict the general tendency (time with respect to the difficulty of the task), together with the imprecision associated with this tendency. In the first time we will present the imprecise regression approach. Then, we will discuss and compare the results obtained with linear regression and with fuzzy imprecise from the data obtained with a mouse pointing experimentation. I.",
            "group": 2889,
            "name": "10.1.1.144.8550",
            "keyword": "",
            "title": "Learning Fitts \u2019 law with imprecise regression Mathieu Serrurier"
        },
        {
            "abstract": "Abstract \u2014 Many studies on machine learning, and more specifically on regression, focus on the search for a precise model, when precise data are available. Therefore, it is well-known that the model thus found may not exactly describe the target concept, due to the existence of learning bias. In order to overcome the problem of too much illusionary precise models, this paper provides a general framework for imprecise regression from non-fuzzy input and output data. The goal of imprecise regression is to find a model that has the better tradeoff between faithfulness w.r.t. data and (meaningful) precision. We propose an algorithm based on simulated annealing for linear and non-linear imprecise regression with triangular and trapezoidal fuzzy sets. This approach is compared with the different fuzzy regression frameworks, especially with possibilistic regression. Experiments on an environmental database show promising results. I.",
            "group": 2890,
            "name": "10.1.1.144.9668",
            "keyword": "",
            "title": "A general framework for imprecise regression Mathieu Serrurier DSNA/R&D"
        },
        {
            "abstract": "c\u25cbLeo Jingyu Lee 2004I hereby declare that I am the sole author of this thesis. I authorize the University of Waterloo to lend this thesis to other institutions or individuals for the purpose of scholarly research.",
            "group": 2891,
            "name": "10.1.1.145.2928",
            "keyword": "",
            "title": "Hidden Dynamic Models for Speech Processing Applications"
        },
        {
            "abstract": "Metaheuristics have been established as one of the most practical approach to simulation optimization. However, these methods are generally designed for combinatorial optimization, and their implementations do not always adequately account for the presence of simulation noise. Research in simulation optimization, on the other hand, has focused on convergent algorithms, giving rise to the impression of a gap between research and practice. This chapter surveys the use of metaheuristics for simulation optimization, focusing on work bridging the current gap between the practical use of such methods and research, and points out some promising directions for research in this area. The main emphasis is on two issues: accounting for simulation noise in the implementation of metaheuristics, and convergence analysis of metaheuristics that is both rigorous and of practical value. To illustrate the key points, three metaheuristics are discussed in some detail and used for examples throughout, namely genetic algorithms, tabu search, and the nested partitions method.",
            "group": 2892,
            "name": "10.1.1.145.3411",
            "keyword": "Simulation optimizationmetaheuristicstabu searchgenetic algorithmsnested partitions",
            "title": "Chapter 23"
        },
        {
            "abstract": " ",
            "group": 2893,
            "name": "10.1.1.145.3807",
            "keyword": "",
            "title": "Stochastic Nested Aggregation for Images and Random Fields"
        },
        {
            "abstract": "Abstract. GRASP, or greedy randomized adaptive search procedure, is a multi-start metaheuristic that repeatedly applies local search starting from solutions constructed by a randomized greedy algorithm. In this chapter we consider ways to hybridize GRASP to create new and more effective metaheuristics. We consider several types of hybridizations: constructive procedures, enhanced local search, memory structures, and cost reformulations. 1.",
            "group": 2894,
            "name": "10.1.1.145.4809",
            "keyword": "",
            "title": "METAHEURISTIC HYBRIDIZATION WITH GRASP"
        },
        {
            "abstract": "This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies\u2019 foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the ACO metaheuristic are defined. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO metaheuristic.",
            "group": 2895,
            "name": "10.1.1.145.6569",
            "keyword": "ant",
            "title": "Ant algorithms for discrete optimization"
        },
        {
            "abstract": "Mobile agents have received much attention recently as a way toe ciently access distributed resources in a low bandwidth network. Planning allows mobile agents to make the best use of the available resources. This thesis studies several planning problems that arise in mobile agent information retrieval and data-mining applications. The general description of the planning problems is as follows: We are given sites at which a certain task might be successfully performed. Each site has an independent probability of success associated with it. Visiting a site and trying the task there requires time (or some other cost matrix) regardless of whether the task is completed successfully or not. Latencies between sites, that is, the travel time between those two sites also have to be taken into account. If the task is successfully completed at a site then the remaining sites need not be visited. The planning problems involve nding the best sequence of sites to be visited, which minimizes the expected time to complete the task. We name the problems Traveling Agent Problems due to their analogy with the Traveling Salesman Problem. This Traveling Agent Problem is NPcomplete in the general formulation. However, in this thesis a polynomial-time algorithm has been",
            "group": 2896,
            "name": "10.1.1.145.6705",
            "keyword": "",
            "title": "Mobile Agent Planning Problems"
        },
        {
            "abstract": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1",
            "group": 2897,
            "name": "10.1.1.145.7155",
            "keyword": "",
            "title": "Forest rescoring: Faster decoding with integrated language models"
        },
        {
            "abstract": "ANT COLONY optimization (ACO) is part of a larger field of research termed ant algorithms or swarm intelligence that deals with algorithmic approaches that are inspired by the behavior of ant colonies and other social insects [1]\u2013[3], [5]. Of particular interest are the collective activities of members of a colony, such as foraging, brood care, or nest building, which utilize mechanisms of self-organization, stigmergic communication [5], [13], and task partitioning. Ant algorithms have been proposed as a novel computational model that replaces the traditional emphasis on control, preprogramming, and centralization with designs featuring autonomy, emergence, and distributed functioning. These designs are proving flexible and robust, they are able to adapt quickly to changing environments, and they continue functioning when individual elements fail. A particularly successful research direction in ant algorithms,",
            "group": 2898,
            "name": "10.1.1.145.7550",
            "keyword": "",
            "title": "Guest Editorial Special Section on Ant Colony Optimization"
        },
        {
            "abstract": "Abstract- The design of Boolean functions with properties of cryptographic significance is a hard task. In this paper, we adopt an unorthodox approach to the design of such functions. Our search space is the set of functions that possess the required properties. It is \u2018Booleanness\u2019 that is evolved. 1",
            "group": 2899,
            "name": "10.1.1.145.7605",
            "keyword": "",
            "title": "Almost Boolean Functions: The Design of Boolean Functions by Spectral Inversion"
        },
        {
            "abstract": "",
            "group": 2900,
            "name": "10.1.1.145.7644",
            "keyword": "",
            "title": "Forest-Based Algorithms in Natural Language Processing"
        },
        {
            "abstract": "Abstract. This paper investigates whether optimisation techniques can be used to evolve artifacts of cryptographic significance which are apparently secure, but which have hidden properties that may facilitate cryptanalysis. We show how this might be done and how such sneaky use of optimisation may be detected. 1",
            "group": 2901,
            "name": "10.1.1.145.7700",
            "keyword": "",
            "title": "Secret agents leave big footprints: how to plant a cryptographic trapdoor, and why you might not get away with it"
        },
        {
            "abstract": "Abstract. Many desirable properties have been identified for Boolean functions with cryptographic applications. Obtaining optimal tradeoffs among such properties is hard. In this paper we show how simulated annealing, a search technique inspired by the cooling processes of molten metals, can be used to derive functions with profiles of cryptographicallyrelevant properties as yet unachieved by any other technique.",
            "group": 2902,
            "name": "10.1.1.145.7740",
            "keyword": "Heuristic OptimisationBoolean FunctionsNonlinearityAutocorrelationCorrelation Immunity",
            "title": "Evolving Boolean Functions Satisfying Multiple Criteria"
        },
        {
            "abstract": "The efficient and effective generation of test-data from high-level models is of crucial importance in advanced modern software engineering. Empirical studies have shown that mutation testing is highly effective. This paper describes how search-based automatic test-data generation methods can be used to find mutation adequate test-sets for Matlab/Simulink models. Categories and Subject Descriptors",
            "group": 2903,
            "name": "10.1.1.145.7922",
            "keyword": "mutation testingtest-data generationautomation",
            "title": "Search-based mutation testing for Simulink models"
        },
        {
            "abstract": " The allocation of scarce spectral resources to support as many user applications as possible while maintaining reasonable quality of service is a fundamental problem in wireless communication. We argue that the problem is best formulated in terms of decision theory. We propose a scheme that takes decision-theoretic concerns (like preferences) into account and discuss the difficulties and subtleties involved in applying standard techniques from the theory of Markov Decision Processes (MDPs) in constructing an algorithm that is decision-theoretically optimal. As an example of the proposed framework, we construct such an algorithm under some simplifying assumptions. Additionally, we present analysis and simulation results that show that our algorithm meets its design goals. Finally, we investigate how far from optimal one well-known heuristic is. The main contribution of our results is in providing insight and guidance for the design of near-optimal admission-control policies.",
            "group": 2904,
            "name": "10.1.1.145.8304",
            "keyword": "",
            "title": "A Decision-Theoretic Approach to Resource Allocation in Wireless Multimedia Networks"
        },
        {
            "abstract": "Abstract \u2014 Substitution boxes are important components in many modern day block and stream ciphers. Their study has attracted a great deal of attention over many years. The development of a variety of cryptosystem attacks has lead to the development of criteria for resilience to such attacks. Some general criteria such as high non-linearity and low autocorrelation have been proposed (providing some protection against attacks such as linear cryptanalysis and differential cryptanalysis). There has been little application of evolutionary search to the development of S-boxes. In this paper we show how a cost function that has found excellent single-output Boolean functions can be generalised to provide improved results for small S-boxes. I.",
            "group": 2905,
            "name": "10.1.1.145.8525",
            "keyword": "",
            "title": "The Design of S-Boxes by Simulated Annealing"
        },
        {
            "abstract": "Security protocols play an important role in modern communications. However, security protocol development is a delicate task, and experience shows that computer security protocols are notoriously difficult to get right. Recently, Clark and Jacob provided a framework for automatic protocol generation based on combinatorial optimization techniques and the symmetric key part of BAN logic. This paper shows how such an approach can be further developed to encompass the full BAN logic without loss of efficiency and thereby synthesize public key protocols and hybrid protocols. 1",
            "group": 2906,
            "name": "10.1.1.145.8540",
            "keyword": "",
            "title": "Automated Design of Security Protocols"
        },
        {
            "abstract": "This is an open access article distributed under the terms of the Creative Commons Attribution License",
            "group": 2907,
            "name": "10.1.1.145.8976",
            "keyword": "",
            "title": "BioMed Central Proceedings Improving the Performance of SVM-RFE to Select Genes in Microarray Data"
        },
        {
            "abstract": "Antenna designers are constantly challenged with the temptation to search for optimum solutions for the design of complex electromagnetic devices. The ability of using numerical methods to accurately and efficiently characterizing the relative quality of a particular design has excited the \u00a9 2008 C. Roy Keys Inc. \u2014 http://redshift.vif.comApeiron, Vol. 15, No. 1, January 2008 79 engineers to apply stochastic global evolutionary optimizers (EO) for this objective. EO techniques have been applied with growing applications to the design of complex electromagnetic systems. Among various EO techniques, Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) have attracted considerable attention and shown superior performance. These schemes are finding popularity in electromagnetics as design tools and problem solvers because of their flexibility, versatility and ability to optimize in complex multimodal search spaces. This paper discusses the design optimization of antenna arrays with special emphasis on evolutionary optimization techniques.",
            "group": 2908,
            "name": "10.1.1.145.9656",
            "keyword": "Antenna ArraysEvolutionary Approaches",
            "title": "Antenna Array Optimization using Evolutionary Approaches"
        },
        {
            "abstract": "The information provided is the sole responsibility of the authors and does not necessarily reflect the opinion of the members of IRIDIA. The authors take full responsability for any copyright breaches that may result from publication of this paper in the IRIDIA \u2013 Technical Report Series. IRIDIA is not responsible for any use that might be made of data appearing in this publication. Preprocessing Fitness Functions",
            "group": 2909,
            "name": "10.1.1.146.62",
            "keyword": "Fitness LandscapePreprocessingMAXSATTSPSelection",
            "title": "et de D\u00e9veloppements en Intelligence Artificielle"
        },
        {
            "abstract": "Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test-data to achieve 100 % coverage of a given structural coverage metric is labour intensive and expensive. This paper presents an approach to automate the generation of such test-data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test-data. The same approach can be be generalised to solve other test-data generation problems. Three such applications are discussed { boundary value analysis, assertion/run-time exception testing and component re-use testing. Aprototype tool-set has been developed to facilitate the automatic generation of test-data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the e ciency and e ectiveness of this approach. 1",
            "group": 2910,
            "name": "10.1.1.146.308",
            "keyword": "",
            "title": "An automated framework for structural test-data generation"
        },
        {
            "abstract": "Abstract. The need for effective testing techniques for architectural level descriptions is widely recognised. However, due to the variety of domain-specific architectural description languages, there remains a lack of practical techniques in many application domains. We present a simulation-based testing framework that applies optimisation-based search to achieve high-performance testing for a type of architectural model. The search based automatic test-data generation technique forms the core of the framework. Matlab/Simulink is popularly used in embedded systems engineering as an architectural-level design notation. Our prototype framework is built on Matlab for testing Simulink models. The technology involved should apply to the other architectural notations provided that the notation supports execution or simulation. 1 Automatic Testing at the Architecture Level Software testing is an expensive procedure. It typically consumes more than 50 % of the total development budget [1]. Failure to detect errors can result in significant financial loss or even disaster in the case of safety critical systems. Complete testing",
            "group": 2911,
            "name": "10.1.1.146.1007",
            "keyword": "",
            "title": "Search Based Automatic Test-Data Generation at an Architectural Level"
        },
        {
            "abstract": "Absfract-Substitution boxes are important components in many modem day block and stream ciphers. Their study has attracted a great deal of attention over many years. The development of a variety of cryptosystem attacks has lead to the development of criteria for resilience to such attacks. Some general criteria such as high non-linearity and low autocorrelation have been proposed (providing some protection against attacks such as linear cryptanalysis and differential cryptanalysis). There has been little application of evolutionary search to the development of S-boxes. In this paper we show how a cost function that has found excellent single-output Boolean functions can he generalised to provide improved results for small S-boxes. I.",
            "group": 2912,
            "name": "10.1.1.146.1694",
            "keyword": "",
            "title": "The Design of SBoxes by Simulated Annealing"
        },
        {
            "abstract": "Typically verification focuses on demonstrating consistency between an implementation and a functional specification. For safety critical systems this is not sufficient, the implementation must also meet the system safety constraints and safety requirements. The work presented in this paper builds on the authors ' previous work in developing a general framework for dynamically generating test-data using heuristic global optimisation techniques. This framework has been adapted to allow automated test-data generation to be used to support the application of software fault tree analysis. Using the framework a search for testdata that causes an identified software hazard condition can be performed automatically. The fact that a hazardous condition can arise may be discovered much earlier than with conventional testing using this automated approach. If no testdata is located then SFTA, or other forms of static analysis, can be performed to give the necessary assurance that no such data exists. A number of extensions to this basic approach are also outlined. These are, integration with fault injection, testing for exception conditions and testing for safe component reuse and integration. Preliminary results are encouraging and show that the approach justifies further research.",
            "group": 2913,
            "name": "10.1.1.146.1704",
            "keyword": "software fault treeautomated testingoptimisationsafety verification",
            "title": "Integrating Safety Analysis with Automatic Test-Data Generation for Software Safety Verification"
        },
        {
            "abstract": "Many problems in high assurance systems design are only tractable using computationally expensive search algorithms. For these algorithms to be useful, designers must be provided with guidance as to how to configure the algorithms appropriately. This paper presents an experimental methodology for deriving such guidance that remains efficient when the algorithm requires substantial computing resources or takes a long time to find solutions. The methodology is shown to be effective on a highly-constrained task allocation algorithm that provides design solutions for high integrity systems. Using the methodology, an algorithm configuration is derived in a matter of days that significantly outperforms one resulting from months of \u2018trial-and-error\u2019 optimisation. 1",
            "group": 2914,
            "name": "10.1.1.146.1895",
            "keyword": "",
            "title": "An Efficient Experimental Methodology for Configuring Search-Based Design Algorithms"
        },
        {
            "abstract": "The linear ordering problem is an-hard problem that arises in a variety of applications. Due to its interest in practice, it has received considerable attention and a variety of algorithmic approaches to its solution have been proposed. In this paper we give a detailed search space analysis of available LOP benchmark instance classes that have been used in various researches. The large fitness-distance correlations observed for many of these instances suggest that adaptive restart algorithms like iterated local search or memetic algorithms, which iteratively generate new starting solutions for a local search based on previous search experience, are promising candidates for obtaining high performing algorithms. We therefore experimentally compared two such algorithms and the final experimental results suggest that, in particular, the memetic algorithm is the new state-of-the-art approach to the LOP. 1",
            "group": 2915,
            "name": "10.1.1.146.2143",
            "keyword": "",
            "title": "The linear ordering problem: instances, search space analysis and algorithms"
        },
        {
            "abstract": "One of the major costs in a software project is the construction of test-data. This paper outlines a generalised test-case data generation framework based on optimisation techniques. The framework can incorporate a number of testing criteria, for both functional and non-functional properties. Application of the optimisation framework to testing specification failures and exception conditions is illustrated. The results of a number of small case studies are presented and show the efficiency and effectiveness of this dynamic optimisation-base approach to generating test-data. 1.1 Keywords Automatic test-case generation, software testing, formal specifications, exception conditions, optimisation techniques, simulated annealing. 2",
            "group": 2916,
            "name": "10.1.1.146.2364",
            "keyword": "",
            "title": "Automated Program Flaw Finding using Simulated Annealing"
        },
        {
            "abstract": "An incomplete method for solving the propositional satisfiability problem (or a general constraint satisfaction problem) is one that does not provide the guarantee that it will eventually either report a satisfying assignment or declare that the given formula is unsatisfiable. In practice, most such methods are biased towards the satisfiable side: they are typically run with a pre-set resource limit, after which they either produce a valid solution or report failure; they never declare the formula to be unsatisfiable. These are the kind of algorithms we will discuss in this chapter. In complexity theory terms, such algorithms are referred to as having one-sided error. In principle, an incomplete algorithm could instead be biased towards the unsatisfiable side, always providing proofs of unsatisfiability but failing to find solutions to some satisfiable instances, or be incomplete with respect to both satisfiable and unsatisfiable instances (and thus have two-sided error). Unlike systematic solvers often based on an exhaustive branching and backtracking search, incomplete methods are generally based on stochastic local search,",
            "group": 2917,
            "name": "10.1.1.147.148",
            "keyword": "",
            "title": " Incomplete Algorithms"
        },
        {
            "abstract": "Propositional model counting or #SAT is the problem of computing the number of models for a given propositional formula, i.e., the number of distinct truth assignments to variables for which the formula evaluates to true. For a propositional formula F, we will use #F to denote the model count of F. This problem is also referred to as the solution counting problem for SAT. It generalizes SAT and is the canonical #P-complete problem. There has been significant theoretical work trying to characterize the worst-case complexity of counting problems, with some surprising results such as model counting being hard even for some polynomial-time solvable problems like 2-SAT. The model counting problem presents fascinating challenges for practitioners and poses several new research questions. Efficient algorithms for this problem will have a significant impact on many application areas that are inherently beyond SAT (\u2018beyond \u2019 under standard complexity theoretic assumptions), such as bounded-length adversarial and contingency planning, and probabilistic reasoning. For example, various probabilistic inference problems, such as Bayesian net reasoning, can be effectively translated into model counting problems [cf.",
            "group": 2918,
            "name": "10.1.1.147.221",
            "keyword": "",
            "title": "Model Counting"
        },
        {
            "abstract": "Topical query decomposition (TQD) is a paradigm recently introduced in [1] , which, given a query, returns to the user a set of queries that cover the answer set of the original query. The TQD problem was studied as a variant of the set-cover problem and solved by means of a greedy algorithm. This paper aims to strengthen the original formulation by introducing a new global objective function, and thus formalising the problem as a combinatorial optimisation one. Such a reformulation defines a common framework allowing a formal evaluation and comparison of different approaches to TQD. We apply simulated annealing, a sub-optimal metaheuristic, to the problem of topical query decomposition and we show, through a large experimentation on a data sample extracted from an actual query log, that such meta-heuristic achieves better results than the greedy algorithm.",
            "group": 2919,
            "name": "10.1.1.147.967",
            "keyword": "Categories and Subject Descriptors H.2.8 [Database ManagementDatabase Applications- Data Mining H.4.3 [Information Systems ApplicationsCommunications Applications General Terms Algorithms Keywords Query logsQuery DecompositionQuery recommendationObjective FunctionSimulated Annealing",
            "title": "Optimising Topical Query Decomposition"
        },
        {
            "abstract": "We propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner. We focus on problems specified as a Boolean formula, i.e., on SAT instances. Sampling for SAT problems has been shown to have interesting connections with probabilistic reasoning, making practical sampling algorithms for SAT highly desirable. The best current approaches are based on Markov Chain Monte Carlo methods, which have some practical limitations. Our approach exploits combinatorial properties of random parity (XOR) constraints to prune away solutions near-uniformly. The final sample is identified amongst the remaining ones using a state-of-the-art SAT solver. The resulting sampling distribution is provably arbitrarily close to uniform. Our experiments show that our technique achieves a significantly better sampling quality than the best alternative. 1",
            "group": 2920,
            "name": "10.1.1.147.1211",
            "keyword": "",
            "title": "Near-uniform sampling of combinatorial spaces using xor constraints"
        },
        {
            "abstract": "The past few years have seen an enormous progress in the performance of Boolean satisfiability (SAT) solvers. Despite the worst-case exponential run time of all known algorithms, satisfiability solvers are increasingly leaving their mark as a generalpurpose tool in areas as diverse as software and hardware verification [29\u201331, 228], automatic test pattern generation [138, 221], planning [129, 197], scheduling [103], and even challenging problems from algebra [238]. Annual SAT competitions have led to the development of dozens of clever implementations of such solvers [e.g. 13,",
            "group": 2921,
            "name": "10.1.1.147.2247",
            "keyword": "",
            "title": "  Satisfiability Solvers"
        },
        {
            "abstract": "We report on GADGET, a new software test generation system that uses combinatorial optimization to obtain condition/decision coverage of C/C++ programs. The GADGET system is fully automatic and supports all C/C++ language constructs. This allows us to generate tests for programs more complex than those previously reported in the literature. We address a number of issues that are encountered when automatically generating tests for complex software systems. These issues have not been discussed in earlier work on test-data generation, which concentrates on small programs (most often single functions) written in restricted programming languages. 1 Dynamic test data generation In this paper, we introduce the GADGET system, which uses a test data generation paradigm commonly known as dynamic test data generation. Dynamic test data generation was originally proposed by [Miller and Spooner, 1976] and then investigated further with the TESTGEN system of [Korel, 1990, Korel, 1996], the",
            "group": 2922,
            "name": "10.1.1.147.2749",
            "keyword": "",
            "title": "Automated software test data generation for complex programs"
        },
        {
            "abstract": "This work presents a study about feature selection and weighting for improving the recognition of handwritten words coming from Brazilian bank check lexicon. For this purpose, two global optimization methods are used: Tabu Search(TS) and Simulated Annealing(SA). These methods were combined with k-NN composing two hybrid approaches for features selection and weighting: SA/k-NN and TS/k-NN. The results show that feature sets optimized by the studied models are very efficient when compared with k-NN. Both, accuracy classification and number of features in the resultant set are considered in the conclusions. Furthermore, some new structural features extracted from image upper and lower profiles are proposed.",
            "group": 2923,
            "name": "10.1.1.147.2869",
            "keyword": "legal amount recognitionfeature selection",
            "title": "Feature Set Selection and Weighting for Legal Amount Recognition on Brazilian Bank Checks"
        },
        {
            "abstract": "Abstract\u2014The generalization ability of artificial neural networks (ANNs) is greatly dependent on their architectures. Constructive algorithms provide an attractive automatic way of determining a near-optimal ANN architecture for a given problem. Several such algorithms have been proposed in the literature and shown their effectiveness. This paper presents a new constructive algorithm (NCA) in automatically determining ANN architectures. Unlike most previous studies on determining ANN architectures, NCA puts emphasis on architectural adaptation and functional adaptation in its architecture determination process. It uses a constructive approach to determine the number of hidden layers in an ANN and of neurons in each hidden layer. To achieve functional adaptation, NCA trains hidden neurons in the ANN by using different training sets that were created by employing a similar concept used in the boosting algorithm. The purpose",
            "group": 2924,
            "name": "10.1.1.147.7055",
            "keyword": "generalization ability",
            "title": "A New Constructive Algorithm for Architectural and Functional Adaptation of Artificial Neural Networks"
        },
        {
            "abstract": "For an information agent to support a human in a personalized way, having a model of the trust the human has in information sources may be essential. As humans differ a lot in their characteristics with respect to trust, a trust model crucially depends on specific personalized values for a number of parameters. This paper contributes an adaptive agent model for trust with parameters that are automatically tuned over time to a specific individual. To obtain the adaptation, four different techniques have been developed. In order to evaluate these techniques, simulations have been performed. The results of these were formally verified. 1.",
            "group": 2925,
            "name": "10.1.1.147.7478",
            "keyword": "",
            "title": "An Adaptive Agent Model Estimating Human Trust in Information Sources"
        },
        {
            "abstract": " ",
            "group": 2926,
            "name": "10.1.1.147.7717",
            "keyword": "Software testingObject-oriented softwareContainersSearch algorithmsNature inspired algorithmsSearch based software engineeringTestability transformationsWhite box testing",
            "title": "Search based software testing of object-oriented containers "
        },
        {
            "abstract": "Abstract\u2014This paper presents a new algorithm, called adaptive merging and growing algorithm (AMGA), in designing artificial neural networks (ANNs). This algorithm merges and adds hidden neurons during the training process of ANNs. The merge operation introduced in AMGA is a kind of a mixed mode operation, which is equivalent to pruning two neurons and adding one neuron. Unlike most previous studies, AMGA puts emphasis on autonomous functioning in the design process of ANNs. This is the main reason why AMGA uses an adaptive not a predefined fixed strategy in designing ANNs. The adaptive strategy merges or adds hidden neurons based on the learning ability of hidden neurons or the training progress of ANNs. In order to reduce the amount of retraining after modifying ANN architectures, AMGA prunes hidden neurons by merging correlated hidden neurons and adds hidden neurons by splitting existing hidden neurons. The proposed AMGA has been tested on a number of benchmark problems in machine learning and ANNs, including breast cancer, Australian credit card assessment, and diabetes, gene, glass, heart, iris, and thyroid problems. The experimental results show that AMGA can design compact ANN architectures with good generalization ability compared to other algorithms. Index Terms\u2014Adding neurons, artificial neural network (ANN) design, generalization ability, merging neurons, retraining.",
            "group": 2927,
            "name": "10.1.1.147.8659",
            "keyword": "",
            "title": "An adaptive merging and growing algorithm for designing artificial neural networks"
        },
        {
            "abstract": "Almost all analyses of time complexity of evolutionary algorithms (EAs) have been conducted for (1 + 1) EAs only. Theoretical results on the average computation time of population-based EAs are few. However, the vast majority of applications of EAs use a population size that is greater than one. The use of population has been regarded as one of the key features of EAs. It is important to understand in depth what the real utility of population is in terms of the time complexity of EAs, when EAs are applied to combinatorial optimization problems. This paper compares (1 + 1) EAs and ( +) EAs theoretically by deriving their first hitting time on the same problems. It is shown that a population can have a drastic impact on an EA\u2019s average computation time, changing an exponential time to a polynomial time (in the input size) in some cases. It is also shown that the first hitting probability can be improved by introducing a population. However, the results presented in this paper do not imply that population-based EAs will always be better than (1 + 1) EAs for all possible problems. ",
            "group": 2928,
            "name": "10.1.1.148.2275",
            "keyword": "",
            "title": "From an Individual to a Population:     An Analysis of the First Hitting Time of Population-Based Evolutionary Algorithms"
        },
        {
            "abstract": "This paper presents research to address the temperature challenge in multicore processors through the lever of thermally-aware floorplanning. Specifically, it examines the thermal benefit in a variety of placement choices available in a multicore processor including alternative core orientation and insertion of L2 cache banks between cores as cooling buffers. In comparison with an idealized scheme that scatters the functional blocks of a multicore across the entire chip area to maximize uniformity, a combination of core orientation and L2 cache bank insertion achieves about 75 % of the peak temperature reduction with negligible performance impact. On an average, the improvement in temperature is about 20 % of the magnitude above the ambient temperature. 1",
            "group": 2929,
            "name": "10.1.1.148.3683",
            "keyword": "",
            "title": "Microarchitectural floorplanning for thermal management: A technical report"
        },
        {
            "abstract": "A metaheuristic-based algorithm is presented for the post enrolment-based course timetabling problem used in track-2 of the Second International Timetabling Competition (ITC2007). The featured algorithm operates in three distinct stages \u2013 a constructive phase followed by two separate phases of simulated annealing \u2013 and is time dependent, due to the fact that various run-time parameters are calculated automatically according to the amount of computation time available. Overall, the method produces results in line with the official finalists to the timetabling competition, though experiments show that this algorithm also seems to find certain instances more difficult to solve than others. A number of reasons for this latter feature are discussed.",
            "group": 2930,
            "name": "10.1.1.148.3991",
            "keyword": "",
            "title": "A Time-Dependent Metaheuristic Algorithm for Post Enrolment-based Course Timetabling"
        },
        {
            "abstract": "Abstract\u2014Recently, categorical data clustering has been gaining significant attention from researchers, because most of the real life data sets are categorical in nature. In contrast to numerical domain, no natural ordering can be found among the elements of a categorical domain. Hence no inherent distance measure, like the Euclidean distance, would work to compute the distance between two categorical objects. In this article, genetic algorithm and simulated annealing based categorical data clustering algorithms have been proposed. The performances of the proposed algorithms have been compared with that of different well known categorical data clustering algorithms and demonstrated for a variety of artificial and real life categorical data sets. Keywords: Genetic Algorithm",
            "group": 2931,
            "name": "10.1.1.148.4525",
            "keyword": "Clustering (SACK-medoids AlgorithmMinkowski",
            "title": "Genetic Algorithm and Simulated Annealing based Approaches to Categorical Data Clustering"
        },
        {
            "abstract": "the date of receipt and acceptance should be inserted later Abstract Network applications often require that a trust relationship is established between a trusted host (e.g., the server) and an untrusted host (e.g., the client). The remote entrusting problem is the problem of ensuring the trusted host that whenever a request from an untrusted host is served, the requester is in a genuine state, unaffected by malicious modifications or attacks. Barrier slicing helps solve the remote entrusting problem. The computation of the sensitive client state is sliced and moved to the server, where it is not possible to tamper with it. However, this solution might involve unacceptable computation and communication costs for the server, especially when the slice to be moved is large. In this paper, we investigate the trade-off between security loss and performance overhead associated with moving only a portion of the barrier slice to the server and we show that this trade-off can be reduced to a multi-objective optimization problem. We describe how to make decisions in practice with reference to a case study, for which we show how to choose among the alternative options.",
            "group": 2932,
            "name": "10.1.1.148.4676",
            "keyword": "security",
            "title": "Automated Software Engineering manuscript No. (will be inserted by the editor) Trading-off Security and Performance in Barrier Slicing for Remote Software Entrusting"
        },
        {
            "abstract": "Managing data centers is a challenging endeavor. Stateof-the-art management systems often rely on analytical modeling to assess the performance, availability, and/or energy implications of potential management decisions or system configurations. In this paper, we argue that actual experiments are cheaper, simpler, and more accurate than models for many management tasks. To support this claim, we built an infrastructure for experimentbased management of virtualized data centers, called JustRunIt. The infrastructure creates a sandboxed environment in which experiments can be run\u2014on a very small number of machines\u2014using real workloads and real system state, but without affecting the on-line system. Automated management systems or the system administrator herself can leverage our infrastructure to perform management tasks on the on-line system. To evaluate the infrastructure, we apply it to two common tasks: server consolidation/expansion and evaluating hardware upgrades. Our evaluation demonstrates that JustRunIt can produce results realistically and transparently, and be nicely combined with automated management systems",
            "group": 2933,
            "name": "10.1.1.148.5583",
            "keyword": "",
            "title": "JustRunIt: Experiment-Based . . . "
        },
        {
            "abstract": "Despite recent developments in protein structure prediction, an accurate new fold prediction algorithm remains elusive. One of the challenges facing current techniques is the size and complexity of the space containing possible structures for a query sequence. Traditionally, to explore this space fragment assembly approaches to new fold prediction have used stochastic optimization techniques. Here we examine deterministic algorithms for optimizing scoring functions in protein structure prediction. Two previously unused techniques are applied to the problem, called the Greedy algorithm and the Hill-climbing algorithm. The main difference between the two is that the latter implements a technique to overcome local minima. Experiments on a diverse set of 276 proteins show that the Hill-climbing algorithms consistently outperform existing approaches based on Simulated Annealing optimization (a traditional stochastic technique) in optimizing the root mean squared deviation (RMSD) between native and working structures. 1.",
            "group": 2934,
            "name": "10.1.1.148.5617",
            "keyword": "",
            "title": "19 1 EFFECTIVE OPTIMIZATION ALGORITHMS FOR FRAGMENT-ASSEMBLY BASED PROTEIN STRUCTURE PREDICTION"
        },
        {
            "abstract": "Abstract\u2014Hybrid genetic algorithms have received significant interest in recent years and are being increasingly used to solve real-world problems. A genetic algorithm is able to incorporate other techniques within its framework to produce a hybrid that reaps the best from the combination. In this paper, different forms of integration between genetic algorithms and other search and optimization techniques are reviewed. This paper also aims to examine several issues that need to be taken into consideration when designing a hybrid genetic algorithm that uses another search method as a local search tool. These issues include the different approaches for employing local search information and various mechanisms for achieving a balance between a global genetic algorithm and a local search method. Index Terms\u2014Genetic algorithms, evolutionary computation, hybrid genetic algorithms, genetic-local hybrid algorithms, memetic algorithms, Lamarckian search, Baldwinian search. I.",
            "group": 2935,
            "name": "10.1.1.148.6231",
            "keyword": "",
            "title": "Hybrid Genetic Algorithms: A Review"
        },
        {
            "abstract": " ",
            "group": 2936,
            "name": "10.1.1.148.6819",
            "keyword": "",
            "title": "Simplifying Swarm Optimization"
        },
        {
            "abstract": "Abstract\u2014Data miners have access to a significant number of classifiers and use them on a variety of different types of dataset. This large selection makes it difficult to know which classifier will perform most effectively in any given case. Usually an understanding of learning algorithms is combined with detailed domain knowledge of the dataset at hand to lead to the choice of a classifier. We propose an empirical framework that quantitatively assesses the accuracy of a selection of classifiers on different datasets, resulting in a set of classification rules generated by the J48 decision tree algorithm. Data miners can follow these rules to select the most effective classifier for their work. By optimising the parameters used for learning and the sampling techniques applied, a set of rules were learned that select with 78 % accuracy (with 0.5 % classification accuracy tolerance), the most effective classifier.",
            "group": 2937,
            "name": "10.1.1.148.8677",
            "keyword": "Bayesian networksData miningClassificationSearch algorithmDecision tree",
            "title": "An Empirical Framework for Automatically Selecting the Best Bayesian Classifier"
        },
        {
            "abstract": "Abstract. We present Fitness Expectation Maximization (FEM), a novel method for performing \u2018black box \u2019 function optimization. FEM searches the fitness landscape of an objective function using an instantiation of the well-known Expectation Maximization algorithm, producing search points to match the sample distribution weighted according to higher expected fitness. FEM updates both candidate solution parameters and the search policy, which is represented as a multinormal distribution. Inheriting EM\u2019s stability and strong guarantees, the method is both elegant and competitive with some of the best heuristic search methods in the field, and performs well on a number of unimodal and multimodal benchmark tasks. To illustrate the potential practical applications of the approach, we also show experiments on finding the parameters for a controller of the challenging non-Markovian double pole balancing task. 1",
            "group": 2938,
            "name": "10.1.1.148.8789",
            "keyword": "",
            "title": "Fitness Expectation Maximization"
        },
        {
            "abstract": "Abstract\u2014Clustering is a widely used technique in data mining application for discovering patterns in underlying data. Most traditional clustering algorithms are limited in handling datasets that contain categorical attributes. However, datasets with categorical types of attributes are common in real life data mining problem. For these data sets, no inherent distance measure, like the Euclidean distance, would work to compute the distance between two categorical objects. In this article, we have described two algorithms based on genetic algorithm and simulated annealing in the field of crisp and fuzzy domain. The performance of the proposed algorithms has been compared with that of different well known categorical data clustering algorithms in crisp and fuzzy domain and demonstrated for a variety of artificial and real life categorical data sets. Also statistical significance tests have been performed to establish the superiority of the proposed algorithms.",
            "group": 2939,
            "name": "10.1.1.148.9061",
            "keyword": "Genetic Algorithm based ClusteringSimulated Annealing based ClusteringK-medoids AlgorithmFuzzy C-Medoids AlgorithmCluster Validity IndicesStatistical significance test",
            "title": "Improved Crisp and Fuzzy Clustering Techniques for Categorical Data \u2217"
        },
        {
            "abstract": " ",
            "group": 2940,
            "name": "10.1.1.148.9823",
            "keyword": "Cache memoryMultimedia applicationsCompositionality",
            "title": "Task Centric Memory . . . "
        },
        {
            "abstract": "Abstract\u2014Metaheuristics are sequential processes that perform exploration and exploitation in the solution space aiming to efficiently find near optimal solutions with natural intelligence as a source of inspiration. One of the most well-known metaheuristics is called Ant Colony Optimisation, ACO. This paper is conducted to give an aid in complicatedness of using ACO in terms of its parameters: number of iterations, ants and moves. Proper levels of these parameters are analysed on eight noisy continuous non-linear continuous response surfaces. Considering the solution space in a specified region, some surfaces contain global optimum and multiple local optimums and some are with a curved ridge. ACO parameters are determined through Modified Simplex, MSM and Steepest Ascent methods, SAM, including their hybridisation. SAM was introduced to enhance a performance of MSM via the",
            "group": 2941,
            "name": "10.1.1.149.915",
            "keyword": "Index Terms\u2014Ant Colony OptimisationModified SimplexTaguchi\u2019s Signal to Noise RatioSteepest Ascent and Response Surface Methodology. *J. Ratanaphanyarat is with the Industrial Statist",
            "title": "A Hybrid of Modified Simplex and Steepest Ascent Methods with Signal to Noise Ratio for Optimal Parameter Settings of ACO"
        },
        {
            "abstract": "In this paper a method for the extraction of shading and reflectance intrinsic images from a single uncalibrated image is presented. It is based on the classification of the image derivatives as either caused by shading or reflectance effects, using an illumination-invariant image to guide this classification. Our approach avoids the learning process \u2013 which requires ground truth intrinsic images \u2013 and obtain results comparable with the state of the art. Index Terms \u2014 Reflectance Recovery, Intrinsic Images 1.",
            "group": 2942,
            "name": "10.1.1.149.918",
            "keyword": "",
            "title": "RECOVERING INTRINSIC IMAGES USING AN ILLUMINATION INVARIANT IMAGE"
        },
        {
            "abstract": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (01, 02, 04), the Lucent Technologies PAC/EPAC/MPAC, the Dolby1 AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.",
            "group": 2943,
            "name": "10.1.1.149.1231",
            "keyword": "",
            "title": "Perceptual Coding of Digital Audio"
        },
        {
            "abstract": "In many scientific applications, significant time is spent tuning codes for a particular high-performance architecture. Tuning approaches range from the relatively nonintrusive (e.g., by using compiler options) to extensive code modifications that attempt to exploit specific architecture features. Intrusive techniques often result in code changes that are not easily reversible, which can negatively impact readability, maintainability, and performance on different architectures. We introduce an extensible annotation-based empirical tuning system called Orio, which is aimed at improving both performance and productivity by enabling software developers to insert annotations in the form of structured comments into their source code that trigger a number of low-level performance optimizations on a specified code fragment. To maximize the performance tuning opportunities, we have designed the annotation processing infrastructure to support both architecture-independent and architecture-specific code optimizations. Given the annotated code as input, Orio generates many tuned versions of the same operation and empirically evaluates the versions to select the best performing one for production use. We have also enabled the use of the PLuTo automatic parallelization tool in conjunction with Orio to generate efficient OpenMP-based parallel code. We describe our experimental results involving a number of computational kernels, including dense array and sparse matrix operations.  ",
            "group": 2944,
            "name": "10.1.1.149.1902",
            "keyword": "",
            "title": "Annotation-Based Empirical Performance Tuning Using Orio"
        },
        {
            "abstract": "Abstract: Event-Related Potentials (ERPs) provide non invasive measurements of the electrical activity on the scalp that are linked to the presentation of stimuli and events. Brain mapping techniques are able to provide evidence on the solution of debatable issues in cognitive science. In this paper, an effective signal classification approach is proposed, extending the use of two inversion techniques: the Brain Electrical Tomography using Algebraic Reconstruction Technique (BET-ART) and the Low Resolution Brain Electromagnetic Tomography (LORETA). The first step of the methodology applied is the feature extraction, which is based on the combination of the Multivariate Autoregressive model with the Simulated Annealing technique, in order to extract optimum features, in terms of classification rate. The classification, as the second step of the methodology, is implemented by means of an Artificial Neural Network (ANN) trained with the back-propagation algorithm under the \u201cleave-one-out cross-validation \u201d scenario. The ANN is a multi-layer perceptron, the architecture of which is selected after a detailed search. The proposed methodology has been applied for the classification of First Episode Schizophrenic (FES) patients and normal controls using the intracranial activity distributions obtained by ERPs. A comparative analysis was performed using BET-ART and LORETA inversion methods. Implementation of the proposed methodology provided classification rates of up to 93.1%, for both types of input signals. Additionally, for both BET-ART and LORETA signals, the",
            "group": 2945,
            "name": "10.1.1.149.3027",
            "keyword": "",
            "title": "An artificial neural network approach to the classification of inferred intracranial signals"
        },
        {
            "abstract": "Abstract \u2014 This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued \u2018black box \u2019 function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the \u2018vanilla \u2019 gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. I.",
            "group": 2946,
            "name": "10.1.1.149.3899",
            "keyword": "",
            "title": "Natural evolution strategies"
        },
        {
            "abstract": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth'value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:",
            "group": 2947,
            "name": "10.1.1.149.6007",
            "keyword": "",
            "title": "Optimal perceptual inference"
        },
        {
            "abstract": "For an information agent to support a human in a personalized way, having a model of the trust the human has in information sources may be essential. As humans differ a lot in their characteristics with respect to trust, a trust model crucially depends on specific personalized values for a number of parameters. This paper contributes an adaptive agent model for trust with parameters that are automatically tuned over time to a specific individual. To obtain the adaptation, four different techniques have been developed. In order to evaluate these techniques, simulations have been performed. The results of these were formally verified. 1.",
            "group": 2948,
            "name": "10.1.1.149.6013",
            "keyword": "",
            "title": "An Adaptive Agent Model Estimating Human Trust in Information Sources"
        },
        {
            "abstract": "In this paper, we develop a novel algorithm that allows service consumer agents to automatically select and provision service provider agents for their workflows in highly dynamic and uncertain computational service economies. In contrast to existing work, our algorithm reasons explicitly about the impact of failures on the overall feasibility of a workflow, and it mitigates them by proactively provisioning multiple providers in parallel for particularly critical tasks and by explicitly planning for contingencies. Furthermore, our algorithm provisions only part of its workflow at any given time, in order to retain flexibility and to decrease the potential for missing negotiated service time slots. We show empirically that current approaches are unable to achieve a high utility in such uncertain and dynamic environments; whereas our algorithm consistently outperforms them over a range of environments. Specifically, our approach can achieve up to a 27-fold increase in utility and successfully completes most workflows within a strict deadline, even when the majority of providers do not honour their contracts.",
            "group": 2949,
            "name": "10.1.1.149.6572",
            "keyword": "General Terms AlgorithmsExperimentationReliability Keywords Service-oriented computing",
            "title": "Flexible service provisioning with advance agreements"
        },
        {
            "abstract": "In this paper we introduce a novel and efficient approach to dense image registration, which does not require a derivative of the employed cost function. In such a context the registration problem is formulated using a discrete Markov Random Field objective function. First, towards dimensionality reduction on the variables we assume that the dense deformation field can be expressed using a small number of control points (registration grid) and an interpolation strategy. Then, the registration cost is expressed using a discrete sum over image costs (using an arbitrary similarity measure) projected on the control points, and a smoothness term that penalizes local deviations on the deformation field according to a neighborhood system on the grid. Towards a discrete approach the search space is quantized resulting in a fully discrete model. In order to account for large deformations and produce results on a high resolution level a multi-scale incremental approach is considered where the optimal solution is iteratively updated. This is done through successive morphings of the source towards the target image. Efficient linear programming using the primal dual principles is considered to recover the lowest potential of the cost function. Very promising results using synthetic data with known deformations and real data demonstrate the potentials of our approach.",
            "group": 2950,
            "name": "10.1.1.150.222",
            "keyword": "Key wordsDiscrete OptimizationDeformable RegistrationLinear Programming",
            "title": " Dense Image Registration through MRFs and Efficient Linear Programming"
        },
        {
            "abstract": "Clustering data by identifying a subset of representative examples is important for detecting patterns in data and in processing sensory signals. Such \u201cexemplars \u201d can be found by randomly choosing an initial subset of data points as exemplars and then iteratively refining it, but this works well only if that initial choice is close to a good solution. This thesis describes a method called \u201caffinity propagation \u201d that simultaneously considers all data points as potential exemplars, exchanging real-valued messages between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. Affinity propagation takes as input a set of pairwise similarities between data points and finds clusters on the basis of maximizing the total similarity between data points and their exemplars. Similarity can be simply defined as negative squared Euclidean distance for compatibility with other algorithms, or it can incorporate richer domain-specific models (e.g., translation-invariant distances for comparing images). Affinity propagation\u2019s computational and memory requirements scale linearly with the number of similarities input; for non-sparse problems where all possible similarities are computed, these requirements scale quadratically with the number of data points. Affinity propagation is demonstrated on several applications",
            "group": 2951,
            "name": "10.1.1.150.5946",
            "keyword": "",
            "title": "AFFINITY PROPAGATION: CLUSTERING DATA BY PASSING MESSAGES"
        },
        {
            "abstract": "Managing data centers is a challenging endeavor. Stateof-the-art management systems often rely on analytical modeling to assess the performance, availability, and/or energy implications of potential management decisions or system configurations. In this paper, we argue that actual experiments are cheaper, simpler, and more accurate than models for many management tasks. To support this claim, we built an infrastructure for experimentbased management of virtualized data centers, called JustRunIt. The infrastructure creates a sandboxed environment in which experiments can be run\u2014on a very small number of machines\u2014using real workloads and real system state, but without affecting the on-line system. Automated management systems or the system administrator herself can leverage our infrastructure to perform management tasks on the on-line system. To evaluate the infrastructure, we apply it to two common tasks: server consolidation/expansion and evaluating hardware upgrades. Our evaluation demonstrates that JustRunIt can produce results realistically and transparently, and be nicely combined with automated management systems. 1",
            "group": 2952,
            "name": "10.1.1.150.7451",
            "keyword": "",
            "title": "Rutgers University"
        },
        {
            "abstract": "Today, many large organizations operate multiple data centers. The reasons for this include natural business distribution, the need for high availability and disaster tolerance, the sheer size of their computational infrastructure,",
            "group": 2953,
            "name": "10.1.1.150.7653",
            "keyword": "",
            "title": "Rutgers University"
        },
        {
            "abstract": "Heuristics for unrelated machine scheduling with precedence",
            "group": 2954,
            "name": "10.1.1.150.9614",
            "keyword": "SchedulingParallel resourcesMakespan",
            "title": "constraints"
        },
        {
            "abstract": "The previous research on cluster-based servers has focused on homogeneous systems. However, real-life clusters are almost invariably heterogeneous in terms of the performance, capacity, and power consumption of their hardware components. In this paper, we argue that designing efficient servers for heterogeneous clusters requires defining an efficiency metric, modeling the different types of nodes with respect to the metric, and searching for request distributions that optimize the metric. To concretely illustrate this process, we design a cooperative Web server for a heterogeneous cluster that uses modeling and optimization to minimize the energy consumed per request. Our experimental results for a cluster comprised of traditional and blade nodes show that our server can consume 42 % less energy than an energy-oblivious server, with only a negligible loss in throughput. The results also show that our server conserves 45 % more energy than an energy-conscious server that was previously proposed for homogeneous clusters. 1",
            "group": 2955,
            "name": "10.1.1.151.1185",
            "keyword": "",
            "title": "Energy Conservation in Heterogeneous Server Clusters"
        },
        {
            "abstract": "The Ternary Tree Solver (tts) is a complete solver for propositional satisfiability which was designed to have good performance on the most difficult small instances. It uses a static ternary tree data structure to represent the simplified proposition under all permissible partial assignments and maintains a database of derived propositions known to be unsatisfiable. In the SAT2007 competition version 4.0 won the silver medal for the category handmade, speciality UNSAT solvers and was the top qualifier for the second stage for handmade benchmarks, solving 11 benchmarks which were not solved by any other entrant. We describe the methods used by the solver and analyse the competition Phase 1 results on small benchmarks. We propose a first version of a comprehensive suite of small difficult satisfiability benchmarks (sdsb) and compare the worst-case performance of the competition medallists on these benchmarks. Keywords: SAT-solver, difficult instance, variable ordering, simulated annealing, clause memoisation",
            "group": 2956,
            "name": "10.1.1.151.2726",
            "keyword": "",
            "title": "tts: A SAT-Solver for Small, Difficult Instances"
        },
        {
            "abstract": "Abstract\u2014Recursive bisection is a popular approach for large scale circuit placement problems, combining a high degree of scalability with good results. In this paper, we present a bisection-based approach for both standard cell and mixed block placement; in contrast to prior work, our horizontal cut lines are not restricted to row boundaries. This technique, which we refer to as a fractional cut, simplifies mixed block placement and also avoids a narrow region problem encountered in standard cell placement. Our implementation of these techniques in the placement tool Feng Shui 2.6 retains the speed and simplicity for which bisection is known, while making it competitive with leading methods on standard cell designs. On mixed block placement problems, we obtain substantial improvements over recently published work. Half perimeter wire lengths are reduced by 29 % on average, compared to a flow based on Capo and Parquet; compared to mPG-ms, wire lengths are reduced by 26 % on average. Index Terms\u2014Circuit placement, design automation, mixed size placement, placement legalization, recursive bisection. I.",
            "group": 2957,
            "name": "10.1.1.151.4398",
            "keyword": "",
            "title": "Mixed block placement via fractional cut recursive bisection"
        },
        {
            "abstract": "In this chapter, we present constrained simulated annealing (CSA), an algorithm that extends conventional simulated annealing to look for constrained local minima of constrained optimization problems. The algorithm is based on the theory of extended saddle points (ESPs) that shows the one-to-one correspondence between a constrained local minimum of the problem and an ESP of the corresponding penalty function. CSA finds ESPs by systematically controlling probabilistic descents in the original variable space of the penalty function and probabilistic ascents in the penalty space. Based on the decomposition of the necessary and sufficient ESP condition into multiple necessary conditions, we also describe constraint-partitioned simulated annealing (CPSA) that exploits the locality of constraints in nonlinear optimization problems. CPSA leads to much lower complexity as compared to that of CSA by partitioning the constraints of a problem into exponentially simpler subproblems, solving each independently, and resolving those violated global constraints across the subproblems. We evaluate CSA and CPSA by applying them to solve some continuous constrained optimization problems and compare their performance to that of other penalty methods. Finally, we apply CSA to solve two real-world applications, one on sensor-network placement design and another on out-of-core compiler code generation. 1",
            "group": 2958,
            "name": "10.1.1.151.7545",
            "keyword": "",
            "title": "Theory and Applications of Simulated Annealing for Nonlinear Constrained Optimization \u2217"
        },
        {
            "abstract": "In this research we present new results on discrete Lagrangian methods (DLM) and extend our previous (incomplete and highly simplified) theory on the method. Our proposed method forms a strong mathematical foundation for solving general nonlinear discrete optimization problems. Specifically, we show for continuous Lagrangian methods the relationship among local minimal solutions satisfying constraints, solutions found by the first-order necessary and second-order sufficient conditions, and saddle points. Since there is no corresponding definition of gradients in discrete space, we propose a new vector-based definition of gradient, develop first-order conditions similar to those in continuous space, propose a heuristic method to find saddle points, and show the relationship between saddle points and local minimal solutions satisfying constraints. We then show, when all the constraint functions are non-negative, that the set of saddle points is the same as the set of local minimal points satisfying constraints. Our formal results for solving discrete",
            "group": 2959,
            "name": "10.1.1.151.8020",
            "keyword": "",
            "title": " THE DISCRETE LAGRANGIAN THEORY AND ITS APPLICATION TO SOLVE NONLINEAR DISCRETE CONSTRAINED OPTIMIZATION PROBLEMS"
        },
        {
            "abstract": "3D packaging via System-On-Package (SOP) is a viable alternative to System-On-Chip (SOC) to meet the rigorous requirements of today\u2019s mixed signal system integration. In this work, we propose a 3D module and decap (decoupling capacitance) placement algorithm that simultaneously reduces the power supply noise and wire congestion. We provide efficient algorithms for 3D power supply noise and congestion analysis to guide our 3D module placement process. In addition, we allocate white spaces around the modules that require decaps to suppress the power supply noise while minimizing the area overhead. In our experimentation, we achieve improvements in both decap amount and congestion with only small increase in area, wirelength, and runtime.",
            "group": 2960,
            "name": "10.1.1.151.8459",
            "keyword": "General Terms AlgorithmsDesign Keywords System-On-Package3D Module PlacementCongestionPower Noise Reduction",
            "title": "3D Module Placement for Congestion and Power Noise Reduction"
        },
        {
            "abstract": "Abstract \u2014 We introduce a new approach for QoS provisioning in packet networks based on the notion of differentiated traffic engineering (DTE). We consider a single AS network capable of source based multi-path routing. We do not require sophisticated queuing or per-class scheduling at individual routers; instead, if a link is used to forward QoS sensitive packets, we maintain its utilization below a threshold. As a consequence, DTE eliminates the need for per-flow (IntServ) or per-class (DiffServ) packet processing tasks such as traffic classification, queueing, shaping, policing and scheduling in the core, and hence poses a lower burden on the network management unit. Conversely, DTE utilizes network bandwidth much more efficiently than simple over-provisioning. In this paper, we propose a complete architecture and an algorithmic structure for DTE. We show that our scheme can be formulated as a non-convex optimization problem, and we present an optimal solution framework based on simulated annealing. We present a simulation-based performance evaluation of DTE, and compare our scheme to existing (Gradient Projection) methods. Index Terms \u2014 Mathematical programming / optimization.",
            "group": 2961,
            "name": "10.1.1.152.919",
            "keyword": "",
            "title": "1 Differentiated Traffic Engineering for QoS Provisioning"
        },
        {
            "abstract": "",
            "group": 2962,
            "name": "10.1.1.152.1091",
            "keyword": "",
            "title": "A High-Speed Timing-Aware Router for FPGAs"
        },
        {
            "abstract": "An Entropy-based gene selection method for cancer classification",
            "group": 2963,
            "name": "10.1.1.152.1234",
            "keyword": "",
            "title": "using microarray data"
        },
        {
            "abstract": "The creation of integrated circuits has progressed from custom design and layout to the less time-intensive implementation media of ASICs and FPGAs. FPGAs provide the lowest development cost and fastest development time; however, the design of the FPGA itself is still a time-consuming, expensive, custom layout task that takes at least 50 person-years to complete. This work explores new techniques to automate the design and layout of FPGAs. An existing automatic layout system is improved by changing the grouping of transistors that form the basic building blocks of the system. These improvements result in a 16.8 % area savings over previous versions and only a 36% area increase compared to equivalent custom designs. The system was also extended to create the first automatic layout of an FPGA from a generic architecture description. These improvements and additions suggest that the automatic layout system is a viable alternative to custom layout of FPGAs. ii Acknowledgements I would like to thank my supervisor, Professor Jonathan Rose, for his advice and guidance in all aspects of this work and my education. Also, Ian Kuon deserves my profound thanks and gratitude for his achievements and co-operation that led to the completion of this work. This work would not have been possible without the people who worked on it",
            "group": 2964,
            "name": "10.1.1.152.2084",
            "keyword": "",
            "title": "Enhancing and Using an Automatic Design System for Creating FPGAs"
        },
        {
            "abstract": " ",
            "group": 2965,
            "name": "10.1.1.152.2322",
            "keyword": "",
            "title": "Supporting High-Performance Pipelined Computation in Commodity-style FPGAs"
        },
        {
            "abstract": "In this paper, we describe the application of two parallelization strategies to the Quartus II FPGA placer. The first uses a pipelining approach and achieves speedups of 1.3x on two processing cores. The second uses a parallel moves approach and achieves speedups of 2.2x on four cores. Unlike all previous parallel moves algorithms, ours is deterministic and always gives the same answer as the serial version of the algorithm, without any significant reduction in performance. We also describe a process to quantify multi-core performance effects, such as memory subsystem limitations and explicit synchronization overhead, and fully describe these effects on a CAD tool for the first time. Memory limitations alone are found to cost up to 35 % of total runtime. Unlike previous algorithms, our algorithms have negligible explicit synchronization overhead. These results are relevant to both CAD designers and to any developers seeking to parallelize existing software. Categories and Subject Descriptors",
            "group": 2966,
            "name": "10.1.1.152.2490",
            "keyword": "AlgorithmsPerformanceDesign Keywords Parallel placementFPGAsTiming-driven placement",
            "title": "General Terms"
        },
        {
            "abstract": "As process geometries shrink into the deep-submicron region, interconnect resistance and capacitance account for an increasingly significant portion of the delay of circuits implemented in Field-Programmable Gate Arrays (FPGAs). One way to improve FPGA speed is to employ logiccluster-based architectures which have high-speed local connections among groups of logic elements. In this work we show what size logic-cluster results in the best area-speed trade-off. To obtain the best choices for a cluster-based architecture, we use computer aided design (CAD) tools to experimentally evaluate architectures with different sized logic clusters. As part of this CAD flow, we develop a timing-driven algorithm that packs logic elements into these clusters. In addition, we develop a timing-driven placement algorithm that results in significant improvements in FPGA speed over existing non-timing-driven algorithms.  ",
            "group": 2967,
            "name": "10.1.1.152.2994",
            "keyword": "",
            "title": "Cluster-Based Architecture, Timing-Driven Packing, and Timing-Driven Placement for FPGAs"
        },
        {
            "abstract": "Abstract. GRASP, or greedy randomized adaptive search procedure, is a multi-start metaheuristic that repeatedly applies local search starting from solutions constructed by a randomized greedy algorithm. In this chapter we review the basic building blocks of GRASP. We cover solution construction schemes, local search methods, and hybridization with path-relinking. Combinatorial optimization can be defined by a finite ground set E = {1,...,n}, a set of feasible solutions F \u2286 2E, and an objective function f: 2E \u2192 R, all three defined for each specific problem. In this chapter, we consider the minimization version of the problem, where we seek an optimal solution S \u2217  \u2208 F such that f(S \u2217 )  \u2264 f(S), \u2200S \u2208 F. Combinatorial optimization finds applications in many settings, including routing, scheduling, inventory and production planning, and facility location. While much progress has been made in finding provably optimal solutions to combinatorial optimization problems employing techniques such as branch and bound, cutting planes, and dynamic programming, as well as provably near-optimal solutions",
            "group": 2968,
            "name": "10.1.1.152.3519",
            "keyword": "",
            "title": "GRASP: GREEDY RANDOMIZED ADAPTIVE SEARCH PROCEDURES"
        },
        {
            "abstract": "Abstract \u2014 In this paper we present a method for FPGA datapath precision optimization subject to user-defined area and error constraints. This work builds upon our previous research [1] which presented a methodology for optimizing for dynamic range\u2014the most significant bit position. In this work, we present an automated optimization technique for the least-significant bit position of circuit datapaths. We present results describing the effectiveness of our methods on typical signal and image processing kernels. I.",
            "group": 2969,
            "name": "10.1.1.152.4415",
            "keyword": "",
            "title": "Automated least-significant bit datapath optimization for FPGAs"
        },
        {
            "abstract": "The demand for high-speed Field-Programmable Gate Array (FPGA) compilation tools has escalated for three reasons: first, as FPGA device capacity has grown, the computation time devoted to placement and routing of circuits has grown more dramatically than the available computer power. Second, there exists a subset of users who are willing to accept a reduction in the quality of result (using a larger FPGA or more resources on a given FPGA) in exchange for a high-speed compilation. Third, high-speed compile has been a long-standing desire of users of FPGA-based custom computing machines, since their compile time requirements are ideally closer to those of regular computers. This thesis focuses on the placement phase of the compile process, and presents an ultrafast placement algorithm for FPGAs. The algorithm is based on a combination of multiple-level, bottom-up clustering and hierarchical simulated annealing. It provides superior area results over a known high-quality placement tool on a set of large benchmark circuits, when both are restricted to a short run time. For example, in 10 seconds of placement time on a 300 MHz Sun UltraSPARC, the ultra-fast tool realizes an average wirelength improvement of 30 % compared to the high-quality tool. It can also generate a placement for a",
            "group": 2970,
            "name": "10.1.1.152.6290",
            "keyword": "",
            "title": "Ultra-Fast Automatic Placement for FPGAs"
        },
        {
            "abstract": "In this paper we present SPR, a new architecture-adaptive mapping tool for use with Coarse-Grained Reconfigurable Architectures (CGRAs). It combines a VLIW style scheduler and FPGA style placement and pipelined routing algorithms with novel mechanisms for integrating and adapting the algorithms to CGRAs. We introduce a latency padding technique that provides feedback from the placer to the scheduler to meet the constraints of a fixed frequency device with configurable interconnect. Using a new dynamic clustering method during placement, we achieved a 1.3x improvement in throughput of mapped designs. Finally, we introduce an enhancement to the PathFinder algorithm for targeting architectures with a mix of dynamically multiplexed and statically configurable interconnects. The enhanced algorithm is able to successfully share statically configured interconnect in a time-multiplexed way, achieving an average channel width reduction of.5x compared to nonshared static interconnect.",
            "group": 2971,
            "name": "10.1.1.152.6546",
            "keyword": "Categories and Subject Descriptors D.3.4 [Pro-cessorsRetargetable compilersB.7.2 [Design AidsPlacement and routing General Terms AlgorithmsDesignExperimentationPerformance",
            "title": "SPR: An Architecture-Adaptive CGRA Mapping Tool"
        },
        {
            "abstract": "In several applications of data mining to high-dimensional data, clustering techniques developed for low-to-moderate sized problems obtain unsatisfactory results. This is an aspect of the curse of dimensionality issue. A traditional approach is based on representing the data in a suitable similarity space instead of the original high-dimensional attribute space. In this paper, we propose a solution to this problem using the projection of data onto a so-called Membership Embedding Space obtained by using the memberships of data points on fuzzy sets centered on some prototypes. This approach can increase the efficiency of the popular Fuzzy C-Means method in the presence of high-dimensional data sets, as we show in an experimental comparisons. We also present a constructive method for prototypes selection based on simulated annealing that is viable for semi-supervised clustering problems.",
            "group": 2972,
            "name": "10.1.1.152.6942",
            "keyword": "",
            "title": "Clustering in the Membership Embedding Space"
        },
        {
            "abstract": "Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA\u2019s defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization",
            "group": 2973,
            "name": "10.1.1.152.8204",
            "keyword": "Bayesian Optimization AlgorithmsDiversityPopulation-based Optimization Methods",
            "title": "A Comparison Study between Genetic Algorithms and Bayesian Optimize Algorithms by Novel Indices"
        },
        {
            "abstract": "Service-oriented computing is an increasingly popular approach for providing applications, computational resources and business services over highly distributed and open systems (such as the Web, computational Grids and peer-to-peer systems). In this approach, service providers advertise their offerings by means of standardised computer-readable descriptions, which can then be used by software applications to discover and consume appropriate services without human intervention. However, despite active research in service infrastructures, and in service discovery and composition mechanisms, little work has recognised that services are offered by inherently autonomous and self-interested entities. This autonomy implies that providers may choose not to honour every service request, demand remuneration for their efforts, and, in general, exhibit uncertain behaviour. This uncertainty is especially problematic for the service consumers when services are part of complex workflows, as is common in many application domains, such as bioinformatics, large-scale data analysis and processing, and commercial supply-chain management. In order to address this uncertainty, we propose a novel algorithm for provisioning services for complex workflows (i.e., for selecting suitable services for the constituent tasks of a workflow). This algorithm uses probabilistic performance information about providers to reason about service",
            "group": 2974,
            "name": "10.1.1.152.8218",
            "keyword": "",
            "title": " Flexible Service Provisioning in Multi-Agent Systems"
        },
        {
            "abstract": "In this paper we discuss the application of a certain class of Monte Carlo methods to stochastic optimization problems. Particularly, we study variable-sample techniques, in which the objective function is replaced, at each iteration, by a sample average approximation. We first provide general results on the schedule of sample sizes, under which variable-sample methods yield consistent estimators as well as bounds on the estimation error. Because the convergence analysis is performed sample-path wise, we are able to obtain our results in a flexible setting, which includes the possibility of using different sampling distributions along the algorithm, without making strong assumptions on the underlying distributions. In particular, we allow the distributions to depend on the decision variables x. We illustrate these ideas by studying a modification of the wellknown simulated annealing method, adapting it to the variable-sample scheme, and show conditions for convergence of the algorithm.",
            "group": 2975,
            "name": "10.1.1.152.8915",
            "keyword": "Stochastic optimizationMonte Carlo methodssimulated annealingMarkov chainssample-path bounds",
            "title": "Variable-Sample Methods and Simulated Annealing for Discrete Stochastic Optimization"
        },
        {
            "abstract": "For the traveling salesman problem various search algorithms have been suggested for decades. In the eld of genetic algorithms, many genetic operators have beenintroduced for the problem. Most genetic encoding schemes have some restrictions that cause more-or-less loss of information contained in problem instances. We suggest a new encoding/crossover pair which pursues minimal information loss in chromosomal encoding and minimal restriction in recombination for the 2D Euclidean traveling salesman problem. The most notable feature of the suggested crossover is that it is based on a totally new concept of encoding. We also prove the theoretical validity of the new crossover by an equivalence-class analysis. The proposed encoding/crossover pair outperformed both distance-preserving crossover and edge-assembly crossover, two state-of-the-art crossovers in the literature. 1",
            "group": 2976,
            "name": "10.1.1.152.9230",
            "keyword": "",
            "title": "The Natural Crossover for the 2D Euclidean TSP"
        },
        {
            "abstract": "Simulated annealing and the (1+1) EA, a simple evolutionary algorithm, are both general randomized search heuristics that optimize any objective function with probability converging to 1. But they use very different techniques to achieve this global convergence. The (1+1) EA applies global mutations than can reach any point in the search space in one step together with an elitist selection mechanism. Simulated annealing restricts its search to a neighborhood but employs a randomized selection scheme where the probability for accepting a move to a new point in the search space depends on the difference in function values as well as on the current time step. Otherwise, the two algorithms are equal. It is known that the different philosophies of search implemented in the two heuristics can lead to exponential performance gaps between the two algorithms with respect to the expected optimization time. Even for very restricted classes of objective functions where the differences in function values between neighboring points are strictly limited the performance differences can be huge. Here, a more local point of view is taken. Considering obstacles in the fitness landscapes it is proven that the local performance of the two algorithms is remarkably similar in spite of their different search behaviors.",
            "group": 2977,
            "name": "10.1.1.152.9357",
            "keyword": "run time analysis",
            "title": "On the Local Performance of Simulated Annealing and the (1+1) Evolutionary Algorithm  "
        },
        {
            "abstract": "Abstract. In this paper we report preliminary results of experiments with finding efficient circuits (over binary bases) using SAT-solvers. We present upper bounds for functions with constant number of inputs as well as general upper bounds that were found automatically. We focus mainly on MOD-functions. Besides theoretical interest, these functions are also interesting from a practical point of view as they are the core of the residue number system. In particular, we present a circuit of size 3n + c over the full binary basis computing MOD n 3. 1",
            "group": 2978,
            "name": "10.1.1.152.9489",
            "keyword": "",
            "title": "OneSpin Solutions GmbH"
        },
        {
            "abstract": "The paper studies hybrid solution methods for a general class of arc routing problems arising in the context of garbage collection. Important differences of basic local optimizers for arc oriented compared to node oriented problems are worked out. The initial problem is split into its routing (sequencing) and clustering part. For both problems meta-procedures that make use of the modified local search procedures are proposed. The routing part is a well defined problem called Mixed Rural Postman Problem with Turn Penalties. For this problem an Evolutionary Algorithm is implemented and compared to known solution methods. It is able to provide the best known solution quality at the expense of high computational effort. The clustering part is shown to be very application dependent. To offer a flexible modeling of the problem a multi-agent-system using the formerly presented local search operators is proposed. 1",
            "group": 2979,
            "name": "10.1.1.153.196",
            "keyword": "",
            "title": "Local Search and Evolutionary Computation for Arc Routing in Garbage Collection. Proceedings of the Genetic and Evolutionary Computation Conference 2001"
        },
        {
            "abstract": " ",
            "group": 2980,
            "name": "10.1.1.153.305",
            "keyword": "",
            "title": "Adaptive Scheduling in Heterogeneous Distributed Computing Systems"
        },
        {
            "abstract": "Abstract. GRASP, or greedy randomized adaptive search procedure, is a multi-start metaheuristic that repeatedly applies local search starting from solutions constructed by a randomized greedy algorithm. In this chapter we review the basic building blocks of GRASP. We cover solution construction schemes, local search methods, and the use of path-relinking as a memory mechanism in GRASP. Combinatorial optimization can be defined by a finite ground set E = {1,...,n}, a set of feasible solutions F \u2286 2E, and an objective function f: 2E \u2192 R, all three defined for each specific problem. In this chapter, we consider the minimization version of the problem, where we seek an optimal solution S \u2217  \u2208 F such that f(S \u2217 )  \u2264 f(S), \u2200S \u2208 F. Combinatorial optimization finds applications in many settings, including routing, scheduling, inventory and production planning, and facility location. While much progress has been made in finding provably optimal solutions to combinatorial optimization problems employing techniques such as branch and bound,",
            "group": 2981,
            "name": "10.1.1.153.1082",
            "keyword": "",
            "title": "GRASP: GREEDY RANDOMIZED ADAPTIVE SEARCH PROCEDURES"
        },
        {
            "abstract": "This paper introduces the concept of a critical backbone as a minimal set of variables or part of the solution necessary to be within the basin of attraction of the global optimum. The concept is illustrated with a new class of test problems Backbone in which the critical backbone structure is completely transparent. The performance of a number of standard heuristic search methods is measure for this problem. It is shown that a hybrid genetic algorithm that incorporates a descent algorithm solves this problem extremely efficiently. Although no rigorous analysis is given the problem is sufficiently transparent that this result is easy to understand. The paper concludes with a discussion of how the emergence of a critical backbone may be the salient feature in a phase transition from typically easy to typically hard problems.",
            "group": 2982,
            "name": "10.1.1.153.1260",
            "keyword": "BackboneGenetic AlgorithmCrossover",
            "title": "Finding Critical Backbone Structures with Genetic Algorithms"
        },
        {
            "abstract": "For each \u025b> 0, let {X \u025b n} be an irreducible, time-homogeneous Markov chain with a finite state space S and transition function p \u025b (i, j)  = pi,j\u025b U(i,j) (1 + o(1)) where 0 \u2264 U(i, j)  \u2264  \u221e is a cost function. (We assume pi,j = 0 iff U(i, j)  = \u221e.) It has been shown [2] that independent of the initial distribution, there are constants h(i)  \u2265 0 and \u03b2i> 0 such that lim \u025b\u21930 \u00b5 \u025b (i)/\u025b h(i)  = \u03b2i for any i \u2208 S, where \u00b5 \u025b is the invariant distribution of {X \u025b n}. Let S = {i \u2208 S: h(i)  = 0}, which is called the global minimum set. Various asymptotic probabilities related to S have been established in [3]. Among others, starting with the uniform or invariant distribution, the expected hitting time E \u025b T of S is of order \u025b \u2212\u03b4 and the constants \u03b4 and h(i) above can be expressed in terms of a complicated hierarchy of \u201ccycles \u201d related to the cost function U. In this paper, we shall express these constants in terms of Ventcel graphs (minimum cost spanning trees) to simplify the concept and computation of these constants. We also establish some new properties of optimal Ventcel graphs. 1.",
            "group": 2983,
            "name": "10.1.1.153.1416",
            "keyword": "",
            "title": "OPTIMAL VENTCEL GRAPHS, MINIMAL COST SPANNING TREES AND ASYMPTOTIC PROBABILITIES"
        },
        {
            "abstract": "The graph partitioning problem has numerous applications in various scientific fields. It usually involves the effective partitioning of a graph into a number of disjoint sub-graphs/ zones, and hence becomes a combinatorial optimization problem whose worst case complexity is NP-complete. The inadequacies of exact methods, like linear and integer programming approaches, to handle large-size instances of the combinatorial problems have motivated heuristic techniques to these problems. In the present work, a multi-objective evolutionary algorithm (MOEA), a kind of heuristic techniques, is developed for partitioning a graph under multiple objectives and constraints. The developed MOEA, which is a modified form of NSGA-II, is applied to four randomly generated graphs for partitioning them by optimizing three common objectives under five general constraints. The applications show that the MOEA is successful, in most of the cases, in achieving the expected results by partitioning a graph into a variable number of zones.",
            "group": 2984,
            "name": "10.1.1.153.1437",
            "keyword": "1",
            "title": "Management Studies"
        },
        {
            "abstract": "Management decisions involving groundwater supply and remediation often rely on optimization techniques to determine an effective strategy. We introduce several derivative-free sampling methods for solving constrained optimization problems that have not yet been considered in this field, and we include a genetic algorithm for completeness. Two well-documented community problems are used for illustration purposes: a groundwater supply problem and a hydraulic capture problem. The community problems were found to be challenging applications due to the objective functions being nonsmooth, nonlinear, and having many local minima. Because the results were found to be sensitive to initial iterates for some methods, guidance is provided in selecting initial iterates for these problems that improve the likelihood Preprint submitted to Elsevier 14 January 2008of achieving significant reductions in the objective function to be minimized. In addition, we suggest some potentially fruitful areas for future research.",
            "group": 2985,
            "name": "10.1.1.153.2685",
            "keyword": "\u2217 Corresponding author",
            "title": "Comparison of Derivative-Free Optimization Methods for Groundwater Supply and Hydraulic Capture Community Problems"
        },
        {
            "abstract": "This paper describes about a new method for reconstructing a shape of a skin surface replica from three shading images taken with three different lightings. Since the shading images include shadows caused by surface height fluctuation, the conventional photometric stereo method is not suitable for reconstructing its surface shape. In the proposed method, the evaluation function of the surface shape is de ned in consideration of the effects of shadow, then the shape is reconstructed by optimizing the evaluation using simulated annealing. The experiments to reconstruct the shape from synthesized images and real images demonstrate that the proposed method is effective for shape reconstruction from shading images which include shadows.  ",
            "group": 2986,
            "name": "10.1.1.153.4679",
            "keyword": "",
            "title": " Shape Reconstruction of Skin Surface from Shading Images IJsing Simulated Annealing"
        },
        {
            "abstract": "Abstract. The universe of biochemical reactions in metabolic pathways can be modeled as a complex network structure augmented with domain specific annotations. Based on the functional properties of the involved reactions, metabolic networks are often clustered into so-called pathways inferred from expert knowledge. To support the domain expert in the exploration and analysis process, we follow the well-known Table Lens metaphor with the possibility to select multiple foci. In this paper, we introduce a novel approach to generate an interactive layout of such a metabolic network taking its hierarchical structure into account and present methods for navigation and exploration that preserve the mental map. The layout places the network nodes on a fixed rectilinear grid and routes the edges orthogonally between the node positions. Our approach supports bundled edge routes heuristically minimizing a given cost function based on the number of bends, the number of edge crossings and the density of edges within a bundle. 1",
            "group": 2987,
            "name": "10.1.1.153.6038",
            "keyword": "",
            "title": "A Novel Grid-based Visualization Approach for Metabolic Networks with Advanced Focus&Context View"
        },
        {
            "abstract": "Embedding images into a low dimensional space has a wide range of applications: visualization, clustering, and pre-processing for supervised learning. Traditional dimension reduction algorithms assume that the examples densely populate the manifold. Image databases tend to break this assumption, having isolated islands of similar images instead. In this work, we propose a novel approach that embeds images into a low dimensional Euclidean space, while preserving local image similarities based on their scale invariant feature transform (SIFT) vectors. We make no neighborhood assumptions in our embedding. Our algorithm can also embed the images in a discrete grid, useful for many visualization tasks. We demonstrate the algorithm on images with known categories and compare our accuracy favorably to those of competing algorithms. 1",
            "group": 2988,
            "name": "10.1.1.153.7246",
            "keyword": "",
            "title": "Unsupervised Image Embedding Using Nonparametric Statistics"
        },
        {
            "abstract": "We describe a design methodology which allows a fast design and prototyping of dedicated hardware devices to be used in heterogeneous computations. The platforms used in heterogeneous computations consist of a general-purpose COTS architecture which hosts a dedicated hardware device; parts of the computation are mapped onto the former, parts onto the latter, in a way to improve the overall computation efficiency. We report the design and the prototyping of a FPGA-based hardware board to be used in the search of low-autocorrelation binary sequences. The circuit has been designed by using a recently developed Parallel Hardware Generator (PHG) package which produces a synthesizable VHDL code starting from the specific algorithm expressed as a System of Affine Recurrence Equations (SARE). The performance of the realized devices has been compared to those obtained on the same numerical application on several computational platforms.",
            "group": 2989,
            "name": "10.1.1.153.7266",
            "keyword": "General Terms AlgorithmsPerformanceDesignTheory. Keywords Dedicated Hardware DeviceSystems of Affine Recurrence EquationsLow-Autocorrelation Binary Sequences",
            "title": "Parallel Dedicated Hardware Devices for Heterogeneous Computations"
        },
        {
            "abstract": "Operational risk is an important quantitative topic as a result of the Basel II regulatory requirements. Operational risk models need to incorporate internal and external loss data observations in combination with expert opinion surveyed from business specialists. Following the Loss Distributional Approach, this article considers three aspects of the Bayesian approach to the modeling of operational risk. Firstly we provide an overview of the Bayesian approach to operational risk, before expanding on the current literature through consideration of general families of non-conjugate severity distributions, g-and-h and GB2 distributions. Bayesian model selection is presented as an alternative to popular frequentist tests, such as Kolmogorov-Smirnov or Anderson-Darling. We present a number of examples and develop techniques for parameter estimation for general severity and frequency distribution models from a Bayesian perspective. Finally we introduce and evaluate recently developed stochastic sampling techniques and highlight their application to operational risk through the models developed.",
            "group": 2990,
            "name": "10.1.1.153.7481",
            "keyword": "Approximate Bayesian ComputationBasel II Advanced Measurement ApproachBayesian InferenceCompound ProcessesLoss Distributional ApproachMarkov Chain Monte CarloOperational Risk",
            "title": "Bayesian inference, Monte Carlo sampling and operational risk"
        },
        {
            "abstract": "The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. Increasingly, services are exploring dynamic methods to minimize energy costs while respecting their service-level agreements (SLAs). Furthermore, it will soon be important for these services to manage their usage of \u201cbrown energy \u201d (produced via carbon-intensive means) relative to renewable or \u201cgreen \u201d energy. This paper introduces a general, optimization-based framework for enabling multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their SLAs and minimizing energy costs. Based on the framework, we propose policies for request distribution across the data centers. Our policies can be used to abide by caps on brown energy consumption, such as those that might arise from Kyotostyle carbon limits, from corporate pledges on carbon-neutrality, or from limits imposed on services to encourage brown energy conservation. We evaluate our framework and policies extensively through simulations and real experiments. Our results show how our policies allow a service to trade off consumption and cost. For example, using our policies, the service can reduce brown energy consumption by 24 % for only a 10 % increase in cost, while still abiding by SLAs. 1",
            "group": 2991,
            "name": "10.1.1.154.500",
            "keyword": "",
            "title": "Managing the Cost, Energy Consumption, and Carbon Footprint of Internet Services"
        },
        {
            "abstract": "The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure. 1.",
            "group": 2992,
            "name": "10.1.1.154.1370",
            "keyword": "",
            "title": "A learning algorithm for Boltzmann machines"
        },
        {
            "abstract": "Abstract\u2014This paper considers the problem of Gaussian symbols detection in MIMO systems in the presence of channel estimation errors. Under this framework we develop a computationally efficient approximations of the MAP detector. The new detectors are based on a relaxation of the discrete nature of the digital constellation and on the channel estimation error statistics. This leads to a non-convex program that is solved efficiently via a hidden convexity minimization approach. Additionally, we show that using a Bayesian EM approach, comparable BER performance to that of the MAP detector can be achieved. Next we extend the detection scheme to the case where the noise variance is unknown. We present a modified Bayesian EM approach with annealed Gibbs sampling to perform joint noise variance estimation and symbols detection. Simulation results in a random MIMO system show that the proposed algorithm outperforms the linear MMSE receiver in terms of BER. Index Terms\u2014MIMO, MAP estimation, Gaussian constellations, Bayesian EM, Gibbs Sampler",
            "group": 2993,
            "name": "10.1.1.154.2043",
            "keyword": "",
            "title": "1 Detection of Gaussian Constellations in MIMO Systems under Imperfect CSI"
        },
        {
            "abstract": "",
            "group": 2994,
            "name": "10.1.1.154.4524",
            "keyword": "",
            "title": " Boosting a Variable Neighborhood Search for the Periodic Vehicle Routing Problem with Time Windows by ILP Techniques "
        },
        {
            "abstract": "haben enormen Zulauf erhalten. Diese Portale erlauben Zusammenarbeit in bisher ungeahnten Dimensionen. Interessensgemeinschaften entstehen ad-hoc, wachsen auf tausende Teilnehmer an und zerfallen schlussendlich wieder. Die zugrundeliegende Dynamik solcher Kollaborationen ist weitgehend unvorhersehbar und f\u00fchrt zu kontinuierlich wechselnden Systemanforderungen. W\u00e4hrend Menschen sich an unterschiedliche Umst\u00e4nde vergleichsweise leicht anpassen k\u00f6nnen, passt sich Software von selbst, wenn \u00fcberhaupt, nur eingeschr\u00e4nkt an wechselnde Bedingungen an. Diese Dissertation behandelt das Problem wie sich Software- speziell Web Services- an den Gesamtkontext und die Anforderungen von Massenzusammenarbeit anpassen kann. Wenn tausende oder mehr technische und menschliche Entit\u00e4ten zusammenarbeiten, kann kein einzelnes Element die Gesamtbed\u00fcrfnisse erfassen. Infolgedessen erkennt niemand Situationen, welche die Umgestaltung des Gesamtsystems erfordern w\u00fcrden. Ohne entsprechende Anpassungstechniken l\u00e4uft die Zusammenarbeit Gefahr ineffizient zu werden oder gar fr\u00fchzeitig auseinanderzubrechen. Diese Dissertation pr\u00e4sentiert Techniken auf drei Ebenen. Den meisten Einfluss auf",
            "group": 2995,
            "name": "10.1.1.154.6507",
            "keyword": "",
            "title": "eingereicht an der"
        },
        {
            "abstract": " Often adaptive, distributed control can be viewed as an iterated game between independent players. The coupling between the players\u2019 mixed strategies, arising as the system evolves from one instant to the next, is determined by the system designer. Information theory tells us that the most likely joint strategy of the players, given a value of the expectation of the overall control objective function, is the minimizer of a Lagrangian function of the joint strategy. So the goal of the system designer is to speed evolution of the joint strategy to that Lagrangian minimizing point, lower the expectated value of the control objective function, and repeat. Here we elaborate the theory of algorithms that do this using local descent procedures, and that thereby achieve efficient, adaptive, distributed control. ",
            "group": 2996,
            "name": "10.1.1.154.6664",
            "keyword": "",
            "title": "Distributed Control by Lagrangian Steepest Descent "
        },
        {
            "abstract": "A wide range of niching techniques have been investigated in evolutionary and genetic algorithms. In this article, we focus on niching using crowding techniques in the context of what we call local tournament algorithms. In addition to deterministic and probabilistic crowding, the family of local tournament algorithms includes the Metropolis algorithm, simulated annealing, restricted tournament selection, and parallel recombinative simulated annealing. We describe an algorithmic and analytical framework which is applicable to a wide range of crowding algorithms. As an example of utilizing this framework, we present and analyze the probabilistic crowding niching algorithm. Like the closely related deterministic crowding approach, probabilistic crowding is fast, simple, and requires no parameters beyond those of classical genetic algorithms. In probabilistic crowding, sub-populations are maintained reliably, and we show that it is possible to analyze and predict how this maintenance takes place. We also provide novel results for deterministic crowding, show how different crowding replacement rules can be combined in portfolios, and discuss population sizing. Our analysis is backed up by experiments that further increase the understanding of probabilistic crowding.",
            "group": 2997,
            "name": "10.1.1.154.8018",
            "keyword": "local tournaments",
            "title": "The Crowding Approach to Niching in Genetic Algorithms"
        },
        {
            "abstract": "Abstract\u2014There is significant interest in using mobile sensors to protect geographical regions against hazards, in which the sensing resources are distributed according to the varying importance of the sub-regions, such as their numbers of residents exposed to the hazards. The quality of monitoring (QoM) resulting from such proportional-share allocation of the coverage time, in terms of the amount of information captured, is not well understood. In this paper, we analyze the QoM properties of proportional-share mobile sensor coverage, at different fairness time scales, as a function of a wide range of event types, stochastic event staying times, and stochastic event arrival/departure dynamics. Based on the QoM analysis, we optimize a class of periodic mobile coverage schedules that achieve accurate proportional sharing while maximizing the QoM of the total system. I.",
            "group": 2998,
            "name": "10.1.1.154.8483",
            "keyword": "",
            "title": "Quality of Monitoring of Stochastic Events by Proportional-Share Mobile Sensor Coverage"
        },
        {
            "abstract": "In this work we extend a VNS for the periodic vehicle routing problem with time windows (PVRPTW) to a multiple VNS (mVNS) where several VNS instances are applied cooperatively in an intertwined way. The mVNS adaptively allocates VNS instances to promising areas of the search space. Further, an intertwined collaborative cooperation with a generic ILP solver applied on a suitable set covering ILP formulation with this mVNS is proposed, where the mVNS provides the exact method with feasible routes of the actual best solutions, and the ILP solver takes a global view and seeks to determine better feasible route combinations. Experimental results were conducted on newly derived instances and show the advantage of the mVNS as well as of the hybrid approach. The latter yields for almost all instances a statistically significant improvement over solely applying the VNS in a standard way, often requiring less runtime, too. ",
            "group": 2999,
            "name": "10.1.1.154.8532",
            "keyword": "",
            "title": "Multiple Variable Neighborhood Search Enriched with ILP Techniques for the Periodic Vehicle Routing Problem with Time Windows "
        },
        {
            "abstract": "The most recent, and arguably one of the most difficult obstacles to the exponential growth in transistor density predicted by Moore\u2019s Law is that of removing the large amount of heat generated within the tiny area of a microprocessor. The exponential increase in power density and its direct relation to on-chip temperature have, in recent processors, led to very high cooling costs. Since temperature also has an exponential effect on lifetime reliability and leakage power, it has become a first-class design constraint in microprocessor development akin to performance. This dissertation describes work to address the temperature challenge from the perspective of the architecture of the microprocessor. It proposes both the infrastructure to model the problem and several mechanisms that form part of the solution. This research describes HotSpot, an efficient and extensible microarchitectural thermal modeling tool that is used to guide the design and evaluation of various thermal management techniques. It presents several Dynamic Thermal Management (DTM) schemes that distribute heat both over time and space by controlling the level of computational activity. Processor temperature is not only a function of the power density but also the placement and adjacency of hot and cold functional blocks, determined by the floorplan of the microprocessor. Hence, this dissertation also explores various thermally mitigating placement choices",
            "group": 3000,
            "name": "10.1.1.154.8987",
            "keyword": "",
            "title": "Thermal Modeling and Management of Microprocessors"
        },
        {
            "abstract": "",
            "group": 3001,
            "name": "10.1.1.155.443",
            "keyword": "",
            "title": "A unifying framework for iterative approximate best-response algorithms for distributed constraint optimisation problems "
        },
        {
            "abstract": "Abstract................................................................................................................................................................................................1",
            "group": 3002,
            "name": "10.1.1.155.1717",
            "keyword": "",
            "title": "Designing Perceptually Optimized Displays"
        },
        {
            "abstract": "In this working document, we report on a new approach to high performance simulation. The main inspiration to this approach is the concept of complex systems: disparate elements with well defined interactions rules and non nonlinear emergent macroscopic behavior. We provide arguments and mechanisms to abstract temporal and spatial locality from the application and to incorporate this locality into the complete design cycle of modeling and simulation on parallel architectures. Although the main application area discussed here is physics, the presented Virtual Particle (VIP) paradigm in the context of Dynamic Complex Systems (DCS), is applicable to other areas of compute intensive applications. Part I deals with the concepts behind the VIP and DCS models. A formal approach to the mapping of application task-graphs to machine task-graphs is presented. The major part of section 3 has recently (July 1997) been accepted for publication in Complexity. In Part II we will elaborate on the execution behavior of",
            "group": 3003,
            "name": "10.1.1.155.3443",
            "keyword": "",
            "title": "Large Scale Simulations of Complex Systems Part I: Conceptual Framework"
        },
        {
            "abstract": "In this working document, we report on a new approach to high performance simulation. The main inspiration to this approach is the concept of complex systems: disparate elements with well defined interactions rules and non nonlinear emergent macroscopic behavior. We provide arguments and mechanisms to abstract temporal and spatial locality from the application and to incorporate this locality into the complete design cycle of modeling and simulation on parallel architectures. Although the main application area discussed here is physics, the presented Virtual Particle (VIP) paradigm in the context of Dynamic Complex Systems (DCS), is applicable to other areas of compute intensive applications. Part I deals with the concepts behind the VIP and DCS models. A formal approach to the mapping of application task-graphs to machine task-graphs is presented. The major part of section 3 has recently (July 1997) been accepted for publication in Complexity. In Part II we will elaborate on the execution behavior of",
            "group": 3004,
            "name": "10.1.1.155.3443",
            "keyword": "",
            "title": "Large Scale Simulations of Complex Systems Part I: Conceptual Framework"
        },
        {
            "abstract": "Permission to make digital or hard copies of portions of this work for personal or classroom use is granted provided that the copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise requires prior specific permission by the publisher mentioned above.",
            "group": 3005,
            "name": "10.1.1.155.4255",
            "keyword": "",
            "title": "Resources"
        },
        {
            "abstract": "Abstract \u2014 Cross-situational learning is based on the idea that a learner can determine the meaning of a word by finding something in common across all observed uses of that word. Although cross-situational learning is usually modeled through stochastic guessing games in which the input data vary erratically with time (or rounds of the game), here we investigate the possibility of applying the deterministic Neural Modeling Fields (NMF) categorization mechanism to infer the correct object-word mapping. Two different representations of the input data were considered. The first is termed object-word representation because it takes as inputs all possible objectword pairs and weighs them by their frequencies of occurrence in the stochastic guessing game. A re-interpretation of the problem within the perspective of learning with noise indicates that the cross-situational scenario produces a too low signal-tonoise ratio, explaining thus the failure of NMF to infer the correct object-word mapping. The second representation, termed context-word, takes as inputs all the objects that are in the pupil\u2019s visual field (context) when a word is uttered by the teacher. In this case we show that use of two levels of hierarchy of NMF allows the inference of the correct object-word mapping. I.",
            "group": 3006,
            "name": "10.1.1.155.4595",
            "keyword": "",
            "title": "A cross-situational algorithm for learning a lexicon using Neural Modeling Fields"
        },
        {
            "abstract": " ",
            "group": 3007,
            "name": "10.1.1.155.4871",
            "keyword": "branch-cut-and-price Zusammenfassung",
            "title": "Relaxation and Decomposition Methods for Mixed Integer Nonlinear  Programming"
        },
        {
            "abstract": "The appearance of commodity multi-core processors, has spawned a wide interest in\r\nparallel programming, which is widely-regarded as more challenging than sequential\r\nprogramming. KPNs are a model of concurrency that relies exclusively on message\r\npassing, and that has some advantages over parallel programming tools in wide use today:\r\nsimplicity, graphical representation, and determinism. Because of determinism, it\r\nis possible to reliably reproduce faults, an otherwise notoriously difficult problem with\r\nparallel programs. KPNs have gained acceptance in simulation and signal-processing\r\ncommunities. In this thesis, we investigate the applicability of KPNs to implementing\r\ngeneral-purpose parallel computations for multi-core machines. In particular, we investigate\r\n1) how KPNs can be used for modeling general-purpose problems; 2) how an\r\nefficient KPN run-time can be implemented; 3) what KPN scheduling strategies give\r\ngood run-time performance.\r\nFor these purposes, we have developed Nornir, an efficient run-time system for executing\r\nKPNs. With Nornir, we show that it is possible to develop a high-performance\r\nKPN run-time for multi-core machines. We experimentally demonstrate that problems\r\nexpressed in the Kahn model resemble very much their sequential implementations,\r\nyet perform much better than when expressed in the MapReduce model, which has\r\nbecome widely-recognized as a simple parallel programming model. Lastly, we use\r\nNornir to evaluate several load-balancing methods: static assignment, work-stealing,\r\nour improvement of work-stealing, and amethod based on graph partitioning. The understanding\r\nbrought by this evaluation is significant not only in the context of the Kahn\r\nmodel, but also in the more general context of load-balancing (potentially distributed)\r\napplications written in message-passing style.",
            "group": 3008,
            "name": "10.1.1.156.241",
            "keyword": "",
            "title": "Implementation and performance aspects of Kahn process networks"
        },
        {
            "abstract": "Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many AI related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. The aim of the thesis is to demonstrate that deep generative models that contain many layers of latent variables and millions of parameters can be learned efficiently, and that the learned high-level feature representations can be successfully applied in a wide spectrum of application domains, including visual object recognition, information retrieval, and classification and regression tasks. In addition, similar methods can be used for nonlinear dimensionality reduction. The first part of the thesis focuses on analysis and applications of probabilistic generative models called Deep Belief Networks. We show that these deep hierarchical models can learn useful feature representations from a large supply of unlabeled sensory inputs. The learned high-level representations capture a lot of structure in the input data, which is useful for subsequent problem-specific tasks, such as classification, regression or information retrieval, even though these tasks are unknown when the",
            "group": 3009,
            "name": "10.1.1.156.1804",
            "keyword": "",
            "title": " Learning Deep Generative Models"
        },
        {
            "abstract": "This is an Open Access article distributed under the terms of the Creative Commons Attribution License",
            "group": 3010,
            "name": "10.1.1.156.1985",
            "keyword": "",
            "title": "BMC Systems Biology BioMed Central Methodology article Transforming Boolean models to continuous models: methodology and application to T-cell receptor signaling"
        },
        {
            "abstract": "Abstract. Learning To Rank (LTR) techniques aim to learn an effective document ranking function by combining several document features. While the function learned may be uniformly applied to all queries, many studies have shown that different ranking functions favour different queries, and the retrieval performance can be significantly enhanced if an appropriate ranking function is selected for each individual query. In this paper, we propose a novel Learning To Select framework that selectively applies an appropriate ranking function on a per-query basis. The approach employs a query feature to identify similar training queries for an unseen query. The ranking function which performs the best on this identified training query set is then chosen for the unseen query. In particular, we propose the use of divergence, which measures the extent that a document ranking function alters the scores of an initial ranking of documents for a given query, as a query feature. We evaluate our method using tasks from the TREC Web and Million Query tracks, in combination with the LETOR 3.0 and LETOR 4.0 feature sets. Our experimental results show that our proposed method is effective and robust for selecting an appropriate ranking function on a per-query basis. In particular, it always outperforms three state-of-the-art LTR techniques, namely Ranking SVM, AdaRank, and the automatic feature selection method. 1",
            "group": 3011,
            "name": "10.1.1.156.2092",
            "keyword": "",
            "title": "Learning to select a ranking function"
        },
        {
            "abstract": "This paper proposes a construction heuristic and an adaptive large neighborhood search heuristic for the technician and task scheduling problem arising in a large telecommunications company. This problem was solved within the framework of the 2007 challenge set up by the French Operational Research Society (ROADEF). The paper describes the authors \u2019 entry in the competition which tied for second place.",
            "group": 3012,
            "name": "10.1.1.156.4361",
            "keyword": "Heuristics",
            "title": "Scheduling technicians and tasks in a telecommunications company"
        },
        {
            "abstract": "This paper shows how to design good biorthogonal FIR filters for wavelet image compression by balancing the space and frequency dispersions of analysis and synthesis lowpass filters. A quality metric is proposed which can be computed directly from the filter coefficients. By optimizing over the space of FIR filter coefficients, a filter bank can be found which minimizes the metric in about 60 seconds on a high performance workstation. The metric contains three parameters which weight the space and frequency dispersions of the low pass analysis and synthesis filters. A series of biorthogonal, symmetric wavelet filters of length 10 was found, each optimized for different weightings. Each of these filter banks was then evaluated by compressing and decompressing five test images at three compression ratios. Selecting each optimum provides fifteen sets of parameters corresponding to filter banks which maximize the PSNR in each case. The average of these parameters was used to define a \u2018mean \u2019 filter bank, which was then evaluated on the test images. Individual images can produce substantially different weightings of the time dispersion at the optimum, but the PSNR of the mean filter is normally close to the optimum. The mean filter also compares favourably with a maximum regularity biorthogonal filter of the same length. 1. BACKGROUND The theory of continuous and discrete wavelet transforms [1, 2] has inspired much basic and applied research in signal and image processing, as well as revitalizing the study of sub-band filtering [3, 4, 5]. The Discrete Wavelet Transform (DWT) is obtained by repeated filtering and sub-sampling into two bands with low- and high-pass Finite Impulse Response (FIR) filters called the analysis filters. The inverse process makes use of the synthesis FIR filters, and gives perfect reconstruction if the wavelet is biorthogonal. This is easily shown to be the case [4] if the lowpass analysis filter coefficients {c,\u2026, c}",
            "group": 3013,
            "name": "10.1.1.156.4634",
            "keyword": "",
            "title": "Space-frequency balance in biorthogonal wavelets"
        },
        {
            "abstract": " Archival storage systems designed to preserve scientific data, business data, and consumer data must maintain and safeguard tens to hundreds of petabytes of data on tens of thousands of media for decades. Such systems are currently designed in the same way as higherperformance, shorter-term storage systems, which have a useful lifetime but must be replaced in their entirety via a \u201cfork-lift\u201d upgrade. Thus, while existing solutions can provide good energy efficiency and relatively low cost, they do not adapt well to continuous improvements in technology, becoming less efficient relative to current technology as they age. In an archival storage environment, this paradigm implies an endless series of wholesale migrations and upgrades to remain efficient and up to date. Our approach, Logan, manages node addition, removal, and failure on a distributed network of intelligent storage appliances, allowing the system to gradually evolve as device technology advances. By automatically handling most of the common administration chores\u2014integrating new devices into the system, managing groups of devices that work together to provide redundancy, and recovering from failed devices\u2014Logan reduces management overhead and thus cost. Logan can also improve cost and space efficiency by identifying and decommissioning outdated devices, thus reducing space and power requirements for the archival storage system. ",
            "group": 3014,
            "name": "10.1.1.156.5767",
            "keyword": "",
            "title": "Logan: Automatic Management for Evolvable, Large-Scale, Archival Storage"
        },
        {
            "abstract": "Topographic mappings are important in several contexts, including data visualization, connectionist representation, and cortical structure. Many different ways of quantifying the degree of topography of a mapping have been proposed. In order to investigate the consequences of the varying assumptions that these diierent approaches embody, we have optimized the mapping with respect to a number of different measures for a very simple problem- the mapping from a square to a line. The principal results are that (1) different objective functions can produce very different maps, (2) only a small number of these functions produce mappings which match common intuitions as to what a topographic mapping \"should\" actually look like for this problem, (3) the objective functions can be put into certain broad categories based on the overall form of the maps, and (4) certain categories of objective functions may be more appropriate for particular types of problem than other categories. ",
            "group": 3015,
            "name": "10.1.1.156.5833",
            "keyword": "",
            "title": " Objective Functions for Topography: A Comparison of Optimal Maps"
        },
        {
            "abstract": "vorgelegt von",
            "group": 3016,
            "name": "10.1.1.156.6695",
            "keyword": "",
            "title": "List of Abbreviations"
        },
        {
            "abstract": "Abstract. There is growing interest in bio(logy)-inspired approaches that are inspired by the principles of biology and that can solve difficult problems. In this paper, we propose a new computational algorithm that is inspired by molecular mechanics for the solution of complex problems. There is a deep and useful connection between mechanics mechanics and combinatorial optimization. This connection exposes new information and allows an unfamiliar perspective on traditional optimization problems and approaches. The alternative of molecular mechanics algorithm (MMA) to traditional approaches has the advantages of inherent parallelism and the ability to deal with a variety of complicated social interactions, autonomous behaviors and multiple objectives. Keywords: Bio-inspired algorithm, multi-objective optimization, molecular mechanics algorithm (MMA), molecular dynamics. 1",
            "group": 3017,
            "name": "10.1.1.156.8359",
            "keyword": "",
            "title": "Optimization Using a New Bio-inspired Approach"
        },
        {
            "abstract": "The host-seeking behavior of mosquitoes is very interesting. In this paper, we propose a novel mosquito host-seeking algorithm (MHSA) as a new branch of biology-inspired algorithms for solving TSP problems. The MHSA is inspired by the host-seeking behavior of mosquitoes. We present the mathematical model, the algorithm, the motivation, and the biological model. The MHSA can work out the theoretical optimum solution, which is important and exciting, and we give the theoretical foundation and present experiment results that verify this fact. ",
            "group": 3018,
            "name": "10.1.1.156.8383",
            "keyword": "",
            "title": "A New Bio-inspired Approach to the Traveling Salesman Problem"
        },
        {
            "abstract": "Abstract. Middleware for pervasive spaces has to meet conflicting requirements. It has to both maximize the utility of the information exposed and ensure that this information does not violate users \u2019 privacy. In order to resolve these conflicts, we propose a framework grounded in utility theory where users dynamically control the level of disclosure about their information. We begin by providing appropriate definitions of privacy and utility for the type of applications that would support collaborative work in an office environment\u2014current definitions of privacy and anonymity do not apply in this context. We propose a distributed solution that, given a user\u2019s background knowledge, maximizes the utility of the information being disclosed to information recipients while meeting the privacy requirements of users. We implement our solution in the context of a real pervasive space middleware and provide experiments that demonstrate its behaviour. 1",
            "group": 3019,
            "name": "10.1.1.156.8647",
            "keyword": "",
            "title": "Middleware for Pervasive Spaces: Balancing Privacy and Utility"
        },
        {
            "abstract": "Protein sequence alignment is the task of identifying evolutionarily or structurally related positions in a collection of amino acid sequences. Although the protein alignment problem has been studied for several decades, many recent studies have demonstrated considerable progress in improving the accuracy or scalability of multiple and pairwise alignment tools, or in expanding the scope of tasks handled by an alignment program. In this chapter, we review state-of-the-art protein sequence alignment and provide practical advice for users of alignment tools. ",
            "group": 3020,
            "name": "10.1.1.156.8988",
            "keyword": "",
            "title": "  Protein Multiple Sequence Alignment"
        },
        {
            "abstract": "Abstract\u2014In many environments where autonomous air or ground vehicles are used to collect information, there will be a known prioritization of areas of the environment where most valuable information will be found. Over time, priorities may change with areas losing value or suddenly becoming important. In this paper, we present an approach to planning paths for vehicles collecting information in such environments, such that they maximize the overall system information gain over time. A key feature of this path planning problem is that there is not a single or small set of goal points to which the vehicles should try to reach, instead information is collected over the entire path without a particular goal in mind. We present a planning approach, which rapidly expands a search tree, inspired by an RRT planner by choosing promising nodes to expand and expanding them randomly. Genetic algorithms are used to learn sets of configuration parameters for the planner, i.e., how to expand which nodes. Results show that the learned planner gets more substantially information than pre-defined paths in a variety of domains.",
            "group": 3021,
            "name": "10.1.1.156.9294",
            "keyword": "Path planning under uncertaintyRapidlyexploring Random TreeGenetic algorithmRandomized",
            "title": "Path Planning for Autonomous Information Collecting Vehicles"
        },
        {
            "abstract": "Abstract\u2014The concept of multiple-input multiple-output (MIMO) radars has drawn considerable attention recently. Unlike the traditional single-input multiple-output (SIMO) radar which emits coherent waveforms to form a focused beam, the MIMO radar can transmit orthogonal (or incoherent) waveforms. These waveforms can be used to increase the system spatial resolution. The waveforms also affect the range and Doppler resolution. In traditional (SIMO) radars, the ambiguity function of the transmitted pulse characterizes the compromise between range and Doppler resolutions. It is a major tool for studying and analyzing radar signals. Recently, the idea of ambiguity function has been extended to the case of MIMO radar. In this paper, some mathematical properties of the MIMO radar ambiguity function are first derived. These properties provide some insights into the MIMO radar waveform design. Then a new algorithm for designing the orthogonal frequency-hopping waveforms is proposed. This algorithm reduces the sidelobes in the corresponding MIMO radar ambiguity function and makes the energy of the ambiguity function spread evenly in the range and angular dimensions. Index Terms\u2014Ambiguity function, frequency-hopping codes, linear frequency modulation (LFM), MIMO radar, simulated",
            "group": 3022,
            "name": "10.1.1.156.9450",
            "keyword": "annealingwaveform design",
            "title": "MIMO Radar Ambiguity Properties and Optimization Using Frequency-Hopping Waveforms"
        },
        {
            "abstract": "vorgelegt von",
            "group": 3023,
            "name": "10.1.1.156.9601",
            "keyword": "",
            "title": "List of Abbreviations"
        },
        {
            "abstract": "A voxel-based shape representation when integrated with an evolutionary algorithm offers a number of potential advantage for shape optimisation. Topology need not be predefined, geometric constraints are easily imposed and, with adequate resolution, any shape can be approximated to arbitrary accuracy. However, lack of boundary smoothness, length of chromosome and inclusion of small holes in the final shape have been stated as problems with this representation. This paper describes two experiments performed in an attempt to address some of these problems. Firstly, a design problem with only a small computational cost of evaluating candidate shapes was used as a testbed for designing genetic operators for this shape representation. Secondly, these operators were refined for a design problem using a more costly finite element evaluation. It was concluded that the voxel representation can, with careful design of genetic operators, be useful in shape optimisation.",
            "group": 3024,
            "name": "10.1.1.156.9803",
            "keyword": "shape optimisationevolutionary algorithmsvoxel representation. 1. Introduction",
            "title": "Title: A Voxel Based Representation for Evolutionary Shape"
        },
        {
            "abstract": "The reconstruction of surfaces from measured contour points is an obviously difficult task. The most common technique in this field is to approach the original surface by triangulation of the given points. A mesh of triangles",
            "group": 3025,
            "name": "10.1.1.157.1248",
            "keyword": "",
            "title": "Optimal Triangulation by Means of Evolutionary Algorithms"
        },
        {
            "abstract": "Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assume a single file/layer being multicast, layered coding introduces new challenges into the paradigm due to its cumulative decoding nature. Layered coding is designed to handle heterogeneity in receiver capacities, and a node may decode layer k only if it successfully receives all layers in 1..k. We show that recently proposed optimization models for layered multicast do not correctly address this challenge. We argue that in order to achieve the absolute maximum throughput (or minimum cost), it is necessary to decouple application layer throughput from network layer throughput. In particular, a node should be able to receive a non-consecutive layer or a partial layer even if it cannot decode and utilize it (e.g., for playback in media streaming applications). The rationale is that nodes at critical network locations need to receive data just for helping other peers. We present a mathematical programming model that addresses the above challenges and achieves the absolute optimal performance. Simulation results show considerable throughput gain (cost reduction) compared with previous models, in a broad range of network scenarios. We then provide a formal proof that the layered multicast problem is NP-complete. We",
            "group": 3026,
            "name": "10.1.1.157.1434",
            "keyword": "Categories and Subject DescriptorsC.2.0 [Computer-Communication NetworksGeneralC.2.2 [Computer-Communication NetworksNetwork ProtocolsF.2.2 [Analysis of Algorithms and Problem ComplexityNonnumerical Algorithms and ProblemsG.1.6 [Numerical AnalysisOptimization\u2014Integer programmingLinear ProgrammingSimulated Annealing General",
            "title": "Optimal Layered Multicast"
        },
        {
            "abstract": "In recent years the functionality required of computer based control systems for safetycritical real-time applications has increased dramatically. Inevitably this has led to an explosion in the complexity ofsuchsystems and an understanding, in both academia and industry, that existing design methods are no longer adequate. One design issue that has traditionally been addressed in an ad hoc and rather simplistic manner is that of setting the topology of a distributed computer based control system. A topology consists of a con gured set of hardware and software units employed to ful l a set of logical control actions. A topology may employ multiple, possibly diverse, copies of these units to ensure that dependability, timing and functional requirements are met. A designer aims to determine the set of units to be employed and how they should be con gured. A maintainer aims to discover the e ect of a change in functionality, or the units employed, on the e ectiveness of an existing topology. Potentially there are a large number of alternative feasible topologies. Unfortunately, existing techniques rely on past experience and typically set a topology very early in the design process. At best only a fraction of the admissible topologies are considered and",
            "group": 3027,
            "name": "10.1.1.157.2663",
            "keyword": "",
            "title": "Selecting a Topology for Safety-Critical Real-Time Control Systems"
        },
        {
            "abstract": "Abstract. The problem of maintaining geometric structures for points in motion has been well studied over the years. Much theoretical work to date has been based on the assumption that point motion is continuous and predictable, but in practice, motion is typically presented incrementally in discrete time steps and may not be predictable. We consider the problem of maintaining a data structure for a set of points undergoing such incremental motion. We present a simple online model in which two agents cooperate to maintain the structure. One defines the data structure and provides a collection of certificates, which guarantee the structure\u2019s correctness. The other checks that the motion over time satisfies these certificates and notifies the first agent of any violations. We present efficient online algorithms for maintaining both nets and net trees for a point set undergoing incremental motion in a space of constant dimension. We analyze our algorithms \u2019 efficiencies by bounding their competitive ratios relative to an optimal algorithm. We prove a constant factor competitive ratio for maintaining a slack form of nets, and our competitive ratio for net trees is proportional to the square of the tree\u2019s height. 1",
            "group": 3028,
            "name": "10.1.1.157.4617",
            "keyword": "",
            "title": "Maintaining Nets and Net Trees under Incremental Motion \u22c6"
        },
        {
            "abstract": "Today, with digitally stored information available in abundance, even for many minor languages, this information must by some means be filtered and extracted in order to avoid drowning in it. Automatic summarization is one such technique, where a computer summarizes a longer text to a shorter non-rendundant form. Apart from the major languages of the world there are a lot of languages for which large bodies of data aimed at language technology research to a high degree are lacking. There might also not be resources available to develop such bodies of data, since it is usually time consuming and requires substantial manual labor, hence being expensive. Nevertheless, there will still be a need for automatic text summarization for these languages in order to subdue this constantly increasing amount of electronically produced text. This thesis thus sets the focus on automatic summarization of text and the evaluation of summaries using as few human resources as possible. The resources that are used should to as high extent as possible be already existing, not specifically aimed at summarization or evaluation of summaries and, preferably, created as part of natural literary processes.",
            "group": 3029,
            "name": "10.1.1.157.5376",
            "keyword": "",
            "title": "Resource Lean and Portable Automatic Text Summarization"
        },
        {
            "abstract": "",
            "group": 3030,
            "name": "10.1.1.157.6076",
            "keyword": "",
            "title": "Metaheuristic approaches for the quartet method of hierarchical clustering"
        },
        {
            "abstract": "Abstract\u2014Power consumption is a crucial concern in nanometer chip design. Researchers have shown that multiple supply voltage (MSV) is an effective method for power consumption reduction. The underlying idea behind MSV is the tradeoff between power saving and performance. In this paper, we present an effective voltage-assignment technique based on dynamic programming. For circuits without reconvergent fan-outs, an optimal solution for the voltage assignment is guaranteed; for circuits with reconvergent fan-outs, a near-optimal solution is obtained. We then generate a level shifter for each net that connects two blocks in different voltage domains and perform power-network-aware floorplanning for the MSV design. Experimental results show that our floorplanner is very effective in optimizing power consumption under timing constraints. Index Terms\u2014Floorplanning, layout, low power, multiple supply voltage (MSV), physical design. I.",
            "group": 3031,
            "name": "10.1.1.157.6357",
            "keyword": "",
            "title": "Voltage-Island Partitioning and Floorplanning Under Timing Constraints"
        },
        {
            "abstract": "Abstract. In this chapter we introduce the area of research that attempts to study the evolution of communication in embodied agents through adaptive techniques, such us artificial evolution. More specifically, we illustrate the theoretical assumptions behind this type of research, we present the methods that can be used to realize embodied and communicating artificial agents, and we discuss the main research challenges and the criteria for evaluating progresses in this field. 1",
            "group": 3032,
            "name": "10.1.1.158.730",
            "keyword": "years the field has been raising increasing interestprobably because of a general",
            "title": "Evolving communication in embodied agents: Theory, Methods, and Evaluation"
        },
        {
            "abstract": "National Taiwan University Droplet-based microfluidic biochips have recently gained much attention and are expected to revolutionize the biological laboratory procedures. As biochips are adopted for the complex procedures in molecular biology, its complexity is expected to increase due to the need of multiple and concurrent assays on a chip. In this article, we formulate the placement problem of digital microfluidic biochips with a tree-based topological representation, called T-tree. To the best knowledge of the authors, this is the first work that adopts a topological representation to solve the placement problem of digital microfluidic biochips. We also consider the defect tolerant issue to avoid to use defective cells due to fabrication. Experimental results demonstrate that our approach is more efficient and effective than the previous unified synthesis and placement framework.",
            "group": 3033,
            "name": "10.1.1.158.1509",
            "keyword": "Categories and Subject DescriptorsB.7.2 [Integrated CircuitsDesign Aids General TermsAlgorithmsPerformanceDesign Additional Key Words and PhrasesMicrofluidicsbiochipplacement ACM Reference Format",
            "title": "Placement of Defect-Tolerant Digital Microfluidic Biochips Using the T-tree Formulation"
        },
        {
            "abstract": "Abstract\u2014Many proteins undergo extensive conformational changes as part of their functionality. Tracing these changes is important for understanding the way these proteins function. Traditional biophysics-based conformational search methods require a large number of calculations and are hard to apply to large-scale conformational motions. In this work we investigate the application of a robotics-inspired method, using backbone and limited side chain representation and a coarse grained energy function to trace large-scale conformational motions. We tested the algorithm on three well known medium to large proteins and we show that even with relatively little information we are able to trace low-energy conformational pathways efficiently. The conformational pathways produced by our methods can be further filtered and refined to produce more useful information on the way proteins function under physiological conditions. Contact: Lydia E. Kavraki,",
            "group": 3034,
            "name": "10.1.1.158.1766",
            "keyword": "",
            "title": "1 Tracing Conformational Changes in Proteins"
        },
        {
            "abstract": "The FlexRay bus is the prospective automotive standard communication system. For the sake of a high flexibility, the protocol includes a static time-triggered and a dynamic event-triggered segment. This paper is dedicated to the scheduling of the static segment in compliance with the automotive-specific AUTOSAR standard. For the determination of an optimal schedule in terms of the number of used slots, a fast greedy heuristic as well as a complete approach based on Integer Linear Programming are presented. For this purpose, a scheme for the transformation of the scheduling problem into a bin packing problem is proposed. Moreover, a metric and optimization method for the extensibility of partially used slots is introduced. Finally, the provided experimental results give evidence of the benefits of the proposed methods. On a realistic case study, the proposed methods are capable of obtaining better results in a significantly smaller amount of time compared to a commercial tool. Additionally, the experimental results provide a case study on incremental scheduling, a scalability analysis, an exploration use case, and an additional test case to emphasis the robustness and flexibility of the proposed methods. Categories and Subject Descriptors C.3 [Special-purpose and application-based systems]: Real-time and embedded systems",
            "group": 3035,
            "name": "10.1.1.158.2079",
            "keyword": "General Terms DesignAlgorithms",
            "title": "FlexRay Schedule Optimization of the Static Segment"
        },
        {
            "abstract": "Abstract This paper provides an overview of the one-stage R&D portfolio optimization problem. It provides a novel problem model that can be solved with stochastic combinatorial optimization methods. Current solution methods are reviewed and a new method that scales to large problems, Stochastic Gradient Portfolio Optimization (SGPO), is proposed. Although SGPO is a heuristic method, we prove global convergence in certain conditions. SGPO is numerically compared to current optimization methods on a test case involving Solid Oxide Fuel Cells. Keywords R&D portfolio \u00b7 Stochastic combinatorial optimization \u00b7 Solid oxide fuel cell 1",
            "group": 3036,
            "name": "10.1.1.158.2273",
            "keyword": "",
            "title": "with an application to solid oxide fuel cells"
        },
        {
            "abstract": "With the thermal effect, improper analog placements may degrade circuit performance because the thermal impact from power devices can affect electrical characteristics of the thermally-sensitive devices. There is not much previous work that considers the desired placement configuration between power and thermally-sensitive devices for a better thermal profile to reduce the thermally-induced mismatches. In this paper, we first introduce the properties of a desired thermal profile for better thermal matching of the matched devices. We then propose a thermal-driven analog placement methodology to achieve the desired thermal profile and to consider the best device matching under the thermal profile while satisfying the symmetry and the common-centroid constraints. Experimental results based on real analog circuits show that our approach can achieve the best analog circuit performance/accuracy with the least impact due to the thermal gradient, among existing works. Digital Circuitry DAC",
            "group": 3037,
            "name": "10.1.1.158.2525",
            "keyword": "General TermsAlgorithmsDesignReliability KeywordsAnalog placementthermal matching",
            "title": "Thermal-driven Analog Placement Considering Device Matching"
        },
        {
            "abstract": "Abstract\u2014Peer-to-peer (P2P) file sharing systems such as Gnutella have been widely acknowledged as the fastest-growing Internet applications ever. The P2P model has many potential advantages, including high flexibility and serverless management. However, these systems suffer from the well-known performance mismatch between the randomly constructed overlay network topology and the underlying IP-layer topology. This paper proposes to structure the P2P overlay topology using a heterogeneity-aware multitier topology to better balance the load at peers with heterogeneous capacities and to prevent low-capability nodes from throttling the performance of the system. An analytical model is developed to enable the construction and maintenance of heterogeneity-aware overlay topologies with good node connectivity and better load balance. We also develop an efficient routing scheme, called probabilistic selective routing, that further utilizes heterogeneity-awareness to enhance the routing performance. We evaluate our design through simulations. The results show that our multitier topologies alone can provide eight to 10 times improvement in the messaging cost, two to three orders of magnitude improvement in terms of load balancing, and seven to eight times lower topology construction and maintenance costs when compared to Gnutella\u2019s random power-law topology. Moreover, our heterogeneity-aware routing scheme provides further improvements on all evaluation metrics, when used with our heterogeneity-aware overlay topologies. Index Terms\u2014Peer-to-peer systems, overlay topology, overlay routing, node heterogeneity, load balancing. \u00c7 1",
            "group": 3038,
            "name": "10.1.1.158.2648",
            "keyword": "",
            "title": "Large Scaling Unstructured Peer-to-Peer Networks with Heterogeneity-Aware Topology and Routing"
        },
        {
            "abstract": "Abstract\u2014To reduce the effect of parasitic mismatches and circuit sensitivity to thermal gradients or process variations for analog circuits, some pairs of modules need to be placed symmetrically with respect to a common axis, and the symmetric modules are preferred to be placed at closest proximity for better electrical properties. Most previous works handle the problem with symmetry constraints by imposing symmetric-feasible conditions in floorplan representations and using cost functions to minimize the distance between symmetric modules. Such approaches are inefficient due to the large search space and cannot guarantee the closest proximity of symmetry modules. In this paper, we present the first linear-time-packing algorithm for the placement with symmetry constraints using the topological floorplan representations. We first introduce the concept of a symmetry island which is formed by modules of the same symmetry group in a single connected placement. Based on this concept and the B \u2217-tree representation, we propose automatically symmetric-feasible (ASF) B \u2217-trees to directly model the placement of a symmetry island. We then present hierarchical B \u2217-trees (HB \u2217-trees) which can simultaneously optimize the placement with both symmetry islands and nonsymmetric modules. Unlike the previous works, our approach can place the symmetry modules in a symmetry group in close proximity and significantly reduce the search space based on the symmetry-island formulation. In particular, the packing time for an ASF-B \u2217-tree or an HB \u2217-tree is the same as that for a plain B \u2217-tree (only linear) and much faster than previous works. Experimental results show that our approach achieves the best-published quality and runtime efficiency for analog placement. Index Terms\u2014Analog circuit, floorplanning, physical design, placement. I.",
            "group": 3039,
            "name": "10.1.1.158.2741",
            "keyword": "",
            "title": "Analog Placement Based on Symmetry-Island Formulation"
        },
        {
            "abstract": " In many Digital Signal Processors (DSPs) with limited memory, programs are loaded in the ROM and thus it is very important to optimize the size of the code to reduce the memory requirement. Many DSP processors include address generation units (AGUs) that can perform address arithmetic (auto-increment and auto-decrement) in parallel to instruction execution, and without the need for extra instructions. Much research has been conducted to optimize the layout of the variables in memory to get the most benefit from auto-increment and autodecrement. The simple offset assignment (SOA) problem concerns the layout of variables for machines with one address register and the general offset assignment (GOA) deals with multiple address registers. Both these problems assume that each variable needs to be allocated for the entire duration of a program. Both SOA and GOA are NP-complete. In this paper, we present a heuristic for SOA that considers coalescing two or more non-interfering variables into the same memory location. SOA with variable coalescing is intended to decrease the cost of address arithmetic instructions as well as to decrease the memory requirement for variables by maximizing the number of variables mapped to the same memory slot. Results on several benchmarks show the significant improvement of our solution compared to other heuristics. In addition, we have adapted simulated annealing to further improve the solution from our heuristic. ",
            "group": 3040,
            "name": "10.1.1.158.2777",
            "keyword": "",
            "title": "An Effective Heuristic for Simple Offset Assignment with Variable Coalescing"
        },
        {
            "abstract": "A methodology is presented for real-time control of unmanned aerial vehicles (UAV) in the absence of apriori knowledge of location of sites in an inhospitable flight territory. Our proposed hostile control methodology generates a sequence of waypoints to be pursued on the way to the target. Waypoints are continually computed with new information about the nature of changing threat. The Dijkstra algorithm is used to account for a weighted combination of threat measures arising from the probability of encountering hostile ground to air fire as well as the internal urgency to complete the mission in the shortest time. UAVs broadcast latest sensed data to their counterparts. The sequence of waypoints defines the trajectory of the UAV to its target. By varying components of cost function, paths are altered to obtain a desired performance criterion. Validation of our methodology is offered by a series of agentbased simulations. Povzetek: Predstavljena je metoda za upravljanje brezpilotnega letala ali helikopterja v sovra\u017enem okolju.",
            "group": 3041,
            "name": "10.1.1.158.2834",
            "keyword": "agentscollaborationUAVformation flightaffect",
            "title": "Coordinated UAV Manoeuvring Flight Formation"
        },
        {
            "abstract": "Abstract\u2014Compressed sensing is a technique for efficiently sampling signals which are sparse in some transform domain. Recently, the idea of compressed sensing has been used in the radar system. When the number of targets on the range-Doppler plane is small, the target scene can be reconstructed by employing the compressed sensing techniques. In this paper, we extend this idea to the MIMO radar. In the MIMO radar, the compressed sensing technique can be used to reconstruct the target scene when the signals are sparse in the range-Doppler-angle space. To effectively reconstruct the target scene, it is required that the correlation between the target responses be small. In this paper, a waveform design method is introduced to reduce the correlations between target responses. Because of the increased dimensionality in MIMO radars as compared to phased array radars, the impact of compressed sensing will be very significant there. 1 (  \ufffd , f,  \ufffd) (  \ufffd , ,  \ufffd ) f D x \u2026 0 x1 xM-1 y0 y1 yN-1 u0(t) u1(t) uM-1(t) r0(t) r1(t) rN-1(t) Transmitter",
            "group": 3042,
            "name": "10.1.1.158.3250",
            "keyword": "",
            "title": "Compressed Sensing in MIMO Radar"
        },
        {
            "abstract": " Improving logic capacity by time-sharing, dynamically reconfigurable Field Gate Programmable Arrays (FPGAs) are employed to handle designs of high complexity and functionality. In this paper, we use a novel graph-based topological floorplan representation, named 3D-subTCG (3-Dimensional Transitive Closure subGraph), to deal with the 3-dimensional (temporal) floorplanning/placement problem, arising from dynamically reconfigurable FPGAs. The 3D-subTCG uses three transitive closure graphs to model the temporal and spatial relations between modules. We derive the feasibility conditions for the precedence constraints induced by the execution of the dynamically reconfigurable FPGAs. Because the geometric relationship is transparent to the 3D-subTCG and its induced operations (i.e., we can directly detect the relationship between any two tasks from the representation), we can easily detect any violation of the temporal precedence constraints on 3DsubTCG. We also derive important properties of the 3D-subTCG to reduce the solution space and shorten the running time for 3D (temporal) foorplanning/placement. Experimental results show that our 3D-subTCG-based algorithm is very effective and efficient.",
            "group": 3043,
            "name": "10.1.1.158.3636",
            "keyword": "Categories and Subject DescriptorsB.7.2 [Integrated CircuitsDesign Aids General TermsAlgorithmPerformanceDesign Additional Key Words and PhrasesReconfigurable computingpartially dynamical reconfigurationtemporal",
            "title": "Temporal Floorplanning Using the Three-Dimensional Transitive Closure subGraph"
        },
        {
            "abstract": "Abstract \u2014 Publishing person specific data while protecting privacy is an important problem. Existing algorithms that enforce the privacy principle called l-diversity are heuristic based due to the NP-hardness. Several questions remain open: can we get a significant gain in the data utility from an optimal solution compared to heuristic ones; can we improve the utility by setting a distinct privacy threshold per sensitive value; is it practical to find an optimal solution efficiently for real world datasets. This paper addresses these questions. Specifically, we present a pruning based algorithm for finding an optimal solution to an extended form of the l-diversity problem. The novelty lies in several strong techniques: a novel structure for enumerating all solutions, methods for estimating cost lower bounds, strategies for dynamically arranging the enumeration order and updating lower bounds. This approach can be instantiated with any reasonable cost metric. Experiments on real world datasets show that our algorithm is efficient and improves the data utility. I.",
            "group": 3044,
            "name": "10.1.1.158.4127",
            "keyword": "",
            "title": "On Optimal Anonymization for l +-Diversity"
        },
        {
            "abstract": "Abstract\u2014The subject of extracting high-resolution data from low-resolution images is one of the most important digital processing applications in recent years, attracting much research. In this work the authors show how to improve the resolution of an image when a small part of the image is given in high-resolution. To obtain this result the authors use an iterative procedure imposing the low frequencies complete data of the original low-resolution image and the high-resolution data present only in a fraction of the image. The result is a clearer image, with higher correlation to the required high-resolution image. The authors show the use of such a procedure on Rosetta images to demonstrate the higher frequencies obtained and on a text sample to show improvement in textual understanding. Index Terms\u2014Image Processing, Signal reconstruction, Superresolution. I.",
            "group": 3045,
            "name": "10.1.1.158.4497",
            "keyword": "",
            "title": "Iterative Single-Image Digital Super-Resolution Using Partial High-Resolution Data"
        },
        {
            "abstract": "Abstract \u2014 This article presents results from experiments where a detector for defects in visual inspection images was learned from scratch by EANT2, a method for evolutionary reinforcement learning. The detector is constructed as a neural network that takes as input statistical data on filter responses from a bank of image filters applied to an image region. Training is done on example images with weakly labelled defects. Experiments show good results of EANT2 in an application area where evolutionary methods are rare. I.",
            "group": 3046,
            "name": "10.1.1.158.6300",
            "keyword": "",
            "title": "Learning Defect Classifiers for Visual Inspection Images by Neuro-Evolution using Weakly Labelled Training Data"
        },
        {
            "abstract": "Abstract. This paper describes a clustering method for unsupervised classification of objects in large data sets. The new methodology combines the mixture likelihood approach with a sampling and subsampling strategy in order to cluster large data sets efficiently. This sampling strategy can be applied to a large variety of data mining methods to allow them to be used on very large data sets. The method is applied to the problem of automated star/galaxy classification for digital sky data and is tested using a sample from the Digitized Palomar Sky Survey (DPOSS) data. The method is quick and reliable and produces classifications comparable to previous work on these data using supervised clustering. Keywords: clustering algorithm, mixture likelihood, sampling, star/galaxy classification 1.",
            "group": 3047,
            "name": "10.1.1.158.7253",
            "keyword": "",
            "title": "Sampling and subsampling for cluster analysis in data mining: With applications to sky survey data"
        },
        {
            "abstract": "Abstract \u2014 Linear antenna array design is one of the most important electromagnetic optimization problems of current interest. This article describes the application of a recently developed metaheuristic algorithm, known as the Invasive Weed Optimization (IWO), to optimize the spacing between the elements of the linear array to produce a radiation pattern with minimum side lobe level and null placement control.The results of the IWO algorithm have been shown to meet or beat the results obtained using other state-of-the-art metaheuristics like",
            "group": 3048,
            "name": "10.1.1.158.7265",
            "keyword": "null controlinvasive weeds optimization",
            "title": "2009 International Conference of Soft Computing and Pattern Recognition Linear Antenna Array Synthesis with Invasive Weed Optimization Algorithm"
        },
        {
            "abstract": "Abstract. In this article we describe EANT2, Evolutionary",
            "group": 3049,
            "name": "10.1.1.158.8152",
            "keyword": "",
            "title": "Evolutionary reinforcement learning of artificial neural networks"
        },
        {
            "abstract": "Abstract\u2014Distributed estimation of an unknown signal is a common task in sensor networks. The scenario usually envisioned consists of several nodes, each making an observation correlated with the signal of interest. The acquired data is then wirelessly transmitted to a fusion center that aims at estimating the desired signal within a prescribed accuracy. Motivated by the obvious processing limitations inherent to such distributed infrastructures, we seek to find efficient compression schemes that account for limited available power and communication bandwidth. In this paper, we propose a transform-based approach to this problem where each sensor provides the fusion center with a low-dimensional approximation of its local observation by means of a suitable linear transform. Under the mean squared error criterion, we derive the optimal solution to apply at one sensor assuming all else being fixed. This naturally leads to an",
            "group": 3050,
            "name": "10.1.1.158.8225",
            "keyword": "",
            "title": "Dimensionality reduction for distributed estimation in the infinite dimensional regime"
        },
        {
            "abstract": "Abstract Memetic Algorithms have become one of the key methodologies behind solvers that are capable of tackling very large, real-world, optimisation problems. They are being actively investigated in research institutions as well as broadly applied in industry. In this chapter we provide a pragmatic guide on the key design issues underpinning Memetic Algorithms (MA) engineering. We begin with a brief contextual introduction to Memetic Algorithms and then move on to define a Pattern Language for MAs. For each pattern, an associated design issue is tackled and illustrated with examples from the literature. In the last section of this chapter we \u201cfast forward \u201d to the future and mention what, in our mind, are the key challenges that scientistis and practitioner will need to face if Memetic Algorithms are to remain a relevant technology in the next 20 years. 1",
            "group": 3051,
            "name": "10.1.1.158.8587",
            "keyword": "",
            "title": "Memetic algorithms"
        },
        {
            "abstract": "ABSTRACT 1 This paper explores the robust routing of messages among individuals. Traditional routing assumes individuals provide messages to a device connected to a communications network that assumes all responsibility for message delivery. Although each individual may have links to multiple communication devices (office computer, PDA, cell phone), messages are delivered only if there is an end-to-end communication path between communication devices available to each individual. To improve robustness of communication, especially in dynamic ad hoc military networks, this paper models a novel routing paradigm using an integrated communication and social network. The understanding is that individuals can and do route messages through a social network in conjunction with the communication network. An example of this is an individual asking another in his immediate social network to place a call on his behalf when the official communication system is not convenient or is unavailable. We show that it is possible to route messages through the integrated social and communication network by: a) using the ORA social analysis tool to select normalized costs for the social and communication network links (e.g., to reflect the link delay, quality or robustness), and b) using the MONOPATI communication design tool to model the integrated sociocommunication network as a graph and performing QoS routing. Results show that the robustness of message delivery can be improved by 5X through this joint routing, without unnecessary impacts on end to end latency.",
            "group": 3052,
            "name": "10.1.1.158.9613",
            "keyword": "providing Delay/Disruption Tolerant Networking (DTN",
            "title": "Routing Through an Integrated Communication and Social Network"
        },
        {
            "abstract": "Artificial neural networks are computer constructs inspired by the neural structure of the brain. The aim is to approximate the vast learning and signal processing power of the human brain by mimicking its structure and mechanisms. In an artificial neural network (often simply called \u201cneural network\u201d),",
            "group": 3053,
            "name": "10.1.1.158.9873",
            "keyword": "",
            "title": "Evolutionary Learning of Neural Structures for Visuo-Motor Control"
        },
        {
            "abstract": "Acquisition of Neural Topologies\u201d, a method that creates neural networks (NNs) by evolutionary reinforcement learning. The structure of NNs is developed using mutation operators, starting from a minimal structure. Their parameters are optimised using CMA-ES. EANT can create NNs that are very specialised; they achieve a very good performance while being relatively small. This can be seen in experiments where our method competes with a different one, called NEAT, \u201cNeuroEvolution of Augmenting Topologies\u201d, to create networks that control a robot in a visual servoing scenario. 1",
            "group": 3054,
            "name": "10.1.1.159.423",
            "keyword": "Neural NetworksEvolutionary AlgorithmsReinforcement Learning Abstract \u2014 In this article we present EANT\u201cEvolutionary",
            "title": "Self-Organisation of Neural Topologies by Evolutionary Reinforcement Learning"
        },
        {
            "abstract": "Abstract: In this paper, we propose a search technique for nurse scheduling, which deals with it as a multi-objective problem. For each nurse, we first randomly generate a set of legal shift patterns which satisfy all shift-related hard constraints. We then employ an adaptive heuristic to quickly find a solution with the least number of violations on the coverage-related hard constraint, by assigning one of the available shift patterns to each nurse. Next, we apply a coverage repairing procedure to make the resulting solution feasible, by adding / removing any under-covered / over-covered shifts. Finally, to satisfy the soft constraints (or preferences), we present a simulated annealing based search method with the following two options: one with a weighted-sum evaluation function which encourages moves towards users \u2019 predefined preferences, and another one with a domination-based evaluation function which encourages moves towards a more diversified approximated Pareto set. Computational results demonstrate that the proposed technique is applicable to modern hospital environments. 1",
            "group": 3055,
            "name": "10.1.1.159.1601",
            "keyword": "",
            "title": "Pareto-Based Optimization for Multi-objective Nurse Scheduling * Corresponding author"
        },
        {
            "abstract": "Abstract. In this article we present EANT2, a method that creates neural networks (NNs) by evolutionary reinforcement learning. The structure of NNs is developed using mutation operators, starting from a minimal structure. Their parameters are optimised using CMA-ES. EANT2 can create NNs that are very specialised; they achieve a very good performance while being relatively small. This can be seen in experiments where our method competes with a different one, called NEAT, to create networks that control a robot in a visual servoing scenario. 1",
            "group": 3056,
            "name": "10.1.1.159.2030",
            "keyword": "",
            "title": "Efficient learning of neural networks with evolutionary algorithms"
        },
        {
            "abstract": "This paper proposes a method to estimate the truncated right tail of the annual wage and salary income distribution 2 using the rest of the distribution. The right tail of the wage and salary income distribution is the distribution of earners over large wage and salary incomes. See figure 1. The metaphor of regenerating a tail that has been cut off suggests the name of this paper\u2019s model: the Salamander. The model is in need of a name more compact than \u201ca mixture of gamma probability density functions (pdfs) with parameters quite restricted in a particular way\u201d. The U.S. Bureau of the Census masks, or 3 \u201ctopcodes\u201d, large annual wage and salary incomes in public use microdata samples (PUMS) of the March Current Population Survey (CPS) (Weinberg, Nelson, Roemer, and Welniak, 1999) and other household surveys and censuses to prevent the disclosure of the identity of respondents reporting large incomes. The Federal Committee on Statistical Methodology (FCSM, 1994:62) sees a growing risk of disclosure of the identity of respondents whose data appear in a PUMS particularly those with rare traits, such as large incomes. Angle and Tolbert (1999) discuss the impact of topcoding on the analysis of annual wage and salary income data in the PUMS of the March, CPS. The Salamander was developed to estimate statistics of annual wage and salary income from a PUMS despite topcoding which effectively truncates the right tail of the annual wage and salary income distribution at the minimum topcodeable income.",
            "group": 3057,
            "name": "10.1.1.159.4183",
            "keyword": "",
            "title": "A Model That Might Facilitate a Lowering of the Minimum Topcodeable Wage and Salary Income"
        },
        {
            "abstract": "It is widely accepted that learning is closely related to theories of optimisation and information. Indeed, there is no need to learn if there is nothing to optimise; if one possesses full information, then there is simply nothing new to learn. The paper considers learning as an optimisation problem with dynamical information constraints. Unlike the standard approach in the optimal control theory, where the solutions are given by the Hamilton\u2013Jacobi\u2013Bellman equation for Markov time evolution, the optimal solution is presented as the system of canonical Euler equations defining the optimal information\u2013 utility trajectory in the conjugate space. The optimal trajectory is parameterised by the information\u2013utility constraints, which are illustrated on examples for finite and infinite\u2013dimensional cases. Without uncertainty, optimisation corresponds to a simple choice problem, where a preference relation (total preorder) is defined over the underlying set \u2126. If one can represent the preference relation by some real utility function u: \u2126 \u2192 R, then the optimal solution corresponds to the extremum (e.g. the maximum) of this function. The utility function may take the form of a Lagrangian incorporating many objectives, and such",
            "group": 3058,
            "name": "10.1.1.159.6715",
            "keyword": "",
            "title": "In PASCAL 2008 Workshop on Approximate Inference in Stochastic Processes and Dynamical Systems, Cumberland Lodge, UK. Information Evolution of Optimal Learning"
        },
        {
            "abstract": "It is often assumed that learning takes place by changing an otherwise stable neural representation. To test this assumption, we studied changes in the directional tuning of primate motor cortical neurons during reaching movements performed in familiar and novel environments. During the familiar task, tuning curves exhibited slow random drift. During learning of the novel task, random drift was accompanied by systematic shifts of tuning curves. Our analysis suggests that motor learning is based on a surprisingly unstable neural representation. To explain these results, we propose that motor cortex is a redundant neural network, i.e., any single behavior can be realized by multiple configurations of synaptic strengths. We further hypothesize that synaptic modifications underlying learning contain a random component, which causes wandering among synaptic configurations with equivalent behaviors but different neural representations. We use a simple model to explore the implications of these assumptions.",
            "group": 3059,
            "name": "10.1.1.159.8191",
            "keyword": "",
            "title": "  Motor Learning with Unstable Neural Representations"
        },
        {
            "abstract": "The performance of Kriging interpolation for enhancement, smoothing, reconstruction and optimization of a test data set is investigated. Specifically, the ordinary twodimensional Kriging and 2D line Kriging interpolation are investigated and compared with the well-known digital filters for data smoothing. We used an analytical 2D synthetic test data with several minima and maxima. Thus, we could perform detailed analyses in a well-controlled manner in order to assess the effectiveness of each procedure. We have demonstrated that Kriging method can be used effectively to enhance and smooth a noisy data set and reconstruct large missing regions (black zones) in lost data. It has also been shown that, with the appropriate selection of the correlation function (variogram model) and its correlation parameter, one can control the \u2018degree \u2019 of smoothness in a robust way. Finally, we illustrate that Kriging can be a viable ingredient in constructing effective global optimization algorithms in conjunction with simulated annealing. 1",
            "group": 3060,
            "name": "10.1.1.159.8893",
            "keyword": "",
            "title": "DATA ENHANCEMENT, SMOOTHING, RECONSTRUCTION AND OPTIMIZATION BY KRIGING INTERPOLATION"
        },
        {
            "abstract": "Often data analysis problems in Bioinformatics concern the fusion of multisensor outputs or the fusion of multi-source information, where one must integrate different kinds of biological data. Natural computing provides several possibilities in Bioinformatics, especially by presenting interesting nature-inspired methodologies for handling such complex problems. In this article we survey the role of natural computing in the domains of protein structure prediction, microarray data analysis and gene regulatory network generation. We utilize the learning ability of neural networks for adapting, uncertainty handling capacity of fuzzy sets and rough sets for modeling ambiguity, and the search potential of genetic algorithms for efficiently traversing large search spaces.",
            "group": 3061,
            "name": "10.1.1.159.9649",
            "keyword": "Key wordsNatural ComputingBioinformaticsprotein structure prediction",
            "title": "Natural Computing Methods in Bioinformatics: A Survey"
        },
        {
            "abstract": "Abstract. An algorithm of simulated annealing for the job shop scheduling problem is presented. The proposed algorithm restarts with a new value every time the previous algorithm finishes. To begin the process of annealing, the starting point is a randomly generated schedule with the condition that the initial value of the makespan of the schedule does not surpass a previously established upper bound. The experimental results show the importance of using upper bounds in simulated annealing in order to more quickly approach good solutions. 1 Introduccion The job shop scheduling problem (JSSP) is considered to be one of the most difficult to solve in combinatorial optimization. It is also one of the most difficult problems in the NP-hard class [1]. The JSSP consists of a set of machines that each carry out the execution of a set of jobs. Each job consists of a certain number of operations, which must be carried out in a specific order. Each operation is carried out by a specific machine",
            "group": 3062,
            "name": "10.1.1.160.480",
            "keyword": "",
            "title": "Simulated Annealing with Restart to Job Shop Scheduling Problem Using Upper Bounds"
        },
        {
            "abstract": " ",
            "group": 3063,
            "name": "10.1.1.160.1517",
            "keyword": "",
            "title": "  RELIABILITY AND POWER-EFFICIENCY IN ERASURE-CODED STORAGE SYSTEMS"
        },
        {
            "abstract": "Abstract- An algorithm of simulated annealing for the job shop scheduling problem is presented. The proposed algorithm restarts with a new value every time the previous algorithm finishes. To begin the process of annealing, the starting point is a randomly generated schedule with the condition that the initial value of the makespan of the schedule does not surpass a previously established upper bound. The experimental results show the importance of using upper bounds in simulated annealing in order to more quickly approach good solutions. Key-Words:- Job shop, upper bound, scheduling, makespan and simulated annealing.",
            "group": 3064,
            "name": "10.1.1.160.1973",
            "keyword": "",
            "title": "Experimental Analysis in Simulated Annealing to Scheduling Problems when Upper Bounds are used"
        },
        {
            "abstract": "",
            "group": 3065,
            "name": "10.1.1.160.1997",
            "keyword": "",
            "title": "Secure, Energy-Efficient, Evolvable, . . . "
        },
        {
            "abstract": "This paper presents an algorithm that applies a new mechanism in order to generate scheduling which allows for evaluation of the quality of solutions that are obtained in the Job Shop Scheduling Problem (JSSP). In this research, the quality of the solution is evaluated by using the makespan as an objective function. It is demonstrated experimentally that the proposed algorithm has better efficiency and efficacy when compared to the classic form of scheduling generation used to evaluate the solution quality in the JSSP. The efficiency and efficacy obtained by the proposed algorithm make it possible to generate and evaluate a greater number of better quality solutions in less time, so a greater exploration of the solution space for the JSSP can be conducted. 1.",
            "group": 3066,
            "name": "10.1.1.160.2075",
            "keyword": "",
            "title": "An Algorithm of scheduling for the Job Shop Scheduling Problem"
        },
        {
            "abstract": "In many real world optimization problems, several optimization goals have to be considered in parallel. For this reason, there has been a growing interest in multi-objective optimization (MOO) in the past many years. Several new approaches have recently been proposed, which produced very good results. However, existing techniques have solved mainly problems of \u201clow dimension\u201d, i.e., with less than 10 optimization objectives. This chapter proposes a new computational algorithm whose design is inspired by particle mechanics in physics. The algorithm is capable of solving MOO problems of high dimensions. There is a deep and useful connection between particle mechanics and high dimensional MOO. This connection exposes new information and provides an unfamiliar perspective on traditional optimization problems and approaches. The alternative of particle mechanics algorithm (PMA) to traditional approaches can deal with a variety of complicated, large scale, high dimensional MOO problems. 1",
            "group": 3067,
            "name": "10.1.1.160.2201",
            "keyword": "",
            "title": "Nature-Inspired Particle Mechanics Algorithm for Multi-Objective Optimization"
        },
        {
            "abstract": "Abstract\u2014Archival storage systems designed to preserve scientific data, business data, and consumer data must maintain and safeguard tens to hundreds of petabytes of data on tens of thousands of media for decades. Such systems are currently designed in the same way as higherperformance, shorter-term storage systems, which have a useful lifetime but must be replaced in their entirety via a \u201cfork-lift \u201d upgrade. Thus, while existing solutions can provide good energy efficiency and relatively low cost, they do not adapt well to continuous improvements in technology, becoming less efficient relative to current technology as they age. In an archival storage environment, this paradigm implies an endless series of wholesale migrations and upgrades to remain efficient and up to date. Our approach, Logan, manages node addition, removal, and failure on a distributed network of intelligent storage appliances, allowing the system to gradually evolve as device technology advances. By automatically handling most of the common administration chores\u2014integrating new devices into the system, managing groups of devices that work together to provide redundancy, and recovering from failed devices\u2014Logan reduces management overhead and thus cost. Logan can also improve cost and space efficiency by identifying and decommissioning outdated devices, thus reducing space and power requirements for the archival storage system. I.",
            "group": 3068,
            "name": "10.1.1.160.2481",
            "keyword": "",
            "title": "Logan: Automatic Management for Evolvable, Large-Scale, Archival Storage"
        },
        {
            "abstract": " Finding maximum likelihood parameter values for Finite Mixture Model (FMM) is often done with the Expectation Maximization (EM) algorithm. However the choice of initial values can severely affect the time to attain convergence of the algorithm and its efficiency in finding global maxima. We alleviate this defect by embedding the EM algorithm within the variable Neighborhood Search (VNS) methaheurestic framework. Computational experiment in several problems in literature as well as some larger ones are reported. ",
            "group": 3069,
            "name": "10.1.1.160.2667",
            "keyword": "Variable Neighborhood SearchMaximum Likelihood EstimationFinite Gaussian Mixture Model and Global Optimization",
            "title": "EM algorithm and Varible Neighborhood Search for fitting Finite Mixture Model parameters"
        },
        {
            "abstract": "Abstract. Simulated annealing (SA) converges by means of a probability of acceptance toward a minimum value of the cost function to a minimum temperature. When the cost function is very high, the probability of acceptance is minimum when temperature descends to a minimum value, for this, the probability is controlled for the temperature. An incorrect tuning of this parameter makes that the distribution of the probabilities of acceptance along the whole process of SA is slanted toward values very low or very high, what cause fall easily in local optimum. In this paper an analysis of correlation between the standard deviation and the distribution of probabilities of Boltzmann is made. The experimental results demonstrate that the standard deviation obtained through a sample of the solutions space of the problem, allow for a good tune of the initial temperature in SA. 1",
            "group": 3070,
            "name": "10.1.1.160.2785",
            "keyword": "",
            "title": "Analysis of the Simulated Annealing Convergence in Function of the Standard Deviation and the Boltzmann Quotient for Scheduling Problems 1"
        },
        {
            "abstract": "We consider the problem of deploying sensors in a large water distribution network, in order to detect the malicious introduction of contaminants. We show that a large class of realistic objective functions \u2013 such as reduction of detection time and the population protected from consuming contaminated water \u2013 exhibit an important diminishing returns effect called submodularity. We exploit the submodularity of these objectives in order to design efficient placement algorithms with provable performance guarantees. Our algorithms do not rely on mixed integer programming, and scale well to networks of arbitrary size. The problem instances considered in our approach are orders of magnitude (a factor of 72) larger than the largest problems solved in the literature. We show how our method can be extended to multicriteria optimization,",
            "group": 3071,
            "name": "10.1.1.160.4940",
            "keyword": "Water distribution networkscontamination detectionoptimizationalgorithms",
            "title": "Efficient sensor placement optimization for securing large water distribution networks"
        },
        {
            "abstract": "Logic emulation enables designers to functionally verify complex integrated circuits prior to chip fabrication. However, traditional FPGA-based logic emulators have poor inter-chip communication bandwidth, commonly limiting gate utilization to less than 20 percent. Global routing contention mandates the use of expensive crossbar and PC-board technology in a system of otherwise low-cost, commodity parts. Even with crossbar technology,current emulators only use a fraction of potential communication bandwidth because they dedicate each FPGA pin (physical wire) to a single emulated signal (logical wire). Virtual Wires overcome pin limitations by intelligently multiplexing each physical wire among multiple logical wires and pipelining these connections at the maximum clocking frequency of the FPGA. The resulting increase in bandwidth allows effective use of low dimension, direct interconnect. The size of the FPGA array can be decreased as well, resulting in low cost logic emulation. This paper covers major contributions of the MIT Virtual Wires project. In the context of a complete emulation system, we analyze phase-based static scheduling and routing algorithms, present Virtual Wires synthesis methodologies, and overview an operational prototype with 20Kgate boards. Results, including in-circuit emulation of a SPARC microprocessor, indicate that Virtual Wires eliminate the need for expensive crossbar technology while increasing FPGA utilization beyond 45 percent. Theoretical analysis predicts that Virtual Wires emulation scales with FPGA size and average routing distance, while traditional emulation does not. 1",
            "group": 3072,
            "name": "10.1.1.160.7267",
            "keyword": "",
            "title": "Page 1 Logic Emulation with Virtual Wires"
        },
        {
            "abstract": "This article presents an algorithm for the automatic detection of circular shapes from complicated and noisy images. The algorithm is based on a hybrid technique composed of simulated annealing and differential evolution. A new fuzzy objective function has been derived for the edge map of a given image. Minimization of this function with a hybrid annealed differential evolution algorithm leads to the automatic detection of circles on the image. Simulation results over several synthetic as well as natural images with varying range of complexity validate the efficacy of the proposed technique in terms of its final accuracy, speed and robustness.",
            "group": 3073,
            "name": "10.1.1.160.7618",
            "keyword": "computer visionhand drawn",
            "title": "Automatic Circle Detection on Images with Annealed Differential Evolution"
        },
        {
            "abstract": "Abstract | We survey learning algorithms for recurrent neural networks with hidden units, and put the various techniques into a common framework. We discuss xedpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non- xedpoint algorithms, namely backpropagation through time, Elman's history cuto, and Jordan's output feedback architecture. Forward propagation, an online technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the uni ed presentation leads to generalizations of various sorts. We discuss advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones, continue with some \\tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. We present somesimulations, and at the end, address issues of computational complexity and learning speed.",
            "group": 3074,
            "name": "10.1.1.160.7894",
            "keyword": "A. Why Recurrent Networks",
            "title": "Gradient calculation for dynamic recurrent neural networks: a survey"
        },
        {
            "abstract": "Semi-supervised learning is an emerging computational paradigm for learning from limited supervision by utilizing large amounts of inexpensive, unsupervised observations. Not only does this paradigm carry appeal as a model for natural learning, but it also has an increasing practical need in most if not all applications of machine learning \u2013 those where abundant amounts of data can be cheaply and automatically collected but manual labeling for the purposes of training learning algorithms is often slow, expensive, and error-prone. In this thesis, we develop families of algorithms for semi-supervised inference. These algorithms are based on intuitions about the natural structure and geometry of probability distributions that underlie typical datasets for learning. The classical framework of Regularization in Reproducing Kernel Hilbert Spaces (which is the basis of state-of-the-art supervised algorithms such as SVMs) is extended in several ways to utilize unlabeled data. These extensions are embodied in the following contributions: (1) Manifold Regularization is based on the assumption that high-dimensional",
            "group": 3075,
            "name": "10.1.1.160.8544",
            "keyword": "",
            "title": "ON SEMI-SUPERVISED KERNEL METHODS "
        },
        {
            "abstract": "",
            "group": 3076,
            "name": "10.1.1.160.8570",
            "keyword": "distributed heterogeneous computing systemsparticle swarm optimizationscheduling Sensors 20099 5340",
            "title": "Metaheuristic Based Scheduling Meta-Tasks in Distributed Heterogeneous Computing Systems"
        },
        {
            "abstract": "This paper introduces a new dynamic neighborhood network for particle swarm optimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO) algorithm, each particle initially joins a default number of what we call \u2018clubs\u2019. Each particle is affected by its own experience and the experience of the best performing member of the clubs it is a member of. Clubs membership is dynamic, where the worst performing particles socialize more by joining more clubs to learn from other particles and the best performing particles are made to socialize less by leaving clubs to reduce their strong influence on other members. Particles return gradually to default membership level when they stop showing extreme performance. Inertia weights of swarm members are made random within a predefined range. This proposed dynamic neighborhood algorithm is compared with other two algorithms having static neighborhood topologies on a set of classic benchmark problems. The results showed superior performance for C-PSO regarding escaping local optima and convergence speed. I.",
            "group": 3077,
            "name": "10.1.1.160.8699",
            "keyword": "",
            "title": "Clubs-based Particle Swarm Optimization"
        },
        {
            "abstract": "Abstract \u2014 This paper presents a new technique for induction motor parameter identification. The proposed technique is based on a simple startup test using a standard V/F inverter. The recorded startup currents are compared to that obtained by simulation of an induction motor model. A Modified PSO optimization is used to find out the best model parameter that minimizes the sum square error between the measured and the simulated currents. The performance of the modified PSO is compared with other optimization methods including line search, conventional PSO and Genetic Algorithms. Simulation results demonstrate the ability of the proposed technique to capture the true values of the machine parameters and the superiority of the results obtained using the modified PSO over other optimization techniques. I.",
            "group": 3078,
            "name": "10.1.1.160.8800",
            "keyword": "",
            "title": "Cairo University"
        },
        {
            "abstract": "Summary. In this chapter, we introduce several nature inspired meta-heuristics for scheduling jobs on computational grids. Our approach is to dynamically generate an optimal schedule so as to complete the tasks in a minimum period of time as well as utilizing the resources in an efficient way. We evaluate the performance of Genetic Algorithm (GA), Simulated Annealing (SA), Ant Colony optimization (ACO) and Particle Swarm Optimization (PSO) Algorithm. Finally, the usage of Multi-objective Evolutionary Algorithm (MOEA) for two scheduling problems are also illustrated.",
            "group": 3079,
            "name": "10.1.1.160.9505",
            "keyword": "Nature Inspired Meta-heuristicsMulti-objective OptimizationJob SchedulingGrid ComputingGenetic AlgorithmsSimulated AnnealingAnt ColonyParticle Swarm Optimization",
            "title": "9 Nature Inspired Meta-heuristics for Grid Scheduling: Single and Multi-objective Optimization Approaches"
        },
        {
            "abstract": "",
            "group": 3080,
            "name": "10.1.1.160.9546",
            "keyword": "",
            "title": "Probabilistic Variational Methods for Vision based Complex Motion Analysis"
        },
        {
            "abstract": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.",
            "group": 3081,
            "name": "10.1.1.161.180",
            "keyword": "General TermsAlgorithms Additional Key Words and PhrasesCluster analysisclustering applicationsexploratory data analysis",
            "title": "AND"
        },
        {
            "abstract": "FPGA-based ASIC development systems have become important tools in contemporary ASIC design. Existing systems exhibit low per-FPGA gate utilization (10 to 20 percent) due to limited inter-chip communication. Attempts at overcoming this limitation through the use of high dimensional interconnection topologies have met with limited success. This paper focuses on the prototype hardware and software interfaces that have been developed for an FPGA-based ASIC emulation system based on a new technique for overcoming inter-chip communication limitations. This technique, referred to as virtual wires, intelligently multiplexes each physical FPGA wire among a number of logical wires. The Virtual Wires Emulation System exhibits high FPGA gate utilization while achieving system speeds comparable to existing logic emulators. A two-dimensional mesh interconnection topology of FPGAs is used to eliminate the cost of signal switching elements and to facilitate scalability. A system capable of emulating 20,000 gates has been constructed for under $3000. This system includes both prototype emulation hardware and a Virtual Wires netlist compiler. Currently, this system is being used as both a simulation accelerator for the LSI Logic LSIM and Cadence Verilog simulators and as an in-circuit emulator. Results from mapping netlists, such as the 18K gate Sparcle microprocessor [2], to this system for simulation acceleration and in-circuit emulation indicate that virtual wires can substantially increase FPGA utilization without adversely affecting emulation speed.",
            "group": 3082,
            "name": "10.1.1.161.535",
            "keyword": "FPGAlogic emulationprototypingmesh topologystatic routingvirtual wires",
            "title": "Page 1 The Virtual Wires Emulation System: A Gate-Efficient ASIC Prototyping Environment"
        },
        {
            "abstract": "The typical planning, design or operations problem has multiple objectives and constraints. Such problems can be solved using only autonomous agents, each specializing in a small and distinct subset of the overall objectives and constraints. No centralized control is necessary. Instead, agents collaborate by observing and modifying one another\u2019s work. Convergence to good solutions for a variety of real and academic problems has been obtained by embedding a few simple rules in each agent. The paper develops these rules and illustrates their use.",
            "group": 3083,
            "name": "10.1.1.161.691",
            "keyword": "A-TeamsAgentAsynchronousAutonomousCollaborationConstraintCooperationNonlinearOptimizationSpecialized",
            "title": "A Collaboration Strategy for Autonomous, Highly Specialized Agents"
        },
        {
            "abstract": "Due to their excellent performance in solving combinatorial optimization problems, metaheuristics algorithms such as Genetic Algorithms (GA), Simulated Annealing (SA) and Tabu Search TS make up another class of search methods that has been adopted to efficiently solve dynamic optimization problem. Most of these methods are confined to the population space and in addition the solutions of nonlinear problems become quite difficult especially when they are heavily constrained. They do not make full use of the historical information and lack prediction about the search space. Besides the knowledge that individuals inherited \"genetic code\" from their ancestors, there is another component called Culture. In this paper, a novel culture-based GA algorithm is proposed and is tested against multidimensional and highly nonlinear real world applications. 1.",
            "group": 3084,
            "name": "10.1.1.161.1422",
            "keyword": "",
            "title": "Cultural-Based Genetic Algorithm: Design and Real World Applications"
        },
        {
            "abstract": "Abstract: Linear antenna array design is one of the most important electromagnetic optimization problems of current interest. This article describes the application of a recently developed metaheuristic algorithm, known as the Invasive Weed Optimization (IWO), to optimize the spacing between the elements of the linear array to produce a radiation pattern with minimum side lobe level and null placement control.The results of the IWO algorithm have been shown to meet or beat the results obtained using other state-of-the-art metaheuristics like the Genetic Algorithm (GA), Particle Swarm",
            "group": 3085,
            "name": "10.1.1.161.1542",
            "keyword": "Antenna arraynull controlinvasive weeds optimizationside lobe",
            "title": "Linear Antenna Array Synthesis with Invasive Weed Optimization Algorithm"
        },
        {
            "abstract": "Abstract \u2014 Meta-heuristic optimization approaches are commonly applied to many discrete optimization problems. Many of these optimization approaches are based on a local search operator like, e.g., the mutate or neighbor operator that are used in Evolution Strategies or Simulated Annealing, respectively. However, the straightforward implementations of these operators tend to deliver infeasible solutions in constrained optimization problems leading to a poor convergence. In this paper, a novel scheme for a local search operator for discrete constrained optimization problems is presented. By using a sophisticated methodology incorporating a backtracking-based ILP solver, the local search operator preserves the feasibility also on hard constrained problems. In detail, an implementation of the local serach operator as a feasibility-preserving mutate and neighbor operator is presented. To validate the usability of this approach, scalable discrete constrained testcases are introduced that allow to calculate the expected number of feasible solutions. Thus, the hardness of the testcases can be quantified. Hence, a sound comparison of different optimization methodologies is presented. I.",
            "group": 3086,
            "name": "10.1.1.161.2765",
            "keyword": "",
            "title": "A feasibility-preserving local search operator for constrained discrete optimization problems"
        },
        {
            "abstract": "In this paper, we present an architecture exploration methodology for low-end embedded systems where the reduction of cost is a primary design concern. The architecture exploration of such systems needs to explore a wide design space spanned by detailed architecture parameters through cycle-accurate performance estimation. For fast exploration, the proposed methodology is based on an efficient evolutionary algorithm, called QEA, and trace-driven simulation to evaluate architecture candidates quickly. We applied the proposed methodology to NAND flashbased Multimedia Card as a case study considering the following design parameters: buffer size, flash memory configuration, clock, communication architecture, and memory allocation. The experimental results validate the proposed methodology by showing the optimal architecture configurations with varying performance constraints and design parameters. 1.",
            "group": 3087,
            "name": "10.1.1.161.3569",
            "keyword": "",
            "title": "Architecture Exploration of NAND Flash-based Multimedia Card"
        },
        {
            "abstract": " ",
            "group": 3088,
            "name": "10.1.1.161.4541",
            "keyword": "",
            "title": "Vehicle Routing 2 Handling Side Constraints"
        },
        {
            "abstract": "Abstract \u2014 Rapid advances of microarray technologies are making it possible to analyze and manipulate large amounts of gene expression data. Clustering algorithms, such as hierarchical clustering, self-organizing maps, k-means clustering and fuzzy k-means clustering, have become important tools for expression analysis of microarray data. However, the need of prior knowledge of the number of clusters, k, and the fuzziness parameter, b, limits the usage of fuzzy clustering. Few approaches have been proposed for assigning best possible values for such parameters. In this paper, we use simulated annealing and fuzzy k-means clustering to determine the optimal parameters, namely the number of clusters, k, and the fuzziness parameter, b. Our results show that a nearly-optimal pair of k and b can be obtained without exploring the entire search space. Index Terms \u2014 Simulated annealing, clustering, DNA microarray analysis, machine learning.",
            "group": 3089,
            "name": "10.1.1.161.4741",
            "keyword": "",
            "title": "1 A Simulated Annealing Approach to Find the Optimal Parameters for Fuzzy Clustering Microarray"
        },
        {
            "abstract": "We describe a code transformation technique that, given code for a vector function F, produces code suitable for computing collections of Jacobian-vector products F \u2032 (x) \u02d9x or Jacobian-transpose-vector products F \u2032 (x) T \u00afy. Exploitation of scarcity \u2014 a measure of the degrees of freedom in the Jacobian matrix \u2014 means solving a combinatorial optimization problem that is believed to be hard. Our heuristics transform the computational graph for F, producing, in the form of a transformed graph G \u2032 , a representation of the Jacobian F \u2032 (x) that is both concise and suitable for the evaluation of large collections of Jacobian-vector products or Jacobian-transpose-vector products. Our heuristics are randomized in nature and compare favorably in all cases with the best known heuristics. 1",
            "group": 3090,
            "name": "10.1.1.161.7050",
            "keyword": "",
            "title": "Randomized Heuristics for Exploiting Jacobian Scarcity"
        },
        {
            "abstract": "This thesis is about estimating probabilistic models to uncover useful hidden structure in data; specifically, we address the problem of discovering syntactic structure in natural language text. We present three new parameter estimation techniques that generalize the standard approach, maximum likelihood estimation, in different ways. Contrastive estimation maximizes the conditional probability of the observed data given a \u201cneighborhood\u201d of implicit negative examples. Skewed deterministic annealing locally maximizes likelihood using a cautious parameter search strategy that starts with an easier optimization problem than likelihood, and iteratively moves to harder problems, culminating in likelihood. Structural annealing is similar, but starts with a heavy bias toward simple syntactic structures and gradually relaxes the bias. Our estimation methods do not make use of annotated examples. We consider their performance in both an unsupervised model selection setting, where models trained under different initialization and regularization settings are compared by evaluating the training objective on a small set of unseen, unannotated development data, and supervised model selection, where the most accurate model on the development set (now with annotations)",
            "group": 3091,
            "name": "10.1.1.161.7564",
            "keyword": "",
            "title": "Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text"
        },
        {
            "abstract": "",
            "group": 3092,
            "name": "10.1.1.162.2009",
            "keyword": "1.1 Single Frame Methods............................ 2",
            "title": " SINGLE AND MULTI-FRAME VIDEO QUALITY ENHANCEMENT  "
        },
        {
            "abstract": "The use of low cost passive RFID tags has become very prevalent. Lightweight cryptography has evolved to address the security issues in these tags. The Gossamer protocol is a lightweight mutual authentication protocol which has guaranteed considerable security in these passive tags. As power is a major constraint in these tags, a detailed analysis of the power consumed by the Gossamer protocol was undertaken and results presented. It was determined that optimizing the power consumed by the protocol can result in substantial power savings and hence significantly enhance the performance of the RFID system. This paper proposes a new optimization model based on the simulated annealing method, that ensures the bounding of the number of power-crunching operations carried out in each run of the Gossamer protocol. The validity of the model is also established by the presentation and discussion of an experimental instance.",
            "group": 3093,
            "name": "10.1.1.163.4757",
            "keyword": "GossamerRFIDMutual Authentication ProtocolOptimizationSimulated Annealing",
            "title": "Simulated Annealing based Optimization Model for the Gossamer Protocol"
        },
        {
            "abstract": "In this work we study a hybrid local search algorithm for the solution of timetabling problems, and we undertake a systematic statistical study of the relative influence of the relevant features on the performances of the algorithm. In particular, we apply statistical methods for the design and analysis of experiments. This work is still ongoing, and its ultimate objective is to develop a procedure for obtaining the best combination of parameters for the algorithm for a given instance and predicting them for the unseen ones. 1",
            "group": 3094,
            "name": "10.1.1.163.5817",
            "keyword": "",
            "title": "A Statistical Analysis of the Features of a Hybrid Local Search Algorithm For Course Timetabling Problems"
        },
        {
            "abstract": "Is the result that equilibrium trading outcomes are efficient in markets without frictions robust to a scenario where agents\u2019 beliefs and plans aren\u2019t already aligned at their equilibrium values? In this paper, starting from a situation where agents\u2019 beliefs and plans aren\u2019t already aligned at their equilibrium values, we study whether out-ofequilibrium trading converges to efficient allocations. We show that out-of-equilibrium trading does converge with probability 1 to an efficient allocation even when traders have limited information and trade cautiously. In economies where preferences can be represented by Cobb-Douglass utility functions, we show, numerically, that the rate of convergence will be exponential. We show that experimentation leads to convergence in some examples where multilateral exchange is essential to achieve gains from trade. We prove that experimentation does converge with probability 1 to an efficient allocation and the speed of convergence remains exponential with Cobb-Douglass utility functions.",
            "group": 3095,
            "name": "10.1.1.163.6826",
            "keyword": "out-of-equilibriumcautioustradingefficiency",
            "title": "OUT-OF-EQUILIBRIUM DYNAMICS WITH DECENTRALIZED EXCHANGE: CAUTIOUS TRADING AND CONVERGENCE TO EFFICIENCY"
        },
        {
            "abstract": "How do multiple elements/agents self-organize into global patterns based on local communications and interactions? This paper describes a theoretical and simulation model called \u201cDigital Hormone Model \u201d (DHM) for such a self-organization task. The model is inspired by two facts: complex biological patterns are results of self-organization of homogenous cells regulated by hormone-like chemical signals (Jiang et al. 1999), and distributed controls can enable self-reconfigurable agents to performance locomotion and reconfiguration",
            "group": 3096,
            "name": "10.1.1.163.7357",
            "keyword": "",
            "title": "Digital Hormone Models for Self-Organization"
        },
        {
            "abstract": "Despite the rapid adoption of Voice over IP (VoIP), its security implications are not yet fully understood. Since VoIP calls may traverse untrusted networks, packets should be encrypted to ensure confidentiality. However, we show that when the audio is encoded using variable bit rate codecs, the lengths of encrypted VoIP packets can be used to identify the phrases spoken within a call. Our results indicate that a passive observer can identify phrases from a standard speech corpus within encrypted calls with an average accuracy of 50%, and with accuracy greater than 90 % for some phrases. Clearly, such an attack calls into question the efficacy of current VoIP encryption standards. In addition, we examine the impact of various features of the underlying audio on our performance and discuss methods for mitigation. 1",
            "group": 3097,
            "name": "10.1.1.163.7653",
            "keyword": "",
            "title": "Spot me if you can: Uncovering spoken phrases in encrypted VoIP conversations"
        },
        {
            "abstract": "functions with simulated annealing*",
            "group": 3098,
            "name": "10.1.1.163.7928",
            "keyword": "Key wordsSimulated annealingGlobal optimizationEstimation algorithms",
            "title": "Global optimization of statistical"
        },
        {
            "abstract": null,
            "group": 3099,
            "name": "10.1.1.163.8747",
            "keyword": "",
            "title": "4 5 6"
        },
        {
            "abstract": "We show that the natural evolutionary algorithm for the all-pairs shortest path problem is significantly faster with a crossover operator than without. This is the first theoretical analysis proving the usefulness of crossover for a non-artificial problem. 1",
            "group": 3100,
            "name": "10.1.1.163.9588",
            "keyword": "",
            "title": "Crossover can provably be useful in evolutionary computation"
        },
        {
            "abstract": "This paper presents a path planning technique for a mobile manipulator whose end-effector path is imposed by a given task. The planning is done decoupling the kinematics of the mobile platform and the manipulator, and planning for the former as a normal mobile robot. Two criteria for planning the path were implemented, one minimizing the need to turn the mobile robot and the other minimizing the gravity induced torque components of the manipulator. The linear velocity for following the path is also calculated, as a function of the given endeffector speed. Several results are presented in some typical applications of this planning.",
            "group": 3101,
            "name": "10.1.1.164.910",
            "keyword": "Mobile manipulatorPath planningNonholonomyMinimization",
            "title": "SMOOTH LOCAL PATH PLANNING FOR A MOBILE MANIPULATOR"
        },
        {
            "abstract": " ",
            "group": 3102,
            "name": "10.1.1.164.1171",
            "keyword": "",
            "title": "Separating Figure from Ground with a Boltzmann Machine "
        },
        {
            "abstract": "An important idea within cognitive science is that much general knowledge can be represented as constraints between the slot-fillers of a schcma. The central idea of \"connectionism \" is that knowledge is represented by the strengths of the connections in a large nctwork of simple processing elements. The relation between these two ideas is complex. Several ways of using connectionist networks to implement schemas have been proposed. The obvious, \"localist \" approach is to identify processing units in the physical network with concepts, and to treat the physical links as if they were direct implementations of the pointers that are conventionally used to represent the filling of a schema-slot by an object (Feldman and Ballard, 1982; Fahlman, 1979). An alternative, \"distributed approach is to allocate a large number of units to each slot of a schema, and to represent the filler of that slot by the pattern of activity of that set of units (Hinton, 1981). The main difference is in how the physical parallelism is used. In the distributed approach, only one instantiation of a particular schema is possible at a time because the units dedicated to each slot can only have one pattern of activity at a time. The physical links are used to implement constraints between slot-fillers. By setting the strengths of the links appropriately, it is possible to make a pattern of activity in one set of units cause (or prohibit) a pattern in another set of units. If each component of a pattern of activity is viewed as a semantic feature of the object",
            "group": 3103,
            "name": "10.1.1.164.1437",
            "keyword": "",
            "title": "Carnegie-Mellon University"
        },
        {
            "abstract": "This paper is concerned with classifying high dimensional data into one of two categories. In various settings, such as when dealing with fMRI and microarray data, the number of variables is very large, which makes well-known classification techniques impractical. The number of variables might be reduced via principal component analysis or some robust analog, but these methods are usually unsatisfactory for the purpose of classification because they are unsupervised learning methods and not designed to minimize classification errors. In this paper, we propose a classification guided dimensionality reduction approach incorporating a stochastic search algorithm in order to look for the optimal subspace in the context of classification. Two different versions of the simulated annealing algorithm are implemented to produce sparse and dense models, respectively. Using data from both simulation and real world studies, situations are found where the misclassification rate can be reduced by the proposed approach.",
            "group": 3104,
            "name": "10.1.1.164.2232",
            "keyword": "KEY WORDSData reductionClassificationStochastic search",
            "title": "Data Reduction in Classification: A Simulated Annealing Based Projection Method"
        },
        {
            "abstract": "This paper introduces a new algorithm to parse discourse within the framework of Rhetorical Structure Theory (RST). Our method is based on recent advances in the field of statistical machine learning (multivariate capabilities of Support Vector Machines) and a rich feature space. RST offers a formal framework for hierarchical text organization with strong applications in discourse analysis and text generation. We demonstrate automated annotation of a text with RST hierarchically organised relations, with results comparable to those achieved by specially trained human annotators. Using a rich set of shallow lexical, syntactic and structural features from the input text, our parser achieves, in linear time, 73.9 % of professional annotators\u2019 human agreement F-score. The parser is 5 % to 12 % more accurate than current state-of-the-art parsers. 1",
            "group": 3105,
            "name": "10.1.1.164.2526",
            "keyword": "",
            "title": "A Novel Discourse Parser Based on Support Vector Machine Classification"
        },
        {
            "abstract": "Abstract. We present Fitness Expectation Maximization (FEM), a novel method for performing \u2018black box \u2019 function optimization. FEM searches the fitness landscape of an objective function using an instantiation of the well-known Expectation Maximization algorithm, producing search points to match the sample distribution weighted according to higher expected fitness. FEM updates both candidate solution parameters and the search policy, which is represented as a multinormal distribution. Inheriting EM\u2019s stability and strong guarantees, the method is both elegant and competitive with some of the best heuristic search methods in the field, and performs well on a number of unimodal and multimodal benchmark tasks. To illustrate the potential practical applications of the approach, we also show experiments on finding the parameters for a controller of the challenging non-Markovian double pole balancing task. 1",
            "group": 3106,
            "name": "10.1.1.164.2624",
            "keyword": "",
            "title": "Fitness Expectation Maximization"
        },
        {
            "abstract": " ",
            "group": 3107,
            "name": "10.1.1.164.3299",
            "keyword": "General TermsAlgorithms Additional Key Words and PhrasesCluster analysisclustering applicationsexploratory data analysis",
            "title": "An ERS Model for Tense and Aspect Information  . . . "
        },
        {
            "abstract": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth'value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:",
            "group": 3108,
            "name": "10.1.1.164.4982",
            "keyword": "",
            "title": "Optimal perceptual inference"
        },
        {
            "abstract": "Chaque &ment rspi#nl ww hypdae, a ba intMactknr mtm k t m m/(hodr aaWqua fownbmmt une description du w nl.tkn trb JmOk w ~ rht ~ s global et lea valeun &a A cnW computatbnd problem in ~ercepUon b march. Ohrm 8 ~#(dqndklatehypolh.scw;rbouthowtok~prmor mpecwdm~,ndrrtdplawibkcoMtrainle~ tl~rm,m.vrkrsrmudbslrignrdtO~~~mto minim& @ lha total vidaUon d the daudbk csc\\atninla Thk kdo~byrHowlngrndworkdcompuUnq~reLolmtle into a rtrM. atate. Each element rcpnrentS r Mpsthsd & urb the intaractionrbetween thedmrnlmP#(NMlt ttn~WtmkW Until recently it haa bden vl~y diAlcutt to analyze thb Wnd d ooopMatlw computation, bmxusa tho dsmmlrr mrwt bs nan-linw and cross~coupkd. If, howcllwan, the demnb dopt discrete date8 according to a partkuh sto~AI8tk full~tion of thdr inputs, it b pdMe to use atatbtkd mchank ~ to deWib8 the behavior of the whok system. fhb bad ^ to a very dWk rdationshlp between the pobrPlriHIy Qs lintling the network in a particular global atate and the sUm@b d the local inl(KaClion8 between the krdividud computiryl drmsnk This rslatloMniP allowe r caopentlvs system to bm WI impkit conatrainto in a domain simply by Wng shown rxunpw from that donuin. Th. kuning pr0crd~fO CfMtw krWlld",
            "group": 3109,
            "name": "10.1.1.164.5059",
            "keyword": "",
            "title": "RESUME SUMMARY"
        },
        {
            "abstract": "2. Spatial \u00a2ltering 1263 (a) Functional^anatomical variability 1263 (b) Image smoothing and signal detection 1264",
            "group": 3110,
            "name": "10.1.1.164.5210",
            "keyword": "",
            "title": "CONTENTS"
        },
        {
            "abstract": "In this work we focus on efficient heuristics for solving a class of stochastic planning problems that arise in a variety of business, investment, and industrial applications. The problem is best described in terms of future buy and sell contracts. By buying less reliable, but less expensive, buy (supply) contracts, a company or a trader can cover a position of more reliable and more expensive sell contracts. The goal is to maximize the expected net gain (profit) by constructing a close to optimum portfolio out of the available buy and sell contracts. This stochastic planning problem can be formulated as a two-stage stochastic linear programming problem with recourse. However, this formalization leads to solutions that are exponential in the number of possible failure combinations. Thus, this approach is not feasible for large scale problems. In this work we investigate heuristic approximation techniques alleviating the efficiency problem. We primarily focus on the clustering approach and devise heuristics for finding clusterings leading to good approximations. We illustrate the quality and feasibility of the approach through experimental data. 1",
            "group": 3111,
            "name": "10.1.1.164.7182",
            "keyword": "",
            "title": "A clustering approach to solving large stochastic matching problems"
        },
        {
            "abstract": "Today, with digitally stored information available in abundance, even for many minor languages, this information must by some means be filtered and extracted in order to avoid drowning in it. Automatic summarization is one such technique, where a computer summarizes a longer text to a shorter non-rendundant form. Apart from the major languages of the world there are a lot of languages for which large bodies of data aimed at language technology research to a high degree are lacking. There might also not be resources available to develop such bodies of data, since it is usually time consuming and requires substantial manual labor, hence being expensive. Nevertheless, there will still be a need for automatic text summarization for these languages in order to subdue this constantly increasing amount of electronically produced text. This thesis thus sets the focus on automatic summarization of text and the evaluation of summaries using as few human resources as possible. The resources that are used should to as high extent as possible be already existing, not specifically aimed at summarization or evaluation of summaries and, preferably, created as part of natural literary processes.",
            "group": 3112,
            "name": "10.1.1.164.7973",
            "keyword": "",
            "title": "Resource Lean and Portable Automatic Text Summarization"
        },
        {
            "abstract": "Vol. 6, 61-82 (1 996) Quantifying neighbourhood preservation in topographic mappings",
            "group": 3113,
            "name": "10.1.1.164.9268",
            "keyword": "",
            "title": "Proceedings of the 3rd Joint"
        },
        {
            "abstract": "into a rSlbk state. Each element ropnrentll a hg(pothdd & Md the kr 4hnb no pdvm pnndn qua dim Aats discrsta coopcnatlva computation, becm80 the elemml mud be nonlinw and crowcoupbd. If, ftowmw, Yhc dmtb adlopt discrete atate8 according ta a partkulw stochrtk function of hdr inputs, it h posdMe to wu statbtlcall meehanica to-W the behavior of the whd. ayst~n. Thb leala to a w dmple hat tho nlwork b ahown. Learning in Boltzmann Machines",
            "group": 3114,
            "name": "10.1.1.165.432",
            "keyword": "",
            "title": "LEARNING IN BOLTZMANN MACHINES RESUME SUMMARY"
        },
        {
            "abstract": "Abstract-- Location area (LA) planning plays an important role in cellular networks because of the trade-off caused by paging and registration signaling. The upper bound on the size of an LA is the service area of a mobile switching center (MSC). In that extreme case, the cost of paging is at its maximum, but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal, but the registration cost is the largest. In general, the most important component of these costs is the load on the signaling resources. Between the extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we try to find an optimal method for determining the location areas. For that purpose, we use the available network information to formulate a realistic optimization problem. We propose an algorithm based on simulated annealing (SA) for the solution of the resulting problem. Then, we investigate the quality of the SA technique by comparing its results to greedy search and random generation methods.",
            "group": 3115,
            "name": "10.1.1.165.1218",
            "keyword": "Index terms\u2014Location AreaCellular NetworksSimulated",
            "title": "Location area planning in cellular networks using simulated annealing"
        },
        {
            "abstract": "Abstract \u2014 This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued \u2018black box \u2019 function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the \u2018vanilla \u2019 gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. I.",
            "group": 3116,
            "name": "10.1.1.165.1649",
            "keyword": "",
            "title": "Natural evolution strategies"
        },
        {
            "abstract": " ",
            "group": 3117,
            "name": "10.1.1.165.2068",
            "keyword": "",
            "title": "Optimization of Entropy with Neural Networks"
        },
        {
            "abstract": "Multi-dimensional transfer functions are widely used to provide appropriate data classification for direct volume rendering. Nevertheless, the design of a multi-dimensional transfer function is a complicated task. In this paper, we propose to use parallel coordinates, a powerful tool to visualize high-dimensional geometry and analyze multivariate data, for multi-dimensional transfer function design. This approach has two major advantages: (1) Combining the information of spatial space (voxel position) and parameter space; (2) Selecting appropriate highdimensional parameters to obtain sophisticated data classification. Although parallel coordinates offers simple interface for the user to design the high-dimensional transfer function, some extra work such as sorting the coordinates is inevitable. Therefore, we use a local linear embedding technique for dimension reduction to reduce the burdensome calculations in the high dimensional parameter space and to represent the transfer function concisely. With the aid of parallel coordinates, we propose some novel high-dimensional transfer function widgets for better visualization results. We demonstrate the capability of our parallel coordinates based transfer function (PCbTF) design method for direct volume rendering using CT and MRI datasets. Categories and Subject Descriptors (according to ACM CCS): Generation\u2014Line and curve generation",
            "group": 3118,
            "name": "10.1.1.165.6610",
            "keyword": "",
            "title": "Multi-dimensional Reduction and Transfer Function Design using Parallel Coordinates"
        },
        {
            "abstract": "Abstract: The MAXimum propositional SATisfiability problem (MAXSAT) is a well known NP-hard optimization problem with many theoretical and practical applications in artificial intelligence and mathematical logic. Heuristic local search algorithms are widely recognized as the most effective approaches used to solve them. However, their performance depends both on their complexity and their tuning parameters which are controlled experimentally and remain a difficult task. Extremal Optimization (EO) is one of the simplest heuristic methods with only one free parameter, which has proved competitive with the more elaborate general-purpose method on graph partitioning and coloring. It is inspired by the dynamics of physical systems with emergent complexity and their ability to self-organize to reach an optimal adaptation state. In this paper, we propose an extremal optimization procedure for MAXSAT and consider its effectiveness by computational experiments on a benchmark of random instances. Comparative tests showed that this procedure improves significantly previous results obtained on the same benchmark with other modern local search methods like WSAT, simulated annealing and Tabu Search (TS).",
            "group": 3119,
            "name": "10.1.1.167.12",
            "keyword": "Constraint satisfactionMAXSATheuristic local searchextremal optimization",
            "title": "Solving the Maximum Satisfiability Problem Using an Evolutionary Local Search Algorithm"
        },
        {
            "abstract": "Abstract: Ramsey numbers are known to be hard combinatorial problems that have many important applications including number theory, algebra, geometry, topology, set theory, logic, ergodic theory, information theory, and theoretical computer science. The evaluation of Ramsey numbers using intelligent algorithms has been extensively studied in the last decades and only few numbers are currently known. Almost all of these methods failed to find the exact value of Ramsey numbers as they are over constraints problem. They have succeeded only to improve some upper and lower bounds of these numbers. In this work, we have tested the following intelligent algorithm: Backtracking, local search, tabu search and simulated annealing on some extremely hard instances of Ramsey numbers namely R (5, 9)- 120 and R (6, 8)- 121. As we failed to solve these hard instances using the previous techniques, we decided to combine them together in a hybrid metaheuristic algorithm and succeeded to generate the expected solutions. This new hybrid algorithm seems efficient and promising. It can be applied also on different combinatorial problems even if deep mathematical properties of the problems ' domain are not on hand.",
            "group": 3120,
            "name": "10.1.1.167.1319",
            "keyword": "Metaheuristichybrid algorithmsoptimizationcombinatoricsRamsey numbers",
            "title": "Experiments of Intelligent Algorithms on Ramsey Graphs"
        },
        {
            "abstract": "ABSTRACT: Recently a new metaheuristic called harmony search was developed. It mimics the behaviors of musicians improvising to find the better state harmony. In this paper, this algorithm is described and applied to solve the container storage problem in the harbor. The objective of this problem is to determine a valid containers arrangement, which meets customers \u2019 delivery deadlines, reduces the number of container rehandlings and minimizes the ship idle time. In this paper, an adaptation of the harmony search algorithm to the container storage problem is detailed and some experimental results are presented and discussed. The proposed approach was compared to a genetic algorithm previously applied to the same problem and recorded a good results.",
            "group": 3121,
            "name": "10.1.1.167.1786",
            "keyword": "Harmony searchtransport schedulingmetaheuristicoptimizationcontainer storage",
            "title": "\u201cEvaluation and optimization of innovative production systems of goods and services\u201d HARMONY SEARCH ALGORITHM FOR THE CONTAINER STORAGE PROBLEM"
        },
        {
            "abstract": "Abstract. A key problem in statistics and machine learning is inferring suitable structure of a model given some observed data. A Bayesian approach to model comparison makes use of the marginal likelihood of each candidate model to form a posterior distribution over models; unfortunately for most models of interest, notably those containing hidden or latent variables, the marginal likelihood is intractable to compute. We present the variational Bayesian (VB) algorithm for directed graphical models, which optimises a lower bound approximation to the marginal likelihood in a procedure similar to the standard EM algorithm. We show that for a large class of models, which we call conjugate exponential, the VB algorithm is a straightforward generalisation of the EM algorithm that incorporates uncertainty over model parameters. In a thorough case study using a small class of bipartite DAGs containing hidden variables, we compare the accuracy of the VB approximation to existing asymptotic-data approximations such as the Bayesian Information Criterion (BIC) and the Cheeseman-Stutz (CS) criterion, and also to a sampling based gold standard, Annealed Importance Sampling (AIS). We find that the VB algorithm is empirically superior to CS and BIC, and much faster than AIS. Moreover, we prove that a VB approximation can always be constructed in such a way that guarantees it to be more accurate than the CS approximation.",
            "group": 3122,
            "name": "10.1.1.167.3031",
            "keyword": "Approximate Bayesian InferenceBayes FactorsDirected Acyclic GraphsEM AlgorithmGraphical ModelsMarkov Chain Monte CarloModel SelectionVariational Bayes",
            "title": "Variational bayesian learning of directed graphical models with hidden variables, Bayesian Analysis 1"
        },
        {
            "abstract": "The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. Increasingly, services are exploring dynamic methods to minimize energy costs while respecting their service-level agreements (SLAs). Furthermore, it will soon be important for these services to manage their usage of \u201cbrown energy \u201d (produced via carbon-intensive means) relative to renewable or \u201cgreen \u201d energy. This paper introduces a general, optimization-based framework for enabling multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their SLAs and minimizing energy costs. Based on the framework, we propose policies for request distribution across the data centers. Our policies can be used to abide by caps on brown energy consumption, such as those that might arise from Kyotostyle carbon limits, from corporate pledges on carbon-neutrality, or from limits imposed on services to encourage brown energy conservation. We evaluate our framework and policies extensively through simulations and real experiments. Our results show how our policies allow a service to trade off consumption and cost. For example, using our policies, the service can reduce brown energy consumption by 24 % for only a 10 % increase in cost, while still abiding by SLAs. 1",
            "group": 3123,
            "name": "10.1.1.167.3105",
            "keyword": "",
            "title": "Managing the Cost, Energy Consumption, and Carbon Footprint of Internet Services \u2217"
        },
        {
            "abstract": " Interacting and annealing are two powerful strategies that are applied in different areas of stochastic modelling and data analysis. Interacting particle systems approximate a distribution of interest by a finite number of particles where the particles interact between the time steps. In computer vision, they are commonly known as particle filters. Simulated annealing, on the other hand, is a global optimization method derived from statistical mechanics. A recent heuristic approach to fuse these two techniques for motion capturing has become known as annealed particle filter. In order to analyze these techniques, we rigorously derive in this paper two algorithms with annealing properties based on the mathematical theory of interacting particle systems. Convergence results and sufficient parameter restrictions enable us",
            "group": 3124,
            "name": "10.1.1.167.8505",
            "keyword": "Particle FilterSimulated AnnealingAnnealed Particle FilterMotion CaptureContents",
            "title": "Interacting and annealing particle filters: Mathematics and a recipe for applications"
        },
        {
            "abstract": "",
            "group": 3125,
            "name": "10.1.1.167.8718",
            "keyword": "",
            "title": "An efficient computational approach for prior sensitivity analysis and cross-validation"
        },
        {
            "abstract": "Simple knots with just a few crossings can be made into attractive 3D geometric sculptures. Various approaches to achieve such a transformation are discussed and illustrated with examples. High-lighted are two larger-scale sculptures based on knots which were assembled in a few hours by participants of G4G9. 1. Knot Theory Primer To a mathematician, knots are closed curves embedded in three-dimensional space. They are typically classified by the minimal number of crossings that will appear when they are projected onto a plane. A simple loop without any crossings is called the trivial knot or the unknot. The simplest true knot is the 3-crossing trefoil knot; it comes in a right-handed and a left-handed version. The next more complicated knot is the mirrorsymmetric Figure-8 knot with 4 crossings. There are two different knot types with five crossings, three with six crossings, seven with seven crossings, and then the number of different knots with a given number of crossings rises very quickly. For the case of 9 crossings, there are already 49 different knots. Knots are typically drawn projected onto a plane, so that they exhibit the minimal number of strand crossings. This means that many of their intrinsic properties may remain hidden. Often the depictions found in the classical knot tables do not even try to reveal the maximal number of symmetries possible in this planar depiction. Moreover, knots are actually 3-dimensional structures, but their possible 3D forms and symmetries are completely lost in those tabulations. Once we have closed a strand into a knotted loop, we can deform and squash this knot as much as we want, but its knot type cannot be changed without breaking and re-closing the strand. Thus if we start with a closedloop rubber band, we can further warp and twist and knot this loop \u2013 it will always still be the unknot!",
            "group": 3126,
            "name": "10.1.1.168.6358",
            "keyword": "",
            "title": "The Beauty of Knots"
        },
        {
            "abstract": "Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required on ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches: CI tests are used to generate an ordering on the nodes from the database, which is then used to recover the underlying Bayesian network structure using a non-Cl-test-based method. Results of the evaluation of the algorithm on a number of databases (e.g., ALARM, LED, and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems.",
            "group": 3127,
            "name": "10.1.1.169.1441",
            "keyword": "Bayesian networksprobabilistic networksprobabilistic model constructionconditional",
            "title": " Construction of Bayesian Network Structures From Data: A Brief Survey and an Efficient Algorithm"
        },
        {
            "abstract": "Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This article introduces diffracting trees, novel data structures for shared counting and load balancing in a distributed/parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message-passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting networks have depth O(log 2 w). Diffracting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queue-locking in the design of many concurrent data structures.",
            "group": 3128,
            "name": "10.1.1.169.5956",
            "keyword": "Organization and Design\u2014distributed systemsE.1 [DataData Structures General TermsDesignPerformance Additional Key Words and PhrasesContentioncounting networksindex distributionlock freewait",
            "title": "Diffracting trees"
        },
        {
            "abstract": "We introduce a classifier based on the L\u221e norm. This classifier, called Linf, is a composition of four stages (transforming, projecting, binning, and covering) that are designed to deal with both the curse of dimensionality and computational complexity. Linf is not a hybrid or modification of existing classifiers; it employs a new covering algorithm. The accuracy of Linf on widely-used benchmark datasets is comparable to the accuracy of competitive classifiers and, in some important cases, exceeds the accuracy of competitors. Its computational complexity is sub-linear in number of instances and number of variables and quadratic in number of classes.",
            "group": 3129,
            "name": "10.1.1.169.6845",
            "keyword": "Supervised classifiersDecision treesSupport vector machinesDecision lists",
            "title": " Linf: An L-infinity Classifier"
        },
        {
            "abstract": "Abstract \u2014 Often adaptive, distributed control can be viewed as an iterated game between independent players. The coupling between the players \u2019 mixed strategies, arising as the system evolves from one instant to the next, is determined by the system designer. Information theory tells us that the most likely joint strategy of the players, given a value of the expectation of the overall control objective function, is the minimizer of a Lagrangian function of the joint strategy. So the goal of the system designer is to speed evolution of the joint strategy to that Lagrangian minimizing point, lower the expectated value of the control objective function, and repeat. Here we elaborate the theory of algorithms that do this using local descent procedures, and that thereby achieve efficient, adaptive, distributed control. I.",
            "group": 3130,
            "name": "10.1.1.170.610",
            "keyword": "",
            "title": "Distributed control by lagrangian steepest descent"
        },
        {
            "abstract": "Global optimization methods tend to focus on exploitation of known optima, often getting stuck in local optima. For problems with costly evaluations, it is more effective to initially identify the high-performance regions. This thesis proposes an algorithm designed to provide good coverage of the highperformance regions of an objective function using few objective function evaluations. The algorithm performs consecutive Metropolis-Hastings random walks on an RBFN meta-model of the objective function. After each walk, it adds the endpoint to the training set, then retrains the RBFN. Experiments show that the algorithm explores good solutions in significantly fewer objective function evaluations than state of the art algorithms, such Niching ES. The efficiency of the algorithm can be significantly increased by raising theacceptance function to some power. The mapof thehigh-performance regions obtained can be used to initialize a more greedy optimization method. Moreover, the MIMH algorithm can be readily used to sample efficiently from a distribution the shape of which is determined by a costly evaluation.",
            "group": 3131,
            "name": "10.1.1.170.1996",
            "keyword": "",
            "title": "Exploring High-Performance Regions with Model Induced Metropolis-Hastings"
        },
        {
            "abstract": "Constraint satisfaction is the core of a large number of problems, notably scheduling. Because of their potential for containing the combinatorial explosion problem in constraint satisfaction, local search methods have received a lot of attention in the last few years. The problem with these methods is that they can be trapped in local minima. GENET is a connectionist approach to constraint satisfaction. It escapes local minima by means of a weight adjustment scheme, which has been demonstrated to be highly effective. The tunneling algorithm described in this paper is an extension of GENET for optimization. The main idea is to introduce modifications to the function which is to be optimized by the network (this function mirrors the objective function which is specified in the problem). We demonstrate the outstanding performance of this algorithm on constraint satisfaction problems, constraint satisfaction optimization problems, partial constraint satisfaction problems, radio frequency allocation problems and traveling salesman problems. 1.",
            "group": 3132,
            "name": "10.1.1.170.2669",
            "keyword": "",
            "title": "The tunneling algorithm for partial CSPs and combinatorial optimization problems"
        },
        {
            "abstract": "Parameter estimation using Simulated Annealing for S-system",
            "group": 3133,
            "name": "10.1.1.170.4041",
            "keyword": "",
            "title": "models of biochemical networks"
        },
        {
            "abstract": "Advanced NMR and mass spectroscopy permit simultaneous measurements of time-course in-vivo metabolite concentrations within an organism. These metabolic profiles are loaded with structural and kinetic information regarding the biochemical networks from which they were drawn. Extracting these information will require systematic application of both experimental and computational techniques. S-systems are non-linear dynamical models based on the power-law formalism which provide a general framework for the simulation of integrated biochemical systems exhibiting complex dynamics such those present in genetic circuits, immune and metabolic networks. In this paper we describe complementary heuristic methods for recovering the parameters of S-systems from time-course biochemical data.",
            "group": 3134,
            "name": "10.1.1.170.4568",
            "keyword": "S-System Parameter EstimationSimulated AnnealingEvolutionary Multiobjective Optimization",
            "title": "Heuristic Parameter Estimation Methods for S-System Models of Biochemical Networks"
        },
        {
            "abstract": " ",
            "group": 3135,
            "name": "10.1.1.171.79",
            "keyword": "",
            "title": "Enhancing Realism and . . . "
        },
        {
            "abstract": "This paper studies a class of enhanced diffusion processes in which random walkers perform Levy flights which are frequently observed in statistical physics and biology. Levy flights are shown to have several attributes suitable for the global optimization problem, and those four algorithms are developed based on such properties. We compare new algorithms with the well-known Simulated Annealing on considerably hard test functions and the results are very promising",
            "group": 3136,
            "name": "10.1.1.171.156",
            "keyword": "Levy Flightoptimizationalgorithm",
            "title": "GLOBAL OPTIMIZATION USING LEVY FLIGHT"
        },
        {
            "abstract": "Abstract \u2014 It has been shown that the global cost of the task allocations obtained with fast greedy algorithms can be improved upon by using a class of auction methods called Stochastic Clustering Auctions (SCAs). SCAs use stochastic transfers or swaps between the task clusters assigned to each team member, allow both uphill and downhill cost movements, and rely on simulated annealing. The choice of a key annealing parameter and turning the uphill movements on and off enables the converged solution of a SCA to slide in the region between the global optimal performance and the performance associated with a random allocation. The first SCA, called here GSSCA, was based on a Gibbs sampler, which constrained the stochastic cluster reallocations to simple single transfers or swaps. This paper presents a new and more efficient SCA, called SWSCA, based on the generalized Swendsen-Wang method that enables more complex and efficient movements between clusters by connecting tasks that appear to be synergistic and then stochastically reassigning these connected tasks. For centralized auctioning, extensive numerical experiments are used to compare the performance of SWSCA with GSSCA in terms of costs and computational and communication requirements. Distributed SWSCA is then compared with centralized SWSCA using communication links between robots that were motivated by a generic topology called a \u201cscale free network.\u201d I.",
            "group": 3137,
            "name": "10.1.1.172.1318",
            "keyword": "",
            "title": "A Novel Stochastic Clustering Auction for Task Allocation in Multi-Robot Teams"
        },
        {
            "abstract": "Abstract. The problem of recognizing objects subject to affine transformation in images is examined from a physical perspective using the theory of statistical estimation. Focusing first on objects that occlude zero-mean scenes with additive noise, we derive the Cramer-Rao lower bound on the mean-square error in an estimate of the six-dimensional parameter vector that describes an object subject to affine transformation and so generalize the bound on one-dimensional position error previously obtained in radar and sonar pattern recognition. We then derive two useful descriptors from the object\u2019s Fisher information that are independent of noise level. The first is a generalized coherence scale that has great practical value because it corresponds to the width of the object\u2019s autocorrelation peak under affine transformation and so provides a physical measure of the extent to which an object can be resolved under affine parameterization. The second is a scalar measure of an object\u2019s complexity that is invariant under affine transformation and can be used to quantitatively describe the ambiguity level of a general 6-dimensional affine recognition problem. This measure of complexity has a strong inverse relationship to the level of recognition ambiguity. We then develop a method for recognizing objects subject to affine transformation imaged in thousands of complex real-world scenes. Our method exploits the resolution gain made available by the brightness contrast between the object perimeter and the scene it partially occludes. The level of recognition ambiguity is shown to decrease exponentially with increasing object and scene complexity. Ambiguity is then avoided by conditioning",
            "group": 3138,
            "name": "10.1.1.172.2361",
            "keyword": "object recognitionobject complexitycoherence scalecoherence volumerecognition ambiguityFisher informationbandwidthstatistical estima",
            "title": "c \u25cb 2001 Kluwer Academic Publishers. Manufactured in The Netherlands. Recognition, Resolution, and Complexity of Objects Subject to Affine Transformations \u2217"
        },
        {
            "abstract": "submitted for publication Abstract. Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce generative models for melodies. We decompose melodic modeling into two subtasks. We first propose a rhythm model based on the distributions of distances between subsequences. Then, we define a generative model for melodies given chords and rhythms based on modeling sequences of Narmour features. The rhythm model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases. Using a similar evaluation procedure, the proposed melodic model consistently outperforms an Input/Output Hidden Markov Model. Furthermore, sampling these models given appropriate musical contexts generates realistic melodies. 2 IDIAP\u2013RR 08-51 1",
            "group": 3139,
            "name": "10.1.1.172.3817",
            "keyword": "",
            "title": "Predictive Models for Music"
        },
        {
            "abstract": "Abstract. Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases. 2 IDIAP\u2013RR 08-33 1",
            "group": 3140,
            "name": "10.1.1.172.3980",
            "keyword": "",
            "title": "A Distance Model for Rhythms"
        },
        {
            "abstract": "We examined the impact of temporal dependence between patterns of error in classified time-series imagery through a simulation modeling approach. This research extended the land-cover-change simulation model we previously developed to investigate: (1) the assumption of temporal independence between patterns of error in classified time-series imagery; and (2) the interaction of patterns of change and patterns of error in a postclassification change analysis. In this research, the thematic complexity of the classified land-cover maps was increased by increasing the number of simulated land-cover classes. Simulating maps with increased categorical resolution permitted the incorporation of: (1) higher-order, more complex spatial and temporal interactions between land-cover classes; and (2) patterns of error that better reproduce the complex error interactions that often occur in time-series classified imagery. The overall modeling framework was divided into two primary components: (1) generation of a map representing true change; and (2) generation of a suite of change maps that had been perturbed by specific patterns of error. All component maps in the model were produced using simulated annealing, which enabled us to create a series of map realizations with user-defined spatial and",
            "group": 3141,
            "name": "10.1.1.172.6701",
            "keyword": "",
            "title": "Propagating error in land-cover-change analyses: impact of temporal dependence under increased thematic complexity"
        },
        {
            "abstract": "Abstract \u2014 As there are many good optimization algorithms each with its own characteristics, it is very difficult to choose the best method for optimization problems. Thus, it is important to select and apply the appropriate algorithms according to the complexities of the problem. However, it is difficult to solve very complicated problems with only a single algorithm, and a hybrid optimization approach, which combines multiple optimization algorithms, is necessary. To develop an efficient hybrid optimization algorithm, it is necessary to determine how the optimization process is performed. This paper focuses on the balance between local and broad searches. Multiple optimization methods are controlled to derive both the optimum point and the information of the landscape. To achieve the proposed optimization strategy, three distinguished optimization algorithms are introduced: DIRECT (DIviding RECTangles), GAs (Genetic Algorithms), and SQP (Sequential Quadratic Programming). To integrate these three algorithms, each algorithm, especially DIRECT, was modified and developed. This paper describes a new hybrid method using these three algorithms. The performance of the proposed hybrid algorithm was examined through numerical experiments. From these experiments, not only the optimum point but also the information of the landscape was determined. The information of the landscape verified the reliability of optimization results. I.",
            "group": 3142,
            "name": "10.1.1.172.6796",
            "keyword": "optimization algorithms. SQP is one of the best algorithms",
            "title": "Hybrid Optimization Using DIRECT, GA, and SQP for Global Exploration"
        },
        {
            "abstract": "Abstract\u2014A set of antenna-optimization problems is presented that satisfies the necessary requirements to form a test suite useful for measuring and comparing the performance of different evolutionary optimization algorithms (EAs) when they are applied to solve complex electromagnetic problems. The ability of the proposed test suite to find strong and weak points of any EA is illustrated by a complete study of four broadly used evolutionary algorithms carried out with the aid of the new test functions. Index Terms\u2014Antennas, genetic algorithms (GAs), optimization methods, particle swarm. I.",
            "group": 3143,
            "name": "10.1.1.172.7602",
            "keyword": "",
            "title": "Benchmark Antenna Problems for Evolutionary Optimization Algorithms"
        },
        {
            "abstract": "We present a new learning algorithm for Boltzmann Machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann Machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layerby-layer \u201cpre-training\u201d phase that initializes the weights sensibly. The pre-training also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that Deep Boltzmann Machines learn very good generative models of hand-written digits and 3-D objects. We also show that the features discovered by Deep Boltzmann Machines are a very effective way to initialize the hidden layers of feed-forward neural nets which are then discriminatively fine-tuned. ",
            "group": 3144,
            "name": "10.1.1.172.7709",
            "keyword": "",
            "title": "An Efficient Learning Procedure for Deep Boltzmann Machines"
        },
        {
            "abstract": "The aim of this meeting is to bring together scientists from a range of backgrounds and perspectives on problems of utilizing large or complex data sets for inference. The focus is on the earth and environmental sciences, but we have a truly cross disciplinary set of speakers and registrants. Speakers include experts in spatial statistics, data mining, inversion, grid computing, computer visualization and numerical simulation of natural processes, together with those who need to solve real world problems using these tools. There are participants from academia, government research institutions and private industry. The original idea (concocted some 18 months ago) for this conference was to provide an opportunity to create new synergies between fields that deal with similar problems. Today we are increasingly trying to make sense of new and complex observations about the world around us. Over the course of this meeting we hope we can learn from each other, by being exposed to questions, tools and techniques that are less familiar. There are four sub-themes of the meeting: Data mining and statistical inference Challenges in computational simulation of natural processes Challenges in inverse problems",
            "group": 3145,
            "name": "10.1.1.172.8106",
            "keyword": "",
            "title": "Australian Academy of Science Elizabeth and Frederick White Conference Mastering the Data Explosion in the Earth and Environmental Sciences"
        },
        {
            "abstract": "In this study, the protein tertiary structure prediction systems on the Grid are proposed for progress of the bioinformatics. The prediction is mainly performed by the protein energy minimization. However, this method has many iterated calculation of the protein energy in most cases. To use the Grid as the large-scale computing environment would be valuable for this system. In the system, Parallel Simulated Annealing using Genetic Crossover (PSA/GAc) is a minimization engine and NetSolve is a basic tool to use the Grid. In this study, two types of implementations are prepared. The first naive implementation of the system has a critical overhead due to large communication delay over the Internet. The second system, asynchronous Crossover model, improves the performance in the second implementation. The details of the system and the experimental results solving C-peptide are shown as an example of Grid application. 1.",
            "group": 3146,
            "name": "10.1.1.172.9597",
            "keyword": "",
            "title": "Graduate Student of Engineering"
        },
        {
            "abstract": "Abstract\u2014The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. Increasingly, services are exploring dynamic methods to minimize energy costs while respecting their service-level agreements (SLAs). Furthermore, it will soon be important for these services to manage their usage of \u201cbrown energy \u201d (produced via carbon-intensive means) relative to renewable or \u201cgreen \u201d energy. This paper introduces a general, optimization-based framework for enabling multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their SLAs and minimizing energy costs. Based on the framework, we propose a policy for request distribution across the data centers. Our policy can be used to abide by caps on brown energy consumption, such as those that might arise from Kyoto-style carbon limits, from corporate pledges on carbon-neutrality, or from limits imposed on services to encourage brown energy conservation. We evaluate our framework and policy extensively through simulations and real experiments. Our results show how our policy allows a service to trade off consumption and cost. For example, using our policy, the service can reduce brown energy consumption by 24 % for only a 10 % increase in cost, while still abiding by SLAs. I.",
            "group": 3147,
            "name": "10.1.1.173.1625",
            "keyword": "",
            "title": "CappingtheBrownEnergyConsumptionof"
        },
        {
            "abstract": "Abstract: Many engineering tasks involve the search for good solutions among many possibilities. In most cases, tasks are too complex to be modeled completely and their solution spaces often contain local minima. Therefore, classical optimization techniques cannot, in general, be applied effectively. This paper studies two stochastic search methods, one well-established \ufffdsimulated annealing \ufffd and one recently developed \ufffdprobabilistic global search Lausanne\ufffd, applied to structural shape control. Search results are applied to control the quasistatic displacement of a tensegrity structure with multiple objectives and interdependent actuator effects. The best method depends on the accuracy related to requirements defined by the objective function and the maximum number of evaluations that are allowed.",
            "group": 3148,
            "name": "10.1.1.174.4393",
            "keyword": "",
            "title": "A study of two stochastic search methods for structural control"
        },
        {
            "abstract": "Abstract. This paper provides a brief description of a constraint-based solver that was applied by the author to the problem instances in all three tracks of the International Timetabling Competition 2007 1. 1",
            "group": 3149,
            "name": "10.1.1.174.5132",
            "keyword": "",
            "title": "ITC2007: Solver Description"
        },
        {
            "abstract": "Abstract. Embryological development provides an inspiring example of the creation of complex hierarchical structures by self-organization. Likewise, biological metamorphosis shows how these complex systems can radically restructure themselves. Our research investigates these principles and their application to artificial systems in order to create intricately structured systems that are ordered from the nanoscale up to the macroscale. However these processes depend on mutually interdependent unfoldings of an information process and of the \u201cbody \u201d in which it is occurring. Such embodied computation provides challenges as well as opportunities, and in order to fulfill its promise, we need both formal and informal models for conceptualizing, designing, and reasoning about embodied computation. This paper presents a preliminary design for one such model especially oriented toward artificial morphogenesis.",
            "group": 3150,
            "name": "10.1.1.174.5835",
            "keyword": "embryological development",
            "title": "Models and Mechanisms for Artificial Morphogenesis"
        },
        {
            "abstract": "This paper describes an algorithm for minimizing the non-productive time or \u2018airtime \u2019 for a tool by optimally connecting the toolpaths for that tool. This problem is formulated as a generalized traveling salesman problem with precedence constraints and is solved using a heuristic method. The performance of the heuristic algorithm and the amount of improvement obtained for different problem sizes is also presented. This algorithm has been implemented in our automated process planning system and can be applied easily to other areas of path planning optimization like fused deposition modeling and laser cutting. 1.",
            "group": 3151,
            "name": "10.1.1.174.6612",
            "keyword": "",
            "title": "Tool-path Optimization for Minimizing Airtime during Machining"
        },
        {
            "abstract": "Search result diversification is a natural approach for tackling ambiguous queries. Nevertheless, not all queries are equally ambiguous, and hence different queries could benefit from different diversification strategies. A more lenient or more aggressive diversification strategy is typically encoded by existing approaches as a trade-off between promoting relevance or diversity in the search results. In this paper, we propose to learn such a trade-off on a per-query basis. In particular, we examine how the need for diversification can be learnt for each query\u2014given a diversification approach and an unseen query, we predict an effective tradeoff between relevance and diversity based on similar previously seen queries. Thorough experiments using the TREC ClueWeb09 collection show that our selective approach can significantly outperform a uniform diversification for both classical and state-of-the-art diversification approaches.",
            "group": 3152,
            "name": "10.1.1.175.4850",
            "keyword": "AlgorithmsExperimentationPerformance Keywords Web searchrelevancediversityselective retrievalmachine learningfeature",
            "title": "Search and Retrieval\u2014retrieval models"
        },
        {
            "abstract": "Abstract:- In this work we propose an algorithm for computing the fractal dimension of a software network, and compare its performances with two other algorithms. Object of our study are various large, object-oriented software systems. We built the associated graph for each system, also known as software network, analyzing the binary relationships (dependencies), among classes. We found that the structure of such software networks is self-similar under a length-scale transormation, confirming previous results of a recent paper from the authors. The fractal dimension of these networks is computed using a Merge algorithm, first devised by the authors, a Greedy Coloring algorithm, based on the equivalence with the graph coloring problem, and a Simulated Annealing algorithm, largely used for efficiently determining minima in multi-dimensional problems. Our study examines both efficiency and accuracy, showing that the Merge algorithm is the most efficient, while the Simulated Annealing is the most accurate. The Greeding Coloring algorithm lays in between the two, having speed very close to the Merge algorithm, and accuracy comparable to the Simulated Annealing algorithm. Key-Words:-",
            "group": 3153,
            "name": "10.1.1.175.5744",
            "keyword": "Complex SystemsComplex NetworksSelf-similaritySoftware GraphsSoftware MetricsObject-Oriented Systems",
            "title": "Three Algorithms for Analyzing Fractal Software Networks"
        },
        {
            "abstract": "Abstract. This paper presents an approach to cash management for automatic teller machine (ATM) network. This approach is based on an artificial neural network to forecast a daily cash demand for every ATM in the network and on the optimization procedure to estimate the optimal cash load for every ATM. During the optimization procedure, the most important factors for ATMs maintenance were considered: cost of cash, cost of cash uploading and cost of daily services. Simulation studies show, that in case of higher cost of cash (interest rate) and lower cost for money uploading, the optimization procedure allows to decrease the ATMs maintenance costs around 15-20 %. For practical implementation of the proposed ATMs \u2019 cash management procedure, further experimental investigations are necessary. 1.",
            "group": 3154,
            "name": "10.1.1.175.6224",
            "keyword": "",
            "title": "OPTIMIZATION OF CASH MANAGEMENT FOR ATM NETWORK"
        },
        {
            "abstract": "As integrated-circuit technology continues to scale, process variation is becoming an issue that cannot be ignored at the microarchitecture and system levels. Process variation is particularly detrimental to a processor\u2019s frequency and leakage power. To solve this growing problem, solutions at different levels of the computing stack are needed. This thesis presents a couple of such solutions. The first solution, is a circuits technique that has important implications on the microarchitecture. It is based on the previously-proposed Fine-Grain Body Biasing (FGBB), where different parts of the processor chip are given a voltage bias that changes the speed and leakage properties of their transistors. Previous work proposed determining the optimal body bias voltages at manufacturing time and setting them permanently for the lifetime of the chip. In this thesis, I propose a new technique (called Dynamic FGBB- D-FGBB), which allows the continuous re-evaluation of the bias voltages to adapt to dynamic conditions. Within-die process variation causes individual cores in a Chip Multiprocessor (CMP) to differ substantially in both static power consumed and maximum frequency supported. In this environment, ignoring variation effects when scheduling applications or when managing",
            "group": 3155,
            "name": "10.1.1.175.8201",
            "keyword": "",
            "title": "MULTILAYER TECHNIQUES TO ADDRESS PARAMETER VARIATION "
        },
        {
            "abstract": "The user interface of existing autonomous wheelchairs concentrates on direct control of the wheelchair by the user using mechanical devices or various hand, head or face gestures. However, it is important to monitor the user to ensure safety and comfort of the user, who operates the autonomous wheelchair. In addition, such monitoring of a user greatly improves usablity of an autonomous wheelchair due to the improved communication between the user and the wheelchair. This paper proposes a user monitoring system for an autonomous wheelchair. The feedback of the user and the information about the actions of the user, obtained by such a system, will be used by the autonomous wheelchair for planning of its future actions. As a first step towards creation of the monitoring system, this work proposes and examines the feasibility of a system that is capable of recognizing static facial gestures of the user using a camera mounted on a wheelchair. The prototype of such a system has been implemented and tested, achieving 90% recognition rate with 6 % false positive and 4 % false negative rates.",
            "group": 3156,
            "name": "10.1.1.175.8583",
            "keyword": "Autonomous wheelchairVision Based InterfaceGesture Recognition",
            "title": "EXAMINING THE FEASIBILITY OF FACE GESTURE DETECTION FOR MONITORING USERS OF AUTONOMOUS WHEELCHAIRS"
        },
        {
            "abstract": "Abstract. Rapidly changing business requirements necessitate the adhoc composition of expert teams to handle complex business cases. Expertcentric properties such as skills, however, are insufficient to assemble an effective team. The given interaction structure determines to a large degree how well the experts can be expected to collaborate. This paper addresses the team composition problem which consists of expert interaction network extraction, skill profile creation, and ultimately team formation. We provide a heuristic for finding near-optimal teams that yield the best trade-off between skill coverage and team connectivity. Finally, we apply a real-world data set to demonstrate the applicability and benefits of our approach.",
            "group": 3157,
            "name": "10.1.1.176.5373",
            "keyword": "social networkteam formationsimulated annealingskill connectivity",
            "title": "Composing near-optimal expert teams: a trade-off between skills and connectivity"
        },
        {
            "abstract": "This thesis discusses the background and methodologies necessary for constructing features in order to discover hidden links in relational data. Specifically, we consider the problems of predicting, classifying and annotating friends relations in friends networks, based upon features constructed from network structure and user profile data. I first document a data model for the blog service LiveJournal, and define a set of machine learning problems such as predicting existing links and estimating inter-pair distance. Next, I explain how the problem of classifying a user pair in a social networks, as directly connected or not, poses the problem of selecting and constructing relevant features. In order to construct these features, a genetic programming approach is used to construct multiple symbol trees with base features as their leaves; in this manner, the genetic program selects and constructs features that many not have been considered, but possess better predictive properties than the base features. In order to extract certain graph features from the relatively large social network, a new shortest path search algorithm is presented which computes and operates on a Euclidean embedding of the network. Finally, I present classification results and discuss the properties of the frequently constructed",
            "group": 3158,
            "name": "10.1.1.177.1123",
            "keyword": "",
            "title": "Link Discovery in Very Large . . . "
        },
        {
            "abstract": "The ability to discover groupings in continuous stimuli on the basis of distributional information is present across species and across perceptual modalities. We investigate the nature of the computations underlying this ability using statistical word segmentation experiments in which we vary the length of sentences, the amount of exposure, and the number of words in the languages being learned. Although the results are intuitive from the perspective of a language learner (longer sentences, less training, and a larger language all make learning more difficult), standard computational proposals fail to capture several of these results. We describe how probabilistic models of segmentation can be modified to take into account some notion of memory or resource limitations in order to provide a closer match to human performance. Human adults and infants, non-human primates, and even rodents all show a surprising ability: presented with a stream of syllables with no pauses between them, individuals from each group are able to discriminate statistically coherent sequences from sequences with lower coherence (Aslin, Saffran,",
            "group": 3159,
            "name": "10.1.1.177.1368",
            "keyword": "",
            "title": "Modeling human performance on statistical word segmentation tasks"
        },
        {
            "abstract": "A fast simulated annealing algorithm is developed for automatic object recognition. The object recognition problem is addressed as the problem of best describing a match between a hypothesized object and an image. The normalized correlation coeficient is used as a measure of the match. Templates are generated on-line during the search by transforming model images. Simulated annealing reduces the search time by orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of how landmarks, e.g., trafic signs, can be recognized by a navigating robot. We illustrate the performance of our algorithm with real-world images of complicated scenes with traffic signs. False positive matches occur only for templates with very small information content. To avoid false positive matches, we propose a method to select model images for robust object recognition by measuring the information content of the model images. The algorithm works well in noisy images for model images with high information content. ",
            "group": 3160,
            "name": "10.1.1.177.3141",
            "keyword": "",
            "title": "Fast object recognition in noisy images using simulated annealing"
        },
        {
            "abstract": "Abstract. Self-Organizing Map (SOM) algorithm has been extensively used for analysis and classification problems. For this kind of problems, datasets become more and more large and it is necessary to speed up the SOM learning. In this paper we present an application of the Simulated Annealing (SA) procedure to the SOM learning algorithm. The goal of the algorithm is to obtain fast learning and better performance in terms of matching of input data and regularity of the obtained map. An advantage of the proposed technique is that it preserves the simplicity of the basic algorithm. Several tests, carried out on different large datasets, demonstrate the effectiveness of the proposed algorithm in comparison with the original SOM and with some of its modification introduced to speed-up the learning. 1",
            "group": 3161,
            "name": "10.1.1.177.3439",
            "keyword": "",
            "title": "Improved SOM Learning Using Simulated"
        },
        {
            "abstract": "Abstract\u2014A method that exploits an information theoretic framework to extract optimized audio features using video information is presented. A simple measure of mutual information (MI) between the resulting audio and video features allows the detection of the active speaker among different candidates. This method involves the optimization of an MI-based objective function. No approximation is needed to solve this optimization problem, neither for the estimation of the probability density functions (pdfs) of the features, nor for the cost function itself. The pdfs are estimated from the samples using a nonparametric approach. The challenging optimization problem is solved using a global method: the differential evolution algorithm. Two information theoretic optimization criteria are compared and their ability to extract audio features specific to speech production is discussed. Using these specific audio features, candidate video features are then classified as member of the \u201cspeaker \u201d or \u201cnon-speaker\u201d class, resulting in a speaker detection scheme. As a result, our method achieves a speaker detection rate of 100 % on in-house test sequences, and of 85 % on most commonly used sequences. Index Terms\u2014Audio features, differential evolution, multimodal, mutual information, speaker detection, speech. I.",
            "group": 3162,
            "name": "10.1.1.177.5168",
            "keyword": "",
            "title": "Extraction of audio features specific to speech production for multimodal speaker detection"
        },
        {
            "abstract": "A new approach to applying feedforward neural networks",
            "group": 3163,
            "name": "10.1.1.177.8708",
            "keyword": "",
            "title": "Performance prediction of"
        },
        {
            "abstract": "Abstract. Cost functions provide information about the amount of resources required to execute a program in terms of the sizes of input arguments. They can provide an upper-bound, a lower-bound, or the average-case cost. Motivated by the existence of a number of automatic cost analyzers which produce cost functions, we propose an approach for automatically proving that a cost function is smaller than another one. In all applications of resource analysis, such as resource-usage verification, program synthesis and optimization, etc., it is essential to compare cost functions. This allows choosing an implementation with smaller cost or guaranteeing that the given resource-usage bounds are preserved. Unfortunately, automatically generated cost functions for realistic programs tend to be rather intricate, defined by multiple cases, involving non-linear subexpressions (e.g., exponential, polynomial and logarithmic) and they can contain multiple variables, possibly related by means of constraints. Thus, comparing cost functions is far from trivial. Our approach first syntactically transforms functions into simpler forms and then applies a number of sufficient conditions which guarantee that a set of expressions is smaller than another expression. Our preliminary implementation in the COSTA system indicates that the approach can be useful in practice. 1",
            "group": 3164,
            "name": "10.1.1.177.8992",
            "keyword": "",
            "title": "Comparing Cost Functions in Resource Analysis"
        },
        {
            "abstract": "A model of bilateral trade between an upstream supplier (landlord) and a downstream producer (retailer) is constructed, in which the upstream supplier confers long-term property usage rights to the downstream supplier in return for a base rental fee plus a percentage of verifiable sales production. Our model allows for the possibility that downstream sales production complements other activities of the upstream supplier to increase its total revenues. An optimal contract is designed that balances investment incentives of the downstream producer with initial investment and subsequent reinvestment incentives of the upstream supplier. A number of important stylized empirical facts associated with retail lease contracting are addressed with the model, including why: i) retail leases contain base rents and often (but not always) contain an overage rental feature, ii) stores that generate greater externalities pay lower base rents and have lower overage rent percentages than stores that generate fewer externalities, iii) the overage rent option is typically well out-of-the-money at contract execution, and iv) stand-alone retail operations often sign leases that contain an overage rental feature. Optimal Revenue Sharing Contracts with Externalities and Dual Agency I.",
            "group": 3165,
            "name": "10.1.1.178.569",
            "keyword": "",
            "title": "Optimal Revenue Sharing Contracts with Externalities and Dual Agency"
        },
        {
            "abstract": "Information retrieval systems often use proximity or term dependence models to increase the effectiveness of document retrieval. Many of the existing proximity models examine document-level local statistics, such as the frequencies that pairs of query terms occur within fixed-size windows of each document, before applying standard or adapted weighting functions \u2013 for instance Markov Random Fields. Term weighting models use Inverse Document Frequency (IDF) to control the influence of occurrences of different query terms in documents. Similarly, some proximity models also take into account the frequency of pairs of query terms in the entire corpus of documents. However, pair frequency is an expensive statistic to pre-compute at indexing time, or to compute at retrieval time before scoring documents. In this work, we examine in a uniform setting, the importance of such global statistics for proximity weighting. We investigate two sources of global statistics, namely the target corpus, and the entire Web. Experiments are conducted using the TREC GOV2 and ClueWeb09 test collections. Our results show that local statistics alone are sufficient for effective retrieval, and global statistics usually do not bring any significant improvement in effectiveness, compared to the same proximity approaches that do not use these global statistics.",
            "group": 3166,
            "name": "10.1.1.178.1778",
            "keyword": "Term DependenceGlobal Statistics",
            "title": "Performance, Experimentation"
        },
        {
            "abstract": "Abstract. A new state space representation of a class of combinatorial optimization problems is introduced. The representation enables efficient implementation of exhaustive search for an optimal solution in bounded NP complete problems such as the traveling salesman problem (TSP) with a relatively small number of cities. Furthermore, it facilitates effective heuristic search for sub optimal solutions for problems with large number of cities. This paper surveys structures for representing solutions to the TSP and the use of these structures in iterative hill climbing (ITHC) and genetic algorithms (GA). The mapping of these structures along with respective operators to a newly proposed electro-optical vector by matrix multiplication (VMM) architecture is detailed. In addition, time space tradeoffs related to using a record keeping mechanism for storing intermediate solutions are presented and the effect of record keeping on the performance of these heuristics in the new architecture is evaluated. Results of running these algorithms on sequential architecture as well as a simulation-based estimation of the speedup obtained are supplied. The results show that the VMM architecture can speedup various variants of the TSP algorithm by a factor of 30x to 50x.",
            "group": 3167,
            "name": "10.1.1.178.1843",
            "keyword": "Optical ComputingParallel ProcessingCombinatorial OptimizationThe Traveling Salesman ProblemHeuristic SearchHill ClimbingGenetic Algorithms",
            "title": "Combinatorial Optimization Using Electro-Optical Vector by Matrix Multiplication Architecture"
        },
        {
            "abstract": "",
            "group": 3168,
            "name": "10.1.1.178.2839",
            "keyword": "",
            "title": "Bio-Inspired Distributed Constrained Optimization Technique and its Application in Dynamic Thermal Management"
        },
        {
            "abstract": "Multicore processors have become mainstream and the number of cores in a chip will continue to increase every year. Programming these architectures to effectively exploit their very high computation power is a non trivial task. First, an application program needs to be explicitly restructured using a set of code transformation techniques to optimize for specific architectural features, especially for parallelism and data locality. Then a significant amount of time is spent on tuning the optimized code to find the best optimization parameter values. However, high performance often means lower productivity as the optimized codes become difficult to understand, maintain and modify. In this dissertation, we present techniques to address these issues by automatic generation of efficient parallel programs, and by the use of empirical search for tuning. The research from this dissertation has been implemented and made publicly available as two useful software tools: one for parameterized tiled loop generation, and one for empirical performance tuning using annotations. Tiling is a critical loop transformation that optimizes both for data locality enhancement",
            "group": 3169,
            "name": "10.1.1.178.3939",
            "keyword": "",
            "title": "Tools for Performance Optimizations and Tuning . . . "
        },
        {
            "abstract": " Phylogenetic methods based on optimality criteria are highly desirable for their logic properties, but time-consuming when compared to other methods of tree construction. Traditionally, researchers have been limited to exploring tree space by using multiple replicates of Wagner addition followed by typical hill climbing algorithms such as SPR or/and TBR branch swapping but these methods have been shown to be insuficient for \u201clarge\u201d data sets (or even for small data sets with a complex tree space). Here, I review different algorithms and search strategies used for phylogenetic analysis with the aim of clarifying certain aspects of this important part of the phylogenetic inference exercise. The techniques discussed here apply to both major families of methods based on optimality criteria\u2014parsimony and maximum likelihood\u2014and allow the thorough analysis of complex data sets with hundreds to thousands of terminal taxa. A new technique, called pre-processed searches is proposed for reusing phylogenetic results obtained in previous analyses, to increase the applicability of the previously proposed jumpstarting phylogenetics method. This article is aimed to serve as an educational and algorithmic reference to biologists interested in phylogenetic analysis. Rationale In phylogenetic analysis, numerical methods are preferred over other methods because of their efficiency and repeatability. Within numerical methods, those based on optimality criteria are to be preferred because they allow for hypothesis testing and tree comparisons based on objective measures. However,",
            "group": 3170,
            "name": "10.1.1.178.4909",
            "keyword": "",
            "title": " Efficient Tree Searches with Available Algorithms"
        },
        {
            "abstract": "Abstract. Several activities related to semantically annotated resources can be enabled by a notion of similarity, spanning from clustering to retrieval, matchmaking and other forms of inductive reasoning. We propose the definition of a family of semi-distances over the set of objects in a knowledge base which can be used in these activities. In the line of works on distance-induction on clausal spaces, the family is parameterized on a committee of concepts expressed with clauses. Hence, we also present a method based on the idea of simulated annealing to be used to optimize the choice of the best concept committee. 1",
            "group": 3171,
            "name": "10.1.1.179.1647",
            "keyword": "",
            "title": "F.Esposito. Induction of optimal semantic semi-distances for clausal knowledge bases"
        },
        {
            "abstract": "This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA). We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice. Experiments show the proposed QA algorithm finds better clustering assignments than SA. Furthermore, QA is as easy as SA to implement. 1",
            "group": 3172,
            "name": "10.1.1.179.1744",
            "keyword": "",
            "title": "Quantum Annealing for Clustering"
        },
        {
            "abstract": "Abstract\u2014Linear antenna array design is one of the most important electromagnetic optimization problems of current interest. In contrast to a plethora of recently published articles that formulate the design as the optimization of a single cost function formed by combining distinct and often conflicting design-objectives into a weighted sum, in this work, we take a Multi-objective Optimization (MO) approach to solve the same problem. We consider two design objectives: the minimum average Side Lobe Level (SLL) and null control in specific directions that are to be minimized simultaneously in order to achieve the optimal spacing between the array elements. Our design method employs a recently developed and very competitive multi-objective evolutionary Corresponding author: S. Das",
            "group": 3173,
            "name": "10.1.1.180.527",
            "keyword": "",
            "title": "OPTIMAL SYNTHESIS OF LINEAR ANTENNA AR- RAYS WITH MULTI-OBJECTIVE DIFFERENTIAL EVO- LUTION"
        },
        {
            "abstract": "The logical correctness of security protocols is important. So are efficiency and cost. This paper shows that meta-heuristic search techniques can be used to synthesise protocols that are both provably correct and satisfy various non-functional efficiency criteria. Our work uses a subset of the SVO logic, which we view as a specification language and proof system and also as a \u201cprotocol programming language\u201d. Our system starts from a set of initial security assumptions, carries out meta-heuristic search in the design space, and ends with a protocol (described at the logic level) that satisfies desired goals.",
            "group": 3174,
            "name": "10.1.1.180.5646",
            "keyword": "Security ProtocolsBelief LogicAutomated Protocol SynthesisMeta-heuristic SearchNon-functional Requirements",
            "title": "www.elsevier.com/locate/entcs Synthesising Efficient and Effective Security Protocols"
        },
        {
            "abstract": "Search based test-data generation has proved successful for codelevel testing. In this paper we investigate the application of such approaches at the higher levels of abstraction offered by Matlab-Simulink models. The presence of persistent state has been shown to be problematic at the code level and such difficulties remain when Matlab-Simulink models are to be tested. In such cases, sequences of inputs that can put the model under test into particular states are needed to enable the underlying test goals to be achieved. Simple search guidance appears to be insufficient and results in a \u2018flat \u2019 cost function landscape. To address this problem, we introduce a technique called tracing and deducing, which helps provide better guidance to the search, allowing our developed tools to home in on the targeted test-data.",
            "group": 3175,
            "name": "10.1.1.180.5688",
            "keyword": "structural coveragestate problem",
            "title": "The state problem for test generation in simulink"
        },
        {
            "abstract": "Abstract. Non-linear cryptanalysis is a natural extension to Matsui\u2019s linear cryptanalitic techniques in which linear approximations are replaced by nonlinear expressions. Non-linear approximations often exhibit greater absolute biases than linear ones, so it would appear that more powerful attacks may be mounted. However, their use presents two main drawbacks. The first is that in the general case no joint approximation can be done for more than one round of a block cipher. Despite this limitation, Knudsen and Robshaw showed that they can be still very useful, for they allow the cryptanalist greater flexibility in mounting a classic linear cryptanalysis. The second problem concerning non-linear functions is how to identify them efficiently, given that the search space is superexponential in the number of variables. As the size of S-boxes (the elements usually approximated) increases, the computational resources available to the cryptanalyst for the search become rapidly insufficient. In this work, we tackle this last problem by using heuristic search techniques \u2013particularly Simulated Annealing \u2013 along with a specific representation strategy that greatly facilitates the identification. We illustrate our approach with the 9\u00d732 S-box of the MARS block cipher. For it, we have found multiple approximations with biases considerably larger (e.g. 151/512) than the best known linear mask (84/512) in reasonable time. Finally, an analysis concerning the search dynamics and its effectiveness is also provided. 1",
            "group": 3176,
            "name": "10.1.1.180.6027",
            "keyword": "",
            "title": "Non-linear Cryptanalysis Revisited: Heuristic Search for Approximations to S-Boxes"
        },
        {
            "abstract": "The design of Boolean functions with properties of cryptographic significance is a hard task. In this paper, we adopt an unorthodox approach to the design of such functions. Our search space is the set of functions that possess the required properties. It is \u2018Boolean-ness \u2019 that is evolved. 1",
            "group": 3177,
            "name": "10.1.1.180.6524",
            "keyword": "",
            "title": "Almost Boolean Functions: the Design of Boolean Functions by Spectral Inversion"
        },
        {
            "abstract": "The design of Boolean functions with properties of cryptographic significance is a hard task. In this paper, we adopt an unorthodox approach to the design of such functions. Our search space is the set of functions that possess the required properties. It is \u201cBoolean-ness \u201d that is evolved.",
            "group": 3178,
            "name": "10.1.1.180.6806",
            "keyword": "Key wordsboolean functionsrotational symmetrynonlinearityautocorrelationcorrelation immunity",
            "title": "Almost Boolean Functions: the design of boolean functions by spectral inversion"
        },
        {
            "abstract": "Maximum a Posteriori assignment (MAP) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a Bayesian network. MAP has been shown to be a NP-hard problem [22], even for constrained networks, such as polytrees [18]. Hence, previous approaches often fail to yield any results for MAP problems in large complex Bayesian networks. To address this problem, we propose AnnealedMAP algorithm, a simulated annealing-based MAP algorithm. The AnnealedMAP algorithm simulates a non-homogeneous Markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density. We tested this algorithm on several real Bayesian networks. The results show that, while maintaining good quality of the MAP solutions, the AnnealedMAP algorithm is also able to solve many problems that are beyond the reach of previous approaches. 1",
            "group": 3179,
            "name": "10.1.1.180.8199",
            "keyword": "",
            "title": "628 YUAN ET AL. UAI 2004 Annealed MAP"
        },
        {
            "abstract": "A fundamental issue in real-world systems, such as sensor networks, is the selection of observations which most effectively reduce uncertainty. More specifically, we address the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model. We present the first efficient randomized algorithm providing a constant factor (1 \u2212 1/e \u2212 \u03b5) approximation guarantee for any \u03b5> 0 with high confidence. The algorithm leverages the theory of submodular functions, in combination with a polynomial bound on sample complexity. We furthermore prove that no polynomial time algorithm can provide a constant factor approximation better than (1 \u2212 1/e) unless P = NP. Finally, we provide extensive evidence of the effectiveness of our method on two complex real-world datasets. 1",
            "group": 3180,
            "name": "10.1.1.182.2750",
            "keyword": "",
            "title": "Near-optimal nonmyopic value of information in graphical models"
        },
        {
            "abstract": "optimization method for continuous multidimensional functions",
            "group": 3181,
            "name": "10.1.1.182.3777",
            "keyword": "LONG WRITE UP",
            "title": "Genetically Controlled Random Search: A global"
        },
        {
            "abstract": "Contour fitting using an adaptive spline model",
            "group": 3182,
            "name": "10.1.1.182.4496",
            "keyword": "",
            "title": ""
        },
        {
            "abstract": "A modification of the standard Simulated Annealing (SA) algorithm is presented for finding the global minimum of a continuous multidimensional, multimodal function. We report results of computational experiments with a set of test functions and we compare to methods of similar structure. The accompanying software accepts objective functions coded",
            "group": 3183,
            "name": "10.1.1.182.5170",
            "keyword": "Global optimizationstochastic methodsgenetic programminggrammatical",
            "title": "GenAnneal: Genetically modified Simulated"
        },
        {
            "abstract": " ",
            "group": 3184,
            "name": "10.1.1.182.7629",
            "keyword": "",
            "title": " PROVIDER AND PEER SELECTION IN THE EVOLVING INTERNET ECOSYSTEM  "
        },
        {
            "abstract": "Abstract. Although clause weighting local search algorithms have produced some of the best results on a range of challenging satisfiability (SAT) benchmarks, this performance is dependent on the careful handtuning of sensitive parameters. When such hand-tuning is not possible, clause weighting algorithms are generally outperformed by self-tuning WalkSAT-based algorithms such as AdaptNovelty + and AdaptG 2 WSAT. In this paper we investigate tuning the weight decay parameter of two clause weighting algorithms using the statistical properties of cost distributions that are dynamically accumulated as the search progresses. This method selects a parameter setting both according to the speed of descent in the cost space and according to the shape of the accumulated cost distribution, where we take the shape to be a predictor of future performance. In a wide ranging empirical study we show that this automated approach to parameter tuning can outperform the default settings for two state-of-the-art algorithms that employ clause weighting (PAWS and gNovelty +). We also show that these self-tuning algorithms are competitive with three of the best-known self-tuning SAT local search techniques: RSAPS, AdaptNovelty + and AdaptG 2 WSAT. Key words: Local search, clause weighting, automated parameter tuning, satisfiability.",
            "group": 3185,
            "name": "10.1.1.183.390",
            "keyword": "",
            "title": "Using Cost Distributions to Guide Weight Decay in Local Search for SAT"
        },
        {
            "abstract": "sampling for environmental field estimation",
            "group": 3186,
            "name": "10.1.1.183.4642",
            "keyword": "Index Terms \u2014 Adaptive SamplingModelingField Estimation",
            "title": "using robotic sensors"
        },
        {
            "abstract": "65. Mention of trade names or commercial products does not constitute endorsement or recommendation for use. All research projects making conclusions or recommendations based on environmentally related measurements and funded by the Environmental Protection Agency are required to participate in the Agency",
            "group": 3187,
            "name": "10.1.1.183.7010",
            "keyword": "",
            "title": "Reliability-Based Uncertainty Analysis of Groundwater Contaminant Transport and"
        },
        {
            "abstract": "The performance of several network protocols can be significantly enhanced by tuning their parameters. The optimization of network protocol parameters can be modeled as a \u201cblack-box \u201d optimization problem with unknown, multi-modal and noisy objective functions. In this paper, a recursive random search algorithm is proposed to address this type of optimization problems. The new algorithm takes advantage of the favorable statistical properties of random sampling and achieves high efficiency without imposing extra restrictions, e.g., differentiability, on the objective function. It is also robust to noise in the evaluation of objective function since it use no traditional noise-susceptible local search techniques. The proposed algorithm is tested on classical benchmark functions and its performance compared with a multi-start hillclimbing algorithm. The algorithm is also integrated with a new on-line simulation system which attempts to automate network management by tuning protocol parameters when network conditions change significantly. We present the application of this on-line simulation system in enhancing the performance of network protocols, such as, RED and OSPF. 1",
            "group": 3188,
            "name": "10.1.1.183.7136",
            "keyword": "",
            "title": "A Recursive Random Search Algorithm for Optimization Network Protocol Parameters"
        },
        {
            "abstract": "The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. Increasingly, services are exploring dynamic methods to minimize energy costs while respecting their service-level agreements (SLAs). Furthermore, it will soon be important for these services to manage their usage of \u201cbrown energy \u201d (produced via carbon-intensive means) relative to renewable or \u201cgreen \u201d energy. This paper introduces a general, optimization-based framework for enabling multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their SLAs and minimizing energy costs. Based on the framework, we propose policies for request distribution across the data centers. In some scenarios, our policies can be used to abide by caps on brown energy consumption, such as those that might arise from Kyoto-style carbon limits, from corporate pledges on carbon-neutrality, or from limits imposed on services to encourage brown energy conservation. In other scenarios, without considering brown energy caps or green energy, the policies can be used to minimize energy costs by exploiting data centers located in different time zones with different electricity prices. We evaluate our framework and policies extensively through simulations and real experiments. In scenarios capping brown energy consumption, our results show how our policies allow a service to trade off consumption and cost. For example, using our policies, the service can reduce brown energy consumption by 24% for only a 10 % increase in cost. In scenarios aimed at minimizing energy costs without caps, our results show 24 % lower costs coming from our policies \u2019 ability to exploit differences in electricity prices across data centers, while still abiding by SLAs.",
            "group": 3189,
            "name": "10.1.1.183.8014",
            "keyword": "",
            "title": "Managing the Cost, Energy Consumption, and Carbon Footprint of Internet Services"
        },
        {
            "abstract": "Abstract\u2014In this paper, we consider the problem of synthesizing custom networks-on-chip (NoC) architectures that are optimized for a given application. We consider both unicast and multicast traffic flows in the input specification. Multicast traffic flows are used in a variety of applications, and their direct support with only replication of packets at optimal bifurcation points rather than full end-to-end replication can significantly reduce network contention and resource requirements. Our problem formulation is based on the decomposition of the problem into the inter-related steps of finding good flow partitions, deriving a good physical network topology for each group in the partition, and providing an optimized network implementation for the derived topologies. Our solutions may be comprised of multiple custom networks, each interconnecting a subset of communicating modules. We propose several algorithms that can systematically examine different flow partitions, and we propose Rectilinear\u2013Steiner-Tree (RST)-based algorithms for generating efficient network topologies. Our design flow integrates floorplanning, and our solutions consider deadlock-free routing. Experimental results on a variety of NoC benchmarks showed that our synthesis results can on average achieve a 4.82 times reduction in power consumption over different mesh implementations on unicast benchmarks and a 1.92 times reduction in power consumption on multicast benchmarks. Significant improvements in performance were also achieved, with an average of 2.92 times reduction in hop count on unicast benchmarks and 1.82 times reduction in hop count on multicast benchmarks. To further gauge the effectiveness of our heuristic algorithms, we also implemented an exact algorithm that enumerates all distinct set partitions. For the benchmarks where exact results could be obtained, our algorithms on average can achieve results within 3 % of exact results, but with much shorter execution times. Index Terms\u2014Multicast routing, network-on-chip (NoC), synthesis, system-on-chip (SoC), topology.",
            "group": 3190,
            "name": "10.1.1.183.8435",
            "keyword": "",
            "title": "Custom Networks-on-Chip Architectures with Multicast Routing"
        },
        {
            "abstract": "The performance of several network protocols can be significantly enhanced by tuning their parameters. The optimization of network protocol parameters can be modeled as a \u201cblack-box \u201d optimization problem with unknown, multi-modal and noisy objective functions. In this paper, a recursive random search algorithm is proposed to address this type of optimization problems. The new algorithm takes advantage of the favorable statistical properties of random sampling and achieves high efficiency without imposing extra restrictions, e.g., differentiability, on the objective function. It is also robust to noise in the evaluation of objective function since it use no traditional noise-susceptible local search techniques. The proposed algorithm is tested on classical benchmark functions and its performance compared with a multi-start hillclimbing algorithm. The algorithm is also integrated with a new on-line simulation system which attempts to automate network management by tuning protocol parameters when network conditions change significantly. We present the application of this on-line simulation system in enhancing the performance of network protocols, such as, RED and OSPF.  ",
            "group": 3191,
            "name": "10.1.1.183.9124",
            "keyword": "",
            "title": "A Recursive Random Search Algorithm for Optimizing Network Protocol Parameters"
        },
        {
            "abstract": "Today, many large organizations operate multiple data centers. The reasons for this include natural business distribution, the need for high availability and disaster tolerance, the sheer size of their computational infrastructure,",
            "group": 3192,
            "name": "10.1.1.183.9626",
            "keyword": "",
            "title": "Rutgers University"
        },
        {
            "abstract": "This paper proposes a hybrid optimization method based on the fusion of the Simulated Annealing (SA) and Clonal Selection Algorithm (CSA), in which the SA is embedded in the CSA to enhance its search capability. The novel optimization algorithm is also employed to deal with several nonlinear benchmark functions as well as a practical engineering design problem. Simulation results demonstrate the remarkable advantages of our approach in achieving the diverse optimal solutions and improved convergence speed. 1.",
            "group": 3193,
            "name": "10.1.1.184.275",
            "keyword": "",
            "title": "A SIMULATED ANNEALING-BASED IMMUNE OPTIMIZATION METHOD"
        },
        {
            "abstract": "Abstract \u2014 Recent years have seen the application of evolutionary and other nature-inspired search approaches to achieve human-competitive results in cryptography. We have also seen the emergence of quantum computation as a tremendously exciting computational paradigm with significant potential applications to cryptography. To date there seems to have been no synergistic application of these techniques to cryptographic problems. All applications are geared to the effective exploitation of one computational paradigm or another. Nature-inspired search and quantum computing can, however, be combined to achieve results neither is capable of individually. All that is needed is that classical search get \u2018close enough \u2019 for quantum search to take over and solve the residual problem. This observation has significant implications for the security of crypto-systems and our understanding of the real power of nature-inspired search.",
            "group": 3194,
            "name": "10.1.1.184.4331",
            "keyword": "",
            "title": "COMPUTING BY NATURE"
        },
        {
            "abstract": "Abstract. When evaluating algorithms a very important goal is to perform better than the state-of-the-art techniques.. This requires experimental tests to compare the new algorithm with respect to the rest. It is, in general, hard to make fair comparisons between algorithms such as metaheuristics. The reason is that we can infer di erent conclusions from the same results depending on the metrics we use and how they are applied. This is specially important for non-deterministic methods. This analysis becomes more complex if the study includes parallel metaheuristics, since many researchers are not aware of existing parallel metrics and their meanings, especially concerning the vast literature on parallel programming used well before metaheuristics were rst introduced. In this paper, we focus on the evaluation of parallel algorithms. We give a clear de nition of the main parallel performance metrics and we illustrate how they can be used. 1",
            "group": 3195,
            "name": "10.1.1.184.8185",
            "keyword": "",
            "title": "Evaluation of parallel metaheuristics"
        },
        {
            "abstract": "In mobile environments, mobile device users access and transfer a great deal of data through the online servers. In order to enhance users \u2019 access speed in a wireless network, decentralizing replicated servers appropriately in the network is required. Previous work regarding this issue had focused on the placement of replicated servers along with the moving paths of the users to maximize the hit ratio. When a miss occurs, they simply ignored the file request. Therefore, we suggest a solution to take care of such a miss by sending a file request to a replicated server nearby in the network. This paper is to propose new cost-effective wireless access algorithms incorporating a present replicated server allocation algorithm with more keen analysis of the moving patterns of mobile device users. We propose four different algorithms that allocate available replicated servers in the network so as to minimize the communication costs. The experimental results show that, among the proposed algorithms, the replicated server clustering algorithm allocated replicated servers with near optimal communication costs.",
            "group": 3196,
            "name": "10.1.1.184.8658",
            "keyword": "Shared DataCommunication CostMoving Pattern",
            "title": "EFFECTIVE REPLICATED SERVER ALLOCATION ALGORITHMS IN MOBILE COMPUTING SYSTEMS"
        },
        {
            "abstract": "Abstract Both optimization and learning play important roles in a system for intelligent tasks. On one hand, we introduce three types of optimization tasks studied in the machine learning literature, corresponding to the three levels of inverse problems in an intelligent system. Also, we discuss three major roles of convexity in machine learning, either directly towards a convex programming or approximately transferring a difficult problem into a tractable one in help of local convexity and convex duality. No doubly, a good optimization algorithm takes an essential role in a learning process and new developments in the literature of optimization may thrust the advances of machine learning. On the other hand, we also interpret that the key task of learning is not simply optimization, as sometimes misunderstood in the optimization literature. We introduce the key challenges of learning and the current status of efforts towards the challenges. Furthermore, learning versus optimization has also been examined from a unified perspective under the name of Bayesian Ying-Yang learning, with combinatorial optimization made more effectively in help of learning.",
            "group": 3197,
            "name": "10.1.1.185.562",
            "keyword": "Bayesian Ying-Yang learning",
            "title": "Machine learning problems from optimization perspective"
        },
        {
            "abstract": "the solution of molecular distance geometry",
            "group": 3198,
            "name": "10.1.1.185.2909",
            "keyword": "",
            "title": "Abstract\u2014The Molecular Distance Geometry Problem consists"
        },
        {
            "abstract": " ",
            "group": 3199,
            "name": "10.1.1.185.4576",
            "keyword": "Key WordsSequence alignmentstochastic optimizationsimulated annealinggenetic algorithms",
            "title": "Sequence alignment from the perspective of stochastic optimization: a survey"
        },
        {
            "abstract": "Three modifications to the framework within which hyper-heuristic approaches operate are presented. The first modification automates a self learning mechanism for updating the values of parameters in the choice function used by the controller. Second, a procedure for dynamically configuring a range of lowlevel heuristics is described. Third, in order to effectively use this range of low-level heuristics the controller is redesigned to form a hierarchy of sub-controllers. The second and third modifications improve the inflexibility associated with having a limited number of low-level heuristics available to the controller. Experiments are used to investigate features of the hyper-heuristic framework and the three modifications including comparisons with previously published results. Povzetek: Opisane so tri modifikacije hiper-hevristi\u010dnih pristopov. 1",
            "group": 3200,
            "name": "10.1.1.185.4603",
            "keyword": "choice functiondynamic configurationhierarchical controllerhyper-heuristic frameworklow-level heuristicstimetabling",
            "title": "An Investigation and Extension of a Hyper-heuristic Framework"
        },
        {
            "abstract": "Abstract. We present Fitness Expectation Maximization (FEM), a novel method for performing \u2018black box \u2019 function optimization. FEM searches the fitness landscape of an objective function using an instantiation of the well-known Expectation Maximization algorithm, producing search points to match the sample distribution weighted according to higher expected fitness. FEM updates both candidate solution parameters and the search policy, which is represented as a multinormal distribution. Inheriting EM\u2019s stability and strong guarantees, the method is both elegant and competitive with some of the best heuristic search methods in the field, and performs well on a number of unimodal and multimodal benchmark tasks. To illustrate the potential practical applications of the approach, we also show experiments on finding the parameters for a controller of the challenging non-Markovian double pole balancing task. 1",
            "group": 3201,
            "name": "10.1.1.186.579",
            "keyword": "",
            "title": "Fitness expectation maximization"
        },
        {
            "abstract": "Augmenting visual object classifiers with a full-image latent-topic model for object occurrence",
            "group": 3202,
            "name": "10.1.1.186.1248",
            "keyword": "",
            "title": "Artificial Intelligence"
        },
        {
            "abstract": "The edge of the Internet is increasingly wireless. Enterprises large and small, homeowners, and even whole cities have deployed Wi-Fi networks for their users, and many users never need to \u2014 or never bother to \u2014 use the wired network. With the advent of high-throughput wireless networks (such as 802.11n) some new construction, even of large enterprise buildings, may no longer be wired for Ethernet. To understand Internet traffic, then, we need to understand the wireless edge. Measuring Wi-Fi traffic, however, is challenging. It is insufficient to capture traffic in the access points, or upstream of the access points, because the activity of neighboring networks, ad hoc networks, and physical interference cannot be seen at that level. To truly understand the MAC-layer behavior, we need to capture frames from the air using Air Monitors (AMs) placed in the vicinity of the network. Such a capture is always a sample of the network activity, since it is physically impossible to capture a full trace: all frames from all channels at all times in all places. We have built a monitoring infrastructure that captures frames from the 802.11 network.",
            "group": 3203,
            "name": "10.1.1.186.2279",
            "keyword": "",
            "title": "Examining Committee:"
        },
        {
            "abstract": " ",
            "group": 3204,
            "name": "10.1.1.186.3439",
            "keyword": "",
            "title": "  A Two-Stage Hybrid Local Search for the Vehicle Routing Problem with Time Windows"
        },
        {
            "abstract": "Abstract. We introduce Hegel and Fichte\u2019s dialectic as a search meta-heuristic for constraint satisfaction and optimization. Dialectic is an appealing mental concept for local search as it tightly integrates and yet clearly marks off of one another the two most important aspects of local search algorithms, search space exploration and exploitation. We believe that this makes dialectic search easy to use for general computer scientists and non-experts in optimization. We illustrate dialectic search, its simplicity and great efficiency on four problems from three different problem domains: constraint satisfaction, continuous optimization, and combinatorial optimization. 1",
            "group": 3205,
            "name": "10.1.1.186.3521",
            "keyword": "",
            "title": "Dialectic Search"
        },
        {
            "abstract": "A simulated annealing approach to the traveling",
            "group": 3206,
            "name": "10.1.1.186.3583",
            "keyword": "",
            "title": "tournament problem"
        },
        {
            "abstract": "Local search is a traditional technique to solve combinatorial search problems and has raised much interest in recent years. The design and implementation of local search algorithms is not an easy task in general and may require considerable experimentation and programming effort. However, contrary to global search, little support is available to assist the design and implementation of local search algorithms. This paper is an attempt to support the implementation of local search. It presents the preliminary design of LOCALIZER, a modeling language which makes it possible to express local search algorithms in a notation close to their informal descriptions in scientific papers. Experimental results on our first implementation show the feasibility of the approach.  ",
            "group": 3207,
            "name": "10.1.1.186.4178",
            "keyword": "",
            "title": " LOCALIZER: A Modeling Language for Local Search"
        },
        {
            "abstract": "On-chip speeds and integration densities have grown exponentially over the past several decades creating a corresponding demand for high-bandwidth, chip-to-chip communication. Compared with integrated circuit technology, the technologies for chip-packaging, printed circuit boards, and connectors improve at a much slower rate. This results in a big and growing gap between the I/O bandwidth needed and the I/O bandwidth available. Off-chip bandwidth has become a bottleneck in developing high-speed systems. At high data rates, high-frequency losses, reflections and crosstalk severely degrade signal integrity and limit the performance of off-chip links. To combat these issues, designers increasingly rely on on-chip signal processing methods. This thesis explores the effectiveness of equalizing filters for high-bandwidth, pointto-point, off-chip buses. In this work, we combine modelling, optimization and prototyping to demonstrate that linear programming provides practical, effective and flexible basis for designing equalization filters that greatly increase the bandwidth",
            "group": 3208,
            "name": "10.1.1.186.4243",
            "keyword": "",
            "title": "Equalizing Filter Design for High-Speed Off-Chip Buses"
        },
        {
            "abstract": "Abstract. This paper presents a two-stage hybrid algorithm for pickup and delivery vehicle routing problems with time windows and multiple vehicles (PDPTW). The first stage uses a simple simulated annealing algorithm to decrease the number of routes, while the second stage uses LNS to decrease total travel cost. Experimental results show the effectiveness of the algorithm which has produced many new best solutions on problems with 100, 200, and 600 customers. In particular, it has improved 47 % and 76 % of the best solutions on the 200 and 600-customer benchmarks, sometimes by as much as 3 vehicles. These results further confirm the benefits of two-stage approaches in vehicle routing. They also answer positively the open issue in the original LNS paper, which advocated the use of LNS for the PDPTW and argue for the robustness of LNS with respect to side-constraints. 1",
            "group": 3209,
            "name": "10.1.1.186.4551",
            "keyword": "",
            "title": "A two-stage hybrid algorithm for the pickup and delivery vehicle routing problem with time windows"
        },
        {
            "abstract": "The research presented here examines topological drawing, a new mode of constructing and interacting with mathematical objects in three-dimensional space. In topological drawing, issues such as adjacency and connectedness, which are topological in nature, take precedence over purely geometric issues. Because the domain of application is mathematics, topological drawing is also concerned with the correct representation and display of these objects on a computer. By correctness we mean that the essential topological features of objects are maintained during interaction. We have chosen to limit the scope of topological drawing to knot theory, a domain that consists essentially of one class of object (embedded circles in three-dimensional space) yet is rich enough to contain a wide variety of difficult problems of research interest. In knot theory, two embedded circles (knots) are considered equivalent if one may be smoothly deformed into the other without any cuts or self-intersections. This notion of equivalence may be thought of as the heart of knot theory. We present methods for the computer construction and interactive manipulation of a wide variety of knots. Many of these constructions would be difficult using standard computeraided",
            "group": 3210,
            "name": "10.1.1.186.5585",
            "keyword": "",
            "title": "Interactive topological drawing"
        },
        {
            "abstract": "Partitioning a system \u20183 functionality among interacting hardware and software components is an important part of system design. We introduce a new partitioning approach that caters to the main objective of the hardware/software partitioningproblem, i.e., minimizing hardware,for given performance constraints. We demonstrate results superior to those of previously published algorithms intendedjor hardware/software partitioning. The approach may be genera&able to problems in which one metric must be minimized while other metrics must merely satisfy constraints. 1",
            "group": 3211,
            "name": "10.1.1.186.7204",
            "keyword": "",
            "title": "A. Binary-Constraint Search Algorithm for Minimizing Hardware during Hardware/Software Partitioning"
        },
        {
            "abstract": "The Vehicle Routing Problem with Time Windows (VRPTW) involves servicing a set of customers, with earliest and latest time deadlines, with varying demands using capacitated vehicles with limited travel times. The objective of the problem is to service all customers while minimizing the number of vehicles and travel distance without violating the capacity and travel time of the vehicles and customer time constraints. In this paper we describe a \u03bb-interchange mechanism that moves customers between routes to generate neighborhood solutions for the VRPTW. The \u03bb-interchange neighborhood is searched using Simulated Annealing and Tabu Search strategies. The initial solutions to the VRPTW are obtained using the Push-Forward Insertion heuristic and a Genetic Algorithm based sectoring heuristic. The hybrid combination of the implemented heuristics, collectively known as the GenSAT system, were used to solve 60 problems from the literature with customer sizes varying from 100 to 417 customers. The computational results of GenSAT obtained new best solutions for 40 test problems. For the remaining 20 test problems, 11 solutions obtained by the GenSAT system equal previously known best solutions. The average performance of GenSAT is significantly better than known competing heuristics. For known optimal solutions to the VRPTW problems, the GenSAT system obtained the optimal number of vehicles. Keywords:",
            "group": 3212,
            "name": "10.1.1.186.7231",
            "keyword": "Tabu SearchGenetic AlgorithmsHeuristics",
            "title": "Hybrid Genetic Algorithm, Simulated Annealing and Tabu Search Methods for Vehicle Routing Problems with Time Windows. Working paper"
        },
        {
            "abstract": "In this paper, we present a system-level Congestion-Aware Routing (CAR) framework for designing minimal deterministic routing algorithms. CAR exploits the peculiarities of the application workload to spread the load evenly across the network. To this end, we first formulate an optimization problem of minimizing the level of congestion in the network and then use the simulated annealing heuristic to solve this problem. The proposed framework assures deadlock-free routing, even in the networks without virtual channels. Experiments with both synthetic and realistic workloads show the effectiveness of the CAR framework. Results show that maximum sustainable throughput of the network is improved by up to 205 % for different applications and architectures.",
            "group": 3213,
            "name": "10.1.1.187.1756",
            "keyword": "Categories and Subject Descriptors C.2.1 [Network Architecture and DesignNetwork communications General Terms AlgorithmsDesignPerformance",
            "title": "A Framework for Designing Congestion-Aware Deterministic Routing"
        },
        {
            "abstract": "The importance of high performance algorithms for tackling difficult optimization problems cannot be understated, and in many cases the most effective methods are metaheuristics. When designing a metaheuristic, simplicity should be favored, both conceptually and in practice. Naturally, it must also",
            "group": 3214,
            "name": "10.1.1.187.2089",
            "keyword": "",
            "title": "Iterated Local Search: Framework and Applications"
        },
        {
            "abstract": "DLIMIDs, which can represent decision problems with partial observability and large horizons, constitute an alternative to POMDPs, or rather, they can be viewed as almost the same type of model with two different types of policies and, consequently, two paradigms of evaluation. In this paper, we describe a Markovian model for carcinoid tumors and discuss the effect of evaluating it as a DLIMID or as a POMDP. 1.",
            "group": 3215,
            "name": "10.1.1.187.4342",
            "keyword": "",
            "title": "A Markovian Model for Carcinoid Tumors"
        },
        {
            "abstract": "This paper considers the problem of estimating the power breakdowns for the main appliances inside a building using a small number of power meters and the knowledge of the ON/OFF states of individual appliances. First we solve the breakdown estimation problem within a tree configuration using a single power meter and the knowledge of ON/OFF states and use the solution to derive an estimation quality metric. Using this metric, we then propose an algorithm for optimally placing additional power meters to increase the estimation certainty for individual appliances to the required level. The proposed solution is evaluated using real measurements, numerical simulations and by constructing a scaled down proof-of-concept prototype using binary sensors.",
            "group": 3216,
            "name": "10.1.1.187.5345",
            "keyword": "Electric Load EstimationElectricity Consumption Monitoring",
            "title": "Estimating Building Consumption Breakdowns using ON/OFF State Sensing and Incremental Sub-Meter Deployment"
        },
        {
            "abstract": "We introduce bounds on the finite-time performance of Markov chain Monte Carlo (MCMC) algorithms in solving global stochastic optimization problems defined over continuous domains. It is shown that MCMC algorithms with finite-time guarantees can be developed with a proper choice of the target distribution and by studying their convergence in total variation norm. This work is inspired by the concept of finite-time learning with known accuracy and confidence developed in statistical learning theory. I.",
            "group": 3217,
            "name": "10.1.1.187.6801",
            "keyword": "",
            "title": "Stochastic optimization on continuous domains with finite-time guarantees by Markov chain"
        },
        {
            "abstract": "The statistical mechanics of combinatorial search problems is described using the example of the well-known NP-complete graph coloring problem. We focus on a recently identified phase transition from under- to overconstrained problems, near which are concentrated many hard to solve search problems. Thus, a readily computed measure of problem structure predicts the difficulty of solving the problem, on average. However, this prediction is associated with a large variance and depends on the somewhat arbitrary choice of the problem ensemble. Thus these results are of limited direct use for individual instances. To help address this limitation, additional parameters, describing problem structure as well as heuristic effectiveness, are introduced. This also highlights the distinction between the statistical mechanics of combinatorial search problems, with their exponentially large search spaces, and physical systems, whose interactions are often governed by a simple euclidean metric. Chapter 1",
            "group": 3218,
            "name": "10.1.1.187.8271",
            "keyword": "",
            "title": "Statistical mechanics of combinatorial search"
        },
        {
            "abstract": "We are developing physical design tools for surfacemicromachined MEMS, such as polysilicon microstructures built using MCNC\u2019s Multi-User MEMS Process service. Our initial efforts include automation of layout synthesis and behavioral simulation from a MEMS schematic representation. As an example, layout synthesis of a folded-flexure electrostatic combdrive microresonator is demonstrated. Lumpedparameter electromechanical models with two mechanical degrees-of-freedom link the physical and behavioral parameters of the microresonator. Simulated annealing is used to generate global optimized layouts of five different resonators from 3 kHz to 300 kHz starting with mixed-domain behavioral specifications and constraints. Development of the synthesis tool enforces codification of all relevant MEMS design variables and constraints. The synthesis approach allows a rapid exploration of MEMS design issues. 1.",
            "group": 3219,
            "name": "10.1.1.188.518",
            "keyword": "",
            "title": "Physical Design for Surface-Micromachined MEMS"
        },
        {
            "abstract": "Abstract\u2014A large number of data mining methods are, as such, not applicable to fast, intuitive, and interactive use. Thus, there is a need for visually controllable data mining methods. Such methods should comply with three major requirements: their model structure can be represented visually, they can be controlled using visual interaction, and they should be fast enough for visual interaction. We define a framework for using data mining methods in interactive visualization. These data mining methods are called \u201cvisually controllable \u201d and combine data mining with visualization and user-interaction, bridging the gap between data mining and visual analytics. Our main objective is to define the interactive visualization scenario and the requirements for visually controllable data mining. Basic data mining algorithms are reviewed and it is demonstrated how they can be controlled visually. We also discuss how",
            "group": 3220,
            "name": "10.1.1.188.1629",
            "keyword": "",
            "title": "Visually Controllable Data Mining Methods"
        },
        {
            "abstract": "Recent theoretical work has reported that chaos facilitates biodiversity. In this paper, we study the lowest-dimensional Lotka-Volterra competition model that exhibits chaotic trajectories, a model with four species. We observe that interaction and growth parameters leading respectively to extinction of three species, or coexistence of two, three or four species, are for the most part arranged in large regions with clear boundaries. Small islands of parameters that lead to chaos are also found. These regions where chaos occurs are, in the three cases presented here, situated at the interface between a non-chaotic four-species region and a region where extinction occurs. This implies a high sensitivity of biodiversity with respect to parameter variations in the chaotic regions. Additionally, in regions where extinction occurs which are adjacent to chaotic regions, the computation of local Lyapunov exponents reveals that a possible cause of extinction is the overly strong fluctuations in species abundances induced by local chaos at the beginning of the interval of study. For this model, we conclude that biodiversity is a necessary condition for chaos rather than a consequence of chaos, which can be seen as a signal of a high extinction risk. ",
            "group": 3221,
            "name": "10.1.1.188.1954",
            "keyword": "",
            "title": "Probing chaos and biodiversity in a simple competition model"
        },
        {
            "abstract": "DECEMBER 2010Layout and typography in this thesis are made using L ATEX with the memoir documentclass. Any good idea or nice layout feature can be directly attributed to the book: Introduktion til Latex, by Lars Madsen,",
            "group": 3222,
            "name": "10.1.1.188.2698",
            "keyword": "",
            "title": "Supervisor:"
        },
        {
            "abstract": "Abstract. We develop a stochastic local search algorithm for finding Pareto points for multi-criteria optimization problems. The algorithm alternates between different single-criterium optimization problems characterized by weight vectors. The policy for switching between different weights is an adaptation of the universal restart strategy defined by [LSZ93] in the context of Las Vegas algorithms. We demonstrate the effectiveness of our algorithm on multicriteria quadratic assignment problem benchmarks and prove some of its theoretical properties. 1",
            "group": 3223,
            "name": "10.1.1.188.2850",
            "keyword": "",
            "title": "On Universal Search Strategies for Multi-Criteria Optimization \u22c6"
        },
        {
            "abstract": "In this dissertation, we propose a general approach that can significantly reduce the complexity in solving discrete, continuous, and mixed constrained nonlinear optimization (NLP) problems. A key observation we have made is that most application-based NLPs have structured arrangements of constraints. For example, constraints in AI planning are often localized into coherent groups based on their corresponding subgoals. In engineering design problems, such as the design of a power plant, most constraints exhibit a spatial structure based on the layout of the physical components. In optimal control applications, constraints are localized by stages or time. We have developed techniques to exploit these constraint structures by partitioning the constraints into subproblems related by global constraints. Constraint partitioning leads to much relaxed subproblems that are significantly easier to solve. However, there exist global constraints relating multiple subproblems that must be resolved. Previous methods cannot exploit such structures using constraint partitioning because they cannot resolve inconsistent global constraints efficiently. We have developed a mathematical theory that provides strong necessary and sufficient",
            "group": 3224,
            "name": "10.1.1.188.3344",
            "keyword": "",
            "title": "Solving Nonlinear Constrained Optimization Problems Through Constraint Partitioning"
        },
        {
            "abstract": "c \u25cb Ying Liu 2010I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. Random heterogeneous, scale-dependent structures can be observed from many image sources, especially from remote sensing and scientific imaging. Examples include slices of porous media data showing pores of various sizes, and a remote sensing image including small and large sea-ice blocks. Meanwhile, rather than the images of phenomena themselves, there are many image processing and analysis problems requiring to deal with discrete-state fields according to a labeled underlying property, such as mineral porosity extracted from microscope images, or an ice type map estimated from a sea-ice image. In many cases, if discrete-state problems are associated with heterogeneous, scale-dependent spatial structures, we will have to deal with complex discrete state fields. Although scaledependent image modeling methods are common for continuous-state problems, models for discrete-state cases have not been well studied in the literature. Therefore, a fundamental",
            "group": 3225,
            "name": "10.1.1.188.4422",
            "keyword": "",
            "title": "Hidden Hierarchical Markov Fields for Image Modeling"
        },
        {
            "abstract": "  In this paper, we propose a search technique for nurse scheduling, which deals with it as a multi-objective problem. For each nurse, we first randomly generate a set of legal shift patterns which satisfy all shift-related hard constraints. We then employ an adaptive heuristic to quickly find a solution with the least number of violations on the coverage-related hard constraint by assigning one of the available shift patterns of each nurse. Next, we apply a coverage repairing procedure to make the resulting solution feasible, by adding/removing any under-covered/over-covered shifts. Finally, to satisfy the soft constraints (or preferences), we present a simulated annealing based search method with the following two options: one with a weighted-sum evaluation function which encourages moves towards users\u2019 predefined preferences, and another one with a domination-based evaluation function which encourages moves towards a more diversified approximated Pareto set. Computational results demonstrate that the proposed technique is applicable to modern hospital environments.  ",
            "group": 3226,
            "name": "10.1.1.188.4441",
            "keyword": "",
            "title": "  A Pareto-based search methodology for multi-objective nurse scheduling"
        },
        {
            "abstract": "In this paper we develop a scientific approach to control intercountry conflict. This system makes use of a neural network and a feedback control approach. It was found that by controlling the four controllable inputs: Democracy, Dependency, Allies and Capability simultaneously, all the predicted dispute outcomes could be avoided. Furthermore, it was observed that controlling a single input Dependency or Capability also avoids all the predicted conflicts. When the influence of each input variable on conflict is assessed, Dependency, Capability, and Democracy emerge as key variables that influence conflict. Key Words Artificial intelligence, control, decision-support systems, interstate conflict 1.",
            "group": 3227,
            "name": "10.1.1.188.5531",
            "keyword": "",
            "title": "An integrated humancomputer system for controlling interstate disputes. Working Paper"
        },
        {
            "abstract": "Many problems in high assurance systems design are only tractable using computationally expensive search algorithms. For these algorithms to be useful, designers must be provided with guidance as to how to configure the algorithms appropriately. This paper presents an experimental methodology for deriving such guidance that remains efficient when the algorithm requires substantial computing resources or takes a long time to find solutions. The methodology is shown to be effective on a highly-constrained task allocation algorithm that provides design solutions for high integrity systems. Using the methodology, an algorithm configuration is derived in a matter of days that significantly outperforms one resulting from months of \u2018trial-and-error\u2019 optimisation. 1",
            "group": 3228,
            "name": "10.1.1.188.6179",
            "keyword": "",
            "title": "An Efficient Experimental Methodology for Configuring Search-Based Design Algorithms"
        },
        {
            "abstract": "Radiotherapy planning is a complex problem which requires both expertise and experience of an oncologist. A Case Based Reasoning (CBR) system is developed to generate dose plans for prostate cancer patients. The proposed approach captures the expertise and experience of oncologists in treating previous patients and recommends a dose in phase I and phase II of the treatment of a new patient considering also the success rate of the treatment. The proposed CBR system employs a modified Dempster-Shafer theory to fuse dose plans suggested by the most similar cases retrieved from the case base. In order to mimic the continuous learning characteristic of oncologists, the weights corresponding to each feature used in the retrieval process are updated automatically each time after generating a treatment plan for a new patient. The efficiency of the proposed methodology has been validated using real data sets collected from the Nottingham University Hospitals NHS, City Hospital Campus UK. Experiments demonstrated that for most of the patients, the dose plan generated by our approach is coherent with the dose plan suggested by an experienced oncologist. This methodology can assist both new and experienced oncologists in the treatment planning.",
            "group": 3229,
            "name": "10.1.1.188.7171",
            "keyword": "Case Based ReasoningFuzzy SetsDempster-Shafer theoryProstate CancerRadiotherapy",
            "title": "A novel case based reasoning approach to radiotherapy planning"
        },
        {
            "abstract": "Post-Moore\u2019s Law computing will require an assimilation between computational processes and their physical realizations, both to achieve greater speeds and densities and to allow computational processes to assemble and control matter at the nanoscale. Therefore, we need to investigate \u201cembodied computing, \u201d which addresses the essential interrelationships of information processing and physical processes in the system and its environment in ways that are parallel to those in the theory of embodied cognition. We briefly discuss matters of function and structure, regulation and causation, and the definition of computation. We address both the challenges and opportunities of embodied computation. Analysis is more difficult because physical effects must be included, but information processing may be simplified by dispensing with explicit representations and allowing massively parallel physical processes to process information. Nevertheless, in order to fully exploit embodied computation, we need robust and powerful theoretical tools, but we argue that the theory of Church-Turing computation is not suitable for the task. 1. Post-Moore\u2019s Law Computation Although estimates differ, it is clear that the end of Moore\u2019s Law is in sight; there are physical limits to the density of binary logic devices and to their speed of operation. This will require us to approach computation",
            "group": 3230,
            "name": "10.1.1.188.8217",
            "keyword": "",
            "title": "Bodies \u2014 both informed and transformed: Embodied computation and information processing"
        },
        {
            "abstract": "Optimization is a powerful paradigm for expressing and solving problems in a wide range of areas, and has been successfully applied to many vision problems. While most optimization methods rely on continuous techniques, discrete optimization methods like those traditionally taught in an undergraduate algorithms course have recently become popular in vision. By carefully exploiting problem structure, these methods can often provide non-trivial guarantees concerning solution quality. We survey some of the most important discrete optimization algorithms, and discuss representative examples of how they have been applied to some classical vision problems. We focus on the low-level vision problem of stereo; the mid-level problem of interactive object segmentation; and the high-level problem of model-based recognition. ",
            "group": 3231,
            "name": "10.1.1.188.9757",
            "keyword": "",
            "title": "Discrete Optimization Algorithms in Computer Vision "
        },
        {
            "abstract": "Abstract We present a new approach to learning directed information flow networks from multi-channel spike train data. A novel scoring function, the Snap Shot Score, is used to assess potential networks with respect to their quality of causal explanation for the data. Additionally, we suggest a generic concept of plausibility in order to assess network learning techniques under partial observability conditions. Examples demonstrate the assessment of networks with the Snap Shot Score, and neural network simulations show its performance in complex situations with partial observability. We discuss the application of the new score to real data and indicate how it can be modified to suit other neural data types.",
            "group": 3232,
            "name": "10.1.1.188.9818",
            "keyword": "",
            "title": "DOI 10.1007/s10827-009-0174-2 Causal pattern recovery from neural spike train data using the Snap Shot Score"
        },
        {
            "abstract": "Optimal sensor placement is one that maximizes the likelihood of identifying future damage models. Based on assumptions from engineers, damage models of a structure are simulated and their predictions are computed. Computational approaches are used to place sensors at locations that maximize the chances of identifying damage. This paper studies the application of global search for optimal sensor placement. The global search methodology uses stochastic sampling to find optimal locations for sensors. In a previous study, Robert-Nicoud et al. proposed a greedy strategy that places sensors sequentially at locations where model predictions have maximum entropy. Performance of the two strategies are compared for the Schwandbach bridge in Switzerland. The results show that global search is better for designing measurement systems on a previously unmonitored structure while the greedy algorithm is better for incremental measurementinterpretation strategies. 1.",
            "group": 3233,
            "name": "10.1.1.189.1054",
            "keyword": "",
            "title": "Accepted for the 18th International Workshop on Database and Expert System Applications, Regensburg, Germany (2007) Optimal Sensor Placement for Damage Detection: Role of Global Search"
        },
        {
            "abstract": "Abstract\u2014The paper begins by reviewing a two-level hierarchical multicriteria routing model for MPLS networks with two service classes (QoS and BE services) and alternative routing, as well as the foundations of a heuristic resolution approach, previously proposed by the authors. Afterwards a new approach, of meta-heuristic nature, based on the introduction of simulated annealing and tabu search techniques, in the structure of the dedicated heuristic, is described. The application of the developed procedures to a benchmarking case study will show that, in certain initial conditions, this approach provides improvements in the final results especially in more \u201cdifficult\u201d situations detected through sensitivity analysis. Keywords \u2014 MPLS-Internet, multiobjective optimization, routing models, simulated annealing, tabu search. 1.",
            "group": 3234,
            "name": "10.1.1.189.1945",
            "keyword": "",
            "title": "Paper Hierarchical Multiobjective Routing in MPLS Networks with Two Service Classes \u2013 A Meta-Heuristic Solution"
        },
        {
            "abstract": "The algorithm we present for solving the Car Sequencing Problem as defined in the Roadef Challenge 2005 consists of the following components. First, a constructive heuristic is used to generate an initial sequence. Next, a local search is applied to improve the solution quality. Because a simple local search",
            "group": 3235,
            "name": "10.1.1.189.2469",
            "keyword": "",
            "title": "An algorithm for the car sequencing problem of the ROADEF 2005 challenge"
        },
        {
            "abstract": "We present an optimization approach for service compositions in large-scale service-oriented systems that are subject to Quality of Service (QoS) constraints. In particular, we leverage a composition model that allows a flexible specification of QoS constraints by using constraint hierarchies. We propose an extensible metaheuristic framework for optimizing such compositions. It provides coherent implementation of common metaheuristic functionalities, such as the objective function, improved mutation or neighbor generation. We implement three metaheuristic algorithms that leverage these improved operations. The experiments show the efficiency of these implementations and the improved convergence behavior compared to purely randomized metaheuristic operators.  ",
            "group": 3236,
            "name": "10.1.1.189.2998",
            "keyword": "",
            "title": " Metaheuristic Optimization of Large-Scale QoS-Aware Service compositions"
        },
        {
            "abstract": "Shot boundary detection The shot boundary detection system in 2007 is basically the same as that of last year. We make three major modifications in the system of this year. First, CUT detector and GT detector use block based RGB color histogram with the different parameters instead of the same ones. Secondly, we add a motion detection module to the GT detector to remove the false alarms caused by camera motion or large object movements. Finally, we add a post-processing module based on SIFT feature after both CUT and GT detector. The evaluation results show that all these modifications bring performance improvements to the system. The brief introduction to each run is shown in the following table: Run_id Description Thu01 Baseline system: RGB4_48 for CUT and GT detector, no motion detector, no sift post-processing, only using development set of 2005 as training set Thu02 Same algorithm as thu01, but with RGB16_48 for CUT detector, RGB4_48 for GT detector Thu03 Same algorithm as thu02, but with SIFT post-processing for CUT Thu04 Same algorithm as thu03, but with Motion detector for GT",
            "group": 3237,
            "name": "10.1.1.189.3034",
            "keyword": "",
            "title": "ICRC at TRECVID 2007"
        },
        {
            "abstract": "In this paper we introduce and motivate the static multicast advance reservation (MCAR) problem for all-optical wavelength-routed WDM networks. Under the advanced reservation traffic model, connection requests specify their start time to be some time in the future and also specify their holding times. We investigate the static MCAR problem where the set of advance reservation requests is known ahead of time. We show the problem is NP-complete, formulate the problem mathematically as an integer linear program, and develop three efficient heuristics, seqRWA, ISH, and SA, to solve the problem for practical size networks. We also introduce a theoretical lower bound on the number of wavelengths required. To evaluate our heuristics we compare the results to the ILP for small networks and run simulations over real-world, large scale networks. We find the SA heuristic provides close to optimal results compared to the ILP for our smaller networks, and up to a 33 % improvement over seqRWA and up to a 22 % improvement over ISH on realistic networks. SA provides, on average, solutions 1.5-1.8x times the cost given by our conservative lower bound on large networks. ",
            "group": 3238,
            "name": "10.1.1.189.3241",
            "keyword": "",
            "title": " Static Routing and Wavelength Assignment for Multicast Advance Reservation in All-Optical Wavelength-Routed WDM Networks"
        },
        {
            "abstract": "The complexity of circuit designs has necessitated a top-down approach tolayout synthesis. A large body of work shows that a good layout hierarchy, or partitioning tree, as measured by the associated Rent parameter, will correspond to an area-e cient layout. We de ne the intrinsic Rent parameter of a netlist to be the minimum possible Rent parameter of any partitioning tree for the netlist. Experimental results show that spectra-based ratio cut partitioning algorithms yield partitioning trees with the lowest observed Rent parameter over all benchmarks and over all algorithms tested. For examples where the intrinsic Rent parameter is known, spectral ratio cut partitioning yields a partitioning tree with Rent parameter essentially identical to this theoretical optimum. These results have deep implications with respect to both the choice of partitioning algorithms for top-down layout, as well as new approaches to layout area estimation. The paper concludes with directions for future research, including several promising techniques for fast estimation of the (intrinsic) Rent parameter. 1",
            "group": 3239,
            "name": "10.1.1.189.3674",
            "keyword": "",
            "title": "On the intrinsic rent parameter and spectra-based partitioning methodologies"
        },
        {
            "abstract": "For firms in an oligopoly service network, demand learning based dynamics pricing is an efficient way to maximize their revenues. This paper introduces a Bayesian method to learn demand behavior from the perspective of game-theoretic dynamics, where non-parametric techniques for nonlinear time series are incorporated, such that stringent parametric assumptions are removed. We determine the unknown quantities in our demand learning model from historical market data through a Markov chain Monte Carlo (MCMC) algorithm. Based on the calibrated model, how future demands respond to prices can be predicted, and the optimal pricing policy for the next planning period is obtained by a heuristic optimization algorithm. Simulated examples show that our new method is efficient for integrating demand",
            "group": 3240,
            "name": "10.1.1.190.2851",
            "keyword": "",
            "title": "A Revenue Maximizing Strategy Based on Bayesian Analysis of Demand Dynamics"
        },
        {
            "abstract": "findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.",
            "group": 3241,
            "name": "10.1.1.190.7047",
            "keyword": "",
            "title": "www.samsi.info Sparse Bayes Inference by Annealing Entropy 1 Sparse Bayes Inference by Annealing Entropy"
        },
        {
            "abstract": null,
            "group": 3242,
            "name": "10.1.1.190.9605",
            "keyword": "",
            "title": "Rule Abstraction: Understanding Emergent Behavior in Swarm Systems"
        },
        {
            "abstract": "Abstract\u2014A large number of data mining methods are, as such, not applicable to fast, intuitive, and interactive use. Thus, there is a need for visually controllable data mining methods. Such methods should comply with three major requirements: their model structure can be represented visually, they can be controlled using visual interaction, and they should be fast enough for visual interaction. We define a framework for using data mining methods in interactive visualization. These data mining methods are called \u201cvisually controllable \u201d and combine data mining with visualization and user-interaction, bridging the gap between data mining and visual analytics. Our main objective is to define the interactive visualization scenario and the requirements for visually controllable data mining. Basic data mining algorithms are reviewed and it is demonstrated how they can be controlled visually. We also discuss how",
            "group": 3243,
            "name": "10.1.1.190.9778",
            "keyword": "",
            "title": "Visually Controllable Data Mining Methods"
        },
        {
            "abstract": "Rule abstraction is an intuitive new tool that we propose for implementing swarm systems. The methods presented in this paper encourage a new paradigm for designing swarm applications: (swarm) level instead of the individual (agent) level. This is made possible by modeling and learning how particular swarm-level properties arise from low-level agent behaviors. rules and discuss how they can be used. We also provide experimental results showing that abstract rules can be learned by observation.",
            "group": 3244,
            "name": "10.1.1.191.808",
            "keyword": "",
            "title": "Learning Abstract Rules for Swarm Systems"
        },
        {
            "abstract": "2.1 Multidimensional Scaling (MDS)........................... 4 2.2 Scaling by MAjorizing a COmplicated Function (SMACOF)............ 4",
            "group": 3245,
            "name": "10.1.1.192.1995",
            "keyword": "",
            "title": "Contents"
        },
        {
            "abstract": "to",
            "group": 3246,
            "name": "10.1.1.192.3201",
            "keyword": "",
            "title": "A Corpus-BasedApproach to language Learning"
        },
        {
            "abstract": "Abstract \u2014 Fuzzy logic and fuzzy set theory provide an important framework for representing and managing imprecision and uncertainty in medical expert systems, but the need remains to optimize such systems to enhance performance. This paper presents a general technique for optimizing fuzzy models in fuzzy expert systems (FES\u2019s) by simulated annealing (SA) and N-dimensional hill climbing simplex method. The application of the technique to a FES for the interpretation of the acid-base balance of blood in the umbilical cord of newborn infants is presented. The Spearman rank order correlation statistic was used to assess and to compare the performance of a commercially available crisp expert system, an initial FES, and a tuned FES with experienced clinicians. Results showed that without tuning, the performance of the crisp system was significantly better (correlation of 0.80) than the FES (correlation of 0.67). The performance of the tuned FES was better than the crisp system and effectively indistinguishable from the clinicians (correlation of 0.93) on training data and was the best of the expert systems on validation data. Unlike most applications of fuzzy logic where all fuzzy sets have normalized heights of unity, in this application it was found that a reduction in the height of some fuzzy sets was effective in enhancing performance. This suggests that the height of fuzzy sets may be a generally useful parameter in tuning FESs. Index Terms \u2014 Acid-base balance, fuzzy modeling, neonatal outcome, simplex method, simulated annealing.",
            "group": 3247,
            "name": "10.1.1.192.7952",
            "keyword": "",
            "title": "Application of simulated annealing fuzzy model tuning to umbilical cord acid-base interpretation"
        },
        {
            "abstract": "Abstract\u2014Gas Metal Arc Welding (GMAW) processes is an important joining process widely used in metal fabrication industries. This paper addresses modeling and optimization of this technique using a set of experimental data and regression analysis. The set of experimental data has been used to assess the influence of GMAW process parameters in weld bead geometry. The process variables considered here include voltage (V); wire feed rate (F); torch Angle (A); welding speed (S) and nozzle-to-plate distance (D). The process output characteristics include weld bead height, width and penetration. The Taguchi method and regression modeling are used in order to establish the relationships between input and output parameters. The adequacy of the model is evaluated using analysis of variance (ANOVA) technique. In the next stage, the proposed model is embedded into a Simulated Annealing (SA) algorithm to optimize the GMAW process parameters. The objective is to determine a suitable set of process parameters that can produce desired bead geometry, considering the ranges of the process parameters. Computational results prove the effectiveness of the proposed model and optimization procedure. Keywords\u2014Weld Bead Geometry, GMAW welding, Process parameters Optimization, Modeling, SA algorithm",
            "group": 3248,
            "name": "10.1.1.192.8326",
            "keyword": "",
            "title": "A New Approach for Predicting and Optimizing Weld Bead Geometry in GMAW"
        },
        {
            "abstract": "is becoming a powerful tool for use in the study of biomedical conditions, including cancer diagnosis. As part of an ongoing programme of research into the potential early diagnosis of cervical cancer, Hierarchical Cluster Analysis (HCA) and Fuzzy C-Means (FCM) have been applied to distinguish FTIR spectra obtained from cancerous and non-cancerous cells. In recent experimentation on non pre-processed FTIR spectra data, the FCM method has been shown to achieve significantly better results. Nevertheless, two limitations apply to this technique. Firstly, a priori assumption of the number of clusters is needed. This is a problem because, in general, this is not known in medical diagnosis. The other limitation is that it involves a greedy local search methodology such that sub-optimal solutions may be returned and, thus, misdiagnosis could occur. Bandyopadhyay [8] has recently proposed a Simulated Annealing Fuzzy Clustering algorithm (SAFC) which can avoid these two limitations. However, when we implemented the proposed algorithm, it was found that sub-optimal solutions could be obtained in certain circumstances. In this paper, we extend the SAFC algorithm to overcome this difficulty and apply this modified version to the classification of seven sets of FTIR spectra data which have been taken from three oral cancer patients. With no prior specification of cluster number, our modified SAFC algorithm is shown to obtain the correct (clinical) classification of clusters in 4 out of 7 data sets. In the remaining 3 data sets it produces a number of clusters which, while differing from the clinical classification, appear to better match the underlying data when subjectively visualised using Principal Component Analysis (PCA). I.",
            "group": 3249,
            "name": "10.1.1.192.9226",
            "keyword": "",
            "title": "The Application of a Simulated Annealing Fuzzy Clustering Algorithm for Cancer Diagnosis"
        },
        {
            "abstract": "statistical approach for predicting and",
            "group": 3250,
            "name": "10.1.1.193.356",
            "keyword": "Mathematical modelingSimulated AnnealingOptimization",
            "title": "optimizing depth of cut in AWJ machining for 6063-T6 Al alloy"
        },
        {
            "abstract": "Abstract\u2014The objective of this paper is the introduction to a unified optimization framework for research and education. The OPTILIB framework implements different general purpose algorithms for combinatorial optimization and minimum search on standard continuous test functions. The preferences of this library are the straightforward integration of new optimization algorithms and problems as well as the visualization of the optimization process of different methods exploring the search space exclusively or for the real time visualization of different methods in parallel. Further the usage of several implemented methods is presented on the basis of two use cases, where the focus is especially on the algorithm visualization. First it is demonstrated how different methods can be compared conveniently using OPTILIB on the example of different iterative improvement schemes for the TRAVELING SALESMAN PROBLEM. A second study emphasizes how the framework can be used to find global minima in the continuous domain.",
            "group": 3251,
            "name": "10.1.1.193.1670",
            "keyword": "Particle Swarm OptimizationEnsemble Based Threshold AcceptingRuin and Recreate",
            "title": "An Integrated Framework for the Realtime Investigation of State Space Exploration"
        },
        {
            "abstract": "Nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world. This paper presents a new component-based approach with evolutionary eliminations, for a nurse scheduling problem arising at a major UK hospital. The main idea behind this technique is to decompose a schedule into its components (i.e. the allocated shift pattern of each nurse), and then to implement two evolutionary elimination strategies mimicking natural selection and natural mutation process on these components respectively to iteratively deliver better schedules. The worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there. This demonstration employs an evaluation function which evaluates how well each component contributes towards the final objective. Two elimination steps are then applied: the first elimination eliminates a number of components that are deemed not worthy to stay in the current schedule; the second elimination may also throw out, with a low level of probability, some worthy components. The eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria. Computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems.",
            "group": 3252,
            "name": "10.1.1.193.2129",
            "keyword": "Key wordsnurse rosteringconstructive heuristiclocal searchevolutionary elimination HistoryAccepted by Michel GendreauArea Editor for Heuristic Search & Learning",
            "title": "A Component Based Heuristic Search Method with Evolutionary Eliminations for Hospital Personnel Scheduling"
        },
        {
            "abstract": "Abstract\u2014There are two common types of operational research techniques, optimisation and metaheuristic methods. The latter may be defined as a sequential process that intelligently performs the exploration and exploitation adopted by natural intelligence and strong inspiration to form several iterative searches. An aim is to effectively determine near optimal solutions in a solution space. In this work, a type of metaheuristics called Ant Colonies Optimisation, ACO, inspired by a foraging behaviour of ants was adapted to find optimal solutions of eight non-linear continuous mathematical models. Under a consideration of a solution space in a specified region on each model, sub-solutions may contain global or multiple local optimum. Moreover, the algorithm has several common parameters; number of ants, moves, and iterations, which act as the algorithm\u2019s driver. A series of computational experiments for initialising parameters were conducted through methods of Rigid Simplex, RS, and Modified Simplex, MSM. Experimental results were analysed in terms of the best so far solutions, mean and standard deviation. Finally, they stated a recommendation of proper level settings of ACO parameters for all eight functions. These parameter settings can be applied as a guideline for future uses of ACO. This is to promote an ease of use of ACO in real industrial processes. It was found that the results obtained from MSM were pretty similar to those gained from RS. However, if these results with noise standard deviations of 1 and 3 are compared, MSM will reach optimal solutions more efficiently than RS, in terms of speed of convergence.",
            "group": 3253,
            "name": "10.1.1.193.2357",
            "keyword": "Modified SimplexNon-linearRigid Simplex",
            "title": "2009, \"A Comparative Study of Rigid and Modified Simplex Methods for Optimal Parameter Settings of ACO for Noisy Non-Linear Surfaces"
        },
        {
            "abstract": "Abstract\u2014Several methods are available for weight and shape optimization of structures, among which Evolutionary Structural Optimization (ESO) is one of the most widely used methods. In ESO, however, the optimization criterion is completely case-dependent. Moreover, only the improving solutions are accepted during the search. In this paper a Simulated Annealing (SA) algorithm is used for structural optimization problem. This algorithm differs from other random search methods by accepting non-improving solutions. The implementation of SA algorithm is done through reducing the number of finite element analyses (function evaluations). Computational results show that SA can efficiently and effectively solve such optimization problems within short search time. Keywords\u2014Simulated annealing, Structural optimization, Compliance, C.V. product.",
            "group": 3254,
            "name": "10.1.1.193.3405",
            "keyword": "",
            "title": "Simulated Annealing Application for Structural Optimization"
        },
        {
            "abstract": "It is widely accepted that learning is closely related to theories of optimisation and information. Indeed, there is no need to learn if there is nothing to optimise; if one possesses full information, then there is simply nothing new to learn. The paper considers learning as an optimisation problem with dynamical information constraints. Unlike the standard approach in the optimal control theory, where the solutions are given by the Hamilton\u2013Jacobi\u2013Bellman equation for Markov time evolution, the optimal solution is presented as the system of canonical Euler equations defining the optimal information\u2013 utility trajectory in the conjugate space. The optimal trajectory is parameterised by the information\u2013utility constraints, which are illustrated on examples for finite and infinite\u2013 dimensional cases. Without uncertainty, optimisation corresponds to a simple choice problem, where a preference relation (total preorder) is defined over the underlying set \u2126. If one can represent the preference relation by some real utility function u: \u2126 \u2192 R, then the optimal solution is solved by finding the extremum (e.g. the maximum) of this function. The utility function may take the form of a Lagrangian incorporating many objectives,",
            "group": 3255,
            "name": "10.1.1.193.3472",
            "keyword": "",
            "title": "Information Evolution of Optimal Learning"
        },
        {
            "abstract": "Abstract\u2014This paper presents a hybrid algorithm for solving a timetabling problem, which is commonly encountered in many universities. The problem combines both teacher assignment and course scheduling problems simultaneously, and is presented as a mathematical programming model. However, this problem becomes intractable and it is unlikely that a proven optimal solution can be obtained by an integer programming approach, especially for large problem instances. A hybrid algorithm that combines an integer programming approach, a greedy heuristic and a modified simulated annealing algorithm collaboratively is proposed to solve the problem. Several randomly generated data sets of sizes comparable to that of an institution in Indonesia are solved using the proposed algorithm. Computational results indicate that the algorithm can overcome difficulties of large problem sizes encountered in previous related works. Keywords\u2014Timetabling problem, mathematical programming model, hybrid algorithm, simulated annealing. I.",
            "group": 3256,
            "name": "10.1.1.193.3646",
            "keyword": "",
            "title": "Solving the Teacher Assignment-Course Scheduling Problem by a Hybrid Algorithm"
        },
        {
            "abstract": " This paper describes a new method for affine parameter estimation between image sequences. Usually, the parameter estimation techniques can be done by least squares in a quadratic way. However, this technique can be sensitive to the presence of outliers. Therefore, parameter estimation techniques for various image processing applications are robust enough to withstand the influence of outliers. Progressively, some robust estimation functions demanding non-quadratic and perhaps non-convex potentials adopted from statistics literature have been used for solving these. Addressing the optimization of the error function in a factual framework for finding a global optimal solution, the minimization can begin with the convex estimator at the coarser level and gradually introduce nonconvexity i.e., from soft to hard redescending non-convex estimators when the iteration reaches finer level of multiresolution pyramid. Comparison has been made to find the performance of the results of proposed method with the results found individually using two different estimators. ",
            "group": 3257,
            "name": "10.1.1.193.3989",
            "keyword": "Robust StatisticsRobust M-estimators",
            "title": " A Novel Multiresolution based Optimization Scheme for Robust Affine Parameter Estimation"
        },
        {
            "abstract": "This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classification tasks. FMS is defined as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classification error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classification domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overfitting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge.",
            "group": 3258,
            "name": "10.1.1.193.5162",
            "keyword": "full model selectionmachine learning challengeparticle swarm optimizationexperimentationcross",
            "title": "Particle swarm model selection"
        },
        {
            "abstract": "(University Leipzig), which had a significant influence on the quality of this work. I wish to express my gratitude to a number of people who became involved in one way or another. First, I want to thank Marc Tittgemeyer, who arouse my interest in brain science and to whom I owe the interesting topic. Moreover, I want to thank him for inspiring my ambition. My stay at the Karolinska Institute would not have been possible without Lars-Olof Wahlund, who did not only offer a place to work to me, but also showed great interest in my work and gave me the feeling that I was about to do something important. For many fruitful discussions and lots of motivation I would like to thank Gerik Scheuermann. Many thanks go to the team at the Karolinska, who gave me a warm welcome and showed me many interesting aspects of their daily work. Special thanks go to Eva-Lena Engman, Elin Lundstr\u00f6m, Leszek Stawiarz and Susanne M\u00fcller, who made my time in Sweden so much more pleasant. Tack so mucket! For his persistent explanations concerning programming and Linux, I would like to thank Gert Wollny. Furthermore,",
            "group": 3259,
            "name": "10.1.1.194.1215",
            "keyword": "",
            "title": "Shape Analysis of the Human Hippocampus Using Spherical Harmonics: An Application to Alzheimer\u2019s Disease"
        },
        {
            "abstract": "Abstract: We review the prominent technologies in virtual screening, and their applications in drug discovery.",
            "group": 3260,
            "name": "10.1.1.194.5834",
            "keyword": "",
            "title": "Retrospect and Prospect of Virtual Screening in Drug Discovery"
        },
        {
            "abstract": "A common approach to parallelizing simulated annealing to generate several perturbations to the currentsolutionsimultaneously, requiring synchronization to guarantee correct evaluation of the cost function. The cost of this synchronization may be reduced by allowing inaccuracies in the cost calculations. We provide a framework for understanding the theoretical implications of this approach based on a model of processor interaction under reduced synchronization that demonstrates how errors in cost calculations occur and how to estimate them. We show how bounds on error in the cost calculations in asimulated annealing algorithm can be translated into worst-case bounds on perturbations in the parameters which describe the behavior of the algorithm.",
            "group": 3261,
            "name": "10.1.1.194.8089",
            "keyword": "",
            "title": "Trading Accuracy for Speed in Parallel Simulated Annealing with Simultaneous Moves"
        },
        {
            "abstract": "",
            "group": 3262,
            "name": "10.1.1.195.808",
            "keyword": "translation fall in this category",
            "title": "Mathematical Methods"
        },
        {
            "abstract": "Abstract. A computer-based system for modelling component dependencies and identifying component modules is presented. A variation of the Dependency Structure Matrix (DSM) representation was used to model component dependencies. The system utilises a two-stage approach towards facilitating the identification of a hierarchical modular structure. The first stage calculates a value for a clustering criterion that may be used to group component dependencies together. A Genetic Algorithm is described to optimise the order of the components within the DSM with the focus of minimising the value of the clustering criterion to identify the most significant component groupings (modules) within the product structure. The second stage utilises a \u2018Module Strength Indicator\u2019 (MSI) function to determine a value representative of the degree of modularity of the component groupings. The application of this function to the DSM produces a \u2018Module Structure Matrix \u2019 (MSM) depicting the relative modularity of available component groupings within it. The approach enabled the identification of hierarchical modularity in the product structure without the requirement for any additional domain specific knowledge within the system. The system supports design by providing mechanisms to explicitly represent and utilise component and dependency knowledge to facilitate the nontrivial task of determining near-optimal component modules and representing product modularity. 1.",
            "group": 3263,
            "name": "10.1.1.195.900",
            "keyword": "",
            "title": "Identifying Component Modules"
        },
        {
            "abstract": "Estimation of biological and economic",
            "group": 3264,
            "name": "10.1.1.195.1469",
            "keyword": "Dynamic parameter estimation\ufffdadjoint method\ufffdtwin experiments\ufffd sheries",
            "title": "parameters of"
        },
        {
            "abstract": "ii iiiTHE A-DESIGN INVENTION MACHINE:",
            "group": 3265,
            "name": "10.1.1.196.3886",
            "keyword": "Matthew Ira CampbellAcknowledgments",
            "title": "A MEANS OF AUTOMATING AND INVESTIGATING CONCEPTUAL DESIGN"
        },
        {
            "abstract": "This Report includes data that shall not be disclosed outside the Government and shall not be duplicated, used, or disclosed in whole or in part for any purpose other than to evaluate this Report. This restriction does not limit the right of the Government to use information contained in this Report if it is proprietary data contained herein, if obtained from another source without restriction. The data subject to this restriction are contained in all sheets of this Report. The proprietary data contained herein, if disclosed to the public, would affect ISR\u2019s competitive position in obtaining business; therefore, it is considered to be exempt from public release under the Freedom of Information Act (5 USC \u00a7552, as amended), paragraph (b)(4). IVVNN-LITREV-F002-UNCLASS-111202",
            "group": 3266,
            "name": "10.1.1.196.4737",
            "keyword": "Karen Tucker",
            "title": "Prepared By:"
        },
        {
            "abstract": "review. Views or opinions expressed herein do not necessarily represent those of the Institute, its Birth-and-death processes or, equivalently, nite Markov chains with three-diagonal transition matrices proved to be adequate models for processes in physics [12], biology [4,5], sociology [13] and economics [1,3,10]. The analysis in this case quite often relies on the stationary distribution of the chain. Representing it as a Gibbs distribution, we study its limit behavior as the number of states increases. We show that the limit nests on the set of global minima of the limit Gibbs potential. If the set consists of a nite number k of singletons ai where the second derivatives i of the potential are positive, the limit distribution assigns probability 1 = p i P k j=1 1 = p j",
            "group": 3267,
            "name": "10.1.1.196.8931",
            "keyword": "ii { Contents",
            "title": "Approved by"
        },
        {
            "abstract": "We study the effects of various incentive schemes on the learning behavior of teams in an artificial factory. Modeling the new product development process, we demonstrate, how production and marketing agents learn to coordinate their actions in order to produce the optimal product with respect to their incentive schemes. As a coordinating mechanism between marketing and production, we use the House of Quality framework of Hauser and Clausing [6]. The House of Quality methodology, which is used by real firms, contains important information from marketing and production. It is a procedure that facilitates the search for new promising (from market perspective) and feasible products (from a production/design perspective). We found that the House of Quality approach yields higher life cycle returns than the traditional search for new products- especially for a low number of search steps. This is an important finding recommending the application of the House of Quality since the number of search steps directly influences time to market. Thus, minimizing the number of steps could be an important competitive advantage in todays fast moving consumer markets. 1.",
            "group": 3268,
            "name": "10.1.1.197.958",
            "keyword": "",
            "title": "Proceedings of the 33rd Hawaii International Conference on System Sciences- 2000 Learning in the Artificial Factory"
        },
        {
            "abstract": "The work documented in this reported benefited immensely from the critical input of many people. Gary Knight has provided oversight and strategic direction for all of the Florida Natural Areas Inventory\u2019s contributions to the Florida Forever program. All of the major decision points in this modeling process received consensus support from a working group of natural resource and conservation experts, who also provided guidance and important insights throughout. The",
            "group": 3269,
            "name": "10.1.1.197.3253",
            "keyword": "",
            "title": "Acknowledgments"
        },
        {
            "abstract": "c \u25cb Eliza Chiang, 2003In presenting this thesis in partial fulfilment of the requirements for an advanced degree at the University of British Columbia, I agree that the Library shall make it freely available for reference and study. I further agree that permission for extensive copying of this thesis for scholarly purposes may be granted by the head of my department or by his or her representatives. It is understood that copying or publication of this thesis for financial gain shall not be allowed without my written permission. (Signature)",
            "group": 3270,
            "name": "10.1.1.197.3599",
            "keyword": "",
            "title": "THE REQUIREMENTS FOR THE DEGREE OF"
        },
        {
            "abstract": "This technical report discusses the theoretical background, the implementation, and the experience gained while working on a prototype shape-matching application. Several",
            "group": 3271,
            "name": "10.1.1.197.5978",
            "keyword": "",
            "title": "Matching, Probabilistic Matching, and Simulated Annealing, and Fr\u00e9chet Distance, Partial Fr\u00e9chet Distance, and Symmetric Difference. Contents"
        },
        {
            "abstract": "Abstract. Time-series prediction can be interpreted in a way that is suitable for artificial intelligence",
            "group": 3272,
            "name": "10.1.1.197.9852",
            "keyword": "time-series predictionmodel selectionmeta-heuristicprincipal component analysiskernel principal component analysisneural networkssupport vector machinessimulated annealing",
            "title": "Intelligent Forecast with Dimension Reduction"
        },
        {
            "abstract": "This paper describes various multi-agent architectures including the heterarchical architecture. It reviews the claimed advantages for multi-agent heterarchies and describes the types of factories which could use this architecture. It surveys the three common types of factory control algorithms: dispatching algorithms, scheduling algorithms, and pull algorithms. It then asks the question: which of these algorithms can be implemented in a multi-agent heterarchy? This paper describes how all common factory control algorithms used in industry can be implemented in an multi-agent heterarchy. It discusses how many of the algorithms which are popular in current research can be implemented in a multi-agent heterarchy, while others will require further research.",
            "group": 3273,
            "name": "10.1.1.198.2789",
            "keyword": "Systems and ArchitecturesHeterarchyScheduling and ControlERPDiscrete OptimizationHeuristicsDistributed Artificial IntelligenceAgent TechnologyMulti-Agent Systems",
            "title": " A Survey of Factory Control Algorithms which Can be Implemented in a Multi-Agent Heterarchy: Dispatching, Scheduling, and Pull"
        },
        {
            "abstract": "evolutionary computation, methodology development, production networks. Adaptation and learning are the most crucial skills in the survival of any complex system \u2212 the former one emphasizing the ability to perform structural reorganization and the latter one the use of previously available information \u2212 to reflect on the endlessly changing environment the particular system is embedded in. Humans are such complex systems and also manmade ones that humans manage by the aid of cooperation, science and the multitude of automated tools such as computers, robots, vehicles and their combinations. The survival fitness of individuals, organizations, societies and mankind itself depends on the successful management of the adaptation and learning process that often involves the changing of the environment. In this interplay between man and nature it is crucial to gather useful knowledge of explanatory and predictive power in the \u2212 Aristotelian \u2212 form of science and metaphors. In addition to these, computers have provided a third form or language for knowledge gathering and representation since the middle of the XX th century. The success of a system of knowledge \u2212 a theory \u2212 largely depends on the integrated application of these",
            "group": 3274,
            "name": "10.1.1.198.5415",
            "keyword": "agentbased modelling and simulationcomplex adaptive systemsdissipative structures",
            "title": "Abstract P\u00e1tkai, B\u00e9la: An Integrated Methodology for Modelling Complex Adaptive Production Networks"
        },
        {
            "abstract": "The Stochastic Diffusion Search (SDS) was developed as a solution to the best-fit search problem. Thus, as a special case it is capable of solving the transform invariant pattern recognition problem. SDS is efficient and, although inherently probabilistic, produces very reliable solutions in widely ranging search conditions. However, to date a systematic formal investigation of its properties has not been carried out. This thesis addresses this problem. The thesis reports results pertaining to the global convergence of SDS as well as characterising its time complexity. However, the main emphasis of the work, reports on the resource allocation aspect of the Stochastic Diffusion Search operations. The thesis introduces a novel model of the algorithm, generalising an Ehrenfest Urn Model from statistical physics. This approach makes it possible to obtain a thorough characterisation of the response of the algorithm in terms of the parameters describing the search conditions in case of a unique best-fit pattern in the search space. This model is further generalised in order to account for different search conditions: two solutions in the search space and search for a unique solution in a noisy search space. Also an approximate solution in the case of two alternative solutions is proposed and compared with predictions",
            "group": 3275,
            "name": "10.1.1.199.920",
            "keyword": "",
            "title": "THE UNIVERSITY OF READING Resource Allocation Analysis of the Stochastic Diffusion Search"
        },
        {
            "abstract": "Effective reuse of past designs is critical to achieve the reductions in development time sought by the RASSP program. Improvements in functionality and performance are simpler to achieve if a design is implemented in an \"upgradable \" form, enabling easier recustomization towards different but related mission requirements. Consequently, any methodology that effectively reuses past designs, i.e, Design with Reuse (DwR), is predicated on the implementation and acceptance of an effective Design for Reuse (DfR) methodology. By building an effective DfR methodology, RASSP is enabling \"model year \" improvements in functionality and performance, similar to those achieved by the automobile industry, without extensive redesign or changes in the manufacturing process. The continuous improvement methodologies being developed by RASSP will ensure that RASSP subsystems are at the \"state-ofthe-shelf\" when fielded, and continuously remain so throughout",
            "group": 3276,
            "name": "10.1.1.199.7254",
            "keyword": "",
            "title": "RASSP Digest Theme: Model Year Architectures"
        },
        {
            "abstract": "Iterated Local Search has many of the desirable features of a metaheuristic: it is simple, easy to implement, robust, and highly effective. The essential idea of Iterated Local Search lies in focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for a given optimization engine. The success of Iterated Local Search lies in the biased sampling of this set of local optima. How effective this approach turns out to be depends mainly on the choice of the local search, the perturbations, and the acceptance criterion. So far, in spite of its conceptual simplicity, it has lead to a number of state-of-the-art results without the use of too much problem-specific knowledge. But with further work so that the different modules are well adapted to the problem at hand, Iterated Local Search can often become a competitive or even state of the art algorithm. The purpose of this review is both to give a detailed description of this metaheuristic and to show where it stands in terms of performance. O.M. acknowledges support from the Institut Universitaire de France. This work was partially supported by the \u201cMetaheuristics Network\u201d, a Research Training Network funded by the Improving Human Potential programme of the CEC, grant HPRN-CT-1999-00106. The information provided is the sole responsibility of the authors and does not reflect the Community\u2019s opinion. The Community is not responsible for any use that might be made of data appearing in this publication. 1 1",
            "group": 3277,
            "name": "10.1.1.200.5466",
            "keyword": "",
            "title": "Iterated local search"
        },
        {
            "abstract": "Stochastic Diffusion Search (sds) was introduced by Bishop (1989a) as an algorithm to solve pattern matching problems. It relies on many concurrent partial evaluations of candidate solutions by a population of agents and communication between those agents to locate the optimal match to a target pattern in a search space. In subsequent research, several variations on the original algorithmic formulation were proposed. It also became evident that its main principles \u2013 partial evaluation and communication between agents \u2013 can be employed to problems outside the pattern matching domain. The primary aim of this dissertation is to develop these expansive views further: sds is proposed as a metaheuristic, a generic heuristic procedure for solving problems through search. Furthermore, it is proposed as a challenge to the dominant metaphor in computer science: sequential computation. The thesis proceeds in a structured way by first considering all questions that can be asked about a heuristic procedure like sds: questions of a foundational nature, questions pertaining to mathematical analysis, questions about application domains and questions about physical implementation. It is to the foundational issues that most attention is devoted. Analogies with selective processes in natural and social systems are investigated, as well as analogies with other metaheuristic techniques from artificial intelligence. An attempt is made to categorise potential variants, and to establish what kind of problems sds would be the optimal problem-solving method for. The work aims to provide an expanded but structured understanding of sds, to give guidelines for future work, and to establish how progress in other scientific disciplines can be of use in the study of sds, and vice versa. Preface All sciences characterise the essential nature of the systems they study. These characterisations are invariably qualitative in nature, for they set the terms with which more detailed knowledge can be developed. A. Newell and H. Simon (Newell and Simon, 1976) Cybernetics is the science of defensible metaphors.",
            "group": 3278,
            "name": "10.1.1.201.5399",
            "keyword": "",
            "title": "Foundations of Stochastic Diffusion Search"
        },
        {
            "abstract": "Abstract\u2014The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we will focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment. Index Terms\u2014Emulation, formal verification, high-level synthesis, logic synthesis, physical synthesis, placement, RTL",
            "group": 3279,
            "name": "10.1.1.202.163",
            "keyword": "synthesis",
            "title": "An industrial view of electronic design automation"
        },
        {
            "abstract": "Quantitative science requires the assessment of uncertainty, and this means that measurements and inferences should be described as probability distributions. This is done by building data into a probabilistic likelihood function which produces a posterior \u201canswer \u201d by modulating a prior \u201cquestion\u201d. Probability calculus is the only way of doing this consistently, so that data can be included gradually or all at once while the answer remains the same. But probability calculus is only a language: it does not restrict the questions one can ask by setting one\u2019s prior. We discuss how to set sensible priors, in particular for a large problem like image reconstruction. We also introduce practical modern algorithms (Gibbs sampling, Metropolis algorithm, genetic algorithms, and simulated annealing) for computing probabilistic inference.",
            "group": 3280,
            "name": "10.1.1.203.3311",
            "keyword": "algorithmBayesGibbsimage reconstructioninferenceMetropolisprobabilitysimulated annealinguncertainty. Copyright c \u25cb \u201cJournal of Microscopy\u201d",
            "title": "Probabilistic data analysis: an introductory guide"
        },
        {
            "abstract": "A common event in the consumerpackaged goods industry is the negotiation between a manufacturer and a retailer of the sales promotion calendar. Determining the promotion calendar involves a large number of decisions regarding levels of temporary price reductions, feature ads, and in-store displays, each executed at the level of individual retail accounts and brand SKUs over several months or a year. Though manufacturers spend much of their marketing budget on trade promotions, they lack decision support systems to address the complexity and dynamics of promotion planning. Previous research has produced insights into how to evaluate the effectiveness of promotional events, but has not addressed the planning problem in a dynamic environment. This paper develops a disaggregate-level econometric",
            "group": 3281,
            "name": "10.1.1.203.4650",
            "keyword": "",
            "title": "A Decision Support System for Planning Manufacturers \u2019 Sales Promotion Calendars"
        },
        {
            "abstract": "This paper deals with portfolio optimisation when asset\u2019s return distribution is non-elliptical. We first come back to the Mean-variance and Mean-CVaR approaches. Then we propose as an alternative to apply the new performance evaluation measure called Omega (Keating and Shadwick (2002)) to the portfolio allocation choice problem. Meta-heuristic techniques which belong to the class of Global Optimisation techniques will enable us to solve the investor\u2018s program. In order to model the uncertainty of future returns we generate Monte-Carlo scenarios using conditional copula and Generalised Hansen skewed-t distribution. It allows us to take into account three fundamental characteristics of financial markets: (i) Departure from Gaussian World, (ii) Time-varying dependence and volatility,(iii) Extreme joint movements in financial markets. We apply these methodologies to a portfolio composed of three total stock market index (US, UK, GERMANY). Ce papier traite du choix de portefeuille lorsque les rendements d\u2019actifs suivent des distributions multivari\u00e9es non elliptiques. Nous revenons d\u2019abord sur les approches Esp\u00e9rance-Variance et Esp\u00e9rance-CVaR (Conditional Value-at-Risk). Nous proposons ensuite d\u2019appliquer la nouvelle measure de performance appel\u00e9e fonction Omega (Keating and Shadwick) \u00e0 l\u2019optimisation de",
            "group": 3282,
            "name": "10.1.1.203.4846",
            "keyword": "Portfolio OptimizationDownside Risk MeasureOmega functionMeta-Heuristic OptimizationThreshold Accepting AlgorithmConditional Copula",
            "title": "NON-ELLIPTICAL ASSET RETURN DISTRIBUTIONS AND PORTFOLIO SELECTION WITH OMEGA FUNCTION."
        },
        {
            "abstract": "Meta-heuristics support managers in decision-making with robust tools that provide high-quality solutions to important applications in business, engineering, economics and science in reasonable time horizons. In this paper we give some insight into the state of the art of meta-heuristics. This primarily focuses on the significant progress which general frames within the meta-heuristics field have implied for solving combinatorial optimization problems, mainly those for planning and scheduling. ",
            "group": 3283,
            "name": "10.1.1.203.4989",
            "keyword": "",
            "title": "Meta-heuristics: The state of the art"
        },
        {
            "abstract": "In order to efficiently design complex microelectromechanical systems (MEMS) having large numbers of multi-domain components, a hierarchically structured design approach that is compatible with standard IC design is needed. A graphical-based schematic, or structural, view is presented as a geometrically intuitive way to represent MEMS as a set of interconnected lumpedparameter elements. An initial library focuses on suspended-MEMS technology from which inertial sensors and other mechanical mechanisms can be designed. The schematic representation has a simulation interface enabling the designer to simulate the design at the component level. Synthesis of MEMS cells for common topologies provides the system designer with rapid, optimized component layout and associated macro-models. A synthesis module is developed for the popular folded-flexure micromechanical resonator topology. The algorithm minimizes a combination of total layout area and voltage applied to the electromechanical actuators. Synthesis results clearly show the design limits of behavioral parameters such as resonant frequency for a fixed process technology.",
            "group": 3284,
            "name": "10.1.1.203.5065",
            "keyword": "",
            "title": "Structured Design of Microelectromechanical Systems"
        },
        {
            "abstract": "In this paper we propose to apply a recent performance measure function called Omega (Shadwick and Keating (2002)) to the portfolio allocation choice problem. Threshold Accepting which is one of a number of powerful global Optimisation Heuristic developed during the eighties and neighties will enable to solve the investor\u2019s program. In order to model uncertainty of future returns we generate Monte Carlo scenarios using conditional copula theory. It allows us to take into account three fundamental characteristics of financial markets: (i) Departure from Gaussian World, (ii) Time-varying dependence and volatility,(iii) Extreme joint movements in financial markets. We apply these methodologies to a portfolio composed of three total stock market index (US, UK, GERMANY).",
            "group": 3285,
            "name": "10.1.1.203.5929",
            "keyword": "Sharpe ratioOmega ratioThreshold Accepting Algorithmportfolio optimizationConditional Copula. JELC51C61C63G11",
            "title": "Neto Optimal Asset Allocation with Omega Function, preprint"
        },
        {
            "abstract": "In this paper, we will show how non-photorealistic rendering (NPR) can take a new role in content-based image retrieval (CBIR). The proposed CBIR method applies a novel image similarity measure: Unlike traditional features like color, texture, or shape, our measure is based on a painted representation of the original image. This is produced by a stochastic paintbrush algorithm which simulates a painting process. We use the stroke parameters (color, size, orientation, and location) as features and similarity is measured by matching strokes of a pair of images. The advantage of our approach is that it provides information not only about the color content but also about the structural properties of an image without the segmentation of the image. Experimental results show that the CBIR method using paintbrush features has higher retrieval rate than traditional methods using color or texture features only. 1.",
            "group": 3286,
            "name": "10.1.1.204.2108",
            "keyword": "",
            "title": "Non-Photorealistic Rendering and Content-Based Image Retrieval"
        },
        {
            "abstract": "We propose a binary Markov Random Field (MRF) model that assigns high probability to regions in the image domain consisting of an unknown number of circles of a given radius. We construct the model by discretizing the \u2018gas of circles\u2019 phase field model in a principled way, thereby creating an \u2018equivalent \u2019 MRF. The behaviour of the resulting MRF model is analyzed, and the performance of the new model is demonstrated on various synthetic images as well as on the problem of tree crown detection in aerial images. Index Terms \u2014 segmentation, Markov random field, shape prior",
            "group": 3287,
            "name": "10.1.1.204.2649",
            "keyword": "",
            "title": "A Markov random field model for extracting near-circular shapes"
        },
        {
            "abstract": "Abstract\u2014A novel search principle for optimal feature subset selection using the Branch & Bound method is introduced. Thanks to a simple mechanism for predicting criterion values, a considerable amount of time can be saved by avoiding many slow criterion evaluations. We propose two implementations of the proposed prediction mechanism that are suitable for use with nonrecursive and recursive criterion forms, respectively. Both algorithms find the optimum usually several times faster than any other known Branch & Bound algorithm. As the algorithm computational efficiency is crucial, due to the exponential nature of the search problem, we also investigate other factors that affect the search performance of all Branch & Bound algorithms. Using a set of synthetic criteria, we show that the speed of the Branch & Bound algorithms strongly depends on the diversity among features, feature stability with respect to different subsets, and criterion function dependence on feature set size. We identify the scenarios where the search is accelerated the most dramatically (finish in linear time), as well as the worst conditions. We verify our conclusions experimentally on three real data sets using traditional probabilistic distance criteria.",
            "group": 3288,
            "name": "10.1.1.204.3833",
            "keyword": "\u00e6",
            "title": "THE"
        },
        {
            "abstract": "",
            "group": 3289,
            "name": "10.1.1.204.4487",
            "keyword": "",
            "title": "Decentralized Coordination . . . Gibbs Sampling"
        },
        {
            "abstract": "The Particle Collision Algorithm (PCA) is a recently introduced metaheuristic conceptually similar to Simulated Annealing, without, though, the necessity of estimating the free parameters as in the latter algorithm. It is loosely inspired by the physics of nuclear particle collision reactions, particularly scattering and absorption. A \u201cparticle \u201d that reaches a promising area of the search space is \u201cabsorbed\u201d, while one that hits a low-fitness region is \u201cscattered\u201d. PCA is also a Metropolis algorithm, as a solution worse than the currently best may be accepted with a certain probability. In this article, we introduce a populational version of the Particle Collision Algorithm, the Populational PCA (PopPCA), which is a hybridization of the PCA and the genetic algorithm (GA). At the end of a generation, the particles reproduce and the fittest individuals survive. We apply the new algorithm to an optimization problem that consists in adjusting several reactor cell parameters, such as dimensions, enrichment and materials, in order to minimize the average peakfactor in a three-enrichment-zone reactor, considering restrictions on the average thermal flux, criticality and sub-moderation. The populational PCA is compared to other metaheuristics previously applied to the problem and shows to perform better than them, thus demonstrating its potential for other applications and further development.",
            "group": 3290,
            "name": "10.1.1.204.8381",
            "keyword": "Key WordsMetaheuristicsStochastic OptimizationNuclear Reactor Design",
            "title": "A POPULATIONAL PARTICLE COLLISION ALGORITHM APPLIED TO A NUCLEAR REACTOR CORE DESIGN OPTIMIZATION"
        },
        {
            "abstract": " ",
            "group": 3291,
            "name": "10.1.1.204.8688",
            "keyword": "",
            "title": "Uncoordinated peer selection in P2P backup and storage applications"
        },
        {
            "abstract": "Abstract. In this work we tackle the problem of on-line backup with a peer-to-peer approach. In contrast to current peer-to-peer architectures that build upon distributed hash-tables, we investigate whether an uncoordinated approach to data placement would prove effective in providing embedded incentives for users to offer local resources to the system. By modeling peers as selfish entities striving for minimizing their cost in participating to the system, we analyze equilibrium topologies that materialize from the process of peer selection, whereby peers establish bi-lateral links that involve storing data in a symmetric way. System stratification, that is the emergence of clusters gathering peers with similar contribution efforts, is an essential outcome of the peer selection process: peers are lured to improve the \u201cquality \u201d of local resources they provide to access clusters with lower operational costs. Our results are corroborated by a numerical evaluation of the system that builds upon a polynomial-time best-response algorithm to the selfish neighbor selection game. 1",
            "group": 3292,
            "name": "10.1.1.205.399",
            "keyword": "",
            "title": "Toka: Selfish neighbor selection in peer-to-peer backup and storage applications, EuroPar"
        },
        {
            "abstract": "Facing the increasing needs for large-scale, robust, adaptive, and distributed/decentralized computing capabilities [1, 5] from such fields as Web intelligence, scientific and social computing, Internet commerce, and pervasive computing, an unconventional bottom-up paradigm, based on the notions of Autonomy-Oriented Computing (AOC) and self-organization in open complex systems, offers new opportunities for developing promising architectures, methods, and technologies. The goal of this paper is to describe the key concepts in this computing paradigm, and furthermore, discuss some of the fundamental principles and mechanisms for obtaining self-organized computing solutions. 1.",
            "group": 3293,
            "name": "10.1.1.205.7801",
            "keyword": "",
            "title": "Autonomy-Oriented Computing (AOC): The nature and implications of a paradigm for self-organized computing"
        },
        {
            "abstract": "Absfruct--Simulated annealing is a computational heuristic for obtaining approximate solutions to combinatorial optimization problems. It is used to construct good source codes, error-correcting codes, and spherical codes. For certain sets of parameters codes that are better than any other known in the literature are found. A I.",
            "group": 3294,
            "name": "10.1.1.205.8181",
            "keyword": "",
            "title": "Using simulated annealing to design good codes"
        },
        {
            "abstract": "We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system\u2019s parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance. 1.",
            "group": 3295,
            "name": "10.1.1.205.8465",
            "keyword": "",
            "title": "A QUANTITATIVE EVALUATION OF A TWO STAGE RETRIEVAL APPROACH FOR A MELODIC QUERY BY EXAMPLE SYSTEM"
        },
        {
            "abstract": "Heterogeneous computing (HC) environments are well suited tomeet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (de ned as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a di cult problem, because comparisons are often clouded by di erent underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined",
            "group": 3296,
            "name": "10.1.1.205.8944",
            "keyword": "areOpportunistic Load BalancingUser-Directed AssignmentFast GreedyMin-minMax-minGreedyGenetic AlgorithmSimulated AnnealingGenetic Simulated AnnealingTabuand A*. This study provides",
            "title": "A Comparison Study of Static Mapping Heuristics for a Class of Meta-tasks on Heterogeneous Computing Systems"
        },
        {
            "abstract": "The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, \u03bd-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented. 1.",
            "group": 3297,
            "name": "10.1.1.205.9765",
            "keyword": "",
            "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach"
        },
        {
            "abstract": "Abstract. Path-relinking is major enhancement to heuristic search methods for solving combinatorial optimization problems, leading to significant improvements in both solution quality and running times. We review its fundamentals and implementation strategies, as well as advanced hybridizations with more elaborate metaheuristic schemes such as genetic algorithms and scatter search. Numerical examples are discussed and algorithms compared based on their run time distributions. 1. Introduction and",
            "group": 3298,
            "name": "10.1.1.206.415",
            "keyword": "",
            "title": "PATH-RELINKING INTENSIFICATION ALGORITHMS FOR STOCHASTIC LOCAL SEARCH HEURISTICS"
        },
        {
            "abstract": "views and conclusions contained in this document are those of the authors and should not be interpreted as",
            "group": 3299,
            "name": "10.1.1.206.2180",
            "keyword": "modelregressorsvariable selectionmodel buildingfull modelmulticollinearityridge",
            "title": "Response Surface Methodology 1"
        },
        {
            "abstract": "This paper presents a review of advances that have taken place in the mathematical programming approach to process design and synthesis. A review is first presented on the algorithms that are available for solving MINLP problems, and its most recent variant, Generalized Disjunctive Programming models. The formulation of superstructures, models and solution strategies is also discussed for the effective solution of the corresponding optimization problems. The rest of the paper is devoted to reviewing recent mathematical programming models for the synthesis of reactor networks, distillation sequences, heat exchanger networks, mass exchanger networks, utility plants, and total flowsheets. As will be seen from this review, the progress that has been achieved in this area over the last decade is very significant. 1.",
            "group": 3300,
            "name": "10.1.1.206.2312",
            "keyword": "Process synthesismathematical programmingreactionseparationenergy systems",
            "title": "ADVANCES IN MATHEMATICAL PROGRAMMING FOR THE SYNTHESIS OF PROCESS SYSTEMS"
        },
        {
            "abstract": "The RNA structural alignment is one of the most challenging tasks in bioinformatics. However, finding the accurate conserved structure of a set of RNA sequences is still being a difficult task. In this work, the problem is cast as an optimization problem for which a new framework relaying on hybrid genetic algorithm is proposed. The contribution consists in using a new objective function based on the Structure Conservation Index (SCI). In order to enhance the Genetic Algorithms (GA) performances, a Simulated Annealing (SA) procedure has been used. The proposed algorithm is composed on two phases.The first phase consists of applying a genetic algorithm.In the second phase, the simulated annealing procedure is applied in order to improve the final population given by the genetic algorithm. Experiments on a wide range of data sets have shown the effectiveness of the proposed framework and its ability to achieve good quality solutions comparing to those given by others techniques.",
            "group": 3301,
            "name": "10.1.1.206.4167",
            "keyword": "Genetic AlgorithmSimulated AnnealingStructure Conservation Index",
            "title": "A Hybrid Genetic Algorithm for RNA Structural Alignment"
        },
        {
            "abstract": "In this paper we describe a method for contourbased segmentation of anatomical structures in tomographic images. Our method requires three steps. First we manually trace one or more 2D contours of a structure of interest. Such contours are then used as training examples in designing a non-linear edge-detector using a genetic algorithm. Finally, by applying the edge detector to the whole dataset we perform a 3D segmentation and surface reconstruction. Results obtained on magnetic resonance images of the brain are also reported. 1.",
            "group": 3302,
            "name": "10.1.1.206.4632",
            "keyword": "",
            "title": "INTERACTIVE SEGMENTATION OF MULTI-DIMENSIONAL MEDICAL DATA WITH CONTOUR-BASED APPLICATION OF GENETIC ALGORITHMS"
        },
        {
            "abstract": "Soft computing based approaches are increasingly being used to solve different NP complete problems, the development of efficient parallel algorithms for digital circuit partitioning, circuit testing, logic minimization and simulation etc. is currently a field of increasing research activity. This paper describes evolutionary based approach for solving circuitpartitioning problem. That implies dividing a circuit into nonoverlapping sub circuits while minimizing the number of cuts after the division and balancing the load associated to each one. The paper shows the effective partitioning for achieving peak chip performance and reducing the cost and time of the design and manufacturing process.",
            "group": 3303,
            "name": "10.1.1.206.4669",
            "keyword": "VLSI circuitsCircuit partitioningGenetic algorithmssimulated",
            "title": "Computer Applications,"
        },
        {
            "abstract": "This thesis is available for Library use on the understanding that it is copyright material and that no quotation from the thesis may be published without proper acknowledgement. I certify that all material in this thesis which is not my own work has been identified and that no material has previously been submitted and approved for the award of a degree by this or any other University...................................... 3 In this thesis we are investigating the use of the Evolutionary Algorithm (EA) in application to problems in molecular biology. We examine two topics of current interest in the field of bioinformatics; Gene Expression Regulatory Network Reconstruction and Core Promoter recognition, spending most of this thesis on the latter problem. When exploring Gene network reconstruction we investigate the problem in terms of reconstructing large scale networks. We apply two forms of evolutionary algorithm and evaluate their effect on two classes of time series data, static (where the expression levels do not change over a time series, and dynamic (where expression levels are changing as a result of perturbation). We discover some of the properties of large scale GRN models, and reveal some issues regarding variation among GRNs",
            "group": 3304,
            "name": "10.1.1.206.5516",
            "keyword": "",
            "title": "Inference Submitted by Carey Pridgeon to the University of Exeter"
        },
        {
            "abstract": "This paper describes unifying Active Appearance Model and Simulated Annealing for automatic Cephalometric analysis. A complete model of shape and texture was built from a dataset of manually annotated images, and then tested with unseen images. A comparison with Active Shape Models was presented. Twenty randomly selected cephalograms were used for training and seven cephalograms for testing. 25 % accuracy improvement over Active Shape Model. Suggestions to improve the accuracy and speed were presented.",
            "group": 3305,
            "name": "10.1.1.207.190",
            "keyword": "Active Appearance ModelActive Shape ModelCephalometric AnalysisSimulated Annealing",
            "title": "Automatic cephalometric analysis using active appearance model and simulated annealing"
        },
        {
            "abstract": "A simulated annealing-based algorithm (MSIMPSA) suitable for the optimization of mixed integer non-linear programming (MINLP) problems was applied to the synthesis of a non-equilibrium reactive distillation column. A simulation model based on an extension of conventional distillation is proposed for the simulation step of the optimization problem. In the case of ideal vapor}liquid equilibrium, the simulation results are similar to those obtained by Ciric and Gu (1994, AIChE Journal, 40(9), 1479) using the GAMS environment and to those obtained with the AspenPlus modular simulator. The optimization results are also similar to those previously reported and similar to those using an adaptive random search algorithm (MSGA). The optimizations were also performed with non-ideal vapor}liquid equilibrium, considering either distributed feed and reaction trays or single feed and reaction tray. The results show that the optimized objective function values are very similar, and mostly independent of the number of trays and of the reaction distribution. It is shown that the proposed simulation/optimization equation-oriented environments are capable of providing optimized solutions which are close to the global optimum, and reveal its adequacy for the optimization of reactive distillation",
            "group": 3306,
            "name": "10.1.1.207.412",
            "keyword": "Simulated annealingReactive distillationSimulationOptimization",
            "title": "Optimization of reactive distillation processes with simulated annealing"
        },
        {
            "abstract": "This paper presents the application of Differential Evolution (DE) for the optimal design of shell-and-tube heat exchangers. The main objective in any heat exchanger design is the estimation of the minimum heat transfer area required for a given heat duty, as it governs the overall cost of the heat exchanger. Lakhs of configurations are possible with various design variables such as outer diameter, pitch, and length of the tubes; tube passes; baffle spacing; baffle cut etc. Hence the design engineer needs an efficient strategy in searching for the global minimum. In the present study for the first time DE, an improved version of Genetic Algorithms (GAs), has been successfully applied with different strategies for 1,61,280 design configurations using Bell\u2019s method to find the heat transfer area. In the application of DE 9680 combinations of the key parameters are considered. For comparison, GAs are also applied for the same case study with 1080 combinations of its parameters. For this optimal design problem, it is found that DE, an exceptionally simple evolution strategy, is significantly faster compared to GA and yields the global optimum for a wide range of the key parameters.",
            "group": 3307,
            "name": "10.1.1.207.2823",
            "keyword": "",
            "title": "and S.A.Munawar, \u201cOptimal Design of Shell-andTube Heat Exchangers using Different Strategies of Differential Evolution\u201d, PreJournal.org \u2013 The Faculty Lounge, Article No. 003873, posted on website Journal http://www.prejournal.org"
        },
        {
            "abstract": "  Extremal Optimization (EO) is a relatively new single search-point optimization heuristic based on self-organized criticality. Unlike many traditional optimization heuristics, EO focuses on removing poor characteristics of a solution instead of preserving the good ones. This thesis will examine the physical and biological inspirations behind EO, and will explore the application of EO on four unique search problems in planning, diagnosis, path-finding, and scheduling. Some of the pros and cons of EO will be discussed, and it will be shown that, in many cases, EO can perform as well as or better than many standard search methods. Finally, this thesis will conclude with a survey of the state of the art of EO, mentioning several variations of the algorithm and the benefits of using such modifications.  ",
            "group": 3308,
            "name": "10.1.1.207.3292",
            "keyword": "Mobile subscriber equipmentMultiple fault diagnosisExploring Applications of Extremal Optimization by",
            "title": "Exploring Applications of Extremal Optimization  "
        },
        {
            "abstract": "Today\u2019s electric utilities are confronted with a myriad of challenges that include aging infrastructure, enhanced expectation of reliability, reduced cost, and coping effectively with uncertainties and changing regulation requirements. Utilities rely on Asset Management programs to manage inspections and maintenance activities in order to control equipment conditions. However, development of strategies to make sound decisions in order to effectively improve equipment and system reliability while meeting constraints such as a maintenance budget is a challenge. The primary objective of this dissertation is to develop models and algorithms to study the impact of maintenance toward equipment/system reliability and economic cost, and to optimize maintenance schedules in a substation to improve the overall substation reliability while decreasing the cost. Firstly, stochastic-based equipment-level reliability and economic models are developed depending on maintenance types. Semi-Markov processes are deployed to represent deteriorations, failures, inspection, maintenance and replacement states for reliability modeling; semi-Markov decision processes are implemented for economic costiii",
            "group": 3309,
            "name": "10.1.1.207.5621",
            "keyword": "",
            "title": "MAINTENANCE OPTIMIZATION FOR SUBSTATIONS WITH AGING EQUIPMENT"
        },
        {
            "abstract": "We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semi-autonomous and acts proactively. In this work we show how to design a system to meet a user\u2019s preferences, show how robot pro-activity can be learned and provide an integrated system using verbal instructions. All these behaviors are implemented in a real platform that achieves all these behaviors and is evaluated in terms of user acceptability and efficiency of interaction.",
            "group": 3310,
            "name": "10.1.1.207.5693",
            "keyword": "Learning by Demonstration",
            "title": "Robot Self-Initiative and Personalization by Learning through Repeated Interactions"
        },
        {
            "abstract": "accept\u00e9e sur proposition du jury:",
            "group": 3311,
            "name": "10.1.1.207.5971",
            "keyword": "",
            "title": "POUR L'OBTENTION DU GRADE DE DOCTEUR \u00c8S SCIENCES PAR"
        },
        {
            "abstract": "Advances in Intelligent Transportation Systems (ITS) have resulted in the deployment of surveillance systems that automatically collect and store extensive network-wide traffic data. Dynamic Traffic Assignment (DTA) models have also been developed for a variety of dynamic traffic management applications. Such models are designed to estimate and predict the evolution of congestion through detailed models and algorithms that capture travel demand, network supply and their complex interactions. The availability of rich time-varying traffic data spanning multiple days thus provides the opportunity to calibrate a DTA model\u2019s many inputs and parameters, so that its outputs reflect field conditions.",
            "group": 3312,
            "name": "10.1.1.207.6720",
            "keyword": "",
            "title": "Off-line Calibration of Dynamic Traffic Assignment Models"
        },
        {
            "abstract": "Abstract \u2014 A control strategy inspired by the hunting tactics of ladybugs is presented to simultaneously achieve sensor coverage and exploration of an area with a group of networked robots. The controller is distributed in that it requires only information local to each robot, and adaptive in that it modifies its behavior based on information in the environment. The ladybug controller is developed as a modification to a basic coverage control law, first for the non-adaptive case, then for the adaptive case. Stability is proven for both cases with a Lyapunov-type proof. Results of numerical simulations are presented. I.",
            "group": 3313,
            "name": "10.1.1.208.900",
            "keyword": "",
            "title": "A ladybug exploration strategy for distributed adaptive coverage control"
        },
        {
            "abstract": "Hardware accelerators outperform equivalent software implementations through the exploitation of both spatial and temporal parallelism. Determining the correct levels of each in order to maximize system performance is a challenging task. We present techniques for exploring the design space of a highlevel pipelined architecture to balance pipeline stages and optimize performance. Consideration is given to both memory-intensive and compute-intensive stages. As an example, a custom FPGA-based architecture for a simulated annealing algorithm which runs as a timecritical in-space application is presented. Performance of the novel architecture is compared with that of traditional space-based microprocessors. 1.",
            "group": 3314,
            "name": "10.1.1.208.1475",
            "keyword": "",
            "title": "Deriving an Efficient, Application-Specific, FPGA-Based Pipelined Processor"
        },
        {
            "abstract": "Abstract. The universe of biochemical reactions in metabolic pathways can be modeled as a complex network structure augmented with domain specific annotations. Based on the functional properties of the involved reactions, metabolic networks are often clustered into so-called pathways inferred from expert knowledge. To support the domain expert in the exploration and analysis process, we follow the well-known Table Lens metaphor with the possibility to select multiple foci. In this paper, we introduce a novel approach to generate an interactive layout of such a metabolic network taking its hierarchical structure into account and present methods for navigation and exploration that preserve the mental map. The layout places the network nodes on a fixed rectilinear grid and routes the edges orthogonally between the node positions. Our approach supports bundled edge routes heuristically minimizing a given cost function based on the number of bends, the number of edge crossings and the density of edges within a bundle. 1",
            "group": 3315,
            "name": "10.1.1.208.2510",
            "keyword": "",
            "title": "G.: A novel gridbased visualization approach for metabolic networks with advanced focus & context view"
        },
        {
            "abstract": "",
            "group": 3316,
            "name": "10.1.1.208.2706",
            "keyword": "",
            "title": "2.1. The Floorplanning Task"
        },
        {
            "abstract": "Abstract. The universe of biochemical reactions in metabolic pathways can be modeled as a complex network structure augmented with domain specific annotations. Based on the functional properties of the involved reactions, metabolic networks are often clustered into so-called pathways inferred from expert knowledge. To support the domain expert in the exploration and analysis process, we follow the well-known Table Lens metaphor with the possibility to select multiple foci. In this paper, we introduce a novel approach to generate an interactive layout of such a metabolic network taking its hierarchical structure into account and present methods for navigation and exploration that preserve the mental map. The layout places the network nodes on a fixed rectilinear grid and routes the edges orthogonally between the node positions. Our approach supports bundled edge routes heuristically minimizing a given cost function based on the number of bends, the number of edge crossings and the density of edges within a bundle. 1",
            "group": 3317,
            "name": "10.1.1.208.4397",
            "keyword": "",
            "title": "A novel grid-based visualization approach for metabolic networks with advanced focus&context view"
        },
        {
            "abstract": "of the",
            "group": 3318,
            "name": "10.1.1.208.9747",
            "keyword": "",
            "title": "OFFICE OF GRADUATE STUDIES"
        },
        {
            "abstract": "Abstract-- Location area (LA) planning plays an important role in cellular networks because of the trade-off caused by paging and registration signaling. The upper bound on the size of an LA is the service area of a mobile switching center (MSC). In that extreme case, the cost of paging is at its maximum, but no registration is needed. On the other hand, if each cell is an LA, the paging cost is minimal, but the registration cost is the largest. In general, the most important component of these costs is the load on the signaling resources. Between the extremes lie one or more partitions of the MSC service area that minimize the total cost of paging and registration. In this paper, we try to find an optimal method for determining the location areas. For that purpose, we use the available network information to formulate a realistic optimization problem. We propose an algorithm based on simulated annealing (SA) for the solution of the resulting problem. Then, we investigate the quality of the SA technique by comparing its results to greedy search and random generation methods.",
            "group": 3319,
            "name": "10.1.1.209.918",
            "keyword": "Index terms\u2014Location AreaCellular NetworksSimulated",
            "title": "Location area planning in cellular networks using simulated annealing"
        },
        {
            "abstract": "The Age-Layered Population Structure (ALPS) paradigm is a novel metaheuristic for overcoming premature convergence by running multiple instances of a search algorithm simultaneously. When the ALPS paradigm was first introduceditwascombinedwithagenerationalEvolutionaryAlgorithm(EA)and the ALPS-EA was shown to work significantly better than a basic EA. Here we describe a version of ALPS with a steady-state EA, which is well suited for use in situations in which the synchronization constraints of a generational model are not desired. To demonstrate the effectiveness of our version of ALPS we compare it against a basic steady-state EA (BEA) in two test problems and find that itoutperforms theBEAinbothcases.",
            "group": 3320,
            "name": "10.1.1.210.157",
            "keyword": "",
            "title": "Chapter1 ASTEADY-STATEVERSIONOFTHEAGE-LAYERED POPULATIONSTRUCTUREEA"
        },
        {
            "abstract": "Often, the explanatory power of a learned model must be traded off against model performance. In the case of predicting runaway software projects, we show that the twin goals of high performance and good explanatory power are achievable after applying a variety of data mining techniques (discrimination, feature subset selection, rule covering algorithms). This result is a new high water mark in predicting runaway projects. Measured in terms of precision, this new model is as good as can be expected for our data. Other methods might out-perform our result (e.g. by generating a smaller, more explainable model) but no other method could out-perform the precision of our learned model.",
            "group": 3321,
            "name": "10.1.1.210.1045",
            "keyword": "ExplanationData MiningRunaway",
            "title": "Explanation vs Performance in Data Mining: A Case Study with Predicting Runaway Projects"
        },
        {
            "abstract": "capacity benefits performance on a correlation detection task. They assumed that people with low short-term memory capacity (low spans) perceived the correlations as more extreme because they relied on smaller samples, which are known to exaggerate correlations. The authors consider, as an alternative hypothesis, that low spans do not perceive exaggerated correlations but make simpler predictions. Modeling both hypotheses in ACT-R demonstrates that simpler predictions impair performance if the environment changes, whereas a more exaggerated perception of correlation is advantageous to detect a change. Congruent with differences in the way participants make predictions, 2 experiments revealed a low capacity advantage before the environment changes but a high capacity advantage afterward, although this pattern of results surprisingly only existed for men. Keywords: short-term memory capacity, correlation detection, probability learning, ACT-R, sex differences Nearly 50 years ago, Miller (1956) concluded that people can consider about seven items or categories simultaneously (plus or minus two). The premise of limited cognitive capacities is often directly linked to its supposed negative consequences, such as",
            "group": 3322,
            "name": "10.1.1.210.1416",
            "keyword": "",
            "title": "Simple predictions fueled by capacity limitations: When are they successful"
        },
        {
            "abstract": "Post-Moore\u2019s Law computing will require an assimilation between computational processes and their physical realizations, both to achieve greater speeds and densities and to allow computational processes to assemble and control matter at the nanoscale. Therefore, we need to investigate \u201cembodied computing, \u201d which addresses the essential interrelationships of information processing and physical processes in the system and its environment in ways that are parallel to those in the theory of embodied cognition. We briefly discuss matters of function and structure, regulation and causation, and the definition of computation. We address both the challenges and opportunities of embodied computation. Analysis is more difficult because physical effects must be included, but information processing may be simplified by dispensing with explicit representations and allowing massively parallel physical processes to process information. Nevertheless, in order to fully exploit embodied computation, we need robust and powerful theoretical tools, but we argue that the theory of Church-Turing computation is not suitable for the task. 1. Post-Moore\u2019s Law Computation Although estimates differ, it is clear that the end of Moore\u2019s Law is in sight; there are physical limits to the density of binary logic devices and to their speed of operation. This will require us to approach computation",
            "group": 3323,
            "name": "10.1.1.210.1600",
            "keyword": "",
            "title": "Bodies \u2014 both informed and transformed: Embodied computation and information processing"
        },
        {
            "abstract": "A positioning algorithm based on the relative order of the received signal strengths is discussed. This algorithm in conjunction with the ray-tracing propagation model can have promising performance for indoor environments without any needs for extensive set of a priori training. Enhancements to the positioning algorithm will be proposed and investigated. Two sets of experimental results with 802.11-based infrastructure and MICA2 motes are presented to demonstrate the system capability and performance in practice.",
            "group": 3324,
            "name": "10.1.1.210.7515",
            "keyword": "Indoor PositioningRay-TracingRadio",
            "title": "Robust Indoor Positioning Based on Received Signal Strength"
        },
        {
            "abstract": "mathematical model of the immune system\u2019s role in obesity-related",
            "group": 3325,
            "name": "10.1.1.210.9370",
            "keyword": "",
            "title": "chronic"
        },
        {
            "abstract": "evidence for active sparsification",
            "group": 3326,
            "name": "10.1.1.210.9465",
            "keyword": "",
            "title": "in"
        },
        {
            "abstract": "Abstract: We explore two strategies that resample from previously sampled observations in a Markov Chain Monte Carlo algorithm. In one strategy the MCMC sampler reuses its own past. We show that in general this strategy generates a sampler with slower mixing. We propose another strategy based on multiple chains where some of the chains reuse past samples generated by other chains. This latter algorithm is related to the Equi-Energy sampler of [11]. We show by examples that this strategy yields a viable Monte Carlo methods with mixing properties similar to those of the Equi-Energy sampler.",
            "group": 3327,
            "name": "10.1.1.211.132",
            "keyword": "Monte Carlo methodsAdaptive MCMCImportance resamplingStochastic",
            "title": "Resampling from the past to improve MCMC algorithms"
        },
        {
            "abstract": "mathematicians often assumed differentiability of the optimand as well as constraint functions. Moreover, they often dealt with the equality constraints. Richard Valentine (1937) and William Karush (1939), however, were perhaps the first mathematicians to study optimization of nonlinear functions under inequality constraints. Leonid Kantorovich and George Dantzig are well known for developing and popularizing linear programming, which ushered a new era of \u2018operations research\u2019, a branch of mathematical science that specializes in optimization. The development of linear programming soon prompted the study of the optimization problem of nonlinear functions (often under linear or nonlinear constraints). The joint work of Harold Kuhn and Albert Tucker (1951)  \u2013 that was backed up by the work of Karush \u2013 is a landmark in the history of optimization of nonlinear functions. Initially, optimization of nonlinear functions was methodologically based on the Leibniz-Newton principles and therefore could not easily escape local optima. Hence, its development to deal with nonconvex (multimodal) functions stagnated until the mid 1950\u2019s. Stanislaw Ulam, John von Neumann and Nicolas Metropolis had in the late",
            "group": 3328,
            "name": "10.1.1.211.1677",
            "keyword": "",
            "title": "A Brief History of Optimization Research: The history of optimization of real-valued"
        },
        {
            "abstract": "An execution environment consisting of virtual machines (VMs) interconnected with a virtual overlay network can use the naturally occurring traffic of an existing, unmodified application running in the VMs to measure the underlying physical network. Based on these characterizations, and characterizations of the application's own communication topology, the execution environment can optimize the execution of the application using application-independent means such as VM migration and overlay topology changes. In this paper, we demonstrate the feasibility of such free automatic network measurement by fusing the Wren passive monitoring and analysis system with Virtuoso's virtual networking system. We explain how Wren has been extended to support on-line analysis, and we explain how Virtuoso's adaptation algorithms have been enhanced to use Wren's physical network level information to choose VM-to-host mappings, overlay topology, and forwarding rules.",
            "group": 3329,
            "name": "10.1.1.211.1820",
            "keyword": "",
            "title": "Free network measurement for adaptive virtualized distributed computing"
        },
        {
            "abstract": "Time series analysis for nonlinear dynamical systems with applications to modeling of infectious diseases by",
            "group": 3330,
            "name": "10.1.1.211.1857",
            "keyword": "ii ACKNOWLEDGEMENTS......................... iii LIST OF FIGURES.............................. LIST ",
            "title": "DEDICATION................................."
        },
        {
            "abstract": "Abstract--A fully connected network of spiking neurons modeling motor cortical directional operations is presented and analyzed. The model allows for the basic biological requirements stemming from the results of experimental studies. The dynamical evolution of the network's output is interpreted as the sequential generation of neuronal population vectors representing the combined directional tendency of the ensemble. Adding these population vectors tip-to-tail yields the neural-vector trajectory that describes the upcoming movement trajectory. The key point of the model is that the intra-network interactions provide sustained dynamics, whereas external inputs are only required to initiate the population. The network is trained to generate neural-vector trajectories corresponding to basic types of two-dimensional movements (the network with specified connections can store one trajectory). A simple modification of the simulated annealing algorithm enables training of the network in the presence of noise. Training in the presence of noise yields robustness of the learned dynamical behaviors. Another key point of the model is that the directional preference of a single neuron is determined by the synaptic connections. Accordingly, individual preferred directions as well as tuning curves are not assigned, but emerge as the result of interactions inside the population. For trained networks, the spiking behavior of single neurons and correlations between different neurons as well as the global activity of the population are discussed in the light of experimental findings. Copyright \u00a9 1996 Elsevier Science Ltd Keywords---Simulated annealing, Gaussian noise, Synaptic weight. 1.",
            "group": 3331,
            "name": "10.1.1.211.3445",
            "keyword": "",
            "title": "CONTRIBUTED ARTICLE Modeling of Directional Operations in the Motor Cortex: a Noisy Network of Spiking Neurons is Trained to Generate a Neural-Vector Trajectory"
        },
        {
            "abstract": "In this paper we investigate the properties of the sampled version of the fictitious play algorithm, familiar from game theory, for games with identical payoffs, and propose a heuristic based on fictitious play as a solution procedure for discrete optimization problems of the form max{u(y) : y = (y 1,..., y n)  \u2208 Y 1 \u00d7  \u00b7  \u00b7  \u00b7  \u00d7 Y n}, i.e., in which the feasible region is a Cartesian product of finite sets Y i, i \u2208 N = {1,..., n}. The contributions of this paper are two-fold. In the first part of the paper we broaden the existing results on convergence properties of the fictitious play algorithm on games with identical payoffs to include an approximate fictitious play algorithm which allows for errors in players \u2019 best replies. Moreover, we introduce sampling-based approximate fictitious play which possesses the above convergence properties, and at the same time provides a computationally efficient method for implementing fictitious play. In the second part of the paper we motivate the use of algorithms based on sampled fictitious play to solve optimization problems in the above form with particular focus on the problems in which the objective function u(\u00b7) comes from a \u201cblack box, \u201d such as a simulation model, where significant computational effort is required for each function evaluation. 1 1",
            "group": 3332,
            "name": "10.1.1.211.3842",
            "keyword": "",
            "title": "A fictitious play approach to large-scale optimization"
        },
        {
            "abstract": "Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This paper introduces di racting trees, novel data structures for shared counting and load balancing in a distributed parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting networks have depth O(log 2 w). Di racting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queuelocking in the design of many concurrent data structures.",
            "group": 3333,
            "name": "10.1.1.211.6109",
            "keyword": "",
            "title": "Diffracting Trees"
        },
        {
            "abstract": " ",
            "group": 3334,
            "name": "10.1.1.211.6553",
            "keyword": "",
            "title": "AN INVESTIGATION OF PRODUCTION SCHEDULING PROBLEMS MOTIVATED BY SEMICONDUCTOR MANUFACTURING "
        },
        {
            "abstract": "Back-propagation learning (Rumelhart, Hinton and Williams, 1986) is a useful research tool but it has a number of undesiderable features such as having the experimenter decide from outside what should be learned. We describe a number of simulations of neural networks that internally generate their own teaching input. The networks generate the teaching input by trasforming the network input through connection weights that are evolved using a form of genetic algorithm. What results is an innate (evolved) capacity not to behave efficiently in an environment but to learn to behave efficiently. The analysis of what these networks evolve to learn shows some interesting results.",
            "group": 3335,
            "name": "10.1.1.211.7397",
            "keyword": "",
            "title": "Auto-\u2010teaching: Networks that Develop their own Teaching Input"
        },
        {
            "abstract": "The use of supervised neural networks for the estimation of seismic source parameters from SAR interferometric data is presented in this paper. The RNGCHN software allowed the generation of the input-output pairs necessary for the learning phase of the net. After being trained, the net has been tested on real measured data. The obtained results encourage future developments of such an approach. 1",
            "group": 3336,
            "name": "10.1.1.211.9278",
            "keyword": "",
            "title": "NEURAL NETWORKS TO RETRIEVE SEISMIC SOURCE PARAMETERS BY SAR INTERFEROMETRY"
        },
        {
            "abstract": "Abstract. This paper outlines a description of a solver that was used for all three tracks of the International Timetabling Competition 2007 1. 1",
            "group": 3337,
            "name": "10.1.1.212.217",
            "keyword": "",
            "title": "ITC2007: Solver Description"
        },
        {
            "abstract": "Abstract\u2014In this paper, a stochastic connectionist approach is proposed for solving function optimization problems with real-valued parameters. With the assumption of increased processing capability of a node in the connectionist network, we show how a broader class of problems can be solved. As the proposed approach is a stochastic search technique, it avoids getting stuck in local optima. Robustness of the approach is demonstrated on several multi-modal functions with different numbers of variables. Optimization of a well-known partitional clustering criterion, the squared-error criterion (SEC), is formulated as a function optimization problem and is solved using the proposed approach. This approach is used to cluster selected data sets and the results obtained are compared with that of the K-means algorithm and a simulated annealing (SA) approach. The amenability of the connectionist approach to parallelization enables effective use of parallel hardware. Index Terms\u2014Clustering, connectionist approaches, function optimization, global optimization. I.",
            "group": 3338,
            "name": "10.1.1.212.439",
            "keyword": "",
            "title": "A stochastic connectionist approach for global optimization with application to pattern clustering"
        },
        {
            "abstract": "Available online at www.sciencedirect.com",
            "group": 3339,
            "name": "10.1.1.212.710",
            "keyword": "Shape modelingCorrespondenceShape variationShape meanPrincipal geodesic analysis",
            "title": ""
        },
        {
            "abstract": "Genetical genomics is a useful approach for studying the effect of genetic perturbations on biological systems at the molecular level. However, molecular networks depend on the environmental conditions and, thus, a comprehensive understanding of biological systems requires studying them across multiple environments. We propose a generalization of genetical genomics, which combines genetic and sensibly chosen environmental perturbations, to study the plasticity of molecular networks. This strategy forms a critical step towards understanding why individuals respond differently to drugs, toxins, pathogens, nutrients and other environmental influences. Here we outline a strategy for selecting and allocating individuals to particular treatments, and we discuss the promises and pitfalls of the generalized genetical genomics approach. (111 words now) 2 Multifactorial experimentation Many genetic and environmental factors can influence the functioning of a biological system. Understanding the interplay between these factors is",
            "group": 3340,
            "name": "10.1.1.212.1130",
            "keyword": "",
            "title": "RC: Generalizing genetical genomics: getting added value from environmental perturbation. Trends Genet 2008, 24(10):518-524. Publish with BioMed Central and every scientist can read your work free of charge \"BioMed Central will be the most significant dev"
        },
        {
            "abstract": "The issue of how children learn the meaning of words is fundamental to developmental psychology. The recent attempts to develop or evolve efficient communication protocols among interacting robots or virtual agents have brought that issue to a central place in more applied research fields, such as computational linguistics and neural networks, as well. An attractive approach to learning an object-word mapping is the so-called cross-situational learning. This learning scenario is based on the intuitive notion that a learner can determine the meaning of a word by finding something in common across all observed uses of that word. Here we show how the deterministic Neural Modeling Fields (NMF) categorization mechanism can be used by the learner as an efficient algorithm to infer the correct object-word mapping. To achieve that we first reduce the original on-line learning problem to a batch learning problem where the inputs to the NMF mechanism are all possible objectword associations that could be inferred from the cross-situational learning scenario. Since many of those associations are incorrect, they are considered as clutter or noise and discarded automatically by a clutter detector model",
            "group": 3341,
            "name": "10.1.1.212.1936",
            "keyword": "",
            "title": "Cross-situational learning of object-word mapping using Neural Modeling Fields"
        },
        {
            "abstract": "In this chapter, we will go through the fundamentals of algorithms that are essential for the readers to appreciate the beauty of various EDA technologies covered in the rest of the book. For example, many of the EDA problems can be either represented in graph data structures or transformed into graph problems. We will go through the most representative ones in which the efficient algorithms have been well studied. The readers should be able to use these graph algorithms in solving many of their research problems. Nevertheless, there are still a lot of the EDA problems that are naturally difficult to solve. That is to say, it is computationally infeasible to seek for the optimal solutions for these kinds of problems. Therefore, heuristic algorithms that yield suboptimal, yet reasonably good, results are usually adopted as practical approaches. We will also cover several selected heuristic algorithms in this chapter. At the end, we will talk about the mathematical programming algorithms, which provide the theoretical analysis for the problem optimality. We will especially focus on the mathematical programming problems",
            "group": 3342,
            "name": "10.1.1.212.1939",
            "keyword": "",
            "title": "CHAPTER Fundamentals of"
        },
        {
            "abstract": "Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm\u2019s empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods. To Appear in:",
            "group": 3343,
            "name": "10.1.1.212.2596",
            "keyword": "",
            "title": "IEEE Transactions on Neural NetworksAn Evolutionary Algorithm that Constructs Recurrent Neural Networks"
        },
        {
            "abstract": "As an aside to the more developed aspects of landscape theory, in Section 3 we survey the landscape properties that have been used to prove convergence of simulated annealing. Additionally, in Section 4 some results on the behaviour of local optimization on a few different random landscape classes are reviewed.",
            "group": 3344,
            "name": "10.1.1.212.3095",
            "keyword": "",
            "title": "Landscape theory, spring 2002 Dynamics on Landscapes"
        },
        {
            "abstract": "SUMMARY: Architecture, Engineering, and Construction (AEC) professionals typically generate and analyze very few design alternatives during the conceptual stage of a project. One primary cause is limitations in the processes and software tools used by the AEC industry. The aerospace industry has overcome similar limitations by using Process Integration and Design Optimization (PIDO) software to support Multidisciplinary Design Optimization (MDO), resulting in a significant reduction to design cycle time as well as improved product performance. This paper describes a test application of PIDO to an AEC case study: the MDO of a classroom building for structural and energy performance. We demonstrate how PIDO can enable orders of magnitude improvement in the number of design cycles typically achieved in practice, and assess PIDO\u2019s potential to improve AEC MDO processes and products.",
            "group": 3345,
            "name": "10.1.1.212.3566",
            "keyword": "multidisciplinary design optimizationconceptual building designenergy simulationstructural analysisintegrationautomation",
            "title": "MULTIDISCIPLINARY PROCESS INTEGRATION AND DESIGN OPTIMIZATION OF A CLASSROOM BUILDING"
        },
        {
            "abstract": "Advances in autonomy, in the fields of control, estimation, and diagnosis, have improved immensely, as seen by spacecraft that navigate toward pinpoint landings, or speech recognition enabled in hand-held devices. Arguably the most important step to controlling and improving a system, is to understand that system. For this reason, accurate models are essential for continued advancements in the field of autonomy. Hybrid stochastic models, such as JMLS and LPHA, allow for representational accuracy of a general scope of problems. The goal of this thesis is to develop a robust method for learning accurate hybrid models automatically from data. A robust method should learn a set of model parameters, but should also avoid convergence to locally optimal solutions that reduce accuracy, and should be less sensitive to sparse or poor quality observation data. These three goals are the focus of this thesis. We present the HML-LPHA algorithm that uses approximate EM for learning maximum likelihood model parameters of LPHA, given a sequence of control inputs {u} T 0, and outputs, {y}T+1 1. We implement the algorithm in a scenario that simulates the mechanical wheel failure of the MER",
            "group": 3346,
            "name": "10.1.1.212.5219",
            "keyword": "",
            "title": "Certified by..............................................................................."
        },
        {
            "abstract": "adaptive dynamics;",
            "group": 3347,
            "name": "10.1.1.212.5494",
            "keyword": "",
            "title": "REVIEW Natural selection. II. Developmental variability and evolutionary rate*"
        },
        {
            "abstract": "Abstract\u2014In this paper, an automatic assignment tool, called BSS-AutoAssign, for artifact-related decorrelated components within a second-order blind source separation (BSS) is presented. The latter is based on the recently proposed algorithm dAMUSE, which provides an elegant solution to both the BSS and the denoising problem simultaneously. BSS-AutoAssign uses a local principal component analysis (PCA)to approximate the artifact signal and defines a suitable cost function which is optimized using simulated annealing. The algorithms dAMUSE plus BSS-AutoAssign are illustrated by applying them to the separation of water artifacts from two-dimensional nuclear overhauser enhancement (2-D NOESY) spectroscopy signals of proteins dissolved in water. Index Terms\u2014Blind source separation, generalized eigenvalue decomposition, matrix pencil, simulated annealing, 2-D NOESY NMR. I.",
            "group": 3348,
            "name": "10.1.1.212.7046",
            "keyword": "",
            "title": "On the Use of Simulated Annealing to Automatically Assign Decorrelated Components in Second-Order Blind Source Separation"
        },
        {
            "abstract": "We consider wireless multicast tree construction for energy efficiency. We introduce a novel algorithm, Incremental Shortest Path Tree (ISPT), to generate source-based multicast trees and analyse its performance. Additionally, the potential of further improvements in the tree construction is addressed comparing the performance of the algorithm with the results from a simulated annealing optimisation. 1.",
            "group": 3349,
            "name": "10.1.1.212.7781",
            "keyword": "",
            "title": "Improving multicast tree construction in static ad hoc networks"
        },
        {
            "abstract": "In this work we present the extension of a variable neighborhood search (VNS) with the multilevel refinement strategy for periodic routing problems. The underlying VNS was recently proposed and performs already well on these problems. We apply a path based coarsening scheme by building fixed (route) segments of customers accounting for the periodicity. Starting at the coarsest level the problem is iteratively refined until the original problem is reached again. This refinement is smoothly integrated into the VNS. Further a suitable solution-based recoarsening is proposed. Results on available benchmark test data as well as on newly generated larger instances show the advantage of the multilevel VNS compared to the standard VNS, yielding better results in usually less CPU time. This new approach is especially appealing for large instances. ",
            "group": 3350,
            "name": "10.1.1.212.8292",
            "keyword": "",
            "title": "Multilevel Variable Neighborhood Search for Periodic Routing Problems "
        },
        {
            "abstract": "Computational ecosystems are large distributed systems in which autonomous agents make choices asynchronously based on locally available information which can be uncertain and delayed. They share these characteristics with biological ecosystems, human societies and market economies. We show that, even when designed with a single overall goal in mind as in the case of distributed problem solving, computational ecosystems can face well-known social dilemmas of sustaining cooperative behavior among selfish agents. Specifically, public-goods problems, where a common good is available to all regardless of individual contribution, can arise due to information limitations as well as the commonly recognized incentive conflicts. Some techniques for mitigating the impact of these problems are also presented. 1.",
            "group": 3351,
            "name": "10.1.1.212.8864",
            "keyword": "",
            "title": "Social dilemmas in computational ecosystems"
        },
        {
            "abstract": "Submitted to Bioinformatics. Motivation: Assembling shotgun sequencing data from repetitive DNA sequences is a non-trivial task. In existing sequence assembly methods repeats are resolved by either using statistical analyses to identify and separate fragments corresponding to repeats, or by using extra information, not contained in the fragments. In this paper we take a different approach. Using the simulated-tempering Monte Carlo method, we resolve repeats by performing an extensive search of the solution space. Results: The method is tested on two highly repetitive sequences with a two-copy and a threecopy repeat, respectively. We find that the method is able to correctly assemble these two sequences, except for a twofold degeneracy for the three-copy repeat sequence. The alternative solution obtained in this case is related by a simple symmetry to the correct one. The performance of the method is compared with that of simulated annealing. We find that simulated tempering is a competitive alternative to simulated 1 To whom correspondence should be addressed2 annealing.",
            "group": 3352,
            "name": "10.1.1.212.9573",
            "keyword": "",
            "title": "A Monte Carlo Approach to Sequence Assembly"
        },
        {
            "abstract": "Channel-optimized index assignment of source codewords is arguably the simplest way of improving transmission error resilience, while keeping the source and/or channel codes intact. But optimal design of index assignment is an instance of quadratic assignment problem (QAP), one of the hardest optimization problems in the NP-complete class. In this paper we make a progress in the research of index assignment optimization. We apply some recent results of QAP research to compute the strongest lower bounds so far for channel distortion of BSC among all index assignments. The strength of the resulting lower bounds is validated by comparing them against the upper bounds produced by heuristic index assignment algorithms. Key words: Index assignment, quantization, error resilience, quadratic assignment, In communication systems index assignment (IA) is the problem of labeling source codewords by binary integer numbers (channel codewords). For a source code of fixed",
            "group": 3353,
            "name": "10.1.1.214.276",
            "keyword": "",
            "title": "On Computation of Performance Bounds of Optimal Index Assignment"
        },
        {
            "abstract": "The application of robots in critical missions in hazardous environments requires the development of reliable or fault tolerant manipulators. In this paper, we define fault tolerance as the ability to continue the performance of a task after immobilization of a joint due to failure. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical design procedure can be used, as is illustrated through an example. 1",
            "group": 3354,
            "name": "10.1.1.214.1427",
            "keyword": "",
            "title": "Mapping tasks into fault tolerant manipulators"
        },
        {
            "abstract": "Embedding images into a low dimensional space has a wide range of applications: visualization, clustering, and pre-processing for supervised learning. Traditional dimension reduction algorithms assume that the examples densely populate the manifold. Image databases tend to break this assumption, having isolated islands of similar images instead. In this work, we propose a novel approach that embeds images into a low dimensional Euclidean space, while preserving local image similarities based on their scale invariant feature transform (SIFT) vectors. We make no neighborhood assumptions in our embedding. Our algorithm can also embed the images in a discrete grid, useful for many visualization tasks. We demonstrate the algorithm on images with known categories and compare our accuracy favorably to those of competing algorithms. 1",
            "group": 3355,
            "name": "10.1.1.214.1782",
            "keyword": "",
            "title": "Unsupervised Image Embedding Using Nonparametric Statistics"
        },
        {
            "abstract": "and comparison to DSA \u2014trade-offs between \u2019anytime \u2019 and marginally improved quality, local effort and communication \u2217 \u2014",
            "group": 3356,
            "name": "10.1.1.214.2495",
            "keyword": "",
            "title": "Distributed Simulated Annealing"
        },
        {
            "abstract": "This paper focuses a comparative evaluation of our framework for 3D face recognition and state-of-theart systems. Our method uses a Simulated Annealingbased approach (SA) for range image registration with the Surface Interpenetration Measure (SIM) as the similarity measure, in order to match two face images. The authentication score is obtained by combining the SIM values corresponding to the matching of four different face regions. Experiments were performed on the FRGC v2 database simulating both verification and identification systems and the obtained results were compared to those reported in the literature. By using all the images in the database, a verification rate of 95.9 % was achieved, at a False Acceptance Rate (FAR) of 0.1%. In the identification scenario, a rank-one accuracy of 99.5 % was obtained. To our knowledge, this is the best rank-one score obtained on the FRGC v2 database, as compared to previously published results. 1.",
            "group": 3357,
            "name": "10.1.1.214.2927",
            "keyword": "Graph Matching (HGM) [8Annotated Deformable Model [9] and Fusion Summation [12]. An",
            "title": "3D Face Recognition using the Surface Interpenetration Measure: a Comparative Evaluation on the FRGC database"
        },
        {
            "abstract": "Abstract\u2014This paper describes the application of various search techniques to the problem of automatic empirical code optimization. The search process is a critical aspect of auto-tuning systems because the large size of the search space and the cost of evaluating the candidate implementations makes it infeasible to find the true optimum point by brute force. We evaluate the effectiveness of Nelder-Mead Simplex, Genetic Algorithms, Simulated Annealing, Particle Swarm Optimization, Orthogonal search, and Random search in terms of the performance of the best candidate found under varying time limits. I.",
            "group": 3358,
            "name": "10.1.1.214.4397",
            "keyword": "",
            "title": "A Comparison of Search Heuristics for Empirical Code Optimization"
        },
        {
            "abstract": "Partial optimal labeling search for a NP-hard",
            "group": 3359,
            "name": "10.1.1.214.6995",
            "keyword": "",
            "title": "subclass of (max,+"
        },
        {
            "abstract": "In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments. It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efficiently as possible in simulation experiments. In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes. Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation. 1",
            "group": 3360,
            "name": "10.1.1.214.8935",
            "keyword": "",
            "title": "Adaptive Design Optimization in Experiments with People"
        },
        {
            "abstract": "",
            "group": 3361,
            "name": "10.1.1.215.740",
            "keyword": "",
            "title": "Probabilistic Variational Methods for Vision based Complex Motion Analysis"
        },
        {
            "abstract": "\u00a9 This paper is not for reproduction without permission of the author. The immune system has a complexity sometimes compared to that of the brain. The vast and diverse number of molecules, cells and tissues, and their complicated pathways of communication (with each other and other bodily systems), endow the immune system with cognitive abilities capable of complementing nervous cognition. In addition, there are several processes and theories used to explain the immune functioning that bring to discussion several key aspects of biology and biologically-inspired computing. This paper thus provides two forms of studying the immune system. The first is more of an analytical approach; it presents some cognitive views of the immune system, the intrinsic evolutionary nature of an adaptive immune response, and how immunity influences the evolution of species. The second study is of a synthetic nature; it describes the immune engineering concept as a meta-synthetic process used for the design of computational intelligence approaches by borrowing inspiration from the immune systems. The latter discussion is a personal account, describing how I used ideas from the immune system to solve complex engineering problems. But these are supposed to provide the reader with some insights about the development of biologically-inspired systems. 1",
            "group": 3362,
            "name": "10.1.1.215.4600",
            "keyword": "",
            "title": "Immune Cognition, Micro-evolution, and a Personal Account on Immune Engineering"
        },
        {
            "abstract": "The Commonality-Based Crossover Framework has been presented as a general model for designing problem specific operators. Following this model, the Common Features/Random Sample Climbing operator has been developed for feature subset selection--a binary string optimization problem. Although this problem should be an ideal application for genetic algorithms with standard crossover operators, experiments show that the new operator can find better feature subsets for classifier training. 1",
            "group": 3363,
            "name": "10.1.1.215.4780",
            "keyword": "",
            "title": "Non-Standard Crossover for a Standard Representation"
        },
        {
            "abstract": "The vehicle routing problem with time windows is a hard combinatorial optimization problem which has received considerable attention in the last decades. This paper proposes a two-stage hybrid algorithm for this transportation problem. The algorithm first minimizes the number of vehicles using simulated annealing. It then minimizes travel cost using a large neighborhood search which may relocate a large number of customers. Experimental results demonstrate the effectiveness of the algorithm which has improved 10 (17%) of the 58 best published solutions to the Solomon benchmarks, while matching or improving the best solutions in 46 problems (82%). More important perhaps, the algorithm is shown to be very robust. With a fixed configuration of its parameters, it returns either the best published solutions (or improvements thereof) or solutions very close in quality on all Solomon benchmarks. Very preliminary results on the extended Solomon benchmarks are also given. 1",
            "group": 3364,
            "name": "10.1.1.215.4910",
            "keyword": "",
            "title": "Hentenryck: A Two-Stage Hybrid Local Search for the Vehicle Routing Problem with Time Windows 530 Transportation Science 38(4"
        },
        {
            "abstract": "Abstract. The class of symmetric functions is based on the OneMax function by a subsequent assigning application of a real valued function. In this work we derive a sharp boundary between those problem instances that are solvable in polynomial time by the Metropolis algorithm and those that need at least exponential time. This result is both proven theoretically and illustrated by experimental data. The classification of functions into easy and hard problem instances allows a deep insight into the problem solving power of the Metropolis algorithm and can be used in the process of selecting an optimization algorithm for a concrete problem instance. 1",
            "group": 3365,
            "name": "10.1.1.215.5596",
            "keyword": "",
            "title": "Metropolis and symmetric functions: a swan song"
        },
        {
            "abstract": "In the scenario of functional protein networks reconstruction, we are given: \u2022 An interaction network- such as a Protein Protein Interaction (PPI) network or a Protein DNA interaction (PDI) network. As always, we assume the network is weighted, i.e every edge e is assigned with a confidence level w(e) such that the sum of weights for any subnetwork represents its reliability.",
            "group": 3366,
            "name": "10.1.1.215.6889",
            "keyword": "",
            "title": "Analysis of Biological Networks: Reconstruction of Functional Protein Networks"
        },
        {
            "abstract": "This thesis is about the tuning and simplification of black-box (direct-search, derivative-free) optimization methods, which by definition do not use gradient information to guide their search for an optimum but merely need a fitness (cost, error, objective) measure for each candidate solution to the optimization problem. Such optimization methods often have parameters that influence their behaviour and efficacy. A Meta-Optimization technique is presented here for tuning the behavioural parameters of an optimization method by employing an additional layer of optimization. This is used in a number of experiments on two popular optimization methods, Differential Evolution and Particle Swarm Optimization, and unveils the true performance capabilities of an optimizer in different usage scenarios. It is found that state-of-the-art optimizer variants with their supposedly adaptive behavioural parameters do not have a general and consistent performance advantage but are outperformed in several cases by simplified optimizers, if only the behavioural parameters are tuned properly. ",
            "group": 3367,
            "name": "10.1.1.216.1838",
            "keyword": "Acknowledgement",
            "title": "Tuning & Simplifying Heuristical Optimization"
        },
        {
            "abstract": "This paper investigates the design space for techniques that enable runtime, autonomic program adaptation for high-performance and low-power execution via event-driven performance prediction. The emerging multithreaded and multicore processor architectures enable applications to trade performance for reduced power consumption via regulating concurrency. At the same time however, power and performance adaptation opportunities for multithreaded programs in high-end computing environments are constrained by the fact that users are unwilling to compromise performance for saving power. Runtime systems that enable autonomous program adaptation are an appealing solution in the specific context, due to the challenges that arise in statically identifying the optimal energy-efficient operating points in each program, and the concerns of delegating the complexity of this task to end-users or application developers. System software needs to identify and exploit the power saving opportunities that arise due to the inability of code to effectively utilize all the available resources in the system, or the inability of the system to overcome scalability bottlenecks in parallel code. The techniques investigated in this paper fall into a broader class of methods that collect",
            "group": 3368,
            "name": "10.1.1.216.8264",
            "keyword": "",
            "title": "On the Design of Online Predictors for Autonomic Power-Performance Adaptation of Multithreaded Programs"
        },
        {
            "abstract": "We present a Semantic Optimized Service Discovery (SemOSD) approach capable of handling Web service search requests on a fine-grained level of detail where we augment semantic service descriptions with statistically built predictor functions. Our approach combines ontologies and mathematical functions built using statistical regression over previous Web service interactions. In the search requests we allow for arbitrary, independent and dependent constraints and user preferences expressed using objective functions. Our approach maps to standard Operational Research global optimization problem where algorithms of Simulated Annealing and Differential Evolution are used. It is capable of finding the optimal combination of service input and output parameters (a configuration) to a user request with rich preferences. Our approach is applied to an international package shipment scenario where real (Web) services are used and mined to create price prediction models. We show that the chosen regression method provides price prediction models of high accuracy and our approach supports expressive and complex search requests. 1",
            "group": 3369,
            "name": "10.1.1.216.8287",
            "keyword": "",
            "title": "Discovery of Optimized Web Service Configurations Using a Hybrid Semantic and Statistical Approach"
        },
        {
            "abstract": "We propose a root n consistent estimator for \u03b20 when the qth conditional quantile of Y given X=x and Z=z takes the semi linear form g(x) + z\u2032\u03b20 where g(.) is an unknown real valued function,\u03b20 a finite dimensional parameter and (X,Z)a couple of explanatory variables.Importantly, our estimator attains,under homoscedasticity,the semi parametric efficiency bound.This estimation is conducted in two steps. First, a Robinson\u2019s like demeaning of the original model is employed which provides a new quantile regression whose nuisance terms are estimated via a non parametric procedure.In the second stage, the quantile regression is conducted by smoothing the check function.We show that the previous estimator belongs to a class of estimators we propose to name \u201dtwo stage smooth semi parametric quantile\u201d",
            "group": 3370,
            "name": "10.1.1.217.294",
            "keyword": "",
            "title": "/ Two Stage Smooth Semi Parametric Quantile Regression"
        },
        {
            "abstract": "Modeling the compatibility of biological and economic objectives on a forested landscape",
            "group": 3371,
            "name": "10.1.1.217.1676",
            "keyword": "",
            "title": "Nathan H. Schumaker, U.S. Environmental Protection Agency"
        },
        {
            "abstract": "Abstract\u2014Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assume a single file/layer being multicast, layered coding introduces new challenges into the paradigm due to its cumulative decoding nature. Layered coding is designed to handle heterogeneity in receiver capacities, and a node may decode layer k only if it successfully receives all layers in 1..k. We show that recently proposed optimization models for layered multicast do not correctly address this challenge. We argue that in order to achieve the absolute maximum throughput (or minimum cost), it is necessary to decouple application layer throughput from network layer throughput. In particular, a node should be able to receive a non-consecutive layer or a partial layer even if it cannot decode and utilize it (e.g., for playback in media streaming applications). The rationale is that nodes at critical network locations need to receive data just for helping other peers. We present a mathematical programming model that addresses the above challenges and achieves the absolute optimal performance. Simulation results show considerable throughput gain (cost reduction) compared with previous models, in a broad range of network scenarios. We further generalize our model for studying the optimal progression of layer sizes. We show that such optimization is non-convex, and apply a Simulated Annealing algorithm to solve it, with flexible trade-off between solution quality and running time. We verify the effectiveness of the new model and the Simulated Annealing algorithm through extensive simulations, and point out insights on the connection between optimal layer size progression and node capacity distribution. I.",
            "group": 3372,
            "name": "10.1.1.217.2544",
            "keyword": "",
            "title": "Optimal Layered Multicast with Network Coding: Mathematical Model and Empirical Studies"
        },
        {
            "abstract": "The discovery of events in time series can have important implications, such as identifying microlensing events in astronomical surveys, or changes in a patient\u2019s electrocardiogram. Current methods for identifying events require a sliding window of a fixed size, which is not ideal for all applications and could overlook important events. In this work, we develop probability models for calculating the significance of an arbitrary-sized sliding window and use these probabilities to find areas of significance. Because a brute force search of all sliding windows and all window sizes would be computationally intractable, we introduce a method for quickly approximating the results. We apply our method to over 100,000 astronomical time series from the MACHO survey, in which 56 different sections of the sky are considered, each with one or more known events. Our method was able to recover 100 % of these events in the top 1 % of the results, essentially pruning 99 % of the data. Interestingly, our method was able to identify events that do not pass traditional event discovery procedures. 1",
            "group": 3373,
            "name": "10.1.1.217.5816",
            "keyword": "",
            "title": "Event Discovery in Time Series"
        },
        {
            "abstract": "Neonatal Mortality Rate (NMR) can be defined as the number of newborn deaths \u2019 aged 0 to 28 days. The most significant factor influenced NMR is low birth weight of newborn babies approximately 48%, followed by asphyxia 37 % and sepsis 32.5%. This paper deploys a Burr XII distribution to model the Non-Normal Neonatal Weight data. The parameters of the fitted Burr XII distribution are estimated using Simulated Annealing (SA) method. The mortality rate defined as the proportion of newborn with weight out side the accepted weight specification limits is then estimated using the fitted Burr XII distribution. The efficacy of the model is assessed by comparing the estimated mortality rate based on the fitted distribution with the actual mortality rate obtained from the data. The results indicate that the Bur XII distribution provides an estimate of Mortality rate which is very close to the actual mortality rate.",
            "group": 3374,
            "name": "10.1.1.217.7519",
            "keyword": "DiabetesHypertensionCardiovascular Diseaseand",
            "title": "Modelling Non-Normal Neonatal Weight Data to Estimate Mortality Rate of Newborn Babies"
        },
        {
            "abstract": "In recent years, co-clustering has emerged as a powerful data mining tool that can analyze dyadic data connecting two entities. However, almost all existing co-clustering techniques are partitional, and allow individual rows and columns of a data matrix to belong to only one cluster. Several current applications, such as recommendation systems and market basket analysis, can substantially benefit from a mixed membership of rows and columns. In this paper, we present Bayesian co-clustering (BCC) models, that allow a mixed membership in row and column clusters. BCC maintains separate Dirichlet priors for rows and columns over the mixed membership and assumes each observation to be generated by an exponential family distribution corresponding to its row and column clusters. We propose a fast variational algorithm for inference and parameter estimation. The model is designed to naturally handle sparse matrices as the inference is done only based on the nonmissing entries. In addition to finding a co-cluster structure in observations, the model outputs a low dimensional coembedding, and accurately predicts missing values in the original matrix. We demonstrate the efficacy of the model through experiments on both simulated and real data. 1",
            "group": 3375,
            "name": "10.1.1.217.7933",
            "keyword": "",
            "title": "Bayesian co-clustering"
        },
        {
            "abstract": "Abstract. This paper presents a time-series whole clustering system that incrementally constructs a hierarchy of clusters. The Online Divisive-Agglomerative Clustering (ODAC) system is an incremental implementation of divisive analysis clustering, using the correlation between timeseries as similarity measure. The system tests existing clusters by descending order of diameters, looking for a possible binary split. If no cluster deserves division, then the system searches for possible aggregation of clusters. At each time step, only one splitting or one aggregation might occur. Main features include a splitting criteria supported by the Hoeffding bound, a stopping criteria based on the divisive coefficient and an agglomerative phase which decreases the number of unneeded clusters, also based on the divisive coefficient which measures the amount of divisive structure found. Preliminary results show competitive performance on clustering time-series when compared to a simple batch divisive analysis clustering algorithm. 1",
            "group": 3376,
            "name": "10.1.1.217.9842",
            "keyword": "",
            "title": "J.P.: Hierarchical time-series clustering for datastreams"
        },
        {
            "abstract": "Abstract \u2014 Persistent CAD algorithms offer the potential to optimize power consumption for programmable chips post development and deployment. The basic idea is that algorithms continue to search for better design solutions and as better solutions are found, the design is deployed to programmable chips resulting in better performance. In this work, we further study a persistent placement algorithm for FPGAs and investigate a number of algorithm improvements attempting to delay premature convergence. Our results show that these techniques create some divergence in the solutions, but in all cases what we call \u201cinevitable convergence occurs \u201d in the first 2 hours of execution.",
            "group": 3377,
            "name": "10.1.1.218.177",
            "keyword": "GAPlacementFPGAPersistent",
            "title": "Exploring Inevitable Convergence for a Genetic Algorithm Persistent FPGA Placer"
        },
        {
            "abstract": "Estimation of Distribution Algorithms (EDAs) have been proposed as an extension of genetic algorithms. We assume that the function to be optimized is additively decomposed (ADF). The interaction graph of the ADF is used to create exact or approximate factorizations of the Boltzmann distribution. Convergence of the algorithm MN-GIBBS is proven. MN-GIBBS uses a Markov network easily derived from the ADF and Gibbs sampling. The Factorized Distribution Algorithm (FDA) uses a less general representation, a Bayesian network and probabilistic logic sampling (PLS). We shortly describe the algorithm LFDA which learns a Bayesian network from data. The relation between the network computed by LFDA and the optimal network used by FDA is investigated. Convergence of FDA to the optima is shown for finite samples if the factorization fulfills the running intersection property. The sample size is bounded by O(nm ln nm) where n is the size of the problem and m the number of sub-functions. For the proof results from statistical learning theory and Probably Approximately Correct (PAC) learning are used. Numerical experiments show that even for difficult test functions a sample size which scales linearly with n is often sufficient. We also show that a good approximation of the true distribution is not necessary, it suffices to use a factorization where the global optima have a large enough probability. This explains the success of EDAs in practical applications.",
            "group": 3378,
            "name": "10.1.1.218.2453",
            "keyword": "Bayesian networksMarkov networksBoltzmann distributionGibbs samplingPAC learning",
            "title": "Convergence Theorems of Estimation of Distribution Algorithms"
        },
        {
            "abstract": "In the field of phylogenetics evolution of species is often described as the result of random mutations and natural selection, which gives rise to the notion of phylogenetic trees. The large amount of high quality sequence data available has made the reconstruction of phylogenetic trees from sequence data possible. The maximum likelihood method is a powerful and popular choice for constructing phylogenetic trees from sequence data, however finding the maximum likelihood tree is a hard problem requiring numerical methods, limiting the size of the trees, which can be constructed. The subject of this thesis is speeding up tree reconstruction by making use of prior knowledge. We propose a new heuristic for incorporating a phylogenetic tree in the search for the maximum likelihood tree for similar data set. In this thesis we implement both the proposed heuristic and a simulated annealing algorithm for finding the maximum likelihood tree from a multiple alignment. The experiments performed show, that incorporating prior knowledge leads to a significant speed up, if large amounts are included.",
            "group": 3379,
            "name": "10.1.1.218.3787",
            "keyword": "",
            "title": "Tree Reconstruction by Maximum Likelihood using Prior Knowledge"
        },
        {
            "abstract": "This paper explores the use of the nonextensive q-distribution in the context of adaptive stochastic searching. The proposed approach consists of generating the \u201cprobability \u201d of moving from one point of the search space to another through a probability distribution characterized by the q entropic index of the nonextensive entropy. The potential benefits of this technique are investigated by incorporating it in two different adaptive search algorithmic models to create new modifications of the diffusion method and the particle swarm optimizer. The performance of the modified search algorithms is evaluated in a number of nonlinear optimization and neural network training benchmark problems.",
            "group": 3380,
            "name": "10.1.1.218.4555",
            "keyword": "Adaptive stochastic searchparticle swarmsimulated annealingdiffusionnonextensive",
            "title": "APPROACHES TO ADAPTIVE STOCHASTIC SEARCH BASED ON THE NONEXTENSIVE q-DISTRIBUTION"
        },
        {
            "abstract": "Abstract Service orchestrations are a common means to compose individual services to either higher-level services or potentially complex composite applications. The Web Service Business Process Execution Language (WS-BPEL) is an example for a language that allows for defining automatically executable orchestrations of Web services. As of today, BPEL process are typically executed in a centralized manner; the process model is deployed on a single workflow management system which, during process instance execution, interprets the process definition and interacts with the orchestrated Web services on behalf of the user. In previous work, we have presented an approach which enables decentralized execution of BPEL processes based on a decentralized process model and supporting runtime infrastructure. In this paper we describe a method for automatic splitting of a process among the partners participating in its execution, referred to as process partitioning. Key words: Process partitioning, decentralized process enactment, BPEL 1",
            "group": 3381,
            "name": "10.1.1.218.5226",
            "keyword": "",
            "title": "A Method for Partitioning BPEL Processes for Decentralized Execution *"
        },
        {
            "abstract": "We propose a variant of the simulated annealing method for optimization in the multivhriate analysis of differentiable functions. The method uses global actualizations via the hybrid Monte Carlo algorithm in their generalized version for the proposal of new configurations. We show how this choice can improve upon the performance of simulated annealing methods (mainly when the number of variables is large) by allowing a more effective searching scheme and a faster annealing schedule. KEY WORDS: mization. Simulated annealing; hybrid Monte Carlo; multivariate mini-",
            "group": 3382,
            "name": "10.1.1.218.5777",
            "keyword": "",
            "title": "Journal of Statistical Physics, Vol. 89, Nos. 5/6, 1997 Simulated Annealing Using Hybrid Monte Carlo"
        },
        {
            "abstract": "We consider the problem of sampling a point from an arbitrary distribution \u03c0 over an arbitrary subset S of an integer hyper-rectangle. Neither the distribution \u03c0 nor the support set S are assumed to be available as explicit mathematical equations but may only be defined through oracles and in particular computer programs. This problem commonly occurs in black-box discrete optimization as well as counting and estimation problems. The generality of this setting and high-dimensionality of S precludes the application of conventional random variable generation methods. As a result, we turn to Markov Chain Monte Carlo (MCMC) sampling, where we execute an ergodic Markov chain that converges to \u03c0 so that the distribution of the point delivered after sufficiently many steps can be made arbitrarily close to \u03c0. Unfortunately, classical Markov chains such as the nearest neighbor random walk or the co-ordinate direction random walk fail to converge to \u03c0 as they can get trapped in isolated regions of the support set. To surmount this difficulty, we propose Discrete Hit-and-Run (DHR), a Markov chain motivated by the Hit-and-Run algorithm known to be the most efficient method for sampling from log-concave distributions over convex bodies in Rn. We prove that the limiting distribution of DHR is \u03c0 as desired, thus enabling us to sample approximately from \u03c0 by delivering the last iterate of a sufficiently",
            "group": 3383,
            "name": "10.1.1.218.5958",
            "keyword": "We are interested in the M",
            "title": "Discrete hit-and-run for sampling points from arbitrary distributions over subsets of integer hyper-rectangles"
        },
        {
            "abstract": "",
            "group": 3384,
            "name": "10.1.1.218.6882",
            "keyword": "",
            "title": "A variable neighborhood search for the periodic vehicle routing problem with time windows"
        },
        {
            "abstract": "Markov chain methods for Boltzmann sampling work in phases with decreasing temperatures. The number of transitions in each phase crucially affects terminal state distribution. We employ dynamic programming to allocate iterations to phases to improve guarantees on sample quality. Numerical experiments on the Ising model are presented.",
            "group": 3385,
            "name": "10.1.1.218.7240",
            "keyword": "",
            "title": "A Dynamic Programming Approach to Efficient Sampling from Boltzmann Distributions"
        },
        {
            "abstract": "We propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner. We focus on problems specified as a Boolean formula, i.e., on SAT instances. Sampling for SAT problems has been shown to have interesting connections with probabilistic reasoning, making practical sampling algorithms for SAT highly desirable. The best current approaches are based on Markov Chain Monte Carlo methods, which have some practical limitations. Our approach exploits combinatorial properties of random parity (XOR) constraints to prune away solutions near-uniformly. The final sample is identified amongst the remaining ones using a state-of-the-art SAT solver. The resulting sampling distribution is provably arbitrarily close to uniform. Our experiments show that our technique achieves a significantly better sampling quality than the best alternative. 1",
            "group": 3386,
            "name": "10.1.1.218.7296",
            "keyword": "",
            "title": "Nearuniform sampling of combinatorial spaces using XOR constraints"
        },
        {
            "abstract": "This paper is concerned with finding first-best tolls in static transportation networks with day-to-day variation in network capacity, as accounted for by changes in the volume-delay function. The key question in addressing this problem is that of information, namely, which agents have access to what information when making decisions. In this work, travelers are assumed to be either fully informed about network conditions before embarking on travel, or having no information except the probability distributions; likewise, the network manager (toll-setter) is either able to vary tolls in response to realized network conditions, or must apply the same tolls every day. Further, travelers \u2019 preference for reliable travel is accounted for, representing risk aversion in the face of uncertainty. For each of the scenarios implied by combinations of these assumptions, we present methods to determine system-optimal link prices. A demonstration is provided, using the Sioux Falls test network, suggesting that attempts to incorporate uncertainty into nonresponsive tolls involve significantly higher prices. 1",
            "group": 3387,
            "name": "10.1.1.218.7489",
            "keyword": "",
            "title": "Congestion Pricing under Operational, Supply-Side Uncertainty"
        },
        {
            "abstract": "durch",
            "group": 3388,
            "name": "10.1.1.218.7916",
            "keyword": "",
            "title": "Solving Two Generalized Network Design Problems with Exact and Heuristic Methods"
        },
        {
            "abstract": "Water is a limited resource and the dramatically increasing world population requires a significant increase in food production. For improving both crop yield and water use efficiency, the usual optimization strategy in irrigation at the field level considers scheduling parameters, i.e. when and how much to irrigate, as well as control parameters, i.e. the intensity and the irrigation time, for each water application. Optimizing control and schedule parameters in irrigation is considered as a nested problem. The objective of the global optimization is to achieve maximum crop yield with a given, but limited water volume, which can be arbitrary distributed over the number of irrigations. It is difficult to solve the global optimization problem, because the target function has many locally optimal solutions and the number of optimization variables, i.e. the number of irrigations is unknown a-priori. For this reason, a made to measure evolutionary optimization technique (EA) is employed to find a near-optimal solution of the global optimization problem within acceptable computational time. The results provided by the new optimization strategy are compared with the popular shuffled complex evolution algorithm (SCE-UA) optimization algorithm, simulated annealing (SA) and differential evolution (DE). The comparison demonstrated a striking superiority of the new tool with respect to both the achieved irrigation efficiency and the required computational time. 1.",
            "group": 3389,
            "name": "10.1.1.218.8081",
            "keyword": "",
            "title": "GLOBAL OPTIMIZATION OF DEFICIT IRRIGATION SYSTEMS USING EVOLUTIONARY ALGORITHMS"
        },
        {
            "abstract": "Keywords:Protein structure prediction, conformational space search, multiple energy functions, active learning, Rosetta, Monte Carlo. The most significant impediment for protein structure prediction is the inadequacy of conformation space search. Conformation space is too large and the energy landscape too rugged for existing search methods to consistently find near-optimal minima. To alleviate this problem, we present model-based search, a novel conformation space search method. Model-based search uses highly accurate information obtained during search to build an approximate, partial model of the energy landscape. Modelbased search aggregates information in the model as it progresses, and in turn uses this information to guide exploration towards regions most likely to contain a nearoptimal minimum. We validate our method by predicting the structure of 32 proteins, ranging in length from 49 to 213 amino acids. Our results demonstrate that modelbased search is more effective at finding low-energy conformations in high-dimensional conformation spaces than existing search methods. The reduction in energy translates into structure predictions of increased accuracy. 1",
            "group": 3390,
            "name": "10.1.1.218.9072",
            "keyword": "",
            "title": "Guiding Conformation Space Search with an All-Atom Energy Potential Short title: Model-Based Search for Protein Folding"
        },
        {
            "abstract": "We present an extension of continuous domain Simulated Annealing. Our algorithm employs a globally reaching candidate generator, adaptive stochastic acceptance probabilities, and converges in probability to the optimal value. An application to simulation-optimization problems with asymptotically diminishing errors is presented. Numerical results on a noisy protein-folding problem are included.",
            "group": 3391,
            "name": "10.1.1.218.9733",
            "keyword": "Global OptimizationSimulated AnnealingMarkov Chain Monte CarloSimulation-Optimization",
            "title": "Adaptive Search with Stochastic Acceptance Probabilities for Global Optimization"
        },
        {
            "abstract": "The aim of a protein folding simulation is to determine the native state of a protein from its amino acid sequence. In this paper we describe the development and application of an Immune Algorithm (IA) to find the lowest energy conformations for the 2D (square) HP lattice bead protein model. Here we introduce a modified chain growth constructor to produce the initial population, where intermediate infeasible structures are recorded, thereby reducing the risk of attempting to perform wasteful point mutations during the mutation phase. We also investigate various approaches for population diversity tracking, ultimately allowing a greater understanding of the progress of the optimization. Povzetek: V \u010dlanku je opisan razvoj in izvedba imunskega algoritma (IA) za iskanje najni\u017eje energijske strukture za 2D (kvadratne) HP mre\u017eno nanizanega modela proteina. 1",
            "group": 3392,
            "name": "10.1.1.218.9859",
            "keyword": "HP lattice bead modelimmune algorithmpopulation diversity trackingprotein modelling",
            "title": "Informatica 32 (2008) 245\u2013251 245 Analysis of an Immune Algorithm for Protein Structure Prediction"
        },
        {
            "abstract": "Abstract \u2013 This paper proposes a technique to obtain long term estimates of the motion of a moving object in a structured environment. Objects moving in such environments often participate in typical motion patterns which can be observed consistently. Our technique learns those patterns by observing the environment and clustering the observed trajectories using any pairwise clustering algorithm. We have implemented our technique using both simulated and real data coming from a vision system. The results show that the technique is general, produces long-term predictions and is fast enough for its use in real time applications. I.",
            "group": 3393,
            "name": "10.1.1.219.456",
            "keyword": "",
            "title": "Motion prediction for moving objects: a statistical approach"
        },
        {
            "abstract": "In my master's thesis I elaborate on strip-shredded text document reconstruction. Contrary to conventional document reconstruction which uses color or shape information of images text document reconstruction has not been researched very well. Nowadays it is common to destroy paper documents by shredding them, i.e. producing paper strips. This work tries to nd ways to undo the process. First and foremost I describe the problem formally. Next I de ne a way to evaluate problem instances. A set of improvement strategies are introduced which help the evaluation process. De ned construction heuristics yield good results in reasonable amount of time. Then optimization algorithms try to nd a good arrangement of the strips, ideally the correct one. A demo application simulates the shredding process of a sample page. Then this page is reconstructed using the above mentioned evaluation techniques and several optimization techniques like multistart variable neighborhood search, simulated annealing and iterated local search. Extensive tests were run with a 60 instance test set. The implemented application reconstructed more than half of the problem instances correctly and is also able to reconstruct several pages at once. Zusammenfassung In meiner Masterarbeit arbeite ich die Wiederherstellung von durch Shredder zerst\u00f6rter Textdokumente aus. Im Gegensatz zu herk\u00f6mmlicher Dokumentenwiederherstellung die auf Farb- oder Umrissinformationen beruht ist die Wiederherstellung von Textdokumenten noch nicht eingehend untersucht worden. Normalerweise werden Papierdokumente mittels Shredder zerst\u00f6rt, d.h. in l\u00e4ngliche Papierstreifen zerlegt. In dieser Arbeit wird versucht, diesen Prozess r\u00fcckg\u00e4ngig zu machen. Zuallererst beschreibe ich die Problemstellung formal. Als n\u00e4chstes werde ich eine M\u00f6glichkeit aufzeigen wie L\u00f6sungen zu diesem Problem eingesch\u00e4tzt werden k\u00f6nnen. Weiters werden eine Reihe von Verbesserungsstrategien vorgestellt, die bei der Evaluierung helfen. De nierte Konstruktionsheuristiken ermitteln",
            "group": 3394,
            "name": "10.1.1.219.4247",
            "keyword": "",
            "title": "durch"
        },
        {
            "abstract": "Abstract-We propose a number of pre-distorted mask design techniques for binary and phase-shifting masks. Our approach is based on modeling the imaging mechanism of a stepper by the Hopkins equations and taking advantage of the contrastenhancement characteristics of photoresist. Optimization techniques such as the branch and bound algorithm and simulated annealing algorithm are used to systematically design pre-distorted masks under incoherent and partially coherent illumination. Computer simulations are used to show that the intensity contour shapes and developed resist shapes of our designed mask patterns are sharper than those of conventional masks. The designed phase-shifting masks are shown to result in higher contrast as well as sharper contours than binary masks. An example of phase conflicting mask designed via our algorithm is shown to outperform a simple intuitive design. This example indicates that a fairly general design procedure consisting of alternating phase shifts and our optimized phase-shift mask is a viable candidate for future phase-shifting mask design. I.",
            "group": 3395,
            "name": "10.1.1.219.4490",
            "keyword": "",
            "title": "Binary and phase shift mask design for optical lithography"
        },
        {
            "abstract": "Abstract-We propose a number of pre-distorted mask design techniques for binary and phase-shifting masks. Our approach is based on modeling the imaging mechanism of a stepper by the Hopkins equations and taking advantage of the contrastenhancement characteristics of photoresist. Optimization techniques such as the branch and bound algorithm and simulated annealing algorithm are used to systematically design pre-distorted masks under incoherent and partially coherent illumination. Computer simulations are used to show that the intensity contour shapes and developed resist shapes of our designed mask patterns are sharper than those of conventional masks. The designed phase-shifting masks are shown to result in higher contrast as well as sharper contours than binary masks. An example of phase conflicting mask designed via our algorithm is shown to outperform a simple intuitive design. This example indicates that a fairly general design procedure consisting of alternating phase shifts and our optimized phase-shift mask is a viable candidate for future phase-shifting mask design. I.",
            "group": 3396,
            "name": "10.1.1.219.4490",
            "keyword": "",
            "title": "Binary and phase shift mask design for optical lithography"
        },
        {
            "abstract": "Abstract. There is a perception that teaching space in universities is a rather scarce resource. However, some studies have revealed that in many institutions it is actually chronically under-used. Often, rooms are occupied only half the time, and even when in use they are often only half full. This is usually measured by the \u201cutilisation \u201d which is defined as the percentage of available \u2019seat-hours \u2019 that are employed. Within real institutions, studies have shown that this utilisation can often take values as low as 20-40%. One consequence of such a low level of utilisation is that space managers are under pressure to make a more efficient use of the available teaching space. However, better management is hampered because there does not appear to be a good understanding within space management (nearterm planning) of why this happens. Nor, a good basis within space planning (long-term planning) of how best to accommodate the expected low utilisations. This motivates our two main goals: (i) To understand",
            "group": 3397,
            "name": "10.1.1.219.4601",
            "keyword": "",
            "title": "Towards improving the utilisation of university teaching space"
        },
        {
            "abstract": "Dynamic optimization in business-wide process controlDynamic optimization in business-wide process control PROEFSCHRIFT ter verkrijging van de graad van doctor",
            "group": 3398,
            "name": "10.1.1.219.5221",
            "keyword": "chemical processesoptimizationsupply chain",
            "title": "aan de Technische Universiteit Delft,"
        },
        {
            "abstract": "Abstract \u2014 Over the recent years, there has been increasing research activities made on improving the efficacy of Memetic Algorithm (MA) for solving complex optimization problems. Particularly, these efforts have revealed the success of MA on a wide range of real world problems. MAs not only converge to high quality solutions, but also search more efficiently than their conventional counterparts. Despite the success and surge in interests on MAs, there is still plenty of scope for furthering our understanding on how and why synergy between populationbased and individual learning searchers would lead to successful Memetic Algorithms. In this paper we outline several important design issues of Memetic Algorithms and present a systematic study on each. In particular, we conduct extensive experimental studies on the impact of each individual design issue and their relative impacts on memetic search performances by means of three commonly used synthetic problems. From the empirical studies obtained, we attempt to reveal the behaviors of several MA variants to enhance our understandings on MAs. I.",
            "group": 3399,
            "name": "10.1.1.219.5562",
            "keyword": "",
            "title": "A study on the design issues of Memetic Algorithm"
        },
        {
            "abstract": "Nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world. This paper presents a new component-based approach with evolutionary eliminations, for a nurse scheduling problem arising at a major UK hospital. The main idea behind this technique is to decompose a schedule into its components (i.e. the allocated shift pattern of each nurse), and then to implement two evolutionary elimination strategies mimicking natural selection and natural mutation process on these components respectively to iteratively deliver better schedules. The worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there. This demonstration employs an evaluation function which evaluates how well each component contributes towards the final objective. Two elimination steps are then applied: the first elimination eliminates a number of components that are deemed not worthy to stay in the current schedule; the second elimination may also throw out, with a low level of probability, some worthy components. The eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria. Computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems.",
            "group": 3400,
            "name": "10.1.1.219.5798",
            "keyword": "Key wordsnurse rosteringconstructive heuristiclocal searchevolutionary elimination HistoryAccepted by Michel GendreauArea Editor for Heuristic Search & Learning",
            "title": "A Component Based Heuristic Search Method with Evolutionary Eliminations for Hospital Personnel Scheduling"
        },
        {
            "abstract": "Abstract. A standard problem within universities is that of teaching space allocation which can be thought of as the assignment of rooms and times to various teaching activities. The focus is usually on courses that are expected to fit into one room. However, it can also happen that the course will need to be broken up, or \u2018split\u2019, into multiple sections. A lecture might be too large to fit into any one room. Another common example is that of seminars or tutorials. Although hundreds of students may be enrolled on a course, it is often subdivided into particular types and sizes of events dependent on the pedagogic requirements of that particular course. Typically, decisions as to how to split courses need to be made within the context of limited space requirements. Institutions do not have an unlimited number of teaching rooms, and need to effectively use those that they do have. The efficiency of space usage is usually measured by the overall \u2018utilisation \u2019 which is basically the fraction of the available seat-hours that are actually used. A multi-objective optimisation problem naturally arises; with a trade-off between satisfying preferences on splitting, a desire to increase utilisation, and also to satisfy other constraints such as those based on event location and timetabling conflicts. In this paper, we explore such trade-offs. The explorations themselves are based on a local search method that attempts to optimise the space utilisation by means of a \u2018dynamic splitting \u2019 strategy. The local moves are designed to improve utilisation and satisfy the other constraints, but are also allowed to split, and un-split, courses so as to simultaneously meet the splitting objectives. 1",
            "group": 3401,
            "name": "10.1.1.219.6402",
            "keyword": "",
            "title": "The teaching space allocation problem with splitting"
        },
        {
            "abstract": "ausgef\u00fchrt am",
            "group": 3402,
            "name": "10.1.1.219.7004",
            "keyword": "Eine generische Implementierung bietet vorallem den Vorteil das bei einem neuen",
            "title": "Datum"
        },
        {
            "abstract": "Abstract \u2014 In this paper the problem of dynamic selfreconfiguration of a class of modular robotic systems referred to as metamorphic systems is examined. A metamorphic robotic system is a collection of mechatronic modules, each of which has the ability to connect, disconnect, and climb over adjacent modules. We examine the near-optimal reconfiguration of a metamorphic robot from an arbitrary initial configuration to a desired final configuration. Concepts of distance between metamorphic robot configurations are defined, and shown to satisfy the formal properties of a metric. These metrics, called configuration metrics, are then applied to the automatic self-reconfiguration of metamorphic systems in the case when one module is allowed to move at a time. There is no simple method for computing the optimal sequence of moves required to reconfigure. As a result, heuristics which can give a near optimal solution must be used. We use the technique of Simulated Annealing to drive the reconfiguration process with configuration metrics as cost functions. The relative performance of simulated annealing with different cost functions is compared and the usefulness of the metrics developed in this paper is demonstrated. Index Terms\u2014Metrics, optimal assignment, self-reconfigurable robots, simulated annealing.",
            "group": 3403,
            "name": "10.1.1.219.7161",
            "keyword": "",
            "title": "Useful metrics for modular robot motion planning"
        },
        {
            "abstract": "Iris recognition has been developing for over 20 years, but, only in recent years has it been more accessible and widely accepted as one of the most accurate and un-obtrusive biometric modalities. Over the past few years, many companies have developed iris acquisition systems that are more user-friendly. Iris-On-the-Move (IOM) is one such system, offering significant stand-off acquisition distance (3m), which is extremely convenient for users and very suitable for deployment at airports to check passenger identifications and to control access. However, iris images acquired by the IOM and other long range systems are, in most cases, considerably blurred, of low contrast, and lacking detail in the iris texture compared to images from very close proximity sensors (5cm stand-off). This thesis focuses on how to deal with the three most challenging problems in long-range iris recognition: (1) iris segmentation from long range systems, (2) automatic iris mask generation of occluded regions, (3) iris matching performance enhancement using multiple irises from a video sequence. In particular, we emphasize solutions in the context of the",
            "group": 3404,
            "name": "10.1.1.220.2214",
            "keyword": "Super-ResolutionIris RecognitionIris SegmentationIris Occlusion Estimation",
            "title": "Robust Long Range Iris Recognition from Video Using Super Resolution"
        },
        {
            "abstract": "Fear of increasing prices and concern about climate change are motivating residential power conservation efforts. We investigate the effectiveness of several unsupervised disaggregation methods on low frequency power measurements collected in real homes. Specifically, we consider variants of the factorial hidden Markov model. Our results indicate that a conditional factorial hidden semi-Markov model, which integrates additional features related to when and how appliances are used in the home and more accurately represents the power use of individual appliances, outperforms the other unsupervised disaggregation methods. Our results show that unsupervised techniques can provide perappliance power usage information in a non-invasive manner, which is ideal for enabling power conservation efforts. 1",
            "group": 3405,
            "name": "10.1.1.220.2405",
            "keyword": "",
            "title": "Unsupervised Disaggregation of Low Frequency Power Measurements"
        },
        {
            "abstract": "Abstract\u2014Computing has recently reached an inflection point with the introduction of multicore processors. On-chip thread-level parallelism is doubling approximately every other year. Concurrency lends itself naturally to allowing a program to trade performance for power savings by regulating the number of active cores; however, in several domains, users are unwilling to sacrifice performance to save power. We present a prediction model for identifying energy-efficient operating points of concurrency in well-tuned multithreaded scientific applications and a runtime system that uses live program analysis to optimize applications dynamically. We describe a dynamic phase-aware performance prediction model that combines multivariate regression techniques with runtime analysis of data collected from hardware event counters to locate optimal operating points of concurrency. Using our model, we develop a prediction-driven phase-aware runtime optimization scheme that throttles concurrency so that power consumption can be reduced and performance can be set at the knee of the scalability curve of each program phase. The use of prediction reduces the overhead of searching the optimization space while achieving near-optimal performance and power savings. A thorough evaluation of our approach shows a reduction in power consumption of 10.8 percent, simultaneous with an improvement in performance of 17.9 percent, resulting in energy savings of 26.7 percent. Index Terms\u2014Modeling and prediction, application-aware adaptation, energy-aware systems. \u00c7 1",
            "group": 3406,
            "name": "10.1.1.220.3325",
            "keyword": "",
            "title": "Prediction-Based Power-Performance Adaptation of Multithreaded Scientific Codes"
        },
        {
            "abstract": "Finding reliable, meaningful patterns in data with high numbers of attributes can be extremely difficult. Feature selection helps us to decide what attributes or combination of attributes are most important for finding these patterns. In this chapter, we study feature selection methods for building classification models from high-throughput genomic (microarray) and proteomic (mass spectrometry) data sets. Thousands of feature candidates must be analyzed, compared and combined in such data sets. We describe the basics of four different approaches used for feature selection and illustrate their effects on an MS cancer proteomic data set. The closing discussion provides assistance in performing an analysis in high-dimensional genomic and proteomic data.",
            "group": 3407,
            "name": "10.1.1.220.4700",
            "keyword": "BioinformaticsClassificationFeature selection2 hal-00643496version 1- 29 Nov 2011",
            "title": "Author manuscript, published in \"Fundamentals of Data Mining in Genomics and Proteomics Springer (Ed.) (2006) 149-172\" DOI: 10.1007/978-0-387-47509-7 Chapter 1 FEATURE SELECTION/WEIGHTING AND DIMENSIONALITY REDUCTION IN GENOMICS AND PROTEOMICS"
        },
        {
            "abstract": "Abstract: We use marked point processes to detect an unknown number of trees from high resolution aerial images. This approach turns to be an energy minimization problem, where the energy contains a prior term which takes into account the geometrical properties of the objects, and a data term to match these objects onto the image. This stochastic process is simulated via a Reversible Jump Markov Chain Monte Carlo procedure, which embeds a Simulated Annealing scheme to extract the best configuration of objects. We compare in this paper different cooling schedules of the Simulated Annealing algorithm which could provide some good minimization in a short time. We also study some adaptive proposition kernels.",
            "group": 3408,
            "name": "10.1.1.220.6674",
            "keyword": "Unit\u00e9 de recherche INRIA Sophia Antipolis",
            "title": "de rechercheOptimization Techniques for Energy Minimization Problem in a Marked Point Process Application to Forestry"
        },
        {
            "abstract": "Abstract\u2014In this paper we investigate the static multicast advance reservation (MCAR) problem for all-optical wavelengthrouted WDM networks. Under the advanced reservation traffic model, connection requests specify their start time to be some time in the future and also specify their holding times. We investigate the static MCAR problem where the set of advance reservation requests is known ahead of time. We prove the MCAR problem is NP-complete, formulate the problem mathematically as an integer linear program, and develop three efficient heuristics, seqRWA, ISH, and SA, to solve the problem for practical size networks. We also introduce a theoretical lower bound on the number of wavelengths required. To evaluate our heuristics, we first compare their performances to the ILP for small networks and then simulate them over real-world, large-scale networks. We find the SA heuristic provides close to optimal results compared to the ILP for our smaller networks, and up to a 33 % improvement over seqRWA and up to a 22 % improvement over ISH on realistic networks. SA provides, on average, solutions 1.5-1.8x times the cost given by our conservative lower bound on large networks. Index Terms\u2014advance reservation, scheduled demands, RWA, multicast, NP-complete, ILP, heuristics, simulated annealing",
            "group": 3409,
            "name": "10.1.1.220.7158",
            "keyword": "",
            "title": "1 Static Routing and Wavelength Assignment for Multicast Advance Reservation in All-Optical Wavelength-Routed WDM Networks"
        },
        {
            "abstract": "he developed industrial vision applications for pattern analysis and verification. In 2002, he spent a semester as a research intern at Siemens Corporate Research (SCR) working on autonomous obstacle detection and avoidance for vehicle navigation. During the summers of 2005 and 2006, he worked as a research intern at Intel Applications Research Lab (ARL) on human pose estimation and tracking. His work received the Best",
            "group": 3410,
            "name": "10.1.1.220.7247",
            "keyword": "",
            "title": "Approved by the Graduate Council Date"
        },
        {
            "abstract": "Abstract\u2014In video streaming over multicast network, user bandwidth requirement is often heterogeneous possibly with orders of magnitude difference (say, from hundreds of kb/s for mobile devices to tens of Mb/s for high-definition TV). Multiple description coding (MDC) can be used to address this bandwidth heterogeneity issue. In MDC, the video source is encoded into multiple independent descriptions. A receiver, depending on its available bandwidth, joins different descriptions to meet their bandwidth requirements. An important but challenging problem for MDC video multicast is how to assign bandwidth to each description in order to maximize overall user satisfaction. In this paper, we investigate this issue by formulating it as an optimization problem, with the objective to maximize user bandwidth experience by taking into account the encoding inefficiency due to MDC. We prove that the optimization problem is NP-hard. However, if the description number is larger than or equal to a certain threshold (e.g., if the minimum and maximum bandwidth requirements are 100 kb/s and 10 Mb/s, respectively, such threshold is seven descriptions), there is an exact and simple solution to achieve maximum user satisfaction, i.e., meeting all the bandwidth requirements. For the case when the description number is smaller, we present an efficient heuristic called simulated annealing for MDC bandwidth assignment (SAMBA) to assign bandwidth to each description given the distribution of user bandwidth requirement. We evaluate our algorithm using simulations. SAMBA achieves virtually the same optimal performance based on exhaustive search. By comparing with other assignment algorithms, SAMBA significantly improves user satisfaction. We also show that, if the coding efficiency decreases with the number of descriptions, there is an optimal description number to achieve maximal user satisfaction. Index Terms\u2014Multiple-description-coded video, optimal description bandwidth assignment, simulated annealing, streaming. I.",
            "group": 3411,
            "name": "10.1.1.220.7617",
            "keyword": "",
            "title": "Optimal Bandwidth Assignment for Multiple-Description-Coded Video"
        },
        {
            "abstract": "pour obtenir le grade de L\u2019Habilitation \u00e0 Diriger des Recherches de",
            "group": 3412,
            "name": "10.1.1.220.7786",
            "keyword": "",
            "title": "Composition du jury"
        },
        {
            "abstract": "Evolutionary algorithms applied to reliable communication",
            "group": 3413,
            "name": "10.1.1.220.9184",
            "keyword": "Evolutionary algorithmsNetwork designGeneralized Steiner problem",
            "title": "network design"
        },
        {
            "abstract": "Abstract \u2014 The focus of this survey is on research in applying evolutionary and other metaheuristic search algorithms to automatically generating content for games, both digital and non-digital (such as board games). The term search-based procedural content generation is proposed as the name for this emerging field, which at present is growing quickly. A taxonomy for procedural content generation is devised, centering on what kind of content is generated, how the content is represented and how the quality/fitness of the content is evaluated; searchbased procedural content generation in particular is situated within this taxonomy. This article also contains a survey of all published papers known to the authors in which game content is generated through search or optimisation, and ends with an overview of important open research problems. I.",
            "group": 3414,
            "name": "10.1.1.220.9554",
            "keyword": "",
            "title": "1 Search-based Procedural Content Generation: A Taxonomy and Survey"
        },
        {
            "abstract": "be inserted by the editor)",
            "group": 3415,
            "name": "10.1.1.220.9785",
            "keyword": "Alexandru BalanStefan Roth and Payman Yadollahpour for help in",
            "title": "This work was supported in part by NSF grants IIS-0534858 and IIS-"
        },
        {
            "abstract": "Abstract. Novelty-based diversification approaches aim to produce a diverse ranking by directly comparing the retrieved documents. However, since such approaches are typically greedy, they require O(n 2) documentdocument comparisons in order to diversify a ranking of n documents. In this work, we propose to model novelty-based diversification as a similarity search in a sparse metric space. In particular, we exploit the triangle inequality property of metric spaces in order to drastically reduce the number of required document-document comparisons. Thorough experiments using three TREC test collections show that our approach is at least as effective as existing novelty-based diversification approaches, while improving their efficiency by an order of magnitude. 1",
            "group": 3416,
            "name": "10.1.1.221.893",
            "keyword": "",
            "title": "Sparse Spatial Selection for Novelty-based Search Result Diversification"
        },
        {
            "abstract": "Squeaky Wheel Optimization (SWO) is a relatively new metaheuristic that has proved to be effective on many real-world problems. At each iteration, SWO does a complete construction of a solution starting from the empty assignment. Although the construction uses information from previous iterations, the complete rebuilding does mean that SWO is generally effective at diversification but can suffer from a relatively weak intensification. Evolutionary SWO (ESWO) is a recent extension to SWO that is designed to improve the intensification by keeping the good components of solutions and only using SWO to reconstruct other poorer components of the solution. In such algorithms a standard challenge is understanding how the various parameters affect the search process. In order to support the future study of such issues, we propose a formal framework for the analysis of ESWO. The framework is based on Markov chains, and the main novelty arises because ESWO moves through the space of partial assignments. This makes it significantly different from the analyses used in local search (such as simulated annealing) which only move through complete assignments. Generally, the exact details of ESWO will depend on various heuristics; so we specialise to a case of ESWO that we call \u201cESWO-II \u201d and that has probabilistic as opposed to heuristic selection and construction operators. For ESWO-II, we study a simple problem instance and explicitly compute the stationary distribution probability over the states of the search space. We find interesting properties of the distribution. In particular, we find that the probabilities of states generally, but not always, increase with their fitness. This non-monotonocity is quite different from the monotonicity expected in algorithms such as simulated annealing.",
            "group": 3417,
            "name": "10.1.1.221.4268",
            "keyword": "Markov",
            "title": "Evolutionary Squeaky Wheel Optimization: A New Analysis Framework"
        },
        {
            "abstract": "Comparative study of serial and parallel heuristics used to",
            "group": 3418,
            "name": "10.1.1.221.5076",
            "keyword": "Combinational logic circuitsGenetic algorithmsMetaheuristicsSimulated annealingParallelization",
            "title": "Optimization Methods and Software"
        },
        {
            "abstract": "Abstract- This paper introduces a mobility tracking mechanism that combines a movement-based location update policy with a selective paging scheme. Movement-based location update is selected for its simplicity. It does not require ea & mobile terminal to store information about the arrangement and the distance relationship of all cells. In fact, each mobile terminal only keeps a counter of the number of cells visited. A location update is performed when this counter exceeds a predefined threshold value. This scheme allows the dynamic selection of the movement threshold on a per-user basis. This is desirable as different users may have very different mobility patterns. Selective paging reduces the cost for locating a mobile terminal in the expense of an increase in the paging delay. In this paper, we propose a selective paging scheme which significantly decreases the location tracking cost under a small increase in the allowable paging delay. We introduce an analytical model for the proposed location tracking mechanism which captures the mobility and the incoming call arrival patterns of each mobile terminal. Analytical results are provided to demonstrate the cost-effectiveness of the proposed scheme under various parameters. Index Terms-Personal communication networks, location update, terminal paging, mobile terminal. I.",
            "group": 3419,
            "name": "10.1.1.221.6245",
            "keyword": "",
            "title": "Movement-based location update and selective paging for PCS networks"
        },
        {
            "abstract": "Genetic Algorithms (GAs) are a search heuristic technique modelled on the processes of evolution. They have been used to solve optimisation problems in a wide variety of fields. When applied to the optimisation of intervention schedules for optimal control problems, such as cancer chemotherapy treatment scheduling, GAs have been shown to require more fitness function evaluations than other search heuristics to find fit solutions. This thesis presents extensions to the GA crossover process, termed directed intervention crossover techniques, that greatly reduce the number of fitness function evaluations required to find fit solutions, thus increasing the effectiveness of GAs for problems of this type. The directed intervention crossover techniques use intervention scheduling information from parent solutions to direct the offspring produced in the GA crossover process towards more promising areas of a search space. By counting the number of interventions present in parents and adjusting the number of interventions for offspring schedules around it, this allows for highly fit solutions to be found in less fitness function evaluations. The validity of these novel approaches is illustrated through comparison with conventional GA crossover approaches for optimisation of intervention schedules of bio-control application in mushroom farming and cancer chemotherapy treatment. These involve optimally scheduling the application of a bio-control agent to combat pests in mushroom farming and optimising the timing and dosage strength of cancer chemotherapy treatments to",
            "group": 3420,
            "name": "10.1.1.221.7239",
            "keyword": "",
            "title": "Directed Intervention Crossover Approaches in Genetic Algorithms with Application to Optimal Control Problems"
        },
        {
            "abstract": "We investigate how morphological features in the form of part-of-speech tags impact parsing performance, using Arabic as our test case. The large, fine-grained tagset of the Penn Arabic Treebank (498 tags) is difficult to handle by parsers, ultimately due to data sparsity. However, ad-hoc conflations of treebank tags runs the risk of discarding potentially useful parsing information. The main contribution of this paper is to describe several automated, language-independent methods that search for the optimal feature combination to help parsing. We first identify 15 individual features from the Penn Arabic Treebank tagset. Either including or excluding these features results in 32,768 combinations, so we then apply heuristic techniques to identify the combination achieving the highest parsing performance. Our results show a statistically significant improvement of 2.86 % for vocalized text and 1.88 % for unvocalized text, compared with the baseline provided by the Bikel-Bies Arabic POS mapping (and an improvement of 2.14 % using product models for vocalized text, 1.65 % for unvocalized text), giving state-of-the-art results for Arabic constituency parsing. 1",
            "group": 3421,
            "name": "10.1.1.221.8217",
            "keyword": "",
            "title": "Morphological Features for Parsing Morphologically-rich Languages: A Case of Arabic"
        },
        {
            "abstract": "Bayesian object recognition is applied to the analysis of complex forest object configurations measured in high-density airborne laser scanning (LIDAR) data. With the emergence of high-resolution active remote sensing technologies, highly detailed, spatially explicit forest measurement information can be extracted through the application of statistical object recognition algorithms. A Bayesian approach to object recognition incorporates a probabilistic model of the active sensing process and places a prior probability model on object configurations. LIDAR sensing geometry is explicitly modelled in the domain of scan space, a threedimensional analogue to two-dimensional image space. Prior models for object configurations take the form of Markov marked point processes, where pair-wise object interactions depend upon object attributes. Inferences are based upon the posterior distribution of the object configuration given the observed LIDAR. Given the complexity of the posterior distribution, inferences are based upon dependent samples generated via Markov chain Monte Carlo simulation. This algorithm was applied to a 0.21 ha area within Capitol State Forest, WA, USA. Algorithm-based estimates are compared to photogrammetric crown measurements and field inventory data. 1.",
            "group": 3422,
            "name": "10.1.1.222.4015",
            "keyword": "KEY WORDSForestrylaser scanningLIDARobjectrecognitionremote sensingstatistics ABSTRACT",
            "title": "BAYESIAN OBJECT RECOGNITION FOR THE ANALYSIS OF COMPLEX FOREST SCENES IN AIRBORNE LASER SCANNER DATA"
        },
        {
            "abstract": "Available online at www.sciencedirect.com",
            "group": 3423,
            "name": "10.1.1.222.4529",
            "keyword": "Filamentous cyanobacteriaAnabaenaHeterocyst commitmentLateral inhibitionFixed nitrogenDiazotrophicDifferentiationComputational modelingPattern formationGrowth and division",
            "title": ""
        },
        {
            "abstract": "The journey of writing a PhD dissertation can be discouraging and lonely. I could not have achieved these results without all the kind help from numerous people around me. I could not have started my PhD career and hence written the thesis without the guidance from Dr. Christian R. Shelton, my esteemed advisor. I joined Dr. Shelton\u2019s research group in September 2004 as a PhD student. Since then he gave me careful guidance for my research directions. Every single process of my work also has his effort to make it happen, and make it better. He is almost always available for helping me when I get stuck. I would like to show my most sincere appreciation to Dr. Shelton. Thank you! Many thanks goes to my wife, Jing Xu. Also a PhD student in the same group as I, Jing is always willing to discuss problems with me, is always helpful when I need suggestions, and is always forgiving when I make mistakes. More importantly, as my wife, Jing gave me enormous support during my PhD studies. My thanks also goes to Dr. Eamonn Keogh and Dr. Vassilis Tsotras for their kindness in serving as my defense committee members. They gave great suggestions for making the dissertation better. I would like to thank Yu Fan, Kevin Horan, Kin Fai Kan, Antony Lam, and Teddy Yap, Jr., the other graduate students in our research group. I have benefitted much from discussions with them, and my PhD life would have been less exciting and fruitful without them. Finally, I would like to thank my parents in China. We had very little time get together during my time as a PhD student here in UC Riverside. They gave me whole-hearted support, iv without which my studying would have been much more difficult, if even possible. v ABSTRACT OF THE DISSERTATION",
            "group": 3424,
            "name": "10.1.1.222.5051",
            "keyword": "",
            "title": "Dimensionality Reduction Algorithms With Applications to Collaborative Data and Images"
        },
        {
            "abstract": "The field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. This is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. We give a survey of the nowadays most important metaheuristics from a conceptual point of view. We outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. Two very important concepts in metaheuristics are intensification and diversification. These are the two forces that largely determine the behaviour of a metaheuristic. They are in some way contrary but also complementary to each other. We introduce a framework, that we call the I&D frame, in order to put different intensification and diversification components into relation with each other. Outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization. 1",
            "group": 3425,
            "name": "10.1.1.222.8266",
            "keyword": "",
            "title": "Metaheuristics in combinatorial optimization: Overview and conceptual comparison"
        },
        {
            "abstract": "Abstract. After introducing the \u0393-convergence of a family of symmetric matrices, we study the limits in that sense, of Schr\u00f6dinger operators on a finite graph. The main result is that any such limit can be interpreted as a Schr\u00f6dinger operator on a new graph, the construction of which is described explicitly. The operators to which the construction is applied are reversible, almost reducible Markov generators. An explicit method for computing an equivalent of the spectrum is described. Among possible applications, quasidecomposable processes and low-temperature simulated annealing are studied.",
            "group": 3426,
            "name": "10.1.1.224.1027",
            "keyword": "Schr\u00f6dinger operatorperturbationsspectrumMarkov generators. MSC (200047A5560J27",
            "title": "Singular limits of Schr\u00f6dinger operators and Markov processes"
        },
        {
            "abstract": "Since 1953, researchers have applied the Monte Carlo method to a wide range of areas. Specialized algorithms have also been developed to extend the method\u2019s applicability and efficiency. The author describes some of the algorithms that have been developed to",
            "group": 3427,
            "name": "10.1.1.224.2567",
            "keyword": "",
            "title": "Copublished by the IEEE CS and the AIP"
        },
        {
            "abstract": "The problem of having a restricted amount of data fpr a computational model is one faced by many, including oceanographers, meteorologists and oil companies. How this problem is overcome varies from discipline to discipline. In this dissertation we consider methods for fitting the permeability data associated with a model for flow through porous media. Optimisation techniques namely conjugate gradients and simulated, are presented as a means for solving linear systems of equations which may be under or over determined. We start by considering an inverse polynomial function for the permeability to achieve an equation for the pressure, minimising either the least squares problem \u2016Ax \u2212 b \u2016 or the norm \u2016x\u20162 for the under and over determined cases. This, however, is a highly specific case and so the second part of the dissertation is concerned with methods applicable when both the pressure and permeability are approximated on a discrete mesh. This is achieved by first looking at the pressure field and minimising the curvature of the field, [2]. In one dimension we then consider the first order differential equation for the permeability, which can be integrated and used to find an estimate for the permeability.",
            "group": 3428,
            "name": "10.1.1.224.6449",
            "keyword": "",
            "title": "Flow Through Porous Media: Recovering permeability data from incomplete information by function fitting"
        },
        {
            "abstract": "Int. Journ. of Unconventional Computing, Vol. 0, pp. 1\u201321 Reprints available directly from the publisher Photocopying permitted by license only",
            "group": 3429,
            "name": "10.1.1.224.6763",
            "keyword": "algorithmic assemblyembodied computationembodimentembryological developmentmetamorphosismorphogenesisnanotechnologypost- Moore\u2019s Law computingreconfigurable systemsself-assemblyself-organization",
            "title": "\u00a92011 Old City Publishing, Inc. Published by license under the OCP Science imprint, a member of the Old City Publishing Group. Artificial Morphogenesis as an Example of Embodied Computation"
        },
        {
            "abstract": "Abstract. This paper investigates the impact of the local and global register file architecture on a reconfigurable system based on the ADRES architecture [3]. The register files consume a significant amount of area on the reconfigurable device, and their architecture has a strong impact on the performance. We found that the global registers should be tightly connected to as many functional units as possible, while the connection of the local register files to their neighbours is less critical. We found that the global register file should contain between 12 and 16 registers, while each local register file should only contain one or two registers. We used these results to propose a new architecture that has between 60 % and 95 % higher performance per unit area compared to the original architecture over the set of benchmarks. 1",
            "group": 3430,
            "name": "10.1.1.224.9114",
            "keyword": "",
            "title": "S.J.E.: Register File Architecture Optimization in a Coarse-Grained Reconfigurable Architecture"
        },
        {
            "abstract": "A metaheuristic-based algorithm is presented for the post enrolment-based course timetabling problem used in track-2 of the Second International Timetabling Competition (ITC2007). The featured algorithm operates in three distinct stages \u2013 a constructive phase followed by two separate phases of simulated annealing \u2013 and is time dependent, due to the fact that various run-time parameters are calculated automatically according to the amount of computation time available. Overall, the method produces results in line with the official finalists to the timetabling competition, though experiments show that this algorithm also seems to find certain instances more difficult to solve than others. A number of reasons for this latter feature are discussed.",
            "group": 3431,
            "name": "10.1.1.225.149",
            "keyword": "",
            "title": "A Time-Dependent Metaheuristic Algorithm for Post Enrolment-based Course Timetabling"
        },
        {
            "abstract": "A thesis presented to the University of Waterloo in fulfilment of the thesis requirement for the degree of",
            "group": 3432,
            "name": "10.1.1.225.500",
            "keyword": "",
            "title": "3.1 Porous Media Reconstruction..................... 36"
        },
        {
            "abstract": "ability of core networks to manage data transmission of increasing volume and variation is critical for the success of data-intensive and networkcentric applications as they grow in both scale and complexity. Traditionally, static optical networks were the dominant transport for medium and long distance communication. However, these networks can no longer meet the needs of tomorrow\u2019s applications for higher bandwidth at lower cost. New dynamic optical networks greatly improve the reconfigurability of optical terminal systems and support unprecedented flexibility for high-traffic resource sharing. However, managing dynamic networks poses challenging problems related to scale and traffic volume. Traditional analytical techniques, which rely heavily on simplification of network topologies and route choices, are insufficient to understand the significant performance differences implied by the subtle path preferences of dynamic routing algorithms. This dissertation presents an integrated approach to efficient and robust",
            "group": 3433,
            "name": "10.1.1.225.577",
            "keyword": "",
            "title": "c \u25cb 2011 Xiaolan ZhangRESOURCE MANAGEMENT FOR DYNAMIC OPTICAL NETWORKS BY"
        },
        {
            "abstract": "As integrated circuits become increasingly more complex and expensive, the ability to make post-fabrication changes will become much more attractive. This ability can be realized using programmable logic cores. Currently, such cores are available from vendors in the form of \u201chard \u201d macro layouts. An alternative approach for fine-grain programmability is possible: vendors supply an RTL version of their programmable logic fabric that can be synthesized using standard cells. Although this technique may suffer in terms of speed, density, and power overhead, the task of integrating such cores is far easier than the task of integrating \u201chard \u201d cores into an ASIC or SoC. When the required amount of programmable logic is small, this ease of use may be more important than the increased overhead. In this thesis, we identify potential implementation issues associated with such cores, and investigate in depth the area, speed and power overhead of using this approach. Based on this investigation, we attempt to improve the performance of programmable cores created in this manner. Using a test-chip implementation, we identify three main issues: core size selection, I/O connections, and clock-tree synthesis. Compared to a non-programmable design, the soft core approach exhibited an average area overhead of 200X, speed overhead of 10X, and power",
            "group": 3434,
            "name": "10.1.1.225.1873",
            "keyword": "",
            "title": "EMBEDDED PROGRAMMABLE LOGIC CORES"
        },
        {
            "abstract": "Abstract \u2014 Often adaptive, distributed control can be viewed as an iterated game between independent players. The coupling between the players \u2019 mixed strategies, arising as the system evolves from one instant to the next, is determined by the system designer. Information theory tells us that the most likely joint strategy of the players, given a value of the expectation of the overall control objective function, is the minimizer of a Lagrangian function of the joint strategy. So the goal of the system designer is to speed evolution of the joint strategy to that Lagrangian minimizing point, lower the expectated value of the control objective function, and repeat. Here we elaborate the theory of algorithms that do this using local descent procedures, and that thereby achieve efficient, adaptive, distributed control. I.",
            "group": 3435,
            "name": "10.1.1.225.2757",
            "keyword": "",
            "title": "Distributed control by lagrangian steepest descent"
        },
        {
            "abstract": "As integrated circuits become increasingly complex, the ability to make post-fabrication changes will become more important and attractive. This ability can be realized using programmable logic cores. Currently, such cores are available from vendors in the form of \u201chard \u201d macro layouts. Previous work has suggested an alternative approach: vendors supply a synthesizable version of their programmable logic core and the integrated circuit designer synthesizes the programmable logic fabric using standard cells. Although this technique suffers increased delay, area, and power, the task of integrating such cores is far easier than the task of integrating \u201chard \u201d cores into an ASIC or SoC. When implementing small amount of logic, this case of use may be more important than the increased overhead. This thesis presents a new family of architectures for these \u201csynthesizable \u201d cores; unlike previous architectures which were based on lookup-tables, the new family of architectures is based on a collection of product-term arrays. Compared to lookup-table based architectures, the new architectures result in density improvements of 35 % and speed improvements of 72 % on standard benchmark circuits. In addition, we describe novel architectural designs to enhance synthesizable architectures to support sequential logic. We show that directly embedding flipflops",
            "group": 3436,
            "name": "10.1.1.225.3738",
            "keyword": "",
            "title": "PRODUCT-TERM BASED SYNTHESIZEABLE EMBEDDED PROGRAMMABLE LOGIC CORES"
        },
        {
            "abstract": "Abstract\u2014This paper gives an overview of some recent advances in topological approaches to analog layout synthesis and in layout-aware analog sizing. The core issue in these approaches is the modeling of layout constraints for an efficient exploration process. This includes fast checking of constraint compliance, reducing the search space, and quickly relating topological encodings to placements. Sequence-pairs, B*-trees, circuit hierarchy and layout templates are described as advantageous means to tackle these tasks. I.",
            "group": 3437,
            "name": "10.1.1.225.4205",
            "keyword": "",
            "title": "Analog Layout Synthesis- Recent Advances in Topological Approaches"
        },
        {
            "abstract": "has been examined by the signatories, and we find that both the content and the form meet acceptable presentation standards of scholarly work in the above mentioned discipline. iii Muszala, Stefan P. (Ph.D., Electrical and Computer Engineering) Model Based Load Indices (MBLI) for Scientific Simulation",
            "group": 3438,
            "name": "10.1.1.225.4475",
            "keyword": "",
            "title": "Model Based Load Indices (MBLI) for Scientific Simulation by"
        },
        {
            "abstract": "Any opinions expressed here are those of the author(s) and not those of the institute. Research disseminated by IZA may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit company supported by Deutsche Post World Net. The center is associated with the University of Bonn and offers a stimulating research environment through its research networks, research support, and visitors and doctoral programs. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be available on the IZA website (www.iza.org) or directly from the author. IZA Discussion Paper No. 1161",
            "group": 3439,
            "name": "10.1.1.225.5820",
            "keyword": "",
            "title": "of LaborUnemployment Benefits and Unemployment Rates of Low-Skilled and Elder Workers in West Germany: A Search Equilibrium Approach"
        },
        {
            "abstract": "data center management, virtual machines, workload replay",
            "group": 3440,
            "name": "10.1.1.225.7671",
            "keyword": "",
            "title": "Abstract:"
        },
        {
            "abstract": "de L\u2019Universit\u00e9 Paris\u2013Sud present\u00e9e en vue de l\u2019obtention du grade de Docteur de l\u2019Universit\u00e9 Paris\u2013Sud",
            "group": 3441,
            "name": "10.1.1.226.1705",
            "keyword": "",
            "title": "par"
        },
        {
            "abstract": "Search result diversification has gained momentum as a way to tackle ambiguous queries. An effective approach to this problem is to explicitly model the possible aspects underlying a query, in order to maximise the estimated relevance of the retrieved documents with respect to the different aspects. However, such aspects themselves may represent information needs with rather distinct intents (e.g., informational or navigational). Hence, a diverse ranking could benefit from applying intent-aware retrieval models when estimating the relevance of documents to different aspects. In this paper, we propose to diversify the results retrieved for a given query, by learning the appropriateness of different retrieval models for each of the aspects underlying this query. Thorough experiments within the evaluation framework provided by the diversity task of the TREC 2009 and 2010 Web tracks show that the proposed approach can significantly improve state-of-the-art diversification approaches.",
            "group": 3442,
            "name": "10.1.1.226.2418",
            "keyword": "AlgorithmsExperimentationPerformance Keywords Web SearchRelevanceDiversity",
            "title": "Intent-aware search result diversification"
        },
        {
            "abstract": "Despite recent developments in protein structure prediction, an accurate new fold prediction algorithm remains elusive. One of the challenges facing current techniques is the size and complexity of the space containing possible structures for a query sequence. Traditionally, to explore this space fragment assembly approaches to new fold prediction have used stochastic optimization techniques. Here we examine deterministic algorithms for optimizing scoring functions in protein structure prediction. Two previously unused techniques are applied to the problem, called the Greedy algorithm and the Hill-climbing algorithm. The main difference between the two is that the latter implements a technique to overcome local minima. Experiments on a diverse set of 276 proteins show that the Hill-climbing algorithms consistently outperform existing approaches based on Simulated Annealing optimization (a traditional stochastic technique) in optimizing the root mean squared deviation (RMSD) between native and working structures. 1",
            "group": 3443,
            "name": "10.1.1.226.2575",
            "keyword": "",
            "title": "Effective Optimization Algorithms for Fragment-assembly based Protein Structure Prediction \u2217"
        },
        {
            "abstract": "ABSTRACT By rearranging naturally occurring genetic components, gene networks can be created that display novel functions. When designing these networks, the kinetic parameters describing DNA/protein binding are of great importance, as these parameters strongly influence the behavior of the resulting gene network. This article presents an optimization method based on simulated annealing to locate combinations of kinetic parameters that produce a desired behavior in a genetic network. Since gene expression is an inherently stochastic process, the simulation component of simulated annealing optimization is conducted using an accurate multiscale simulation algorithm to calculate an ensemble of network trajectories at each iteration of the simulated annealing algorithm. Using the three-gene repressilator of Elowitz and Leibler as an example, we show that gene network optimizations can be conducted using a mechanistically realistic model integrated stochastically. The repressilator is optimized to give oscillations of an arbitrary specified period. These optimized designs may then provide a starting-point for the selection of genetic components needed to realize an in vivo system.",
            "group": 3444,
            "name": "10.1.1.226.3229",
            "keyword": "",
            "title": "Optimization of a Stochastically Simulated Gene Network Model via Simulated Annealing"
        },
        {
            "abstract": "NOTICE: this is the authors version of a work that was accepted for publication in Remote Sensing of Environment. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Remote Sensing of Environment, 108:171-178 (2007). doi:10.1016/j.rse.2006.10.023",
            "group": 3445,
            "name": "10.1.1.226.3931",
            "keyword": "",
            "title": "1 An Automatic Statistical Segmentation Algorithm for Extraction of Fire and Smoke Regions"
        },
        {
            "abstract": "This paper considers the problem of estimating the power breakdowns for the main appliances inside a building using a small number of power meters and the knowledge of the ON/OFF states of individual appliances. First we solve the breakdown estimation problem within a tree configuration using a single power meter and the knowledge of ON/OFF states and use the solution to derive an estimation quality metric. Using this metric, we then propose an algorithm for optimally placing additional power meters to increase the estimation certainty for individual appliances to the required level. The proposed solution is evaluated using real measurements, numerical simulations and by constructing a scaled down proof-of-concept prototype using binary sensors.",
            "group": 3446,
            "name": "10.1.1.227.578",
            "keyword": "Electric Load EstimationElectricity Consumption Monitoring",
            "title": "Estimating Building Consumption Breakdowns using ON/OFF State Sensing and Incremental Sub-Meter Deployment"
        },
        {
            "abstract": "We describe a new approach for rescoring speech lattices \u2014 with long-span language models or wide-context acoustic models \u2014 that does not entail computationally intensive lattice expansion or limited rescoring of only an N-best list. We view the set of word-sequences in a lattice as a discrete space equipped with the edit-distance metric, and develop a hill climbing technique to start with, say, the 1-best hypothesis under the lattice-generating model(s) and iteratively search a local neighborhood for the highest-scoring hypothesis under the rescoring model(s); such neighborhoods are efficiently constructed via finite state techniques. We demonstrate empirically that to achieve the same reduction in error rate using a better estimated, higher order language model, our technique evaluates fewer utterance-length hypotheses than conventional N-best rescoring by two orders of magnitude. For the same number of hypotheses evaluated, our technique results in a significantly lower error rate.",
            "group": 3447,
            "name": "10.1.1.227.2332",
            "keyword": "1. SHORTCOMINGS OF N-BEST RESCORING",
            "title": "climbing on speech lattices : A new rescoring framework"
        }
    ]
}
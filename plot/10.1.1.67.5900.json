{
    "nodes": [
        {
            "group": 0,
            "name": "10.1.1.67.5900",
            "keyword": "",
            "author": "James Jenn-jier Lien, Jeffrey F. Cohn",
            "abstract": "We have developed a computer vision system, including both facial feature extraction and recognition, that automatically discriminates among subtly different facial expressions. Expression classification is based on Facial Action Coding System (FACS) action units (AUs), and discrimination is performed using Hidden Markov Models (HMMs). Three methods are developed to extract facial expression information for automatic recognition. The first method is facial feature point tracking using a coarse-to-fine pyramid method. This method is sensitive to subtle feature motion and is capable of handling large displacements with sub-pixel accuracy. The second method is dense flow tracking together with principal component analysis (PCA), where the entire facial motion information per frame is compressed to a lowdimensional weight vector. The third method is high gradient component (i.e., furrow) analysis in the spatiotemporal domain, which exploits the transient variation associated with the facial expression. Upon extraction of the facial information, non-rigid facial expression is separated from the rigid head motion component, and the face images are automatically aligned and normalized using an affine transformation. This system also provides expression intensity estimation, which has significant effect on the actual meaning of the expression. 1.",
            "title": "Subtly different facial expression recognition and expression intensity estimation"
        },
        {
            "group": 1,
            "name": "10.1.1.1.9874",
            "keyword": "",
            "author": "Xiaolei Huang Song, Song Zhang, Yang Wang, Dimitris Metaxas, Dimitris Samaras",
            "abstract": "We present a novel hierarchical framework for high resolution, nonrigid facial expression tracking. The high quality dense point clouds of facial geometry moving at video speeds are acquired using a phase-shifting based structured light ranging technique [19]. To use such data for temporal study of the subtle dynamics in expressions and for face recognition, an efficient nonrigid facial tracking algorithm is needed to establish intra-frame correspondences. In this paper, we propose such an algorithmic framework that uses a multi-resolution 3D deformable face model, and a hierarchical tracking scheme. This framework can not only track global facial motion that is caused by muscle action, but fit to subtler expression details that are generated by highly local skin deformations. Tracking of global deformations is performed efficiently on the coarse level of our face model with one thousand nodes, to recover the changes in a few intuitive parameters that control the motion of several deformable regions. In order to capture the complementary highly local deformations, we use a variational algorithm for non-rigid shape registration based on the integration of an implicit shape representation and the Free Form Deformations (FFD). Due to the strong implicit and explicit smoothness constraints imposed by the algorithm, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one intra-frame correspondences. User-input sparse facial feature correspondences can also be incorporated as hard constraints in the optimization process, in order to guarantee high accuracy of the established correspondences. Extensive tracking experiments using the dynamic facial scan of five different subjects demonstrate the accuracy and efficiency of our proposed frame...",
            "title": "A Hierarchical Framework For High Resolution Facial Expression Tracking"
        },
        {
            "group": 2,
            "name": "10.1.1.7.9346",
            "keyword": "",
            "author": "Lionel Reveret,  Irfan Essa",
            "abstract": "We present a new method for video-based coding of facial motions inherent with speaking. We propose a set of four Facial Speech Parameters (FSP): jaw opening, lip rounding, lip closure, and lip raising, to represent the primary visual gestures in speech articulation. To generate a parametric model of facial actions, first a statistical model is developed by analyzing accurate 3D data of a reference human subject. The FSP are then associated to the linear modes of this statistical model resulting in a 3D parametric facial mesh that is linearly deformed using FSP. For tracking of talking facial motions, the parametric model is adapted and aligned to a subject's face. Then the face motion is tracked by optimally aligning the incoming video frames with the face model, textured with the first image, and deformed by varying the FSP, head rotations, and translations. Finer details of lip and skin deformation are modeled using a blend of textures into an appearance model. We show results of the tracking for different subjects using our method. Finally, we demonstrate the facial activity encoding into the four FSP values to represent speaker-independent phonetic information and to generate different styles of animation.",
            "title": "Visual Coding and Tracking of Speech Related Facial Motions"
        },
        {
            "group": 3,
            "name": "10.1.1.33.1946",
            "keyword": "exchanging knowledge on optical flows, Peter Rander, Henry Rowley, Shumeet Baluja",
            "author": "Jenn-jier James Lien, Jenn-jier James Lien",
            "abstract": "Signature Professor Ching-Chung Li Signature Professor Takeo Kanade Signature Professor Jeffrey F. Cohn AUTOMATIC RECOGNITION OF FACIAL EXPRESSIONS USING HIDDEN MARKOV MODELS AND ESTIMATION OF EXPRSSION INTENSITY Jenn-Jier James Lien, Ph.D. Facial expressions provide sensitive cues about emotional responses and play a major role in the study of psychological phenomena and the development of nonverbal communication. Facial expressions regulate social behavior, signal communicative intent, and are related to speech production. Most facial expression recognition systems focus on v only six basic expressions. In everyday life, however, these six basic expressions occur relatively infrequently, and emotion or intent is more often communicated by subtle changes in one or two discrete features, such as tightening of the lips which may communicate anger. Humans are capable of producing thousands of expressions that vary in complexity, intensity, and meaning. The objective of this dissertati...",
            "title": "Automatic Recognition of Facial Expressions Using Hidden Markov Models and Estimation of Expression Intensity"
        },
        {
            "group": 4,
            "name": "10.1.1.40.8481",
            "keyword": "Key words, face expression recognition, optical flow, high-gradient component detection, hidden Markov model, human-computer interaction",
            "author": "J.J. Lien,  T. Kanade,  J. Cohn,  C. Li",
            "abstract": "Most of the current work on automated facial expression analysis attempt to recognize a small set of prototypic expressions, such as joy and fear. Such prototypic expressions, however, occur infrequently, and human emotions and intentions are communicated more often by changes in one or two discrete features. To capture the full range of facial expression, detection, tracking, and classification of fine-grained changes in facial features are needed. We developed the first version of a computer vision system that is sensitive to subtle changes in the face. The system includes three modules to extract feature information: dense-flow extraction using a wavelet motion model, facial feature tracking, and edge and line extraction. The feature information thus extracted is fed to discriminant classifiers or hidden Markov models that classify it into FACS action units, the descriptive system to code fine-grained changes in facial expression. The system was tested on image sequences from 100 ma...",
            "title": "Detection, Tracking and Classification of Actions Units in Facial Expressions"
        },
        {
            "group": 5,
            "name": "10.1.1.64.6088",
            "keyword": "",
            "author": "Montse Pard\u00e0s, Antonio Bonafonte",
            "abstract": "The video analysis system described in this paper aims at facial expression recognition consistent with the MPEG4 standardized parameters for facial animation, FAP. For this reason, two levels of analysis are necessary: low level analysis to extract the MPEG4 compliant parameters and high level analysis to estimate the expression of the sequence using these low level parameters. The low level analysis is based on an improved active contour algorithm that uses high level information based on Principal Component Analysis to locate the most significant contours of the face (eyebrows and mouth), and on motion estimation to track them. The high level analysis takes as input the FAP produced by the low level analysis tool and, by means of a Hidden Markov Model classifier, detects the expression of the sequence. \u2217 This work has been supported by the European project InterFace and TIC2001-0996 of the Spanish Government 1.",
            "title": "Facial animation parameters extraction and expression detection using HMM"
        },
        {
            "group": 6,
            "name": "10.1.1.65.6795",
            "keyword": "",
            "author": "Yang Wang, Mohit Gupta, Song Zhang, Sen Wang, Xianfeng Gu, Dimitris Samaras, Peisen Huang",
            "abstract": "We present a novel fully automatic method for high resolution, non-rigid dense 3D point tracking. High quality dense point clouds of non-rigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient non-rigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints guarantees that non-rigid deformations will be accurately tracked as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency. 1. Introduction and Previous",
            "title": "High resolution tracking of non-rigid 3D motion of densely sampled data using harmonic maps"
        },
        {
            "group": 7,
            "name": "10.1.1.70.2520",
            "keyword": "",
            "author": "Ying-li Tian, Takeo Kanade, Jeffrey F. Cohn",
            "abstract": "Facial expressions are the facial changes in response to a person\u2019s internal emotional states, intentions, or social communications. Facial expression analysis has been an active research topic for behavioral scientists since the work of Darwin in 1872 [18, 22, 25, 71]. Suwa et al. [76] presented an early attempt to automatically analyze facial expressions by tracking the motion of 20 identified spots on an image sequence in 1978. After that, much progress has been made to build computer systems to help us understand and use this natural form of human",
            "title": "Chapter 11. Facial Expression Analysis"
        },
        {
            "group": 8,
            "name": "10.1.1.76.8220",
            "keyword": "",
            "author": "Jos\u00e9 Luis L, Montse Pard\u00e0s, Antonio Bonafonte",
            "abstract": "This paper discusses the application of a facial expression recognition system in unrestrained video intervals. The system is based on the modeling of the expressions by means of Hidden Markov Models. The observations used to create the models are the MPEG-4 standardized Facial Animation Parameters (FAPs). The FAPs of a video sequence are first extracted and then analyzed using semi-continuous HMM. The basic recognizer, presented in [12], shows good performance in distinguishing entire expressions, previously marked inside video sequences. This paper describes the adaptation of the technique to deal with unrestrained expression intervals in video sequences, that is, expressions the boundaries of which have not been previously marked inside the scene. To design the new system, we have taken advantage of the symmetry of the expressions to extract a new topology of the HMMs and we have trained the models without requiring the recording and analysis of a new database. We have also used a discarding process with the aim to eliminate expressions not comprised within the models together with the intervals without any expressions at all. The combination of the presented techniques is suitable for temporal block sampling with later expression classification or discarding. 1.",
            "title": "HMM recognition of expressions in unrestrained video intervals"
        },
        {
            "group": 9,
            "name": "10.1.1.77.4740",
            "keyword": "",
            "author": "Montse Pard\u00e0s, Antonio Bonafonte, Jos\u00e9 Luis L",
            "abstract": "In this paper a facial expression recognition system is presented. The system is based on the modelling of the expressions by means of Hidden Markov Models. The observations used to create the models are the MPEG-4 standardized Facial Animation Parameters (FAPs). The FAPs of a video sequence are first extracted and then analyzed using semi-continuous HMM. The system shows good performance for distinguishing isolated expressions and can also be used, with lower accuracy, to extract the expressions in long video sequences where speech is mixed with silence frames. 1.",
            "title": "Emotion recognition based on MPEG-4 facial animation parameters"
        },
        {
            "group": 10,
            "name": "10.1.1.81.4834",
            "keyword": "",
            "author": "Sarah L. Snow, Dawn R. Hurst, Herv\u00e9 Abdi",
            "abstract": "Abstract\u2014We describe a database of static images and video clips of human faces and people that is useful for testing algorithms for face and person recognition, head/eye tracking, and computer graphics modeling of natural human motions. For each person there are nine static \u201cfacial mug shots \u201d and a series of video streams. The videos include a \u201cmoving facial mug shot, \u201d a facial speech clip, one or more dynamic facial expression clips, two gait videos, and a conversation video taken at a moderate distance from the camera. Complete data sets are available for 284 subjects and duplicate data sets, taken subsequent to the original set, are available for 229 subjects. Index Terms\u2014Face database, face recognition, face tracking, digital video. 1",
            "title": "A Video Database of Moving Faces and People Alice J. O\u2019Toole, Joshua Harms,"
        },
        {
            "group": 11,
            "name": "10.1.1.88.2731",
            "keyword": "",
            "author": "Jeremy N. Bailenson, James J. Gross, Clifford Nass, Emmanuel D. Pontikakis, Maria E. Jabon, Oliver John, Iris B. Mauss, Cendri A. C. Hutcherson",
            "abstract": "We present automated, real-time models built with machine learning algorithms which use videotapes of subjects\u2019 faces in conjunction with physiological measurements to predict rated emotion (trained coders \u2019 second-by-second assessments of sadness or amusement). Input consisted of videotapes of 41 subjects watching emotionallyevocative films along with measures of their cardiovascular activity, somatic activity, and electrodermal responding. We built algorithms based on extracted points from the subjects \u2019 faces as well as their physiological responses. Strengths of the current approach are 1) we are assessing real behavior of subjects watching emotional videos instead of actors making facial poses, 2) the training data allow us to predict both emotion type (amusement versus sadness) as well as the intensity level of each emotion, 3) we provide a direct comparison between person-specific, gender-specific, and general models. Results demonstrated good fits for the models overall, with better performance for emotion categories than for emotion intensity, for amusement ratings than sadness ratings, for a full model using both physiological measures and facial tracking than for either cue alone, and for person-specific models than for gender-specific or general models. 1.",
            "title": "1"
        },
        {
            "group": 12,
            "name": "10.1.1.89.6062",
            "keyword": "Facial displays indicate emotion ~Ekman",
            "author": "Jeffrey F. Cohn, A Adena J. Zlochower, A James Lien, Takeo Kanade B",
            "abstract": "The face is a rich source of information about human behavior. Available methods for coding facial displays, however, are human-observer dependent, labor intensive, and difficult to standardize. To enable rigorous and efficient quantitative measurement of facial displays, we have developed an automated method of facial display analysis. In this report, we compare the results with this automated system with those of manual FACS ~Facial Action Coding System, Ekman & Friesen, 1978a! coding. One hundred university students were videotaped while performing a series of facial displays. The image sequences were coded from videotape by certified FACS coders. Fifteen action units and action unit combinations that occurred a minimum of 25 times were selected for automated analysis. Facial features were automatically tracked in digitized image sequences using a hierarchical algorithm for estimating optical flow. The measurements were normalized for variation in position, orientation, and scale. The image sequences were randomly divided into a training set and a cross-validation set, and discriminant function analyses were conducted on the feature point measurements. In the training set, average agreement with manual FACS coding was 92 % or higher for action units in the brow, eye, and mouth regions. In the cross-validation set, average agreement was 91%, 88%, and 81 % for action units in the brow, eye, and mouth regions, respectively. Automated face analysis by feature point tracking demonstrated high concurrent validity with manual FACS coding.",
            "title": "Copyright \u00a9 1999 Society for Psychophysiological Research Automated face analysis by feature point tracking has high concurrent validity with manual FACS coding"
        },
        {
            "group": 13,
            "name": "10.1.1.94.6370",
            "keyword": "",
            "author": "Maja Pantic, Marian Stewart Bartlett",
            "abstract": "The human face is the site for major sensory inputs and major communicative outputs. It houses the majority of our sensory apparatus as well as our speech production apparatus. It is used to identify other members of our species, to gather information about age, gender, attractiveness, and personality, and to regulate conversation by gazing or nodding. Moreover, the human face is our preeminent means of communicating and understanding somebody\u2019s affective state and intentions on the basis of the shown facial expression (Keltner & Ekman, 2000). Thus, the human face is a multi-signal input-output communicative system capable of tremendous flexibility and specificity (Ekman & Friesen, 1975). In general, the human face conveys information via four kinds of signals. (a) Static facial signals represent relatively permanent features of the face, such as the bony structure, the soft tissue, and the overall proportions of the face. These signals contribute to an individual\u2019s appearance and are usually exploited for person identification. (b) Slow facial signals represent changes in the appearance of the face that occur gradually over time, such as development of permanent wrinkles and changes in skin texture.",
            "title": "20 Machine Analysis of Facial Expressions 1. Human Face and Its Expression"
        },
        {
            "group": 14,
            "name": "10.1.1.96.1082",
            "keyword": "",
            "author": "Douglas Fidaleo, Ulrich Neumann",
            "abstract": "performance-driven facial animation",
            "title": "Analysis of co-articulation regions for"
        },
        {
            "group": 15,
            "name": "10.1.1.98.9504",
            "keyword": "Facial Analysis, independent component analysis",
            "author": "",
            "abstract": "A facial analysis-synthesis framework based on a concise set of local, independently actuated, Coarticulation Regions (CR) is presented for the control of 2D animated characters. CR\u2019s are parameterized by muscle actuations and thereby provide a physically meaningful description of face state that is easily abstracted to higher-level descriptions of facial expression. Independent component analysis on a set of training images acquired from an actor is used to characterize the appearance space of each CR. Within this framework, actor-independent face reconstruction databases can be created by an artist or extracted from video sequences. In addition, the muscle parameter values may be used to drive any similarly parameterized 3D facial model. The flexibility afforded by such a methodology is demonstrated with applications to 2D facial animation control and sample based video synthesis. The analysis runs in real-time on modest consumer hardware.",
            "title": "CoArt: Co-articulation Region Analysis for Control of 2D Characters"
        },
        {
            "group": 16,
            "name": "10.1.1.102.9255",
            "keyword": "",
            "author": "Toby Collins, Supervisor Prof, Robert B. Fisher",
            "abstract": "",
            "title": "Facial Dynamics for Identity Recognition"
        },
        {
            "group": 17,
            "name": "10.1.1.113.9114",
            "keyword": "",
            "author": "Maja Pantic, Marian Stewart Bartlett",
            "abstract": "The human face is the site for major sensory inputs and major communicative outputs. It houses the majority of our sensory apparatus as well as our speech production apparatus. It is used to identify other members of our species, to gather information about age, gender, attractiveness, and personality, and to regulate conversation by gazing or nodding. Moreover, the human face is our preeminent means of communicating and understanding somebody\u2019s affective state and intentions on the basis of the shown facial expression (Keltner & Ekman, 2000). Thus, the human face is a multi-signal input-output communicative system capable of tremendous flexibility and specificity (Ekman & Friesen, 1975). In general, the human face conveys information via four kinds of signals. (a) Static facial signals represent relatively permanent features of the face, such as the bony structure, the soft tissue, and the overall proportions of the face. These signals contribute to an individual\u2019s appearance and are usually exploited for person identification. (b) Slow facial signals represent changes in the appearance of the face that occur gradually over time, such as development of permanent wrinkles and changes in skin texture.",
            "title": "20 Machine Analysis of Facial Expressions 1. Human Face and Its Expression"
        },
        {
            "group": 18,
            "name": "10.1.1.117.4665",
            "keyword": "",
            "author": "Yang Wang, Mohit Gupta, Song Zhang, Dimitris Samaras, Peisen Huang",
            "abstract": "We present a novel fully automatic method for high resolution, non-rigid dense 3D point tracking. High quality dense point clouds of non-rigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient non-rigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints guarantees that non-rigid deformations will be accurately tracked as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency. 1. Introduction and Previous",
            "title": "High resolution tracking of non-rigid 3D motion of densely sampled data using harmonic maps"
        },
        {
            "group": 19,
            "name": "10.1.1.117.4940",
            "keyword": "Index Terms Vision and Graphics, Face and Gesture, Registration, Motion Analysis and Tracking",
            "author": "Yang Wang, Mohit Gupta, Song Zhang, Sen Wang, Xianfeng Gu, Dimitris Samaras, Peisen Huang",
            "abstract": "We present a novel automatic method for high resolution, non-rigid dense 3D point tracking. High quality dense point clouds of non-rigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient non-rigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints allows to track non-rigid deformations accurately as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency.",
            "title": "High Resolution Tracking of Non-Rigid Motion of Densely Sampled 3D Data Using Harmonic Maps"
        },
        {
            "group": 20,
            "name": "10.1.1.125.7997",
            "keyword": "",
            "author": "Maja Pantic, Marian Stewart Bartlett",
            "abstract": "The human face is the site for major sensory inputs and major communicative outputs. It houses the majority of our sensory apparatus as well as our speech production apparatus. It is used to identify other members of our species, to gather information about age, gender, attractiveness, and personality, and to regulate conversation by gazing or nodding. Moreover, the human face is our preeminent means of communicating and understanding somebody\u2019s affective state and intentions on the basis of the shown facial expression (Keltner & Ekman, 2000). Thus, the human face is a multi-signal input-output communicative system capable of tremendous flexibility and specificity (Ekman & Friesen, 1975). In general, the human face conveys information via four kinds of signals. (a) Static facial signals represent relatively permanent features of the face, such as the bony structure, the soft tissue, and the overall proportions of the face. These signals contribute to an individual\u2019s appearance and are usually exploited for person identification. (b) Slow facial signals represent changes in the appearance of the face that occur gradually over time, such as development of permanent wrinkles and changes in skin texture.",
            "title": "20 Machine Analysis of Facial Expressions 1. Human Face and Its Expression"
        },
        {
            "group": 21,
            "name": "10.1.1.125.8176",
            "keyword": "",
            "author": "Artville Llc",
            "abstract": "Two channels have been distinguished in human interaction [1]: one transmits explicit messages, which may be about anything or nothing; the other transmits implicit messages about the speakers themselves. Both linguistics and technology have invested enormous efforts in understanding the first, explicit channel, but the second is not as well understood. Understanding the other party\u2019s emotions is one of the key tasks associated with the second, implicit channel. To tackle that task, signal processing and analysis techniques have to be developed, while, at the same time, consolidating psychological and linguistic analyses of emotion. This article examines basic issues in those areas. It is motivated by the PHYSTA project, in which we aim to develop a hybrid system capable of using information",
            "title": "32 IEEE SIGNAL PROCESSING MAGAZINE JANUARY 2001"
        },
        {
            "group": 22,
            "name": "10.1.1.153.2130",
            "keyword": "",
            "author": "Yang Wang, Mohit Gupta, Song Zhang, Sen Wang, Xianfeng Gu, Dimitris Samaras, Peisen Huang, Y. Wang, M. Gupta, M. Gupta, S. Wang, X. Gu, D. Samaras, S. Wang, X. Gu, D. Samaras, S. Zhang, P. Huang, S. Zhang, P. Huang",
            "abstract": "Abstract We present a novel automatic method for high resolution, non-rigid dense 3D point tracking. High quality dense point clouds of non-rigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient non-rigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature cor-",
            "title": "High Resolution Tracking of Non-Rigid Motion of Densely Sampled 3D Data Using Harmonic Maps"
        },
        {
            "group": 23,
            "name": "10.1.1.159.23",
            "keyword": "automatic facial expression analysis, dynamics of facial behaviour",
            "author": "Maja Pantic",
            "abstract": "",
            "title": "Machine analysis of facial behaviour: naturalistic and dynamic . . . "
        },
        {
            "group": 24,
            "name": "10.1.1.161.4019",
            "keyword": "",
            "author": "",
            "abstract": "face expression recognition, optical flow, high-gradient component detection, hidden Markov model, human-computer interaction.",
            "title": "In press, Journal of Robotics and Autonomous Systems"
        },
        {
            "group": 25,
            "name": "10.1.1.186.4114",
            "keyword": "",
            "author": "Jesse Hoey",
            "abstract": "We present a vision-based, adaptive, decision-theoretic model of human facial displays and gestures in interaction. Changes in the human face occur due to many factors, including communication, emotion, speech, and physiology. Most systems for facial expression analysis attempt to recognize one or more of these factors, resulting in a machine whose inputs are video sequences or static images, and whose outputs are, for example, basic emotion categories. Our approach is fundamentally different. We make no prior commitment to some particular recognition task. Instead, we consider that the meaning of a facial display for an observer is contained in its relationship to actions and outcomes. Agents must distinguish facial displays according to their affordances, or how they help an agent to maximize utility. To this end, our system learns relationships between the movements of a person\u2019s face, the context in which they are acting, and a utility function. The model is a partially observable Markov decision process, or POMDP. The video observations are integrated into the POMDP using a dynamic Bayesian network, which creates spatial and temoral abstractions amenable to decision making at the high level. The",
            "title": "Decision Theoretic Learning of Human Facial Displays and Gestures"
        }
    ],
    "links": [
        {
            "source": 0,
            "target": 1,
            "value": 0.383901
        },
        {
            "source": 0,
            "target": 2,
            "value": 0.37276
        },
        {
            "source": 0,
            "target": 3,
            "value": 0.201465
        },
        {
            "source": 0,
            "target": 4,
            "value": 0.527027
        },
        {
            "source": 0,
            "target": 5,
            "value": 0.450216
        },
        {
            "source": 0,
            "target": 6,
            "value": 0.350515
        },
        {
            "source": 0,
            "target": 7,
            "value": 0.305164
        },
        {
            "source": 0,
            "target": 8,
            "value": 0.376344
        },
        {
            "source": 0,
            "target": 9,
            "value": 0.35122
        },
        {
            "source": 0,
            "target": 10,
            "value": 0.245968
        },
        {
            "source": 0,
            "target": 11,
            "value": 0.212418
        },
        {
            "source": 0,
            "target": 12,
            "value": 0.449827
        },
        {
            "source": 0,
            "target": 13,
            "value": 0.199357
        },
        {
            "source": 0,
            "target": 14,
            "value": 0.0914286
        },
        {
            "source": 0,
            "target": 15,
            "value": 0.282353
        },
        {
            "source": 0,
            "target": 16,
            "value": 0.0
        },
        {
            "source": 0,
            "target": 17,
            "value": 0.199357
        },
        {
            "source": 0,
            "target": 18,
            "value": 0.350515
        },
        {
            "source": 0,
            "target": 19,
            "value": 0.344948
        },
        {
            "source": 0,
            "target": 20,
            "value": 0.199357
        },
        {
            "source": 0,
            "target": 21,
            "value": 0.161654
        },
        {
            "source": 0,
            "target": 22,
            "value": 0.469072
        },
        {
            "source": 0,
            "target": 23,
            "value": 0.0
        },
        {
            "source": 0,
            "target": 24,
            "value": 0.0864865
        },
        {
            "source": 0,
            "target": 25,
            "value": 0.289655
        },
        {
            "source": 2,
            "target": 14,
            "value": 0.0531915
        },
        {
            "source": 2,
            "target": 15,
            "value": 0.289575
        },
        {
            "source": 2,
            "target": 25,
            "value": 0.355872
        },
        {
            "source": 3,
            "target": 4,
            "value": 0.270742
        },
        {
            "source": 3,
            "target": 24,
            "value": 0.013245
        },
        {
            "source": 4,
            "target": 7,
            "value": 0.268421
        },
        {
            "source": 5,
            "target": 8,
            "value": 0.4
        },
        {
            "source": 6,
            "target": 16,
            "value": 0.0
        }
    ]
}